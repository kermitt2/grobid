<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_1050._CLEF08Working_Notes_QA_Overview"/>
	</teiHeader>
	<text xml:lang="en">
		<front>
<lb/>
	<docTitle>
	<titlePart>OVERVIEW OF THE CLEF 2008<lb/> MULTILINGUAL QUESTION ANSWERING TRACK<lb/></titlePart>
	</docTitle>

	<byline>
	<docAuthor>Pamela Forner 1 , Anselmo Peñas 2 , Eneko Agirre 3 , Iñaki Alegria 4 , Corina<lb/> Forăscu 5 , Nicolas Moreau 6 , Petya Osenova 7 , Prokopis Prokopidis 8 , Paulo Ro-<lb/>cha 9 , Bogdan Sacaleanu 10 , Richard Sutcliffe 11 , and Erik Tjong Kim Sang 12<lb/></docAuthor>
	</byline>

	<byline>
	<affiliation>1 CELCT,</affiliation>
	</byline>

	<address>Trento, Italy</address>

	(
	<email>forner@celct.it</email>
	)<lb/>

	<byline>
	<affiliation>2 Departamento de Lenguajes y Sistemas Informáticos, UNED,</affiliation>
	</byline>

	<address>Madrid, Spain<lb/></address>

	(
	<email>anselmo@lsi.uned.es</email>
	)<lb/>

	<byline>
	<affiliation>3 Computer Science Department, University of Basque Country,</affiliation>
	</byline>

	<address>Spain</address>

	(
	<email>e.agirre@ehu.es</email>
	)

	<byline>
	<affiliation>4<lb/> University of Basque Country, Spain</affiliation>
	</byline>

	(
	<email>i.alegria@ehu.es</email>
	)<lb/>

	<byline>
	<affiliation>5 UAIC and RACAI,</affiliation>
	</byline>

	<address>Romania</address>

	(
	<email>corinfor@info.uaic.ro</email>
	)<lb/>

	<byline>
	<affiliation>6 ELDA/ELRA,</affiliation>
	</byline>

	<address>Paris, France</address>

	(
	<email>moreau@elda.org</email>
	)<lb/> 

	<byline>
	<affiliation>7 BTB,</affiliation>
	</byline> 

	<address>Bulgaria,</address> 
	(
	<email>petya@bultreebank.org</email>
	)<lb/>

	<byline>
	<affiliation>8 ILSP Greece, Athena Research Center</affiliation>
	</byline>

	(
	<email>prokopis@ilsp.gr</email>
	)<lb/>

	<byline>
	<affiliation>9 Linguateca, DEI UC, </affiliation>
	</byline>

	<address>Portugal,</address>

	(
	<email>Paulo.Rocha@di.uminho.pt</email>
	)<lb/> 

	<byline>
	<affiliation>10 DFKI,</affiliation>
	</byline> 

	<address>Germany,</address> 

	(
	<email>bogdan@dfki.de</email>
	)<lb/>
	
	<byline>
	<affiliation>11 DLTG, University of Limerick, </affiliation>
	</byline>

	<address>Ireland</address>

	(
	<email>richard.sutcliffe@ul.ie</email>
	)<lb/>

	<byline>
	<affiliation>12 University of Groningen</affiliation>
	</byline>

	(
	<email>e.f.tjong.kim.sang@rug.nl</email>
	)<lb/> 

	Abstract
	<div type="abstract">The QA campaign at CLEF [1], was manly the same as that proposed<lb/> last year. The results and the analyses reported by last year&apos;s participants sug-<lb/>gested that the changes introduced in the previous campaign had led to a drop in<lb/> systems&apos; performance. So for this year&apos;s competition it has been decided to practi-<lb/>cally replicate last year&apos;s exercise.<lb/> Following last year&apos;s experience some QA pairs were grouped in clusters. Every<lb/> cluster was characterized by a topic (not given to participants). The questions from<lb/> a cluster contained co-references between one of them and the others. Moreover,<lb/> as last year, the systems were given the possibility to search for answers in Wiki-<lb/>pedia 1 as document corpus beside the usual newswire collection.<lb/> In addition to the main task, three additional exercises were offered, namely the<lb/> Answer Validation Exercise (AVE), the Question Answering on Speech Tran-<lb/>scriptions (QAST), which continued last year&apos;s successful pilot, and Word Sense<lb/> Disambiguation for Question Answering (QA-WSD).<lb/> As general remark, it must be said that the task still proved to be very challenging<lb/> for participating systems. In comparison with last year&apos;s results the Best Overall<lb/> Accuracy dropped significantly from 41,75% to 19% in the multi-lingual subtasks,<lb/> while instead it increased a little in the monolingual sub-tasks, going from 54% to<lb/> 63,5%.<lb/> </div>

		</front>
	</text>
</tei>
