<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="23._10.1.1.32.1269"/>
	</teiHeader>
	<text xml:lang="en">
		<front>
<lb/>
	<docTitle>
	<titlePart>Neuronal Goals: E cient Coding and Coincidence Detection<lb/></titlePart>
	</docTitle>

	<byline>
	<docAuthor>Nathan Intrator<lb/></docAuthor>
	</byline>

	<byline>
	<affiliation>School of Mathematical Sciences<lb/> Tel Aviv University<lb/></affiliation>
	</byline>

	<email>nin@cns.brown.edu<lb/></email>

	Abstract| 
	<div type="abstract">Barlow&apos;s seminal work on minimal entropy codes and unsupervised learning is<lb/> reiterated. In particular, the need to transmit the probability of events is put in a practical<lb/> neuronal framework for detecting suspicious events. A variant of the BCM learning rule 15]<lb/> is presented together with some mathematical results suggesting optimal minimal entropy<lb/> coding.<lb/></div>

	Key words:
	<keyword>Sparse coding, Non-Gaussian distributions, BCM Theory, Minimal Entropy</keyword>

		</front>
	</text>
</tei>
