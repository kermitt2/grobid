<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="2101.11214v2"/>
	</teiHeader>
	<text xml:lang="en">
		<front>
<lb/>
	Presented as a 

	<meeting>RobustML and S2D-OLAD workshop paper at ICLR 2021<lb/></meeting> 

	<docTitle>
	<titlePart>TOWARDS ROBUSTNESS TO LABEL NOISE IN TEXT<lb/> CLASSIFICATION VIA NOISE MODELING<lb/></titlePart>
	</docTitle>

	<byline>
	<docAuthor>Siddhant Garg * †<lb/></docAuthor>
	</byline>

	<byline>
	<affiliation>Amazon Alexa AI Search<lb/></affiliation>
	</byline>

	<address>Manhattan Beach, CA, USA<lb/></address>

	<email>sidgarg@amazon.com<lb/></email>

	<byline>
	<docAuthor>Goutham Ramakrishnan * †<lb/></docAuthor>
	</byline>

	<byline>
	<affiliation>Health at Scale Corporation<lb/></affiliation>
	</byline>

	<address>San Jose, CA, USA<lb/></address>

	<email>goutham7r@gmail.com<lb/></email>

	<byline>
	<docAuthor>Varun Thumbe * †<lb/></docAuthor>
	</byline>

	<byline>
	<affiliation>KLA Corporation<lb/></affiliation>
	</byline>

	<address>Ann Arbor, MI , USA<lb/></address>

	<email>thumbevarun@gmail.com</email>

	ABSTRACT<lb/>
	<div type="abstract">Large datasets in NLP suffer from noisy labels, due to erroneous annotation pro-<lb/>cedures. We study the problem of text classification with label noise, and aim to<lb/> capture this noise through an auxiliary noise model over the classifier. We first<lb/> assign a probability score to each training sample of having a noisy label, through<lb/> a beta mixture model fitted on the losses at an early epoch of training. Then, we<lb/> use this score to selectively guide the learning of the noise model and classifier.<lb/> Our empirical evaluation on two text classification tasks shows that our approach<lb/> can improve over the baseline accuracy, and prevent over-fitting to the noise.<lb/></div>

	* Equal contribution by authors<lb/> † Work completed as graduate students at the
	<byline>
	<affiliation>University of Wisconsin-Madison<lb/></affiliation>
	</byline>

	<idno>arXiv:2101.11214v2 [cs.CL]</idno>

	<date>22 Apr 2021</date>

		</front>
	</text>
</tei>
