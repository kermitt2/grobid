<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Mathematical Programming in Machine Learning <lb/>O. L. Mangasarian y <lb/>Mathematical Programming Technical Report 95-06 <lb/>April 1995{Revised July 1995 <lb/>Abstract <lb/>We describe in this work a number of central problems of machine learning and <lb/>show how they can be modeled and solved as mathematical programs of various <lb/>complexity. <lb/></front>

			<body>1 Introduction <lb/>Machine learning can be thought of as generalizing information gleaned from given data to new <lb/>unseen data. As such it can be considered as determining a mapping between an input set and <lb/>an output set in a robust manner that is amenable to generalization. In this work we shall con-<lb/>centrate on a number of fundamental problems of machine learning, and show how mathematical <lb/>programming plays a signi cant role in their formulation and solution. In Section 2 we consider the <lb/>classical problem of discriminating between two point sets in the n-dimensional real space R n , and <lb/>show that its complexity ranges from polynomial-time to NP-complete, depending on the measure <lb/>of error employed. When the traditional distance of a misclassi ed point to a separating plane <lb/>is used as an error, a single linear program 6, 15, 16, 4] usually solves the problem. Recently <lb/>10, 18, 2, 7] a more complex, and for certain applications more realistic, error measure has been <lb/>considered, namely the number of misclassi ed points by a separating plane. This problem, even <lb/>though shown to be NP-complete 7], can be e ectively solved by a parametric 2] or a hybrid <lb/>method 7]. In Section 3 we describe a central problem of machine learning, that of improving <lb/>generalization 27]. We give a very simple model which justi es the often accepted rule-of-thumb of <lb/>machine learning and approximation theory, that over tting leads to poor generalization. In fact <lb/>we go the opposite direction, and show that inexact tting can lead to improved generalization. <lb/>In Section 4 we use an equivalence between the step function and the complementarity problem to <lb/>show that the problem of training a neural network can be represented as mathematical program <lb/>with equilibrium constraints (MPEC) which has been studied recently in the literature 14]. <lb/>A word about our notation now. For a vector x in the n-dimensional real space R n ; x + will <lb/>denote the vector in R n with components (x + ) i := max fx i ; 0g; i = 1; : : :; n: Similarly x will <lb/>denote the vector in R n with components (x ) i := (x i ) ; i = 1; : : :; n , where () is the step <lb/>function that maps a nonpositive number into zero and a positive number into one. The p-norm <lb/>will be denoted by k k p for p = 1; 2; : : :; 1, while k k will denote an arbitrary norm. We will <lb/></body>

			<front>This material is based on research supported by Air Force O ce of Scienti c Research Grant F49620-94-1-000036 <lb/>and National Science Foundation Grant CCR-9322479. <lb/>y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: <lb/>olvi@cs.wisc.edu. <lb/></front>

			<page>1 <lb/></page>

			<body>also make use of the function x ? which will denote the vector in R n with components (x ? ) i := <lb/>min fx i ; 1g; i = 1; : : :; n: The notation A 2 R m n will signify a real m n matrix. For such a <lb/>matrix, A T will denote the transpose while A i will denote row i. For two vectors x and y in R n , x T y <lb/>will denote the scalar product, while x ? y will denote x T y = 0. A vector of ones in a real space of <lb/>arbitrary dimension will be denoted by e: The symbols \:=&quot; and \=:&quot; will denote a de nition of a <lb/>term adjacent to the colon by a term adjacent to the equality. <lb/>2 Linear Discrimination <lb/>We begin with the fundamental problem of constructing a linear discriminator between two given <lb/>point sets A and B in R n . That is we look for a plane <lb/>x T w = ; <lb/>(1) <lb/>such that <lb/>x T w &gt; <lb/>for x 2 A <lb/>x T w &lt; <lb/>for x 2 B: <lb/>(2) <lb/>Here w is the normal to the plane and j j <lb/>kwk 2 <lb/>is the Euclidean distance from the origin to the plane. <lb/>In general it is not possible to satisfy (2) except in the special case when the convex hulls of A and <lb/>B do not intersect. Thus, one resorts in the general case to optimizing some error criterion in the <lb/>satisfaction of (2). The simplest such criterion is to use linear programming in order to construct a <lb/>plane (1) that maximizes a weighted sum of the distances of correctly classi ed points to the plane <lb/>16, 4] as follows: <lb/>max <lb/>w; ;y;z e T y + e T z j Aw e + y; Bw e ? z; y e; z e <lb/>(3) <lb/>Here the rows of the matrices A 2 R m n and B 2 R k n represent the m points in A and the k points <lb/>in B respectively, while e is a vector of ones of appropriate dimension. The objective function of <lb/>(3) represents the sum of distances, positive and truncated to one for correctly classi ed points and <lb/>nonpositive for incorrectly classi ed points, to the plane x T w = , multiplied by kwk 2 . Although, <lb/>apparently di erent from the robust linear program of 4, Proposition 2.4], it is equivalent to it if <lb/>we set the weights 1 = 2 = 1 in the latter and make a simple change of variables. A principal <lb/>advantage of the formulation (3), is that it ties more easily with the classi cation maximization <lb/>formulation (7) below, once we make use of the function <lb/>( ) ? := minf ; 1g = 1 ? (1 ? ) + : <lb/>(4) <lb/>By using this nondecreasing piecewise-linear concave function, the linear program (3) can be written <lb/>as the following unconstrained concave maximization problem: <lb/>max w; e T (Aw ? e ) ? + e T (?Bw + e ) ? <lb/>(5) <lb/>Another simplifying feature of the linear programming formulation (3) is the absence of a nor-<lb/>malization vector e from both terms of (5), and from the rst two constraints of (3). The linear <lb/>program (3) also maintains the non-nullity properties of w 4, Theorems 2.5 &amp; 2.6], which can be <lb/>summarized as follows here. The point (w = 0; ; y; z) is a solution of (3) if and only if e T A = e T B <lb/>and m = k, in which case the solution is never unique in w = 0. If the convex hulls of A and B are <lb/></body>

			<page>2 <lb/></page>

			<body>disjoint, then all points in A and B are correctly classi ed, and the equivalent programs (3) and (5) <lb/>yield a maximum value of m + k, equal to the total number of points in A and B that have been <lb/>completely separated by the plane x T w = . However, in the general case of intersecting convex <lb/>hulls, the linear program (3) obtains an approximate separating plane that maximizes a weighted <lb/>sum of distances of points as described above. For this case the number of correctly classi ed points <lb/>is given by: <lb/>e T y + e T z or equivalently e T ((Aw ? e ) ? ) + e T ((?Bw + e ) ? ) ; <lb/>(6) <lb/>where (w; ; y; z) is a solution of (3) and, as indicated earlier, ( ) is the step function. Although <lb/>the linear programming formulation (3) is very e ective for practical problems 21] and can be <lb/>used in the construction of neural networks 3] as well multi-surface discriminators 1, 4], it does <lb/>not minimize the number of misclassi ed points by the plane (1), which may be an important <lb/>consideration in certain applications. In order to minimize the number of misclassi ed points, we <lb/>need to maximize the number of satis ed components of the inequalities (2). This corresponds to <lb/>solving the following problem: <lb/>max w; e T (Aw ? e ) + e T (?Bw + e ) <lb/>(7) <lb/>We refer to this problem as the classi cation maximization problem, that is the problem of max-<lb/>imizing the number of correctly classi ed points. Although this is an NP-complete problem 7, <lb/>Proposition 2], e ective methods for its solution have been proposed 18, 7] and successfully tested <lb/>on real world problems 2, 7]. We outline two of these approaches brie y now. <lb/>We rst describe a hybrid approach, recently proposed and tested successfully on ten publicly <lb/>available databases 7]. The idea of this approach is to combine the two criteria described above <lb/>as follows. For a xed orientation w, translate the plane (1) by varying , so as to minimize the <lb/>number of misclassi ed points, that is solve (7) for a xed w, which is then a one dimensional <lb/>problem in with a nite number of objective function values. Then for a xed , rotate the <lb/>plane (1) by varying w, so as to minimize the weighted average of the sum of the distances of the <lb/>misclassi ed points to the plane (1), that is solve the linear program (3) in w for a xed value of . <lb/>The algorithm stops, when successive line searches in fail to decrease the number of misclassi ed <lb/>points. Needless to say, there is no guarantee that this approach will give a global solution to the <lb/>NP-complete problem (7). However it seems to have the best generalization 7] as determined by <lb/>tenfold cross-validation 26] on the ten data sets employed. <lb/>We describe now another approach for solving the classi cation maximization problem (7), by <lb/>reducing it to an LPEC, a linear program with equilibrium constraints 18]. We begin with a <lb/>variation of a lemma of 18, Lemma 2.1] which ensures that the backward implication of the lemma <lb/>holds also for zero components of a. <lb/>Lemma 2.1 Equilibrium characterization of step function () For r 2 R m ; u 2 R m ; a 2 R m <lb/>and e, a vector of ones in R m : <lb/>r = (a) ; u = (a ? e) + () 0 r ? u ? a + e 0 <lb/>0 u ? ?r + e 0 ; <lb/>(8) <lb/>where is a su ciently small positive number, that is <lb/>0 &lt; &lt; inf <lb/>a i 6 =0 ja i j: <lb/>(9) <lb/></body>

			<page>3 <lb/></page>

			<body>Proof The points r = (a) and u = (a ? e) + uniquely solve the dual linear programs <lb/>max r f(a ? e) T rj0 r eg and min u fe T uju a ? e; u 0g: <lb/>(10) <lb/>The right hand side of the equivalence (8) is merely the Karush-Kuhn-Tucker necessary and su -<lb/>cient optimality conditions for r = (a) and u = (a ? e) + to solve (10). <lb/>We note that the use of is unnecessary in 18], because of the following equivalence: <lb/>r = (a) ; u = (a) + () (r; u) 2 arg min r;u fe T rj0 r ? u ? a 0; 0 u ? ?r + e 0g; (11) <lb/>and because the term e T r is minimized in 18], but is being maximized here (see (12) for example). <lb/>With Lemma 2.1, we can reformulate the classi cation maximization problem (7) as the following <lb/>LPEC with su ciently small and positive: <lb/>maximize w; ;r;u;s;v <lb/>e T r + e T s <lb/>subject to <lb/>0 r ? u ? Aw + e + e 0 <lb/>0 u ? ?r + e 0 <lb/>0 s ? v + Bw ? e + e 0 <lb/>0 v ? ?s + e 0 <lb/>(12) <lb/>Note that with the exception of the \perp&quot; condition, all constraints and the objective function are <lb/>linear. To overcome the nonlinear e ect of the ?-condition, an implicitly exact penalty function <lb/>formulation has been proposed as well as a parametric approach 18]. The parametric approach <lb/>is preferable, because (12) has in nitely many stationary points as was pointed out in 18]. The <lb/>reason for this anomaly is that any (w; ) determining a plane x T w = that does not contain any <lb/>points from either the sets A or B , is a stationary solution for problem (12). This is so because a <lb/>slight perturbation of the plane does not change the number of misclassi ed points. To overcome <lb/>this di culty a parametric reformulation was proposed in 18] and implemented in 2]. For the <lb/>classi cation maximization problem (7), the parametric reformulation of (12) is the following: <lb/>minimize w; ;r;u;s;v r T (?Aw + e + e) + e T u] + s T (Bw ? e + e) + e T v] =: f( ) <lb/>subject to <lb/>0 r; u ? Aw + e + e 0 <lb/>0 u; ?r + e 0 <lb/>0 s; v + Bw ? e + e 0 <lb/>0 v; ?s + e 0 <lb/>e T r + e T s <lb/>2 0; 1) <lb/>(13) <lb/>Here is a parameter that represents the number of points correctly classi ed. The largest value <lb/>of , such that the objective function has a minimum of zero, is the maximum number of points <lb/>that can be correctly classi ed by a plane x T w = . Note that f( ) is a nondecreasing function <lb/>of , and the largest value for which f( ) = 0, constitutes a maximum to the NP-complete <lb/>classi cation maximization problem (7). The parametric approach consists of starting at some <lb/>large &gt; , solving (13) by a Frank-Wolfe algorithm 8, 5], for decreasing values of until is <lb/>reached. E cient estimation of successive values of can be achieved by a secant method applied <lb/>to f( ). The method seems to work quite well as evidenced by computational results given in 2, 7]. <lb/></body>

			<page>4 <lb/></page>

			<body>3 Improving Generalization <lb/>In this section we shall consider a fundamental problem of machine learning: How to train a system <lb/>on a given training set so as to improve generalization on a new unseen testing set 13, 24, 28]. <lb/>We shall concentrate on some very recent results 27] obtained for a simple linear model and which <lb/>make critical use of mathematical programming ideas. These ideas, although rigorously established <lb/>for a simple linear model only, seem to extend to much more complex systems, including neural <lb/>networks 27]. <lb/>The model that we shall consider here consists of the training set fA; ag where A is a given <lb/>m n real matrix and a is a given m 1 real vector. A vector x in R n is to be \learnt&quot; such that <lb/>the linear system <lb/>Ax = a; <lb/>(14) <lb/>which does not have an exact solution, is satis ed in some approximate fashion, and such that the <lb/>error in satisfying <lb/>Cx = c; <lb/>(15) <lb/>for some unseen testing set (C; c) 2 R k n R k , is minimized. Of course, if we disregard the testing <lb/>set error (15), the problem becomes the standard least-norm problem: <lb/>min <lb/>x2R n kAx ? ak; <lb/>(16) <lb/>where k k is some norm on R m . However with an eye to possible perturbations in the given training <lb/>set fA; ag, we pose the following motivational question: If the vector a of the training set is known <lb/>only to an accuracy of , where is some positive number, does it make sense to attempt to drive <lb/>the error to zero as is done in (16), or is it not better to tolerate errors in the satisfaction of Ax = a <lb/>up to a magnitude of ? In other words, instead of (14), we should try to satisfy the following <lb/>system of inequalities, in some best sense: <lb/>?e Ax ? a e <lb/>(17) <lb/>To do that, we solve the following regularized quadratic program for some nonnegative and a <lb/>small positive : <lb/>minimize x;y;z <lb/>1 <lb/>2 kyk 2 2 + 1 2 kzk 2 2 + 2 kxk 2 2 <lb/>subject to ?z ? e Ax ? a e + y <lb/>y; z 0: <lb/>(18) <lb/>Here is a small xed positive regularization constant that ensures the uniqueness of the x com-<lb/>ponent of the solution. We note immediately, that if = 0, problem (18) degenerates to the <lb/>regularized classical least squares problem: <lb/>min <lb/>x2R n <lb/>1 <lb/>2 kAx ? ak 2 2 + 2 kxk 2 2 : <lb/>(19) <lb/>The key question to ask here, is this: Under what conditions does a solution x( ) of (18), for <lb/>some &gt; 0, give a smaller error on a testing set? We are able to give an answer to this question <lb/>and corroborate it computationally 27], by considering a general testing set (C; c) 2 R k n R k <lb/></body>

			<page>5 <lb/></page>

			<body>as well as a simpler testing set, where only the right side of (14) is perturbed. We begin with the <lb/>latter and simpler perturbation, that is: <lb/>Ax = a + t; <lb/>(20) <lb/>where t is some arbitrary perturbation in R m , and consider the following associated error function: <lb/>f( ) := 1 <lb/>2 kAx( ) ? a ? tk 2 2 : <lb/>(21) <lb/>In particular we would like to know when is f(0) not a local minimum of f( ) on the set f j 0g. <lb/>In fact we are only interested in the -interval 0;^ ], where^ is de ned bŷ <lb/>:= min x kAx ? ak 1 ; <lb/>(22) <lb/>because the minimum value of (18) approaches zero, for <lb/>^ , as approaches zero. Since x( ) is <lb/>continuous and piecewise-linear on <lb/>0 it follows that f( ) de ned by (21) is continuous piecewise-<lb/>quadratic on 0;^ ], and hence attains a minimum at some in 0;^ ]. Since f( ) is directionally <lb/>di erentiable, it follows that if the directional derivative f 0 ( ; 1) at = 0 in the positive direction <lb/>is negative, then = 0 is a strict local maximum of f( ). Hence, as measured by the error criterion <lb/>(21), x( ) for some positive provides a better point. The following theorem gives a su cient <lb/>condition for f 0 (0; 1) &lt; 0 and thus ensuring that solving (18) for some positive produces an x( ) <lb/>that generalizes better on the system (20) than that obtained by solving a plain regularized least <lb/>squares problem (19), that is f( ) &lt; f(0) for some 2 (0;^ ]. <lb/>Theorem 3.1 27] Improved generalization on Ax = a+t with positive training tolerance <lb/>The testing set error function f( ) of (21) has a strict local maximum at 0 and a global minimum <lb/>on 0;^ ], where^ is de ned by (22), at some &gt; 0, whenever <lb/>( x(0) + A T t) T (x( ) ? x(0)) &gt; 0 <lb/>(23) <lb/>for some 2 (0;~ ], for some su ciently small~ . <lb/>For the more general testing model given by Cx = c of (15), we have the following result for <lb/>improved generalization. <lb/>Theorem 3.2 27] Improved generalization on Cx = c with positive training tolerance <lb/>Let x( ) be de ned by the tolerant training of Ax = a by the quadratic program (18) with tolerance <lb/>0. Let g( ) denote the error generated by x( ) in the testing model Cx = c, de ned by: <lb/>g( ) := 1 <lb/>2 kCx( ) ? ck 2 2 : <lb/>(24) <lb/>The zero-tolerance error g(0) generated by x(0) is a strict local maximum over <lb/>0 whenever <lb/>kr(0)k 2 2 &gt; r( ) T r(0) for some 2 (0;~ ] <lb/>(25) <lb/>for some su ciently small~ , where r( ) is de ned by <lb/>r( ) := Cx( ) ? c: <lb/>(26) <lb/>Computational results carried out in 27] have corroborated the improved generalization results <lb/>of Theorem 3.1 above, as well as for more complex models such as neural networks, where a <lb/>threshold tolerance in measuring the error in the backpropagation algorithm 23, 11, 20] is allowed. <lb/></body>

			<page>6 <lb/></page>

			<body>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>B <lb/>B <lb/>B <lb/>A <lb/>A <lb/>A <lb/>A <lb/>x T w 3 = 3 <lb/>x T w 2 = 2 <lb/>x T w 1 = 1 <lb/>110 <lb/>000 <lb/>010 <lb/>011 <lb/>001 <lb/>101 <lb/>100 <lb/>Figure 1: Seven polyhedral regions in R 2 generated by three planes: x T w 1 = 1 ; x T w 2 = 2 <lb/>and x T w 3 = 3 . Each region contains elements of only one set A or B and is tagged by <lb/>a binary number, the ith digit of which denotes whether the region is on the 1-side, <lb/>x T w i &gt; i , or 0-side, x T w i &lt; i , of the ith plane. <lb/>4 Neural Networks as Mathematical Programs with Equilibrium <lb/>Constraints <lb/>A neural network, which is a generalization of a separating plane in R n , can be de ned as a <lb/>nonlinear map from R n into some set, typically f0,1g. One intuitive way to generate such a map <lb/>is to divide R n into various polyhedral regions, each of which containing elements of only one of <lb/>two given disjoint point sets A and B . (See Figure 1.) In its general form, this problem is again <lb/>an extremely di cult and nonconvex problem. However, various greedy sequential constructions <lb/>of the planes determining the various polyhedral regions 16, 19, 1] have been quite successful in <lb/>obtaining very e ective algorithms for training neural networks. These algorithms are much faster <lb/>than the classical online backpropagation (BP) gradient algorithm 23, 11, 20], where the training <lb/>is done on one point at a time. Often online BP is erroneously referred to as a descent algorithm, <lb/>which it is not. <lb/>In this section of the paper we relate the polyhedral regions into which R n is partitioned, to <lb/>a classical neural network with one hidden layer of linear threshold units (LTUs) and one output <lb/>LTU. (See Figure 2.) An LTU is an abstraction of a human neuron which res if its input exceeds <lb/>its threshold value. Thus the LTU, depicted by its threshold value of 1 in Figure 2, will have the <lb/>output (x T w 1 ? 1 ) , where ( ) is the step function de ned earlier. An obvious representation of <lb/>such an LTU would be by the plane x T w 1 = 1 . It turns out that every neural network mapping R n <lb/>into the set f0,1g can be related to a partitioning of R n into polyhedral regions, but not conversely. <lb/>However, any two disjoint point sets in R n can be discriminated between by some polyhedral <lb/>partition that corresponds to a neural network with one hidden layer with a su cient number of <lb/>hidden units 12, 19]. <lb/></body>

			<page>7 <lb/></page>

			<body>y(x) <lb/>x <lb/>w 1 <lb/>w 2 <lb/>w h <lb/>1 <lb/>2 <lb/>h <lb/>t 1 <lb/>t 2 <lb/>t h <lb/>Figure 2: A typical feedforward neural network with a single layer of h hidden linear <lb/>threshold units (LTUs), input x 2 R n , and output y(x) 2 f0; 1g: The output of hidden <lb/>unit i is (x T w i ? i ) ; i = 1; : : :; h. The output y(x) of the output LTU is ( P h i=1 (x T w i ? <lb/>i ) t i ? ) : <lb/>We describe now precisely when a speci c partition of R n by h separating planes <lb/>x T w i = i ; i = 1; : : :; h; <lb/>(27) <lb/>corresponds to a neural network with h hidden units. (See Figures 1 and 2.) The h separating <lb/>planes (27) divide R n into at most p polyhedral regions, where 9] <lb/>p := <lb/>n <lb/>X <lb/>i=0 <lb/>h <lb/>i : <lb/>(28) <lb/>We shall assume that A and B are contained in the interiors of two mutually exclusive subsets of <lb/>these regions. (See Figure 1.) Each of these polyhedral regions can be mapped uniquely into a <lb/>vertex of the unit cube in R h ; <lb/>fzjz 2 R h ; 0 z eg <lb/>(29) <lb/>by using the map: <lb/>(x T w i ? i ) ; i = 1; : : :; h; <lb/>(30) <lb/>where x is a point in R n belonging to some polyhedral region. If the p polyhedral regions of R n <lb/>constructed by the h planes (27) are such that vertices of the cube (29) corresponding to points in <lb/></body>

			<page>8 <lb/></page>

			<body>+ <lb/>+ <lb/>+ <lb/>r T t = <lb/>(0; 0; 0) 2 B <lb/>(1; 1; 0) 2 A <lb/>(0; 1; 0) 2 B <lb/>(0; 1; 1) 2 A <lb/>(0; 0; 1) 2 B <lb/>(1; 0; 0) 2 A <lb/>(1; 0; 1) 2 A <lb/>r 2 <lb/>r 3 <lb/>r 1 <lb/>Figure 3: The vertices of the unit cube into which the sets A and B of Figure 1 are <lb/>mapped by the three planes shown in that gure, or equivalently by three hidden <lb/>LTUs of a neural network. A plane, r T t = , separates the vertices associated with A <lb/>from those associated with B . This plane corresponds to the output LTU of a neural <lb/>network and the weights of its incoming arcs. <lb/>A , are linearly separable in R h from the vertices of (29) corresponding to points in B , by a plane <lb/>r T t = ; <lb/>(31) <lb/>as in the example of Figure 3, then the polyhedral partition of R n corresponds to a neural network <lb/>with h hidden linear threshold units (with thresholds i , incoming arc weights w i ; i = 1; : : :; h) and <lb/>output linear threshold unit (with threshold and incoming arc weights t i ; i = 1; : : :; h) 17]. This <lb/>condition is necessary and su cient for the polyhedral partition of R n in order for it to correspond <lb/>to a neural network with one layer of hidden units. For more details, see 17]. <lb/>\Training&quot; a neural network consists of determining (w i ; i ) 2 R n+1 ; i = 1; : : :; h; (t; ) 2 <lb/>R h+1 ; such that the following nonlinear inequalities are satis ed as best as possible: <lb/>h <lb/>X <lb/>i=1 <lb/>(Aw i ? e i ) t i &gt; e <lb/>h <lb/>X <lb/>i=1 <lb/>(Bw i ? e i ) t i &lt; e <lb/>(32) <lb/>This can be achieved by maximizing a weighted sum of correctly classi ed points in R h by solving <lb/>the following unconstrained maximization problem (as in the equivalent programs (3) and (5)): <lb/></body>

			<page>9 <lb/></page>

			<body>max <lb/>w i ; i ;t i ; <lb/>e T ( <lb/>h <lb/>X <lb/>i=1 <lb/>(Aw i ? e i ) t i ? e ) ? <lb/>+e T ( <lb/>h <lb/>X <lb/>i=1 <lb/>?(Bw i ? e i ) t i + e ) ? ; <lb/>(33) <lb/>where the function ( ) ? is de ned in (4). If instead of the step function ( ) the sigmoid function <lb/>( ) is used in (33), where ( ) := 1 <lb/>1+e ? ; &gt; 0, we obtain an error function similar to the error <lb/>function that backpropagation attempts to nd a stationary point for, and for which a convergence <lb/>proof is given in 20], and stability analysis in 25]. We note that the classical exclusive-or (XOR) <lb/>example 22] for which A = 1 0 <lb/>0 1 ; B = 0 0 <lb/>1 1 , gives a maximum value of four for (33) with <lb/>the following solution: <lb/>(w 1 ; 1 ) = ((2 ? 2); 1); (w 2 ; 2 ) = ((?2 2); 1) <lb/>(v; ) = ((2 2); 1) <lb/>(34) <lb/>This corresponds to correctly separating the two points in A from the two points in B . <lb/>It is interesting to note that the same solution for the XOR example is given by the greedy <lb/>multisurface method tree (MSMT) 1]. MSMT attempts to separate as many points of A and B <lb/>as possible by a rst plane obtained by solving (3), and then repeats the process for each of the <lb/>ensuing halfspaces, until adequate separation is obtained. For this example, the rst plane obtained <lb/>4] is (w 1 ; 1 ) = ((2 ? 2); 1), which separates f(1; 0)g from f(0; 0); (0; 1); (1; 1)g. The second plane <lb/>obtained is (w 2 ; 2 ) = ((?2 2); 1), separates f(0; 1)g from f(0; 0); (1; 1)g, and the separation <lb/>is complete between A and B. These planes correspond to the same neural network obtained by <lb/>solving (33), which of course is not always the case when using the greedy MSMT method. However <lb/>MSMT frequently gives better solutions than those generated by BP and is much faster than BP. <lb/>We now set up the problem (33) as an MPEC. We rst use the equivalence between the step <lb/>function ( ) and an equilibrium condition given by Lemma 2.1 and obtain the following problem, <lb/>where is a su ciently small positive number: <lb/>maximize <lb/>w i ; i ;r i ;u i ;s i ;v i ;t i ; e T ( <lb/>h <lb/>X <lb/>i=1 <lb/>r i t i ? e ) ? + e T ( <lb/>h <lb/>X <lb/>i=1 <lb/>?s i t i + e ) ? <lb/>subject to <lb/>0 r i ? u i ? Aw i + e i + e 0 <lb/>0 u i ? ?r i + e 0 <lb/>0 s i ? v i ? Bw i + e i + e 0 <lb/>0 v i ? ?s i + e 0 <lb/>i = 1; : : :; h: <lb/>(35) <lb/>By using the equivalence between the formulations (3) and (5) we can formulate (35) as the following <lb/>MPEC: <lb/></body>

			<page>10 <lb/></page>

			<body>maximize <lb/>w i ; i ;r i ;u i ;s i ;v i ;t i ; ;y i ;z i <lb/>e T <lb/>h <lb/>X <lb/>i=1 <lb/>y i + e T <lb/>h <lb/>X <lb/>i=1 <lb/>z i <lb/>subject to <lb/>P h <lb/>i=1 r i t i ? e y i ; y i e <lb/>P h <lb/>i=1 ?s i t i + e z i ; z i e <lb/>0 r i ? u i ? Aw i + e i + e 0 <lb/>0 u i ? ?r i + e 0 <lb/>0 s i ? v i ? Bw i + e i + e 0 <lb/>0 v i ? ?s i + e 0 <lb/>i = 1; : : :; h <lb/>(36) <lb/>In a manner similar to the parametric reformulation (13) of the LPEC (12) associated with the <lb/>classi cation maximization problem (7), the above MPEC can be reformulated as the following <lb/>parametric bilinear program: <lb/>minimize <lb/>w i ; i ;r i ;u i ;s i ;v i ;t i ; ;y i ;z i <lb/>h <lb/>X <lb/>i=1 <lb/>(r i ) T (?Aw i + e i + e) + e T u i + <lb/>h <lb/>X <lb/>i=1 <lb/>(s i ) T (?Bw i + e i + e) + e T v i =: g( ) <lb/>subject to <lb/>P h <lb/>i=1 r i t i ? e y i ; y i e <lb/>P h <lb/>i=1 ?s i t i + e z i ; z i e <lb/>0 r i ; u i ? Aw i + e i + e 0 <lb/>0 u i ; ?r i + e 0 <lb/>0 s i ; v i ? Bw i + e i + e 0 <lb/>0 v i ; ?s i + e 0 <lb/>i = 1; : : :; h <lb/>e T P h <lb/>i=1 y i + e T <lb/>h <lb/>X <lb/>i=1 <lb/>z i <lb/>2 0; 1) <lb/>(37) <lb/>Here is a parameter that represents the number of points correctly classi ed, and will equal <lb/>m + k if complete separation is achieved by the neural network. The largest value of for which <lb/>the objective function has a minimum of zero is the maximum value of the MPEC (36), which <lb/>corresponds to training a neural network on the sets A and B . Note that g( ) is a nondecreasing <lb/>function of , and the largest value for which g( ) = 0, constitutes a maximum to problem <lb/>(35). The parametric approach consists of starting at some large , say = m+k, solving (37), for <lb/>decreasing values of for which g( ) &gt; 0, until such that g( ) = 0 is reached. E cient estimation <lb/>of successive values of can be achieved by a secant method applied to the nondecreasing function <lb/>g( ). Note that the nonconvex problem (37) has a bilinear objective and two sets of bilinear <lb/>constraints. Although no computation has been done with this model of a neural network, it is <lb/>felt that the Frank-Wolfe approach utilized to solve e ciently numerous NP-complete problems in <lb/>5] could also be e ective here as well. Brie y the approach would consist of xing t i ; i = 1; : : :; h <lb/>and solving (37) by the bilinear approach of 5] which involves successive linear programs and line <lb/>searches. Then (t i ; y i ; z i ) i = 1; : : :; h and are updated by solving a single linear program. The <lb/>bilinear approach corresponds to adjusting the thresholds and incoming arc weights for the hidden <lb/>units of the neural network as well as adjusting the threshold of the output unit, while holding <lb/>the weights of the incoming arcs to the output unit xed. The linear program then attempts to <lb/>get a best linear separation between vertices of the unit cube in R h that represent A and B , by <lb/></body>

			<page>11 <lb/></page>

			<body>readjusting the threshold of the output unit of the neural network as well as the weights of its <lb/>incoming arcs. <lb/>5 Conclusion <lb/>Signi cant problems associated with machine learning have been cast as a variety of mathematical <lb/>programs, ranging in complexity from polynomial-time-solvable linear programs to NP-complete <lb/>problems. E ective methods for solving some of these problems have been outlined. Modeling and <lb/>e ciently solving many of these problems constitute an important and challenging eld of research <lb/>for mathematical programming. <lb/></body>

			<listBibl>References <lb/>1] K. P. Bennett. Decision tree construction via linear programming. In M. Evans, editor, <lb/>Proceedings of the 4th Midwest Arti cial Intelligence and Cognitive Science Society Conference, <lb/>pages 97{101, Utica, Illinois, 1992. <lb/>2] K. P. Bennett and E. J. Bredensteiner. A parametric optimization method for machine learning. <lb/>Department of Mathematical Sciences Report No. 217, Rensselaer Polytechnic Institute, Troy, <lb/>NY 12180, 1994. <lb/>3] K. P. Bennett and O. L. Mangasarian. Neural network training via linear programming. <lb/>In P. M. Pardalos, editor, Advances in Optimization and Parallel Computing, pages 56{67, <lb/>Amsterdam, 1992. North Holland. <lb/>4] K. P. Bennett and O. L. Mangasarian. Robust linear programming discrimination of two <lb/>linearly inseparable sets. Optimization Methods and Software, 1:23{34, 1992. <lb/>5] K. P. Bennett and O. L. Mangasarian. Bilinear separation of two sets in n-space. Computational <lb/>Optimization &amp; Applications, 2:207{227, 1993. <lb/>6] A. Charnes. Some fundamental theorems of perceptron theory and their geometry. In J. T. <lb/>Lou and R. H. Wilcox, editors, Computer and Information Sciences, pages 67{74, Washington, <lb/>D.C., 1964. Spartan Books. <lb/>7] Chunhui Chen and O. L. Mangasarian. Hybrid misclassi cation minimization. Technical <lb/>Report 95-05, Computer Sciences Department, University of Wisconsin, Madison, Wiscon-<lb/>sin, February 1995. Advances in Computational Mathematics, submitted. Available from <lb/>ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. <lb/>8] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics <lb/>Quarterly, 3:95{110, 1956. <lb/>9] G. M. Georgiou. Comments on hidden nodes in neural nets. IEEE Transactions on Circuits <lb/>and Systems, 38:1410, 1991. <lb/>10] David Heath. A geometric Framework for Machine Learning. PhD thesis, Department of <lb/>Computer Science, Johns Hopkins University{Baltimore, Maryland, 1992. <lb/>11] J. Hertz, A. Krogh, and R. G. Palmer. Introduction to the Theory of Neural Computation. <lb/>Addison-Wesley, Redwood City, California, 1991. <lb/></listBibl>

			<page>12 <lb/></page>

			<listBibl>12] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal <lb/>approximators. Neural Networks, 2:359{366, 1989. <lb/>13] Y. le Cun, J. S. Denker, and S. A. Solla. Optimal brain damage. In D. S. Touretzky, editor, <lb/>Advances in Neural Information Processing Systems II (Denver 1989), pages 598{605, San <lb/>Mateo, California, 1990. Morgan Kaufmann. <lb/>14] Z.-Q. Luo, J.-S. Pang, D. Ralph, and S.-Q. Wu. Exact penalization and stationarity conditions <lb/>of mathematical programs with equilibrium constraints. Technical Report 275, Communica-<lb/>tions Research Laboratory, McMaster University, Hamilton, Ontario, Hamilton, Ontario L8S <lb/>4K1, Canada, 1993. Mathematical Programming, to appear. <lb/>15] O. L. Mangasarian. Linear and nonlinear separation of patterns by linear programming. Op-<lb/>erations Research, 13:444{452, 1965. <lb/>16] O. L. Mangasarian. Multi-surface method of pattern separation. IEEE Transactions on In-<lb/>formation Theory, IT-14:801{807, 1968. <lb/>17] O. L. Mangasarian. Mathematical programming in neural networks. ORSA Journal on Com-<lb/>puting, 5(4):349{360, 1993. <lb/>18] O. L. Mangasarian. Misclassi cation minimization. Journal of Global Optimization, 5:309{323, <lb/>1994. <lb/>19] O. L. Mangasarian, R. Setiono, and W. H. Wolberg. Pattern recognition via linear program-<lb/>ming: Theory and application to medical diagnosis. In T. F. Coleman and Y. Li, editors, <lb/>Large-Scale Numerical Optimization, pages 22{31, Philadelphia, Pennsylvania, 1990. SIAM. <lb/>Proceedings of the Workshop on Large-Scale Numerical Optimization, Cornell University, <lb/>Ithaca, New York, October 19-20, 1989. <lb/>20] O. L. Mangasarian and M. V. Solodov. Serial and parallel backpropagation convergence via <lb/>nonmonotone perturbed minimization. Optimization Methods and Software, 4(2):103{116, <lb/>1994. <lb/>21] O. L. Mangasarian, W. Nick Street, and W. H. Wolberg. Breast cancer diagnosis and prognosis <lb/>via linear programming. Technical Report 94-10, Computer Sciences Department, University <lb/>of Wisconsin, Madison, Wisconsin 53706, 1994. Operations Research 43(4) 1995, to appear. <lb/>Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-10.ps.Z. <lb/>22] M. Minsky and S. Papert. Perceptrons: An Introduction to Computational Geometry. MIT <lb/>Press, Cambridge, Massachusetts, 1969. <lb/>23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by <lb/>error propagation. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed <lb/>Processing, pages 318{362, Cambridge, Massachusetts, 1986. MIT Press. <lb/>24] C. Scha er. Over tting avoidance as bias. Machine Learning, 10:153{178, 1993. <lb/>25] M. V. Solodov and S. K. Zavriev. Stability properties of the gradient projection method with <lb/>applications to the backpropagation algorithm. Computer Sciences Department, Mathematical <lb/>Programming Technical Report 94-05, University of Wisconsin, Madison, Wisconsin, June <lb/>1994. SIAM Journal on Optimization, submitted. <lb/></listBibl>

			<page>13 <lb/></page>

			<listBibl>26] M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the <lb/>Royal Statistical Society, 36:111{147, 1974. <lb/>27] W. Nick Street and O. L. Mangasarian. Improved generalization via tolerant training. Technical <lb/>report, Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, 1995. <lb/>To appear. <lb/>28] D. H. Wolpert, editor. The Mathematics of Generalization, Reading, MA, 1995. Addison-<lb/>Wesley. <lb/></listBibl>

			<page>14 </page>


	</text>
</tei>
