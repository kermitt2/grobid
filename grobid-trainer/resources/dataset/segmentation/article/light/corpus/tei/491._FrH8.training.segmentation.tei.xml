<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Belief Revision: A Critique <lb/>Nir Friedman <lb/>Computer Science Department <lb/>Stanford University <lb/>Gates Building 1A <lb/>Stanford, CA 94305-9010 <lb/>nir@cs.stanford.edu <lb/>Joseph Y. Halpern <lb/>IBM Research Division <lb/>Almaden Research Center, Dept. K53-B2 <lb/>650 Harry Road <lb/>San Jose, CA 95120-6099 <lb/>halpern@almaden.ibm.com <lb/>May 6, 1996 <lb/>Abstract <lb/>The problem of belief change-how an agent should revise her beliefs upon learning new <lb/>information-has been an active area of research in both philosophy and artificial intelligence. <lb/>Many approaches to belief change have been proposed in the literature. Our goal is not to <lb/>introduce yet another approach, but to examine carefully the rationale underlying the approaches <lb/>already taken in the literature, and to highlight what we view as methodological problems in the <lb/>literature. The main message is that to study belief change carefully, we must be quite explicit <lb/>about the &quot;ontology&quot; or scenario underlying the belief change process. This is something that <lb/>has been missing in previous work, with its focus on postulates. Our analysis shows that we <lb/>must pay particular attention to two issues which have often been taken for granted: The first <lb/>is how we model the agent&apos;s epistemic state. (Do we use a set of beliefs, or a richer structure, <lb/>such as an ordering on worlds? And if we use a set of beliefs, in what language are these <lb/>beliefs are expressed?) The second is the status of observations. (Are observations known to <lb/>be true, or just believed? In the latter case, how firm is the belief?) For example, we argue that <lb/>even postulates that have been called &quot;beyond controversy&quot; are unreasonable when the agent&apos;s <lb/>beliefs include beliefs about her own epistemic state as well as the external world. Issues of the <lb/>status of observations arise particularly when we consider iterated belief revision, and we must <lb/>confront the possibility of revising by and then by ¡ . <lb/>Keyword: Belief revision <lb/></front>

			<body>1 Introduction <lb/>The problem of belief change-how an agent should revise her beliefs upon learning new <lb/>information-has been an active area of research in both philosophy and artificial intelligence. <lb/>The problem is a fascinating one in part because it is clearly no unique answer. Nevertheless, <lb/>there is a strong intuition that one wants to make minimal changes, and all the approaches <lb/>to belief change in the literature, such as [AGM85, Gär88, KM91a], try to incorporate this <lb/>principle. However, approaches differ on what constitutes a minimal change. This issue has <lb/>come to the fore with the spate of recent work on iterated belief revision (see, for example, <lb/>[Bou93, BG93, DP94, FL94, Leh95, Lev88, Wil94]). <lb/>The approaches to belief change typically start with a collection of postulates, argue that <lb/>they are reasonable, and prove some consequences of these postulates. Occasionally, a semantic <lb/>model for the postulates is provided and a representation theorem is proved (of the form that <lb/>every semantic model corresponds to some belief revision process, and that every belief revision <lb/>process can be captured by some semantic model). Our goal in this paper is not to introduce <lb/>yet another model of belief change, but to examine carefully the rationale underlying the <lb/>approaches in the literature. The main message of the paper is that describing postulates and <lb/>proving a representation theorem is not enough. While it may have been reasonable when <lb/>research on belief change started in the early 1980s to just consider the implications of a <lb/>number of seemingly reasonable postulates, it is our view that it should no longer be acceptable <lb/>now just to write down postulates and give short English justifications for them. In addition, <lb/>it is important to describe, what, for want of a better word, we call the underlying ontology or <lb/>scenario for the belief change process. Roughly speaking, this means describing carefully what <lb/>it means for something to be believed by an agent and what the status is of new information that <lb/>is received by the agent. This point will hopefully become clearer as we present our critique. <lb/>We remark that even though the issue of ontology is tacitly acknowledged in a number of <lb/>papers (for example, in the last paragraph of [Leh95]), it rarely enters into the discussion in a <lb/>significant way. We hope to show that ontology must play a central role in all discussions of <lb/>belief revision. <lb/>Our focus is on approaches that take as their starting point the postulates for belief revision <lb/>proposed by Alchourrón, Gärdenfors, and Makinson (AGM from now on) [AGM85], but our <lb/>critique certainly applies to other approaches as well. The AGM approach assumes that an <lb/>agent&apos;s epistemic state is represented by a belief set, that is, a set ¢ of formulas in a logical <lb/>language £ . What the agent learns is assumed to be characterized by some formula ¤ , also in <lb/>£ ; ¢ ¦¥ §¤ describes the belief set of an agent that starts with belief set ¢ and learns ¤ . <lb/>There are two assumptions implicit in this notation: <lb/>¨The functional form of ¥ suggests that all that matters regarding how an agent revises her <lb/>beliefs is the belief set and what is learnt. <lb/>¨The notation suggests that the second argument of ¥ can be an arbitrary formula in £ . But <lb/>what does it mean to revise by false? In what sense can false be learnt? More generally, is <lb/>it reasonable to assume that an arbitrary formula can be learnt in a given epistemic state? <lb/>The first assumption is particularly problematic when we consider the postulates that AGM <lb/></body>

			<page>1 <lb/></page>

			<body>require ¥ to satisfy. These essentially state that the agent is consistent in her choices, in the sense <lb/>that she acts as though she has an ordering on the strength of her beliefs [GM88, Gro88], or <lb/>an ordering on possible worlds [Bou94, Gro88, KM91b], or some other predetermined manner <lb/>of choosing among competing beliefs [AGM85]. However, the fact that an agent&apos;s epistemic <lb/>state is characterized by a collection of formulas means that the epistemic state cannot include <lb/>information about relative strength of beliefs (as required for the approach of, say, [GM88]), <lb/>unless this information is expressible in the language. Note that if £ is propositional logic <lb/>or first-order logic, such information cannot be expressed. On the other hand, if £ contains <lb/>conditional formulas of the form © , interpreted as &quot;if © is learnt, then will be believed&quot;, <lb/>then such information can be expressed. <lb/>Problems arise when the language is not rich enough to express relative degrees of strength <lb/>in beliefs. Consider, for example, a situation where ¢ ! &quot;© $# % &apos;&amp; (the logical closure of © $# % ; <lb/>that is, the agent&apos;s beliefs are characterized by the formula © (# ) and its logical consequences), <lb/>and then the agent learns ¤ 10 2© 43 50 6 . We can imagine that an agent whose belief in © is <lb/>stronger than her belief in would have ¢ 1¥ $¤ 7 8 9© A@ . That is, the agent gives up her belief in <lb/>, but retains a belief in © . On the other hand, if the agent&apos;s belief in is stronger than her belief <lb/>in © , it seems reasonable to expect that ¢ B¥ C¤ D E8 F &apos;@ . This suggests that it is unreasonable to <lb/>take ¥ to be a function if the representation language is not rich enough to express what may be <lb/>significant details of an agent&apos;s epistemic state. <lb/>We could, of course, assume that information about the relative strength of beliefs in various <lb/>propositions is implicit in the choice of the revision operator ¥ , even if it is not contained in the <lb/>language. This is perfectly reasonable, and also makes it more reasonable that ¥ be a function. <lb/>However, note that we can then no longer assume that we use the same ¥ when doing iterated <lb/>revision, since there is no reason to believe that the relative strength of beliefs is maintained <lb/>after we learn a formula. In fact, in a number of recent papers [Bou93, BG93, FH95b, Wil94], ¥ <lb/>is defined as a function from (epistemic states G formulas) to epistemic states, but the epistemic <lb/>states are no longer just belief sets; they include information regarding relative strengths of <lb/>beliefs. The revision function on epistemic states induces a mapping from (belief sets G <lb/>formulas) to belief sets, but at the level of belief sets, the mapping may not be functional; for a <lb/>belief set ¢ and formula ¤ , the belief set ¢ H¥ $¤ may depend on what epistemic state induced <lb/>¢ . Thus, the effect of ¥ on belief sets may change over time. 1 <lb/>There is certainly no agreement on what postulates belief change should satisfy. However, <lb/>the following two postulates are almost universal: <lb/>¨¤ PI Q¢ ¦¥ §¤ <lb/>¨if ¢ is consistent and ¤ RI ¢ , then ¢ B¥ $¤ 5 S¢ . <lb/>These postulates have been characterized by Rott [Rot89] as being &quot;beyond controversy&quot;. <lb/>Nevertheless, we argue that they are not as innocent as they may at first appear. <lb/>The first postulate says that the agent believes the last thing she learns. Making sense of <lb/>this requires some discussion of the underlying ontology. For example, imagine a scientist <lb/></body>

			<note place="footnote">1 Freund and Lehmann [FL94] have called the viewpoint that T may change over time the dynamic point of view. <lb/>However, this seems somewhat of a misnomer when applied to papers such as [Bou93, BG93, FH95b, Wil94], <lb/>since there T in fact is static, when viewed as a function on epistemic states and formulas. <lb/></note>

			<page>2 <lb/></page>

			<body>who believes that heavy objects drop faster than light ones, climbs the tower of Pisa, drops a <lb/>5 kilogram textbook and a 500 milligram novel, and observes they hit the ground at the same <lb/>time. Should the scientist necessarily believe that the time for an object to fall to the ground is <lb/>independent of its weight, on the basis of this one experiment? Certainly when scientists make <lb/>an observation that conflicts with their previous beliefs, they do not immediately change those <lb/>beliefs. This is even more true if they get information (perhaps as a result of reading a paper) <lb/>that is inconsistent with their previous beliefs. One could certainly imagine an ontology where <lb/>it takes repeated observations of ¤ before ¤ is accepted. Roughly speaking, an epistemic state <lb/>would then have to keep track of how many times, and under what circumstances, a proposition <lb/>has been observed. This requires either a rich language or an epistemic state that is described <lb/>by more than just a set of formulas. <lb/>Implicit in Gärdenfors&apos; discussion [Gär88] is a somewhat different assumption: if we decide <lb/>to revise by ¤ , it is because we give ¤ very high epistemic importance. In particular, if ¢ <lb/>contains 0 6¤ , we give ¤ higher importance than 0 6¤ . In the case of our scientist, this means <lb/>that the experiment was repeated, perhaps with some variations, and enough times so as to give <lb/>strong support to the new belief. While this position is again not unreasonable, it seems hard to <lb/>believe that false would ever be given such high epistemic importance. More generally, it is far <lb/>from obvious that in a given epistemic state ¢ we should allow arbitrary consistent formulas <lb/>to be given high epistemic importance. <lb/>If we can actually talk about epistemic importance in the language, then the second postulate <lb/>is no longer so reasonable. For suppose that ¤ DI 7¢ . Why should ¢ B¥ C¤ P U¢ ? It could well <lb/>be that being informed of ¤ raises the importance of ¤ in the epistemic ordering. If epistemic <lb/>ordering can be talked about in the language, then a notion of minimal change should still <lb/>allow epistemic ordering to change, even when something expected is learned. Even if we <lb/>cannot talk about epistemic ordering in the language, this observation has an impact on iterated <lb/>revisions. For example, one assumption made by Lehmann [Leh95] (his postulate I4) is that if <lb/>© is believed after revising by ¤ , then revising by V W ¤ X Y© `X ba dc -that is, revising by ¤ then © then <lb/>a -is equivalent to revising by V W ¤ eX fa dc . But consider a situation where after revising by ¤ , the <lb/>agent believes both © and , but her belief in is stronger than her belief in © . We can well <lb/>imagine that after learning 0 2© g3 h0 6 in this situation, she would believe 0 i© and . However, <lb/>if she first learned © and then 0 2© 43 70 6 , she would believe © and 0 6 , because, as a result of <lb/>learning © , she would give © higher epistemic importance than . In this case, we would not <lb/>have V W ¤ hX b© A p0 i© 43 h0 6 &apos;&amp; qc d HV r ¤ 7X &apos; p0 i© 43 h0 6 &apos;&amp; qc . In light of this discussion, it is not surprising that <lb/>the combination of the second postulate with a language that can talk about epistemic ordering <lb/>leads to technical problems such as Gärdenfors&apos; triviality result [Gär88]. <lb/>To give a sense of our concerns here, we discuss two basic ontologies. The first ontology <lb/>that seems (to us) reasonable assumes that the agent has some knowledge as well as beliefs. <lb/>We can think of the formulas that the agent knows as having the highest state of epistemic <lb/>importance. In keeping with the standard interpretation of knowledge, we also assume that the <lb/>formulas that the agent knows are true in the world. Since agents typically do not have certain <lb/>knowledge of very many facts, we assume that the knowledge is augmented by beliefs (which <lb/>can be thought of as defeasible guides to action). Thus, the set of formulas that are known form <lb/></body>

			<page>3 <lb/></page>

			<body>a subset of the belief set. We assume that the agent observes the world using reliable sensors; <lb/>thus, if the agent observes ¤ , then the agent is assumed to know ¤ . After observing ¤ , the agent <lb/>adds ¤ to his stock of knowledge, and may revise his belief set. Since the agent&apos;s observations <lb/>are taken to be knowledge, the agent will believe ¤ after observing ¤ . However, the agent&apos;s <lb/>epistemic state may change even if she observes a formula that she previously believed to be <lb/>true. In particular, if the formula observed was believed to be true but not known to be true, <lb/>after the observation it is known. Note that, in this ontology, the agent never observes false, <lb/>since false is not true of the world. In fact, the agent never observes anything that contradicts <lb/>her knowledge. Thus, ¢ s¥ t¤ is defined only for formulas ¤ that are compatible with the agent&apos;s <lb/>knowledge. Moving to iterated revision, this means we cannot have a revision by ¤ followed <lb/>by a revision by 0 6¤ . This ontology underlies some of our earlier work [FH95a, FH95b]. As <lb/>we show here, a variant of Darwiche and Pearl&apos;s approach [DP94] captures them as well. <lb/>We can consider a second ontology that has a different flavor. In this ontology, if we <lb/>observe something, we believe it to be true and perhaps even assign it a strength of belief. <lb/>But this assignment does not represent the strength of belief of the observation in the resulting <lb/>epistemic state. Rather, the belief in the observation must &quot;compete&quot; against current beliefs <lb/>if it is inconsistent with these beliefs. In this ontology, it is not necessarily the case that <lb/>¤ DI h¢ B¥ §¤ , just as it is not the case that a scientist will necessarily adopt the consequences of <lb/>his most recent observation into his stock of beliefs (at least, not without doing some additional <lb/>experimentation). Of course, to flesh out this ontology, we need to describe how to combine <lb/>a given strength of belief in the observation with the strengths of the beliefs in the original <lb/>epistemic state. Perhaps the closest parallel in the literature is something like the Dempster-<lb/>Shafer rule of combination [Sha76], which gives a rule for combining two separate bodies of <lb/>belief. We do not have a particular suggestion to make along these lines. However, we believe <lb/>that this type of ontology deserves further study. <lb/>The rest of the paper is organized as follows. In Section 2, we review the AGM framework, <lb/>and point out some problems with it. In Section 3, we consider proposals for belief change and <lb/>iterated belief change from the literature due to Boutilier [Bou93], Darwiche and Pearl [DP94], <lb/>Freund and Lehmann [FL94], and Lehmann [Leh95], and try to understand the ontology implicit <lb/>in the proposal (to the extent that one can be discerned). In Section 4, we consider the first <lb/>ontology discussed above in more detail. We conclude with some discussion in Section 5. <lb/>2 AGM Belief Revision <lb/>In this section we review the AGM approach to belief revision. As we said earlier, this approach <lb/>assumes that beliefs and observations are expressed in some language £ . It is assumed that £ is <lb/>closed under negation and conjunction, and comes equipped with a consequence relation u wv that <lb/>contains the propositional calculus and satisfies the deduction theorem. The agent&apos;s epistemic <lb/>state is represented by a belief set, that is, a set of formulas in £ closed under deduction. There <lb/>is also assumed to be a revision operator ¥ that takes a belief set ¢ and a formula ¤ and returns <lb/>a new belief set ¢ U¥ x¤ , intuitively, the result of revising ¢ by ¤ . The following AGM postulates <lb/>are an attempt to characterize the intuition of &quot;minimal change&quot;: <lb/></body>

			<page>4 <lb/></page>

			<body>R1. ¢ ¦¥ §¤ is a belief set <lb/>R2. ¤ PI Q¢ ¦¥ §¤ <lb/>R3. ¢ ¦¥ §¤ Ry ! ¢ 8 F¤ @ &amp; <lb/>R4. If 0 6¤ I ¢ then ! ¢ 8 F¤ @ &amp; y ¢ ¦¥ §¤ <lb/>R5. ¢ ¦¥ §¤ 7 % false&amp; if and only if u w 0 6¤ <lb/>R6. If u ¤ a then ¢ 1¥ $¤ 7 ¢ 1¥ a <lb/>R7. ¢ ¦¥ C ¤ # )a &amp; y ! ¢ ¥ §¤ 8 a C@ &amp; <lb/>R8. If 0 ta I Q¢ ¦¥ §¤ then ! ¢ ¥ §¤ Q e8 a @ &amp; y ¢ 1¥ ¤ # 4a &amp; <lb/>There are several representation theorems for AGM belief revision; perhaps the clearest is <lb/>due to Grove [Gro88]. We discuss a slight modification, due to Boutilier [Bou94] and Katsuno <lb/>and Mendelzon [KM91b]: Let an £ -world be a complete and consistent truth assignment to <lb/>the formulas in £ . Let consist of all the £ -worlds, and let be a ranking, that is, a total <lb/>preorder, on the worlds in . Let mind consist of all the minimal worlds with respect to , <lb/>that is, all the worlds e such that there is no e f with e Cf $g 1e . With we can associate a <lb/>belief set ¢ hd , consisting of all formulas ¤ that are true in all the worlds in mind . Moreover, <lb/>we can define a revision operator ¥ on ¢ d , by taking ¢ d ¥ i¤ to consist of all formulas a that <lb/>are true in all the minimal ¤ -worlds according to . It can be shown that ¥ satisfies the AGM <lb/>postulates (when its first argument is ¢ d ). Thus, we can define a revision operator by taking a <lb/>collection of orderings Cj , one for each belief set ¢ . To define ¢ B¥ C¤ for a belief set ¢ , we <lb/>apply the procedure above, starting with the ranking j corresponding to ¢ . 2 Furthermore, <lb/>in [Bou94, Gro88, KM91b], it is shown that every belief revision operator satisfying the AGM <lb/>axioms can be characterized in this way. <lb/>This elegant representation theorem also brings out some of the problems with the AGM <lb/>postulates. First, note that a given revision operator ¥ is represented by a family of rankings, <lb/>one for each belief set. There is no necessary connection between the rankings corresponding <lb/>to different belief sets. It might seem more reasonable to have a more global setting (perhaps <lb/>one global ranking) from which each element in the family of rankings arises. <lb/>A second important point is that the epistemic state here is represented not by a belief set, <lb/>but by a ranking. Each ranking is associated with a belief set ¢ d , but it is the ranking that <lb/>gives the information required to describe how revision is carried out. The belief set does not <lb/>suffice to determine the revision; there are many rankings for which the associated belief set <lb/>¢ d is ¢ . Since the revision process only gives us the revised belief set, not the revised ranking, <lb/>the representation does not support iterated revision. <lb/>This suggests that we should consider, not how to revise belief sets, but how to revise <lb/>rankings. More generally, whatever we take to be our representation of the epistemic state, it <lb/>seems appropriate to consider how these representations should be revised. This suggests that <lb/>we consider an analogue of the AGM postulates for epistemic states. Such an analogue can be <lb/></body>

			<note place="footnote">2 In this construction, for each belief set k other than the inconsistent belief set, we have k l nm o Rk . The <lb/>inconsistent belief set gets special treatment here. <lb/></note>

			<page>5 <lb/></page>

			<body>defined in a straightforward way (cf. [FH95b]): Taking p to range over epistemic states and <lb/>Bel p q&amp; to represent the belief set associated with epistemic state p , we have <lb/>R1f &quot;r p ¥ $¤ is an epistemic state <lb/>R2f r ¤ RI Bel p ¥ §¤ &amp; <lb/>R3f &quot;r p ¥ $¤ 7y ! Bel p q&amp; s 8 F¤ @ &amp; <lb/>and so on, with the obvious syntactic transformation. In fact, as we shall see in the next section, <lb/>a number of processes for revising epistemic states have been considered in the literature, and <lb/>in fact they all do satisfy these modified postulates. <lb/>Finally, even if we restrict attention to belief sets, we can consider what happens if the <lb/>underlying language £ is rich enough to talk about how revision should be carried out. For <lb/>example, suppose £ conditional formulas, and we want to find some ranking for which the <lb/>corresponding belief set is ¢ . Not just any ranking such that ¢ d t¢ will do here. The <lb/>beliefs in ¢ put some constraints on the ranking. For example, if © 7 E is in ¢ and © u I R¢ , <lb/>then the minimal -worlds satisfying © must all satisfy , since after © is learnt, is believed. <lb/>Once we restrict to rankings that are consistent with the formulas in the worlds that are being <lb/>ranked, then the AGM postulates are no longer sound. This point has essentially been made <lb/>before [Bou92, Rot89]. However, it is worth stressing the sensitivity of the AGM postulates to <lb/>the underlying language and, more generally, to the choice of epistemic state. <lb/>3 Proposals for Iterated Revision <lb/>We now briefly review some of the previous proposals for iterated belief change, and point out <lb/>how the impact of the observations we have been making on the approaches. Most of these <lb/>approaches start with the AGM postulates, and augment them to get seemingly appropriate <lb/>restrictions on iterated revision. This is not an exhaustive review of the literature on iterated <lb/>belief revision by any stretch of the imagination. Rather, we have chosen a few representative <lb/>approaches that allow us to bring out our methodological concerns. <lb/>3.1 Boutilier&apos;s natural revision <lb/>As we said in the previous section, Boutilier takes the agent&apos;s epistemic state to consist of a <lb/>ranking of possible worlds. Boutilier [Bou93] describes a particular revision operator ¥ v on <lb/>epistemic states that he calls natural revision operator. Natural revision maps a ranking of <lb/>possible worlds and an observation ¤ to a revised ranking ¥ v w¤ such that (a) ¥ v w¤ satisfies <lb/>the conditions of the representation theorem described above-the minimal worlds in ¥ v ¤ <lb/>are precisely the minimal ¤ -worlds in , and (b) in a precise sense, ¦¥ v ¤ is the result of <lb/>making the minimal number of changes to required to guarantee that all the minimal worlds <lb/>in E¥ v ¤ satisfy ¤ . Given a ranking and a formula ¤ , the ranking s¥ v ¤ is identical to <lb/>except that the minimal ¤ -worlds according to have the minimal rank in the revised ranking, <lb/>while the relative ranks of all other worlds remains unchanged. <lb/>Boutilier characterizes the properties of natural revision. Suppose that, starting in some <lb/>epistemic state, we revise by ¤ 1 x r br yr x ¤ sz . Further suppose ¤ |{ ~} 1 is consistent with the beliefs <lb/></body>

			<page>6 <lb/></page>

			<body>after revising by ¤ 1 x r br br x ¤ |{ . Then the beliefs after revising by ¤ 1 x r br br x ¤ |z are precisely the <lb/>beliefs after observing ¤ 1 # r br br # 7¤ sz . (More precisely, given any ranking , the belief set <lb/>associated with the ranking ¥ v w¤ 1 ¥ v r yr br ¥ v ¤ |z is the same as that associated with the <lb/>ranking ¥ v ¤ 1 # r br br # ¤ |z &amp; . Note, however, that ¥ v w¤ 1 ¥ v r br br ¥ v ¤ |z e % ¥ v ( ¤ g# r br br # ¤ |z &amp; <lb/>in general.) Thus, as long as the agent&apos;s new observations are not surprising, the agent&apos;s <lb/>beliefs are exactly the ones she would have had had she observed the conjunction of all the <lb/>observations. This is an immediate consequence of the AGM postulates, and thus holds for any <lb/>approach that attempts to extend the AGM postulates to iterated revision. <lb/>What happens when the agent observes a formula ¤ |z } 1 that is inconsistent with her current <lb/>beliefs? Boutilier shows that in this case the new observation nullifies the impact of the all the <lb/>observations starting with the most recent one that is inconsistent with ¤ sz } 1 . More precisely, <lb/>suppose ¤ { } 1 is consistent with the belief after observing ¤ 1 x r br yr x ¤ { for , but ¤ z } 1 is <lb/>inconsistent with the beliefs after observing ¤ 1 x r br br x ¤ z . Let be the maximal index such that <lb/>¤ z } 1 is consistent with the beliefs after learning ¤ 1 x r yr br x ¤ . The agent&apos;s beliefs after observing <lb/>¤ z } 1 are the same as her beliefs after observing ¤ 1 x r yr br x ¤ x ¤ z } 1 . Thus, the agent acts as <lb/>though she did not observe ¤ t } 1 x r br br x ¤ z . <lb/>Boutilier does not provide any argument for the reasonableness of this ontology. In fact, <lb/>Boutilier&apos;s presentation (like almost all others in the literature) is not in terms of an ontology <lb/>at all; he presents natural revision as an attempt to minimize changes to the ranking. While <lb/>the intuition of minimizing changes to the ranking seems reasonable at first, it becomes less <lb/>reasonable when we realize its ontological implications. The following example, due to <lb/>Darwiche and Pearl [DP94], emphasizes this point. Suppose we encounter a strange new <lb/>animal and it appears to be a bird, so we believe it is a bird. On closer inspection, we see that <lb/>it is red, so we believe that it is a red bird. However, an expert then informs us that it is not a <lb/>bird, but a mammal. Applying natural revision, we would no longer believe that the animal is <lb/>red. This does not seem so reasonable. <lb/>One more point is worth observing: As described by Boutilier [Bou93], natural revision <lb/>does not allow revision by false. While we could, of course, modify the definition to handle <lb/>false, it is more natural simply to disallow it. This suggests that, whatever ontology is used to <lb/>justify natural revision, in that ontology, revising by false should not make sense. <lb/>3.2 Freund and Lehmann&apos;s approach <lb/>Freund and Lehmann [FL94] stick close to the original AGM approach. They work with belief <lb/>sets, not more general epistemic states. However, they are interested in iterated revision. They <lb/>consider the effect of adding just one more postulate to the basic AGM postulates, namely <lb/>R9. If 0 6¤ PI ¢ , then ¢ ¦¥ §¤ 7 ¢ h¥ §¤ , <lb/>where ¢ is the inconsistent belief set, which consists of all formulas. <lb/>Suppose ¥ satisfies K1-K9. Just as with Boutilier&apos;s natural revision, if ¤ { ~} 1 is consistent <lb/>with the beliefs after learning ¤ 1 x r br yr x ¤ |{ for C s e 1, then ¢ ¥ C¤ 1 ¥ r br yr ¥ C¤ |z e E¢ ¥ w ¤ 1 # <lb/>r br br # Q¤ |z &amp; . However, if we then observe ¤ sz } 1 , and it is inconsistent with ¢ ¥ C¤ 1 # r br br # ¤ |z , <lb/>then ¢ ¥ d¤ 1 ¥ r yr br ¥ d¤ |z } 1 ¢ g¥ d¤ |z } 1 . That is, observing something inconsistent causes us to <lb/></body>

			<page>7 <lb/></page>

			<body>retain none of our previous beliefs, but to start over from scratch. While the ontology here is <lb/>quite simple to explain, as Freund and Lehmann themselves admit, it is a rather severe form of <lb/>belief revision. Darwiche and Pearl&apos;s red bird example applies to this approach as well. <lb/>3.3 Darwiche and Pearl&apos;s approach <lb/>Darwiche and Pearl [DP94] suggest a set of postulates extending the AGM postulates, and <lb/>claim to provide a semantics that satisfies them. Their intuition is that the revision operator <lb/>should retain as must as possible certain parts of the ordering among worlds in the ranking. In <lb/>particular, if e 1 and e 2 both satisfy ¤ , then a revision by ¤ should not change the relative rank <lb/>of e 1 and e 2 . Similarly, if both e 1 and e 2 satisfy 0 6¤ , then a revision should not change their <lb/>relative rank. They describe four postulates that are meant to embody these intuitions: <lb/>C1. If ¤ u )a , then ¢ 1¥ da &amp; s¥ §¤ 5 ¢ ¦¥ §¤ <lb/>C2. If ¤ u e0 ta , then ¢ 1¥ a C&amp; s¥ $¤ 7 ¢ B¥ $¤ <lb/>C3. If a I ¢ ¦¥ §¤ , then a I 5 ¢ ¦¥ da &amp; s¥ §¤ <lb/>C4. If 0 ta u I Q¢ ¦¥ §¤ , then 0 ta u I h ¢ 1¥ a C&amp; i¥ §¤ <lb/>Freund and Lehmann [FL94] point out that C2 is inconsistent with the AGM postulates. <lb/>This observation seems inconsistent with the fact that Darwiche and Pearl claim to provide a <lb/>semantics for their postulates. What is going on here? It turns out that the issues raised earlier <lb/>help clarify the situation. <lb/>Darwiche and Pearl semantics is based on a special case Spohn&apos;s ordinal conditional <lb/>functions (OCFs) [Spo88] called -rankings [GP92]. A -ranking associates with each world <lb/>either a natural number or , with the requirement that for at least one world e 0 , we have <lb/>w qe 0 &amp; 0. We can think of A e &amp; as the rank of e , or as denoting how surprising it would be to <lb/>discover that e is the actual world. If A e &amp; 0, then world e is unsurprising; if w e &amp; 1, then <lb/>e is somewhat surprising; if w qe (&amp; 2, then e is more surprising, and so on. If w e &amp; C , <lb/>then e is impossible. 3 OCFs provide a way of ranking worlds that is closely related to, but has <lb/>a little more structure than, the orderings considered by Boutilier. The extra structure makes it <lb/>easier to define a notion of conditioning. <lb/>Given a formula ¤ , let w ¤ &amp; min8 A e &amp; : e ¤ @ ; we define w false&amp; . We say that <lb/>¤ is believed with firmness R 0 in OCF if w ¤ &amp; d 0 and w 0 6¤ &amp; d . Thus, ¤ is believed <lb/>with firmness if ¤ is unsurprising and the least surprising world satisfying 0 6¤ has rank . By <lb/>analogy to the definition of ¢ d , we define ¢ ` to consist of all those formulas that are believed <lb/>with firmness at least 1. <lb/>Spohn defined a notion of conditioning on OCFs. Given an OCF , a formula ¤ such that <lb/>w ¤ &amp; C E , and D 0, n¡ ¢ is the unique OCF satisfying the properties desired by Darwiche <lb/>and Pearl-namely, if e and e f both satisfy ¤ or both satisfy 0 6¤ , then £¡ ¢ ¤ e &amp; 6 D £¡ ¢ ¥ qe f &amp; <lb/>3 Spohn allowed ranks to be arbitrary ordinals, not just natural numbers, and did not allow a rank of ¦ , since, <lb/>for philosophical reasons, he did not want to allow a world to be considered impossible. As we shall see, there <lb/>are technical advantages to introducing a rank of ¦ . <lb/></body>

			<page>8 <lb/></page>

			<body>w qe (&amp; t D w qe f &amp; -such that ¤ is believed with firmness in £¡ ¢ . It is defined as follows: <lb/>£¡ ¢ ¤ e &amp; <lb/> § A e &amp; t D w ¤ &amp; <lb/>if e satisfies ¤ <lb/>A e &amp; t D w p0 ¤ &amp; sR if e satisfies 0 6¤ . <lb/>Notice that £¡ ¢ is defined only if w ¤ &amp; d , that is, if ¤ is considered possible. <lb/>Darwiche and Pearl defined the following revision function on OCFs: <lb/>%¥ © sª 7¤ 7 <lb/> § if ¤ is believed with firmness 7 1 in <lb/>¥ £¡ 1 otherwise. <lb/>Thus, if ¤ is already believed with firmness at least 1 in , then is unaffected by a revision <lb/>by ¤ ; otherwise, the effect of revision is to modify by conditioning so that ¤ ends up being <lb/>believed with degree of firmness 1. Intuitively, this means that if ¤ is not believed in , in C¥ 6¤ <lb/>it is believed, but with the minimal degree of firmness. <lb/>It is not hard to show that if we take an agent&apos;s epistemic state to be represented by an <lb/>OCF, then Darwiche and Pearl&apos;s semantics satisfies all the AGM postulates modified to apply <lb/>to epistemic states (that is, R1f -R8f in Section 2), except that revising by false is disallowed, <lb/>just as in natural revision, so that R5f holds vacuously; in addition, this semantics satisfies <lb/>Darwiche and Pearl&apos;s C1-C4, modified to apply to epistemic states. For example, C2 becomes <lb/>C2f &quot;r If ¤ u e0 a , then ¢ g« 9¬ &apos;® ¯¬ ¢ ` 9¬ ° . <lb/>Indeed, as Darwiche and Pearl observe, natural revision also satisfies C1f -C4f , however, it has <lb/>properties that they view as undesirable. Thus, Darwiche and Pearl&apos;s claim that their postulates <lb/>are consistent with AGM is correct, if we think at the level of general epistemic states. On <lb/>the other hand, Freund and Lehmann are quite right that R1-R8 and C1-C4 are incompatible; <lb/>indeed, as they point out, R1-R4 and C2 are incompatible. The importance of making clear <lb/>exactly whether we are considering the postulates with respect to the OCF or the belief set <lb/>¢ ` is particularly apparent here. <lb/>The fact that Boutilier&apos;s natural revision also satisfies C1f -C4f clearly shows that these <lb/>postulates do not capture all of Darwiche and Pearl&apos;s intuitions. Their semantics embodies <lb/>further assumptions. Some of them seem ad hoc. Why is it reasonable to believe ¤ with <lb/>a minimal degree of firmness after revising by ¤ ? Rather than trying to come up with an <lb/>improved collection of postulates (which Darwiche and Pearl themselves suggest might be a <lb/>difficult task), it seems to us a more promising approach is to find an appropriate ontology. <lb/>3.4 Lehmann&apos;s revised approach <lb/>Finally, we consider Lehmann&apos;s &quot;revised&quot; approach to belief revision [Leh95]. With each <lb/>sequence ± of observations, Lehmann associates a belief set that we denote Bel ± &amp; . Intuitively, <lb/>we can think of Bel ± &amp; as describing the agent&apos;s beliefs after making the sequence ± of <lb/>observations, starting from her initial epistemic state. Lehmann allows all possible sequences <lb/>of consistent formulas. Thus, he assumes that the agent does not observe false. We view <lb/>Lehmann&apos;s approach essentially as taking the agent&apos;s epistemic state to be the sequence of <lb/></body>

			<page>9 <lb/></page>

			<body>observations made, with the obvious revision operator that concatenate a new observation <lb/>to the current epistemic state. The properties of belief change depend on the function Bel. <lb/>Lehmann require Bel to satisfy the following postulates (where ± and ² denote sequences of <lb/>formulas, and X is the concatenation operator): <lb/>I1. Bel ± &amp; is a consistent belief set <lb/>I2. ¤ PI Bel ± X ¤ &amp; <lb/>I3. If a I Bel ± X ¤ &amp; , then ¤ 7³ a I Bel ± &amp; <lb/>I4. If ¤ RI Bel ± &amp; , then Bel ± 4X ¤ X ² ¤&amp; Bel ± 4X ² ¤&amp; <lb/>I5. If a Du h¤ , then Bel ± 4X ¤ X ba RX b² ¤&amp; Bel ± X ba 7X ² ¤&amp; <lb/>I6. If 0 ta E I Bel ± 4X ¤ &amp; , then Bel ± X ¤ ´X ba µX ² ¤&amp; Bel ± ¶X y¤ ´X ¤ # a µX ² ¤&amp; <lb/>I7. Bel ± X £0 6¤ X ¤ &amp; dy Cl Bel ± &amp; s e8 F¤ @ &amp; <lb/>We refer the interested reader to [Leh95] for the motivation for these postulates. As Lehmann <lb/>notes, the spirit of the original AGM postulates is captured by these postulates. Lehmann views <lb/>I5 and I7 as two main additions to the basic AGM postulates. He states that &quot;Since postulates <lb/>I5 and I7 seem secure, i.e., difficult to reject, the postulates I1-I7 may probably be considered <lb/>as a reasonable formalization of the intuitions of AGM.&quot; Our view is that it is impossible to <lb/>decide whether to accept or reject postulates such as I5 or I7 (or, for that matter, any of the <lb/>other postulates) without an explicit ontology. There may be ontologies for which I5 and I7 <lb/>are reasonable, and others for which they are not. &quot;Reasonableness&quot; is not an independently <lb/>defined notion; it depends on the ontology. The ontology of the next section emphasizes this <lb/>point. <lb/>4 Taking Observations to be Knowledge <lb/>We now consider an ontology where observations are taken to be knowledge. In this ontology, <lb/>it is impossible to observe false. In fact, it is impossible to make any inconsistent sequence of <lb/>observations. That is, if ¤ 1 x r br br x ¤ sz is observed, then ¤ 1 # r br yr # (¤ sz must be consistent (although <lb/>it may not be consistent with the agent&apos;s original beliefs). <lb/>In earlier work [FH95b], we presented such an ontology, based on Halpern and Fagin&apos;s <lb/>[HF89] framework of multi-agent systems (see [FHMV95] for more details). The key assump-<lb/>tion in the multi-agent system framework is that we can characterize the system by describing <lb/>it in terms of a state that changes over time. Formally, we assume that at each point in time, the <lb/>agent is in some local state. Intuitively, this local state encodes the information the agent has <lb/>observed thus far. There is also an environment, whose state encodes relevant aspects of the <lb/>system that are not part of the agent&apos;s local state. A global state is a tuple • b¸x • ¹ b&amp; consisting of <lb/>the environment state • b¸and the local state • b¹ of the agent. A run of the system is a function <lb/>from time (which, for ease of exposition, we assume ranges over the natural numbers) to global <lb/>states. Thus, if º is a run, then º 0&amp; x º &apos; 1&amp; x r yr br is a sequence of global states that, roughly <lb/>speaking, is a complete description of what happens over time in one possible execution of the <lb/>system. We take a system to consist of a set of runs. Intuitively, these runs describe all the <lb/></body>

			<page>10 <lb/></page>

			<body>possible behaviors of the system, that is, all the possible sequences of events that could occur <lb/>in the system over time. <lb/>Given a system » , we refer to a pair qº x ¼ &amp; consisting of a run º I 7» and a time ¼ as a <lb/>point. If º ¼ &amp; C ½ • b¸x • b¹ f&amp; , we define º y¹ b ¼ &amp; w ¾• b¹ and º bY ¼ &amp; t• b¸. We say two points º x ¼ &amp; <lb/>and º f x ¼ f &amp; are indistinguishable to the agent, and write qº x ¼ &amp; ¿ À¹ qº f x p¼ f &amp; , if º b¹ ¼ &amp; º f <lb/>¹ ¼ f &amp; , <lb/>i.e., if the agent has the same local state at both points. Finally, an interpreted system is a tuple <lb/>q» x Á &amp; , consisting of a system » together with a mapping Á that associates with each point a <lb/>truth assignment to the primitive propositions. <lb/>To capture the AGM framework, we consider a special class of interpreted systems: We <lb/>fix a propositional £ . We assume that the agent makes observations, which are characterized <lb/>by formulas in £ , and that her local state consists of the sequence of observations that she has <lb/>made. We assume that the environment&apos;s local state describes which formulas are actually true <lb/>in the world, so that it is a truth assignment to the formulas in £ . As observed by Katsuno and <lb/>Mendelzon [KM91a], the AGM postulates assume that the world is static; to capture this, we <lb/>assume that the environment state does not change over time. Formally, we are interested in <lb/>the unique interpreted system q» 4Â Ã ¥Ä x Á &amp; that consists of all runs satisfying the following two <lb/>assumptions for every point qº x ¼ &amp; : <lb/>¨Theenvironment&apos;sstate º ¸ ¼ &amp; is a truth assignment to the formulas in £ that agrees with Á <lb/>at º x ¼ &amp; (that is, Á º x p¼ &amp; t º b9 ¼ &amp; ), and º yÅ ¼ &amp; º bY 0&amp; . <lb/>¨The agent&apos;s state º b¹ ¼ &amp; is a sequence of the form AE !¤ 1 x r yr br x ¤ |Ç wÈ , such that ¤ 1 # r br yr # ¤ sÇ is <lb/>true according to the truth assignment º bY ¼ &amp; and º 1 ¼ 1&amp; ¾AE ¤ 1 x r br yr x ¤ |Ç dÉ 1 È . <lb/>Notice that the form of the agent&apos;s state makes explicit an important implicit assumption: that <lb/>the agent remembers all her previous observations. <lb/>In an interpreted system, we can talk about an agent&apos;s knowledge: the agent knows ¤ at <lb/>a point qº x ¼ &amp; if ¤ holds in all points º ff x ¼ f ~&amp; such that qº x ¼ &amp; ¿ ¹ qº f x ¼ f Ê&amp; . It is easy to see <lb/>that, according to this definition, if º ¹ ¼ &amp; UAE ¤ 1 x r br yr x ¤ Ç È , then the agent knows ¤ 1 # r br yr # ¤ Ç <lb/>at the point qº x ¼ &amp; : the agent&apos;s observations are known to be true in this approach. We are <lb/>interested in talking about the agent&apos;s beliefs as well as her knowledge. To allow this, we <lb/>added a notion of plausibility to interpreted systems in [FH95a]. We consider a variant of this <lb/>approach here, using OCFs, since it makes it easier to relate our observations to Darwiche and <lb/>Pearl&apos;s framework. <lb/>We assume that we start with an OCF on runs such that w º &apos;&amp; À for any run º . Intuitively, <lb/>represents our prior ranking on runs. Initially, no runs is viewed as impossible. We then <lb/>associate, with each point qº x ¼ &amp; , an OCF « ÌË ¡ Ç Í® on the runs. We define « ÌË ¡ Ç Í® by induction on <lb/>¼ . We take « ÎË ¡ 0 ® E , and we take « ÎË ¡ Ç } 1 ® E « ÌË ¡ Ç Í® <lb/>bÏ wÐ 1 ¡ Ñ , where º b¹ b ¼ ¨1&amp; d ½AE ¤ 1 x r br yr x ¤ sÇ } 1 È . <lb/>Thus, « ÌË ¡ Ç Í} 1 ® is the result of conditioning « ÎË ¡ Ç ® on the last observation the agent made, <lb/>giving it degree of firmness . Thus, the agent is treating the observations as knowledge in <lb/>a manner compatible with the semantics for knowledge in the interpreted system. Moreover, <lb/>since obervations are known, they are also believed. <lb/>As we show in the full paper, this framework satisfies the AGM postulates R1f -R8f , inter-<lb/>preted on epistemic states. (Here we take the agent&apos;s epistemic state at the point º x ¼ &amp; to consist <lb/>of º b¹ b ¼ &amp; together with « ÌË ¡ Ç Í® .) Moreover, the framework also satisfies Darwiche and Pearl&apos;s <lb/></body>

			<page>11 <lb/></page>

			<body>postulates (appropriately modified to apply to epistemic states), except that the contentious C2 <lb/>is now vacuous, since it is illegal to revise by a and then ¤ if ¤ 5u 0 a . <lb/>How does this framework compare to Lehmann&apos;s? Like Lehmann&apos;s, there is an explicit <lb/>attempt to associate beliefs with a sequence of revisions. However, we have restricted the <lb/>sequence of revisions, since we are treating observations as knowledge. It is easy to see that <lb/>I1-I3 and I5-I7 hold in our framework. However, since we have restricted the sequence of <lb/>observations allowed, some of these postulates are much weaker in our framework than in <lb/>Lehmann&apos;s. In particular, I7 is satisfied vacuously, since we do not allow a sequence of the <lb/>form ± X £0 6¤ X ¤ . On the other hand, I4 is not satisfied in our framework. Our discussion in the <lb/>introduction suggests a counterexample. Suppose that initially, w &quot;© `# e &apos;&amp; d 0, w 0 2© g# e &amp; 1, <lb/>w &quot;© # 70 6 &apos;&amp; ` 2, and A p0 2© ¶# 70 6 &apos;&amp; ` 3. Thus, initially the agent believes both © and , but <lb/>believes © with firmness 1 and with firmness 2. If the agent then observes 0 2© g3 0 6 , he will <lb/>then believe but not © . On the other hand, suppose the agent first observes © . He still believes <lb/>both © and , of course, but now © is believed with firmness . That means if he then observes <lb/>0 2© q3 0 6 , he will believe , but not © , violating I4. However, a weaker variant of I4 does hold <lb/>in our system: if the agent knows ¤ , then observing ¤ will not change her future beliefs. <lb/>5 Discussion <lb/>The goal of this paper was to highlight what we see as some methodological problems in much <lb/>of the literature on belief revision. There has been (in our opinion) too much attention paid <lb/>to postulates, and not enough to the underlying ontology. An ontology must make clear what <lb/>the agent&apos;s epistemic state is, what types of observations the agent can make, the status of <lb/>observations, and how the agent goes about revising the epistemic state. Previous work has <lb/>typically not made clear whether observations are believed to be true or known to be true, and <lb/>if they are believed, what the strength of belief is. This issue is particularly important if we <lb/>have epistemic states like rankings that are richer than belief sets. If observations are believed, <lb/>but not necessarily known, to be true, then it is not clear how to go about revising such a richer <lb/>epistemic state. With what degree of firmness should the new belief be held? No particular <lb/>answer seems to us that well motivated. It may be appropriate for the user to attach degrees of <lb/>firmness to observations, as was done in [Gol92, Wil94, Wob95] (following the lead of Spohn <lb/>[Spo88]); we can even generalize to allowing uncertain observations [DP92]. <lb/>It seems to us that many of the intuitions that researchers in the area have are motivated by <lb/>thinking in terms of observations as known, even if this is not always reflected in the postulates <lb/>considered. We have examined carefully one particular instantiation of this ontology, that of <lb/>treating observations as knowledge. (As shown in [FH95b], this ontology can also capture <lb/>Katsuno and Mendelzon&apos;s belief update [KM91b].) We have shown that, in this ontology, some <lb/>postulates that seem reasonable, such as Lehmann&apos;s I4, do not hold. We do not mean to suggest <lb/>that I4 is &quot;wrong&quot; (whatever that might mean in this context). Rather, it shows that we cannot <lb/>blithely accept postulates without making the underlying ontology clear. We would encourage <lb/>the investigation of other ontologies for belief change. <lb/></body>

			<page>12 <lb/></page>

			<listBibl>References <lb/>[AGM85] C. E. Alchourrón, P. Gärdenfors, and D. Makinson. On the logic of theory change: <lb/>partial meet functions for contraction and revision. Journal of Symbolic Logic, <lb/>50:510-530, 1985. <lb/>[BG93] <lb/>C. Boutilier and M. Goldszmidt. Revising by conditional beliefs. In Proc. National <lb/>Conference on Artificial Intelligence (AAAI &apos;93), pages 648-654. 1993. <lb/>[Bou92] <lb/>C. Boutilier. Normative, subjective and autoepistemic defaults: adopting the <lb/>Ramsey test. In KR &apos;92, pages 685-696. <lb/>[Bou93] <lb/>C. Boutilier. Revision sequences and nested conditionals. In Proc. Thirteenth <lb/>International Joint Conference on Artificial Intelligence (IJCAI &apos;93), pages 519-<lb/>525, 1993. <lb/>[Bou94] <lb/>C. Boutilier. Unifying default reasoning and belief revision in a modal framework. <lb/>Artificial Intelligence, 68:33-85, 1994. <lb/>[DP92] <lb/>D. Dubois and H. Prade. Belief change and possibility theory. In P. Gärdenfors, <lb/>editor, Belief Revision. Cambridge University Press, Cambridge, U.K., 1992. <lb/>[DP94] <lb/>A. Darwiche and J. Pearl. On the logic of iterated belief revision. In Theoretical <lb/>Aspects of Reasoning about Knowledge: Proc. Fifth Conference, pages 5-23. <lb/>1994. <lb/>[FH95a] <lb/>N. Friedman and J. Y. Halpern. Modeling belief in dynamic systems. part I: <lb/>foundations. Technical Report RJ 9965, IBM, 1995. Submitted for publication. A <lb/>preliminary version appears in R. Fagin editor. Theoretical Aspects of Reasoning <lb/>about Knowledge: Proc. Fifth Conference, 1994, pp. 44-64, under the title &quot;A <lb/>knowledge-based framework for belief change. Part I: foundations&quot;. <lb/>[FH95b] <lb/>N. Friedman and J. Y. Halpern. Modeling belief in dynamic systems. Part II: <lb/>revision and update. In preparation. A preliminary version appears in J. Doyle, <lb/>E. Sandewall, and P. Torasso, editors. Principles of Knowledge Representation and <lb/>Reasoning: Proc. Fourth International Conference (KR &apos;94), 1994, pp. 190-201, <lb/>under the title &quot;A knowledge-based framework for belief change. Part II: revision <lb/>and update.&quot;, 1995. <lb/>[FHMV95] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. Reasoning about Knowledge. <lb/>MIT Press, Cambridge, Mass., 1995. <lb/>[FL94] <lb/>M. Freund and D. Lehmann. Belief revision and rational inference. Technical <lb/>Report TR 94-16, Hebrew University, 1994. <lb/>[Gär88] <lb/>P. Gärdenfors. Knowledge in Flux. MIT Press, Cambridge, U.K., 1988. <lb/>[GM88] <lb/>P. Gärdenfors and D. Makinson. Revisions of knowledge systems using epistemic <lb/>entrenchment. In Proc. Second Conference on Theoretical Aspects of Reasoning <lb/>about Knowledge, pages 83-95. 1988. <lb/></listBibl>

			<page>13 <lb/></page>

			<listBibl>[Gol92] <lb/>M. Goldszmidt. Qualitative probabilities: a normative framework for common-<lb/>sense reasoning. PhD thesis, University of California Los Angeles, 1992. <lb/>[GP92] <lb/>M. Goldszmidt and J. Pearl. Rank-based systems: A simple approach to belief <lb/>revision, belief update and reasoning about evidence and actions. In KR &apos;92, pages <lb/>661-672. <lb/>[Gro88] <lb/>A. Grove. Two modelings for theory change. Journal of Philosophical Logic, <lb/>17:157-170, 1988. <lb/>[HF89] <lb/>J. Y. Halpern and R. Fagin. Modelling knowledge and action in distributed systems. <lb/>Distributed Computing, 3(4):159-179, 1989. A preliminary version appeared in <lb/>Proc. 4th ACM Symposium on Principles of Distributed Computing, 1985, with <lb/>the title &quot;A formal model of knowledge, action, and communication in distributed <lb/>systems: preliminary report&quot;. <lb/>[KM91a] H. Katsuno and A. Mendelzon. On the difference between updating a knowledge <lb/>base and revising it. In KR &apos;91, pages 387-394. 1991. <lb/>[KM91b] H. Katsuno and A. Mendelzon. Propositional knowledge base revision and minimal <lb/>change. Artificial Intelligence, 52(3):263-294, 1991. <lb/>[Leh95] <lb/>D. Lehmann. Belief revision, revised. In Proc. Fourteenth International Joint <lb/>Conference on Artificial Intelligence (IJCAI &apos;95), pages 1534-1540. 1995. <lb/>[Lev88] <lb/>I. Levi. Iteration of conditionals and the Ramsey test. Synthese, 76:49-81, 1988. <lb/>[Rot89] <lb/>H. Rott. Conditionals and theory change: revision, expansions, and additions. <lb/>Synthese, 81:91-113, 1989. <lb/>[Sha76] <lb/>G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, <lb/>Princeton, N.J., 1976. <lb/>[Spo88] <lb/>W. Spohn. Ordinal conditional functions: a dynamic theory of epistemic states. <lb/>In W. Harper and B. Skyrms, editors, Causation in Decision, Belief Change and <lb/>Statistics, volume 2, pages 105-134. Reidel, Dordrecht, Netherlands, 1988. <lb/>[Wil94] <lb/>M. Williams. Transmutations of knowledge systems. In KR &apos;94, pages 619-629. <lb/>1994. <lb/>[Wob95] W. Wobcke. Belief revision, conditional logic, and nonmonotonic reasoning. Notre <lb/>Dame Journal of Formal Logic, 36(1):55-102, 1995. <lb/></listBibl>

			<page>14 </page>


	</text>
</tei>
