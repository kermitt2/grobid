<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Accurate 3D Action Recognition using Learning on the <lb/>Grassmann Manifold <lb/>Rim Slama a,b , Hazem Wannous a,b , Mohamed Daoudi b,c , Anuj Srivastava d <lb/>a <lb/>University Lille 1, Villeneuve d&apos;Ascq, France <lb/>b <lb/>LIFL Laboratory / UMR CNRS 8022, Villeneuve d&apos;Ascq, France <lb/>c <lb/>Institut Mines-Telecom / Telecom Lille, Villeneuve d&apos;Ascq, France <lb/>d <lb/>Florida State University, Departement of Statistics, Tallahassee, USA <lb/>Abstract <lb/>In this paper we address the problem of modelling and analyzing hu-<lb/>man motion by focusing on 3D body skeletons. Particularly, our intent is <lb/>to represent skeletal motion in a geometric and e cient way, leading to an <lb/>accurate action-recognition system. Here an action is represented by a dy-<lb/>namical system whose observability matrix is characterized as an element <lb/>of a Grassmann manifold. To formulate our learning algorithm, we pro-<lb/>pose two distinct ideas: (1) In the first one we perform classification using <lb/>a Truncated Wrapped Gaussian model, one for each class in its own tangent <lb/>space. (2) In the second one we propose a novel learning algorithm that <lb/>uses a vector representation formed by concatenating local coordinates in <lb/>tangent spaces associated with di↵erent classes and training a linear SVM. <lb/>We evaluate our approaches on three public 3D action datasets: MSR-action <lb/>3D, UT-kinect and UCF-kinect datasets; these datasets represent di↵erent <lb/>Email addresses: rim.slama@telecom-lille.fr (Rim Slama), <lb/>hazem.wannous@telecom-lille.fr (Hazem Wannous), <lb/>mohamed.daoudi@telecom-lille.fr (Mohamed Daoudi), anuj@stat.fsu.edu (Anuj <lb/>Srivastava) <lb/>Preprint submitted to Pattern Recognition <lb/>August 14, 2014 <lb/>kinds of challenges and together help provide an exhaustive evaluation. The <lb/>results show that our approaches either match or exceed state-of-the-art per-<lb/>formance reaching 91.21% on MSR-action 3D, 97.91% on UCF-kinect, and <lb/>88.5% on UT-kinect. Finally, we evaluate the latency, i.e. the ability to <lb/>recognize an action before its termination, of our approach and demonstrate <lb/>improvements relative to other published approaches. <lb/>Keywords: Human action recognition, Grassmann manifold, observational <lb/>latency, depth images, skeleton, classification. <lb/></front>

			<body>1. Introduction <lb/>Human action and activity recognition is one of the most active research <lb/>topics in the computer vision community due to its many challenging issues. <lb/>The motivation behind the great interest granted to action recognition is <lb/>the large number of possible applications in consumer interactive entertain-<lb/>ment and gaming [1], surveillance systems [2], life-care and home systems <lb/>[3]. An extensive literature around this domain can be found in a number of <lb/>fields including pattern recognition, machine learning, and human-machine <lb/>interaction [4, 5]. <lb/>The main challenges in action recognition systems are the accuracy of <lb/>data acquisition and the dynamic modelling of the movements. The major <lb/>problems, which can alter the way actions are perceived and consequently <lb/>be recognized, are: occlusions, shadows and background extraction, lighting <lb/>condition variations and viewpoint changes. The recent release of consumer <lb/>depth cameras, like Microsoft Kinect, has significantly lighten these di -<lb/>culties that reduce the action recognition performance in 2D video. These <lb/></body>

			<page>2 <lb/></page>

			<body>cameras provide in addition to the RGB image a depth stream allowing to <lb/>discern changes in depth in certain viewpoints. <lb/>More recently, Shotton et al. [6] have proposed a real-time approach for <lb/>estimating 3D positions of body joints using extensive training on synthetic <lb/>and real depth-streams. The accurate estimation obtained by such a low-<lb/>cost acquisition depth sensor has provided new opportunities for human-<lb/>computer-interaction applications, where popular gaming consoles involve <lb/>the player directly in interaction with the computer. While these acquisition <lb/>sensors and their accurate data are within everyone&apos;s reach, the next research <lb/>challenge is activity-driven. <lb/>In this paper we address the problem of modelling and analyzing human <lb/>motion in the 3D human joint space. Particularly, our intent is to represent <lb/>skeletal joint motion in a compact and e cient way that leads to an accurate <lb/>action recognition. Our ultimate goal is to develop an approach that avoids <lb/>an overly complex design of feature extraction and is able to recognize actions <lb/>performed by di↵erent actors in di↵erent contexts. <lb/>Additionally, we study the ability of our approach for reducing latency: <lb/>in other words, to quickly recognize human actions from the smallest number <lb/>of frames possible to permit a reliable recognition of the action occurring. <lb/>Furthermore, we analyze the impact of reducing the number of actions per <lb/>class in the training set on the classifier&apos;s accuracy. <lb/>In our approach, the spatio-temporal aspect of the action is considered <lb/>and each movement is characterized by a structure incorporating the intrinsic <lb/>nature of the data. We believe that 3D human joint motion data captures <lb/>useful knowledge to understand the intrinsic motion structure, and a manifold <lb/></body>

			<page>3 <lb/></page>

			<body>representation of such simple features can provide discriminating structure <lb/>for action recognition. This leads to manifold-based analysis, which has <lb/>been successfully used in many computer vision applications such as visual <lb/>tracking [7] and action recognition in 2D video [8, 9, 10, 11]. <lb/>Our overall approach is sketched in Figure 1, which has the following <lb/>modules: <lb/>Split <lb/>Training <lb/>videos <lb/>Time series <lb/>extrac on <lb/>Temporal <lb/>modelling <lb/>Linear subspace <lb/>representa on <lb/>CT computa on <lb/>LTB representa on <lb/>S <lb/>V <lb/>M <lb/>Evalua on <lb/>LTB representa on <lb/>Dataset <lb/>Test <lb/>videos <lb/>Time series <lb/>extrac on <lb/>Temporal <lb/>modelling <lb/>Linear subspace <lb/>representa on <lb/>Figure 1: Overview of the approach. The illustrated pipeline is composed of two main <lb/>modules: (1) temporal modelling of time series data and manifold representation (2) <lb/>learning approach on the Control Tangent spaces on Grassman manifold, using Local <lb/>Bundle Tangent representation of data. <lb/>First, given training videos recorded from depth camera, motion trajecto-<lb/>ries from the 3D human joint in Euclidean space are extracted as time series. <lb/>Then, each motion represented by its time series is expressed as an autore-<lb/>gressive and moving average model (ARMA) in order to model its dynamic <lb/>process. The subspace spanned by columns of the observability matrix of <lb/>this model represents a point on a Grassmann manifold. <lb/>Second, using the Riemannian geometry of this manifold, we present a so-<lb/>lution for solving the classification problem. We studied statistical modelling <lb/></body>

			<page>4 <lb/></page>

			<body>of inter-and intra-class variations in conjunction with appropriate tangent <lb/>vectors on this manifold. While class samples are presented by a Grass-<lb/>mann point cloud, we propose to learn Control Tangent (CT) spaces which <lb/>represent the mean of each class. <lb/>Third, each observation of the learning process is projected on all CTs <lb/>to form a Local Tangent Bandle (LTB) representation. This step allows <lb/>obtaining a discriminative parameterization incorporating class separation <lb/>properties and providing the input to a linear SVM classifier. <lb/>While given an unknown test video, to recognize its belonging to one of <lb/>N action classes, we apply the first step on the sequence to represent it as <lb/>a point on the Grassmann manifold. Then, this point is presented by its <lb/>LTB as done in learning step. In order to recognize the input action, SVM <lb/>classifier is performed. <lb/>The rest of the paper is organized as follows: In section 1, the state-of-<lb/>the-art is summarized and main contributions of this paper are highlighted. <lb/>In section 2, parametric subspace-based modelling of 3D joint-trajectory is <lb/>discussed. In Section 3, statistical tools developed on a Grassmann manifold <lb/>are presented and a new supervised learning algorithm is introduced. In <lb/>Section 4, the strength of the framework in term of accuracy and latency on <lb/>several datasets are demonstrated. Finally, concluding remarks are presented <lb/>in Section 5. <lb/>2. Related works <lb/>In this section two categories of related works are reviewed from two <lb/>points of view: manifold-based approache and depth data representation. <lb/></body>

			<page>5 <lb/></page>

			<body>We first review some related manifold based approaches for action analysis <lb/>and recognition in 2D video. Then we focus on the most recent methods of <lb/>action recognition from depth cameras. <lb/>2.1. Manifold approaches in 2D videos <lb/>Human action modelling from 2D video is a well studied problem in the <lb/>literature. Recent surveys can be found in the work of Aggarwal et al. [12], <lb/>Weinland et al. [13], and Poppe [4]. Beside classical methods performed <lb/>in Euclidean space, a variety of techniques based on manifold analysis are <lb/>proposed in recent years. <lb/>In the first category of manifold based approaches, each frame of action <lb/>sequence (pose) is represented as an element of a manifold and the whole <lb/>action is represented as a trajectory on this manifold. These approaches give <lb/>solutions in the temporal domain to be invariant to speed and time using <lb/>techniques like Dynamic Time Warping (DTW) to align action trajectories <lb/>on the manifold. Also probabilistic grammatical models like Hidden Markov <lb/>Model (HMM) are used to classify these actions presented as trajectories. <lb/>Indeed, Veeraraghavan et al. [14] propose the use of human silhouettes ex-<lb/>tracted from video images as a representation of the pose. Silhouettes are <lb/>then characterized as points on the shape space manifold and modelled by <lb/>ARMA models in order to compare sequences using a DTW algorithm. In <lb/>another manifold shape space, Abdelkader et al. [15] represent each pose <lb/>silhouette as a point on the shape space of closed curves and each gesture is <lb/>represented as a trajectory on this space. To classify actions, two approaches <lb/>are used: a template-based approach (DTW) and a graphical model approach <lb/>(HMM). Other approaches use skeleton as a representation of each frame, as <lb/></body>

			<page>6 <lb/></page>

			<body>works presented by Gong et al. [16]. They propose a spatio-Temporal Man-<lb/>ifold (STM) model to analyze non-linear multivariate time series with latent <lb/>spatial structure and apply it to recognize actions in the joint-trajectories <lb/>space. Based on STM, they propose a Dynamic Manifold Warping (DMW) <lb/>and a motion similarity metric to compare human action sequences both in <lb/>2D space using a 2D tracker to extract joints from images and in 3D space <lb/>using Motion capture data. Recently, Gong et al. [17] propose a Kernelized <lb/>Temporal Cut (KTC) as an extension of their previous work [16]. They incor-<lb/>porate Hilbert space embedding of distributions to handle the non-parametric <lb/>and high dimensionality issues. <lb/>Some manifold approaches represent the entire action sequence as a point <lb/>on an other special manifold. Indeed, Turaga et al. [18] involve a study of <lb/>the geometric properties of the Grassmann and Stiefel manifolds, and give <lb/>appropriate definitions of Riemannian metrics and geodesics for the purpose <lb/>of video indexing and action recognition. Then, in order to perform the clas-<lb/>sification as a probability density function, a mean and a standard-deviation <lb/>are learnt for each class on class-specific tangent spaces. Turaga et al. [19] <lb/>use the same approach to represent complex actions by a collection of sub-<lb/>sequence. These sub-sequences correspond to a trajectory on a Grassmann <lb/>manifold. Both DTW and HMM are used for action modelling and com-<lb/>parison. Guo et al. [20] use covariance matrices of bags of low-dimensional <lb/>feature vectors to model the video sequence. These feature vectors are ex-<lb/>tracted from segments of silhouette tunnels of moving objects and coarsely <lb/>capture their shapes. <lb/>Without any extraction of human descriptor as silhouette and neither an <lb/></body>

			<page>7 <lb/></page>

			<body>explicit learning, Lui et al. [21] introduce the notion of tangent bundle to <lb/>represent each action sequence on the Grassmann manifold. Videos are ex-<lb/>pressed as a third-order data tensor of raw pixel from action images, which <lb/>are then factorized on the Grassmann manifold. As each point on the mani-<lb/>fold has an associated tangent space, tangent vectors are computed between <lb/>elements on the manifold and obtained distances are used for action clas-<lb/>sification in a nearest neighbour fashion. In the same way, Lui et al. [22] <lb/>factorize raw pixel from images by high-order singular value decomposition <lb/>in order to represent the actions on Stiefel and Grassmann manifolds. How-<lb/>ever, in this work where raw pixels are directly factorized as manifold points, <lb/>there is no dynamic modelling of the sequence. In addition, only distances <lb/>obtained between all tangent vectors are used for action classification and <lb/>there is no training process on data. <lb/>Kernels [23, 24] are also used in order to transform subspaces of a man-<lb/>ifold onto a space where Euclidean metric can be applied. Shirazi et al. <lb/>[23] embed Grassmann manifolds upon a Hilbert space to minimize cluster-<lb/>ing distortions and then apply a locally discriminant analysis using a graph. <lb/>Video action classification is then obtained by a Nearest-Neighbour classi-<lb/>fier applied on Euclidean distances computed on the graph-embedded kernel. <lb/>Similarly, Harandi et al. [24] propose to represent the spatio-temporal as-<lb/>pect of the action by subspaces elements of a Grassmann manifold. Then, <lb/>they embed this manifold into reproducing kernel of Hilbert spaces in order <lb/>to tackle the problem of action classification on such manifolds. Gall et al. <lb/>[25] use multi-view system coupling action recognition on 2D images with <lb/>3D pose estimation, were the action-specific manifolds are acting as a link <lb/></body>

			<page>8 <lb/></page>

			<body>between them. <lb/>All these approaches cited above are based on features extracted from <lb/>2D video sequences as silhouettes or raw pixels from images. However, the <lb/>recent emergence of low-cost depth sensors opens the possibility of revisiting <lb/>the problem of activity modelling and learning using depth data-driven. <lb/>2.2. Depth data-driven approaches <lb/>Maps obtained by depth sensors are able to provide additional body shape <lb/>information to di↵erentiate actions that have similar 2D projections from a <lb/>single view. It has therefore motivated recent research works, to investigate <lb/>action recognition using the 3D information. Recent surveys [26, 27] are re-<lb/>porting works on depth videos. First methods used for activity recognition <lb/>from depth sequences have tendency to extrapolate techniques already de-<lb/>veloped for 2D video sequences. These approaches use points in depth map <lb/>sequences as a gray pixels in images to extract meaningful spatiotemporal <lb/>descriptors. In Wanqing et al. [28], depth maps are projected onto the three <lb/>orthogonal Cartesian planes (X Y , Z X, and Z Y planes) and the <lb/>contours of the projections are sampled for each frame. The sampled points <lb/>are used as bag-of-points to characterize a set of salient postures that corre-<lb/>spond to the nodes of an action graph used to model explicitly the dynamics <lb/>of the actions. Local feature extraction approaches like spatiotemporal inter-<lb/>est points (STIP) are also employed for action recognition on depth videos. <lb/>Bingbing et al.[29] use depth maps to extract STIP and encode Motion His-<lb/>tory Image (MHI) in a framework combining color and depth information. <lb/>Xia et al [30] propose a method to extract STIP a on depth videos (DSTIP). <lb/>Then around these points of interest they build a depth cuboid similarity <lb/></body>

			<page>9 <lb/></page>

			<body>feature as descriptor for each action. In the work proposed by Vieira et al. <lb/>[31], each depth map sequence is represented as a 4D grid by dividing the <lb/>space and time axes into multiple segments in order to extract SpatioTempo-<lb/>ral Occupancy Pattern features (STOP). Also in Wang et al. [32], the action <lb/>sequence is considered as a 4D shape but Random Occupancy Pattern (ROP) <lb/>is used for features extraction. Yang et al.[33] employ Histograms of Oriented <lb/>Gradients features (HOG) computed from Depth Motion Maps (DMM), as <lb/>the representation of an action sequence. These histograms are then used as <lb/>input to SVM classifier. Similarly, Oreifej et al. [34] compute a 4D histogram <lb/>over depth, time, and spatial coordinates capturing the distribution of the <lb/>surface normal orientation. This histogram is created using 4D projectors <lb/>allowing quantification in 4D space. <lb/>The availability of 3D sensors has recently made possible to estimate 3D <lb/>positions of body joints. Especially thanks to the work of Shotton et al. <lb/>[6], where a real-time method is proposed to accurately predict 3D positions <lb/>of body joints. Thanks to this work, skeleton based methods have become <lb/>popular and many approaches in the literature propose to model the dynamic <lb/>of the action using these features. <lb/>Xia et al. [35] compute histograms of the locations of 12 3D joints as a <lb/>compact representation of postures and use them to construct posture visual <lb/>words of actions. The temporal evolutions of those visual words are modeled <lb/>by a discrete HMM. Yang et al. [36] extract three features, as pair-wise dif-<lb/>ferences of joint positions, for each skeleton joint. Then, principal component <lb/>analysis (PCA) is used to reduce redundancy and noise from feature, and it <lb/>is also used to obtain a compact Eigen Joints representation for each frame. <lb/></body>

			<page>10 <lb/></page>

			<body>Finally, a naïve-Bayes nearest-neighbour classifier is used for multi-class ac-<lb/>tion classification. The popular Dynamic Time Warping (DTW) technique <lb/>[37], well-known in speech recognition area, is also used for gesture and action <lb/>recognition using depth data. The classical DTW algorithm was defined to <lb/>match temporal distortions between two data trajectories, by finding an op-<lb/>timal warping path between the two time series. The feature vector of time <lb/>series is directly constructed from human body joint orientation extracted <lb/>from depth camera or 3D Motion Capture sensors. Reyes et al. [38] per-<lb/>form DTW on a feature vector defined by 15 joints on a 3D human skeleton <lb/>obtained using PrimeSense NiTE. Similarly, Sempena et al. [39], by the 3D <lb/>human skeleton model, use quaternions to form a 60-element feature vec-<lb/>tor. The obtained warping path, by classical DTW algorithm, between two <lb/>time series is mainly subjected to some constraints: (1) boundary constraint <lb/>which enforces the first elements of the sequences as well as the last one <lb/>to be aligned to each other (2) monotonicity constraint which requires that <lb/>the points in the warping path are monotonically spaced in time in the two <lb/>sequences. This technique is relatively sensitive to noise as it requires all <lb/>elements of the sequences to be matched to a corresponding elements of the <lb/>other sequence. It also has a drawback related to its computational complex-<lb/>ity incurring in quadratic cost. However, many works have been proposed to <lb/>bypass its drawbacks by means of probabilistic models [40] or incorporating <lb/>manifold learning approach [17, 16]. <lb/>Recent research has carried on more complex challenge of in-line recogni-<lb/>tion systems for di↵erent applications, in which a trade-o↵ between accuracy <lb/>and latency can be highlighted. Ellis et al. [41] study this trade-o↵ and <lb/></body>

			<page>11 <lb/></page>

			<body>employed a Latency Aware Learning (LAL) method, reducing latency when <lb/>recognizing actions. They train a logistic regression-based classifier, on 3D <lb/>joint position sequences captured by kinect camera, to search a single canon-<lb/>ical posture for recognition. Another work is presented by Barnachon et <lb/>al. [42], where a histogram-based formulation is introduced for recognizing <lb/>streams of poses. In this representation, classical histogram is extended to <lb/>integral one to overcome the lack of temporal information in histograms. <lb/>They also prove the possibility of recognizing actions even before they are <lb/>completed using the integral histogram approach. Tests are made on both 3D <lb/>MoCap from TUM kitchen dataset [43] and RGB-D data from MSR-Action <lb/>dataset [28]. <lb/>Some hybrid approaches combining both skeleton data features and depth <lb/>information were recently introduced, trying to combine positive aspects of <lb/>both approaches. Azary et al. [44] propose spatiotemporal descriptors as <lb/>time-invariant action surfaces, combining image features extracted using ra-<lb/>dial distance measures and 3D joint tracking. Wang et al. [45] compute <lb/>local features on patches around joints for human body representation. The <lb/>temporal structure of each joint in the sequence is represented through a tem-<lb/>poral pattern representation called Fourier Temporal Pyramid. In Oreifej et <lb/>al. [34], a spatiotemporal histogram (HON4D) computed over depth, time, <lb/>and spatial coordinates is used to encode the distribution of the surface nor-<lb/>mal orientation. Similarly to Wang et al. [45], HON4D histograms [34] are <lb/>computed around joints to provide the input of an SVM classifier. Althloothi <lb/>et al. [46] represent 3D shape features based on spherical harmonics repre-<lb/>sentation and 3D motion features using kinematic structure from skeleton. <lb/></body>

			<page>12 <lb/></page>

			<body>Both feature are then merged using multi kernel learning method. <lb/>It is important to note that, to date, few works have very recently pro-<lb/>posed to use manifold analysis for 3D action recognition. Devanne et al. [47], <lb/>propose a spatiotemporal motion representation to characterize the action as <lb/>a trajectory which corresponds to a point on Riemannian manifold of open <lb/>curves shape space. These motion trajectories are extracted from 3D joints, <lb/>and the action recognition is performed by K-Nearest-Neighbor method ap-<lb/>plied on geodesic distances obtained on open curve shape space. Azary et al. <lb/>[48] use a Grassmannian representation as an interpretation of depth motion <lb/>image (DMI) computed from depth pixel values. All DMI in the sequence <lb/>are combined to create a motion depth surface representing the action as a <lb/>spatiotemporal descriptor. <lb/>2.3. Contributions and proposed approach <lb/>On the one hand, approaches modelling actions as elements of manifolds <lb/>[49, 50, 9] prove that it is an appropriate way to represent and compare <lb/>videos. On the other hand, very few works deal with this task using depth <lb/>images and it is still possible to improve learning step using these models. <lb/>Besides, linear dynamic systems [51] show more and more promising results <lb/>on the motion modelling since they exhibit the stationary properties in time, <lb/>so they fit for action representation. <lb/>In this paper, we propose the use of geometric structure inherent in the <lb/>Grassmann manifold for action analysis. We perform action recognition by <lb/>introducing a manifold learning algorithm in conjunction with dynamic mod-<lb/>elling process. In particular, after modelling motions as a linear dynamic sys-<lb/>tems using ARMA models, we are interested in a representation of each point <lb/></body>

			<page>13 <lb/></page>

			<body>on the manifold incorporating class separation properties. Our representa-<lb/>tion takes benefit of statistics in the Grassmann manifold and action classes <lb/>representations on tangent spaces. From spatiotemporal point of view, each <lb/>action sequence is represented in our approach as linear dynamical system <lb/>acquiring the time series of 3D joint-trajectory. From geometrical point of <lb/>view, each action sequence is viewed as a point on the Grassmann manifold. <lb/>In terms of machine learning, a discriminative representation is provided for <lb/>each action thanks to a set of appropriate tangent vectors taking benefit <lb/>of manifold proprieties. Finally, the e ciency of the proposed approach is <lb/>demonstrated on three challenging action recognition datasets captured by <lb/>depth cameras. <lb/>3. Spatiotemporal modelling of action <lb/>The human body can be represented as an articulated system composed <lb/>of hierarchical joints that are connected with bones, forming a skeleton. The <lb/>two best-known skeletons provided by the Microsoft Kinect sensor, are those <lb/>obtained by o cial Microsoft SDK, which contains 20 joints, and PrimeSense <lb/>NiTE which contains only 15 joints (see Figure 2). The various joint con-<lb/>figurations throughout the motion sequence produce a time series of skeletal <lb/>poses giving the skeleton movement. In our approach, an action is simply <lb/>described as a collection of time series of 3D positions of the joints in the <lb/>hierarchical configuration. <lb/>3.1. Linear dynamic model <lb/>Let p j <lb/>t denote the 3D position of a joint j at a given frame t i.e., p j = <lb/>[x j , y j , z j ] <lb/>j=1:J , with J is the number of joints. The joint position time-series <lb/></body>

			<page>14 <lb/></page>

			<body>5 <lb/>7 <lb/>10 <lb/>12 <lb/>11 <lb/>13 <lb/>14 <lb/>16 <lb/>18 <lb/>20 <lb/>15 <lb/>19 <lb/>17 <lb/>(a) <lb/>1 <lb/>2 <lb/>3 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>12 <lb/>11 <lb/>13 <lb/>15 <lb/>19 <lb/>14 <lb/>16 <lb/>20 <lb/>(b) <lb/>Figure 2: Skeleton joint locations captured by Microsof Kinect sensor (a) using Microsoft <lb/>SDK (b) using PrimeSense NiTE. Joint signification are: (1) head (2) shoulder center (3) <lb/>spine (4) hip center (5/6) left/right hip (7/8) left/ ight knee (9/10) left/right ankle (11/12) <lb/>left/right foot (13/14) left/right shoulder (15/16) left/right elbow (17/19) left/right wrist <lb/>(19/20) left/right hand. <lb/> of joint j is p j <lb/>t = {x j <lb/>t , y j <lb/>t , z j <lb/>t } t=1:T <lb/>j=1:J , with T the number of frames. A motion <lb/>sequence can then be seen as a matrix collecting all time-series from J joints, <lb/>i.e., M = [p 1 p 2 · · · p T ], p 2 R 3⇤J . <lb/>At this level, we could consider using DTW algorithm [37] to find optimal <lb/>non-linear warping function to match these given time-series as proposed by <lb/>[38, 39, 16]. However, we opted for a system combining a linear dynamic <lb/>modelling with statistical analysis on a manifold, avoiding the boundary and <lb/>the monotonicity constraints presented by classical DTW algorithm. Such a <lb/>system is also less sensitive to noise due to the poor estimation of the joint <lb/>locations, in addition to its reduced computational complexity. <lb/>The dynamic and the continuity of movement imply that the action can <lb/>not be resumed as a simply set of skeletal poses because of the temporal <lb/></body>

			<page>15 <lb/></page>

			<body>information contained in the sequence. Instead of directly using original <lb/>joint position time-series data, we believe that a linear dynamic system, like <lb/>that often used for dynamic texture modelling, is essential before manifold <lb/>analysis. Therefore, to capture both the spatial and the temporal dynamics <lb/>of a motion, linear dynamical system characterized by ARMA models are <lb/>applied to the 3D joint position time-series matrix M . <lb/>The dynamic captured by the ARMA [52, 53] model during an action <lb/>sequence M can be represented as: <lb/>p(t) = Cz(t) + w(t), w(t) ⇠ N (0, R), <lb/>z(t + 1) = Az(t) + v(t), v(t) ⇠ N (0, Q) <lb/>(1) <lb/>where z 2 R d is a hidden state vector, A 2 R d⇥d is the transition matrix <lb/>and C 2 R 3⇤J⇥d is the measurement matrix. w and v are noise components <lb/>modeled as normal with mean equal to zero and covariance matrix R 2 <lb/>R 3⇤J⇥3⇤J and Q 2 R d⇥d respectively. The goal is to learn parameters of the <lb/>model (A, C) given by these equations. Let U <lb/>P V T be the singular value <lb/>decomposition of the matrix M . Then, the estimated model parameters A <lb/>and C are given by:Ĉ = U andÂ = <lb/>P <lb/>V T D 1 V (V T D 2 V ) 1 P 1 , where <lb/>D 1 = [0 0, I <lb/>⌧ 1 0], D 2 = [I <lb/>⌧ 1 0, 0 0] and I <lb/>⌧ 1 is the identity matrix of <lb/>size ⌧ 1. <lb/>Comparing two ARMA models can be done by simply comparing their <lb/>observability matrices. The expected observation sequence generated by an <lb/>ARMA model (A,C) lies in the column space of the extended observability <lb/>matrix given by ✓ T <lb/>1 = [C T , (CA) T , (CA 2 ) T , ...] T . This can be approximated <lb/>by the finite observability matrix ✓ T <lb/>m = [C T , (CA) T , (CA 2 ) T , ..., (CA 2 ) m ] T <lb/></body>

			<page>16 <lb/></page>

			<body>[18]. The subspace spanned by columns of this finite observability matrix <lb/>corresponds to a point on a Grassmann manifold. <lb/>3.2. Grassmann manifold interpretation <lb/>Grassmannian analysis provides a natural way to deal with the problem of <lb/>sequence matching. Especially, this manifold allows to represent a sequence <lb/>by a point on its space and o↵ers tools to compare and to do statistics on <lb/>this manifold. The classification problem of sets of motions represented by a <lb/>collection of features can be transformed to point classification problem on <lb/>the Grassmann manifold. <lb/>In this work we are interested in Grassmann manifolds which definition <lb/>is as below. <lb/>Definition: The Grassmann manifold G <lb/>n⇥d is a quotient space of orthogonal <lb/>group O(n) and is defined as the set of d-dimensional linear subspaces of R n <lb/>. <lb/>Points on the Grassmann manifold are equivalent classes of n ⇥ d orthogonal <lb/>matrices, with d &lt; n, where two matrices are equivalent if their columns span <lb/>the same d-dimensional subspace. <lb/>Let µ denotes an element on G <lb/>n⇥d , the tangent space to this element T <lb/>µ on <lb/>G <lb/>n,d is the tangent plane to the surface of the manifold at µ. It is possible <lb/>to map a point U , of the Grassmann manifold, to a vector in the tangent <lb/>space T <lb/>µ using the logarithm map as defined by Turaga et al. [18]. An other <lb/>important tool in statistics is the exponential map Exp <lb/>µ : T <lb/>µ (G <lb/>n,d ) ! G <lb/>n,d , <lb/>which allows to move on the manifold. <lb/>Two points U 1 and U 2 on G <lb/>n,d are equivalent if one can be mapped into <lb/>the other one by d ⇥ d orthogonal matrix [54]. In other words, U 1 and U 2 are <lb/>equivalent if the d columns of U 1 are rotations of U 2 . The minimum length <lb/></body>

			<page>17 <lb/></page>

			<body>curve connecting these two points is the geodesic between them computed <lb/>as: <lb/>d <lb/>geod (U 1 , U 2 ) =k [✓ , ✓ 2 , · · · , ✓ <lb/>i , · · · , ✓ <lb/>d ] k 2 <lb/>(2) <lb/>where ✓ <lb/>i is the principal angle vector which can be computed through the <lb/>SVD of U T <lb/>1 U 2 . <lb/>4. Learning process on the manifold <lb/>Let {U 1 , · · · U <lb/>N } be N actions represented by points on the Grassmann <lb/>manifold. A common learning approach on manifolds is based on the use <lb/>of only one-tangent space, which usually can be obtained as the tangent <lb/>space to the mean (µ) of the entire data points {U <lb/>i } <lb/>i=1:N without regard <lb/>to class labels. All data points on the manifold are then projected on this <lb/>tangent space to provide the input of a classifier. This assumption provide an <lb/>accommodated solution to use a classical supervised learning on the manifold. <lb/>However, this flattening of the manifold through tangent space is not e cient <lb/>since the tangent space on the global mean can be far from other points. <lb/>A more appropriate way is to consider separate tangent spaces for each <lb/>class at the class-mean. The classification is then performed in these indi-<lb/>vidual tangent spaces as in [18]. <lb/>Some other approaches explore the idea of tangent bundle as in Lui et <lb/>al. [21, 22], in which all tangent planes of all data points on the manifold <lb/>are considered. Tangent vectors are then computed between all points on <lb/>a Grassmann manifold and action classification is performed thanks to ob-<lb/>tained distances. <lb/>We believe that using several tangent spaces, obtained for each class of <lb/></body>

			<page>18 <lb/></page>

			<body>the training data points, is more intuitive. However, the question here is how <lb/>to learn a classifier in this case? <lb/>In the rest of the section, we present a statistical computation of the mean <lb/>in the Grassmann manifold [55]. Then, we propose two learning methods on <lb/>this manifold taking benefit from tangent space class specific and tangent <lb/>bundle [21]: Truncated Wrapped Gaussian (TWG) [56] and Local Tangent <lb/>Bundle SVM (LBTSVM). <lb/>4.1. Mean computation on the Grassmann manifold <lb/>The Karcher mean [55] enables computation of a mean representative for <lb/>a cluster of points on the manifold. This mean should belong to the same <lb/>space as the given points. In our case, we need Karcher mean to compute <lb/>averages on the Grassman manifold and more precisely means of each action <lb/>class which represents the action at best. The algorithm exploits log and exp <lb/>maps in a predictor/corrector loop until convergence to an expected point. <lb/>The computation of a mean can be used to perform an action classification <lb/>solution. This can be done by a s simple comparison of an unknown action, <lb/>represented as a point on the manifold, to all class-means and assigning it to <lb/>the nearest one using the distance presented in Equation 2. <lb/>4.2. Truncated Wrapped Gaussian <lb/>In addition to the mean µ computed by Karcher mean on {U <lb/>i } <lb/>i=1:N , we <lb/>look for the standard deviation value between all actions in each class of <lb/>training data. The must be computed on {V <lb/>i } <lb/>i=1:N where V = exp 1 <lb/>µ (U <lb/>i ) <lb/>are the projections of actions from the Grassmann manifold into the tangent <lb/></body>

			<page>19 <lb/></page>

			<body>space defined on the mean µ. The key idea here is to use the fact that the <lb/>tangent space T <lb/>µ (G <lb/>n,d ) is a vector space. <lb/>Thus, we can estimate the parameters of a probability density function <lb/>such as a Gaussian and then use the exponential map to wrap these param-<lb/>eters back onto the manifold using exponential map operator [18]. However, <lb/>the exponential map is not a bijection for the Grassmann manifold. In fact, a <lb/>line on tangent space, with infinite length, can be warpped around the man-<lb/>ifold many times. Thus, some points of this line are going to have more than <lb/>one image on G <lb/>n,d . It becomes a bijection only if the domain is restricted. <lb/>Therefore, we can restrict the tangent space by a truncation beyond a radius <lb/>of ⇡ in T <lb/>µ (G <lb/>n,d ). By truncation, the normalization constant changes for mul-<lb/>tivariate density in T <lb/>µ (G <lb/>n,d ). In fact, it gets scaled down depending on how <lb/>much of the probability mass is left out of the truncation region. <lb/>Let f (x) denotes the probability density function (pdf) defined on T <lb/>µ (G <lb/>n,d ) <lb/>by : <lb/>f (x) = <lb/>1 <lb/>p <lb/>2⇡ 2 <lb/>e <lb/>x 2 <lb/>2 2 <lb/>(3) <lb/>After truncation, an approximation of f gives: <lb/>424f <lb/>(x) = <lb/>f (x) ⇥ 1 |x|&lt;⇡ <lb/>z <lb/>(4) <lb/>where z is the normalization factor : <lb/>z = <lb/>Z <lb/>⇡ <lb/>⇡ <lb/>f (x) ⇥ 1 |x|&lt;⇡ dx <lb/>(5) <lb/>Using Monte Carlo estimation, it can proved that the estimation of z is given <lb/></body>

			<page>20 <lb/></page>

			<body>by: <lb/>427ẑ <lb/>= <lb/>1 <lb/>N <lb/>N <lb/>X <lb/>i=1 <lb/>|x <lb/>i |&lt;⇡ <lb/>(6) <lb/>In practice, we employ wrapped Gaussians in each class-specific tangent <lb/>space. Separate tangent space is considered for each class at its mean com-<lb/>puted by Karcher mean algorithm. Predicted class of an observation point <lb/>is estimated in these individual tangent spaces. In the training step, the <lb/>mean, standard deviation and normalization factor in each class of actions <lb/>are computed. The predicted label of unknown class action is estimated as <lb/>a function of probability density in class-specific tangent spaces. <lb/>4.3. Local Tangent Bundle <lb/>We intent here to generalize a learning algorithm to work with data points <lb/>which are geometrically lying to a Grassmann manifold. Using multiple class-<lb/>specific tangent spaces is decidedly more relevant than single one. However, <lb/>restrict the learning to only the mean and the standard-deviation in each tan-<lb/>gent space, as in TGW method, is probably insu cient to classify complex <lb/>actions with small inter-class variation. Our idea is to build a supervised clas-<lb/>sifier on the manifold but without limiting the learning process to distances <lb/>computed on the tangent spaces as in [22]. <lb/>We consider such data points to be embedded in higher dimensional rep-<lb/>resentation providing a natural and implicit separation of directions. We <lb/>use the notion of tangent bundle on the manifold to formulate our learning <lb/>algorithm. <lb/>The tangent bundle of a manifold is defined in the literature as the mani-<lb/>fold along with the set of tangent planes taken at all points on it. Each such <lb/></body>

			<page>21 <lb/></page>

			<body>a tangent plane can be equipped with a local Euclidean coordinate system. <lb/>In our approach, we consider several &quot;local&quot; bundles, each one represents the <lb/>tangent planes taken at all points belonging to a class from training dataset <lb/>and expressed as class-specific local bundle. <lb/>We generate Control Tangents (CT) on the manifold, which represent <lb/>all class-specific local bundles of data points. Each CT can be seen as the <lb/>tangent space of the Karcher mean of all points belonging to the same class <lb/>of points from only training data. Karcher mean algorithm can be employed <lb/>here for mean computation. <lb/>We introduce an upswing of the manifold learning so-called Local Tangent <lb/>Bundle (LTB), in which proximities are required between each point on the <lb/>manifold and all CTs. The LTB can be viewed as a parameterization of <lb/>a point on the manifold which incorporates implicitly release properties in <lb/>relation to all class clusters, by mapping this point to all CTs using logarithm <lb/>map. <lb/>The LTBs can provide the input of a classifier, like the linear SVM clas-<lb/>sifier as in our case. In doing so, the learning model of the classifier is con-<lb/>structed using LTBs instead of classifying as function of the local distances <lb/>(mean and standard-deviation) of the point from LTBs as in TWG method. <lb/>We finally notice that training a linear SVM classifier on our represen-<lb/>tation of points provided by LTB is more appropriate than the use of SVM <lb/>with classical Kernel, like rbf, on original points on the manifold. <lb/>In experiments, we compare our learning approach LTBSVM to the clas-<lb/>sical one denoted as One-tangent SVM (TSVM), in which the mean is com-<lb/>puted on the entire training dataset regardless to class labels. Then, all <lb/></body>

			<page>22 <lb/></page>

			<body>points on the manifold are projected on this later to provide the inputs of a <lb/>linear SVM. <lb/>A graphical illustration of the manifold learning by TWG and LTB can <lb/>be shown in Figure 3. <lb/>(a) TWG <lb/>(b) LTB <lb/>Figure 3: Conceptual TWG and LTB learning methods on the Grassmann manifold. <lb/>(a) Actions belonging to the same class, illustrated with same color, are projected to the <lb/>tangent space presented with their mean and then Gaussian function is computed on each <lb/>tangent space, (b) An action is projected on all CTs, and thus construct a new observation <lb/>is represented by its LTB. <lb/>5. Experimental results <lb/>This section summarizes our empirical results and provides an analysis of <lb/>the performances of our proposed approach on several datasets compared to <lb/>the state-of-the-art approaches. <lb/></body>

			<page>23 <lb/></page>

			<body>5.1. Data and features <lb/>We extensively experimented our proposed approach on three public 3D <lb/>action datasets containing various challenges, including MSR-action 3D [28], <lb/>UT-kinect [35] and UCF-kinect [41]. All details about these datasets: di↵er-<lb/>ent types and number of motions, number of subjects executing these motions <lb/>and the experimental protocol used for evaluation are summarized in Table <lb/>1. Examples of actions from these datasets are shown in Figure 4. <lb/>Dataset <lb/>Motions <lb/>Total number of ac-<lb/>tions <lb/>Experimental <lb/>protocol <lb/>MSR-action 3D <lb/>[28] <lb/>RGB + depth (320*240) + 20 <lb/>joints: high arm wave, horizontal <lb/>arm wave, hammer, hand catch, <lb/>forward punch, high throw, draw <lb/>X, draw tick, draw circle, hand <lb/>clap, two hand wave, side-<lb/>boxing, bend, forward kick, side <lb/>kick, jogging, tennis swing, ten-<lb/>nis serve, golf swing, pick up and <lb/>throw <lb/>10 subjects | 20 ac-<lb/>tions | 3 try ) To-<lb/>tal of 520 actions <lb/>50% Learn-<lb/>ing / 50% <lb/>Testing <lb/>UT-kinect [35] <lb/>RGB + depth (320*240) + 20 <lb/>joints: walk, sit down, stand up, <lb/>pick up, carry, throw, push, pull, <lb/>wave and clap hands <lb/>10 subject | 10 ac-<lb/>tions | 2 try ) To-<lb/>tal of 200 actions <lb/>leave-one-<lb/>out <lb/>cross-<lb/>validation <lb/>UCF-kinect [41] 15 joints: balance, climb up, <lb/>climb ladder, duck, hop, vault, <lb/>leap, run, kick, punch, twist left, <lb/>twist right , step forward, step <lb/>back, step left, step right <lb/>16 subjects | 16 ac-<lb/>tions | 5 try ) To-<lb/>tal of 1280 actions <lb/>70% Learn-<lb/>ing / 30% <lb/>Testing <lb/>Table 1: Overview of the datasets used in the experiments. <lb/>In all these datasets, a normalization step is performed in order to make <lb/>the skeletons scale-invariant. For each frame, the hip center joint is first <lb/>placed at the origin of the coordinate system. Then, a skeleton template is <lb/>taken as reference and all the other skeletons are normalized such that their <lb/></body>

			<page>24 <lb/></page>

			<body>(a) MSR-action 3D <lb/>(b) UT-Kinect <lb/>(c) UCF-kinect <lb/>Figure 4: Examples of human actions from datsets used in our experiments: (a) &apos;hand <lb/>clap&apos; from MSR-action 3D , (b) &apos;walk&apos; from UT kinect and (c) &apos;climb ladder&apos; from UCF-<lb/>kinect. <lb/></body>

			<page>25 <lb/></page>

			<body>body part lengths are equal to the corresponding lengths of the reference <lb/>skeleton. Each 3D joint sequence is represented as time series matrix of size <lb/>F ⇥ T with T the number of frames in the sequence and F the number <lb/>of features per frame. The number of features depends on the number of <lb/>estimated joints (60 values for Microsoft SDK skeleton and 45 for PrimeSense <lb/>NiTE skeleton). The dynamic of the activity is then captured using an <lb/>ARMA model. In this process, a dimensionality reduction is needed and best <lb/>subspace dimension &quot;d&quot; have been chosen using a 5-fold cross-validation on <lb/>the training dataset. The parameter giving the best accuracy on the training <lb/>set is kept for all experiments. <lb/>Each action is an element of the Grassmann manifold G <lb/>n⇥d with n = m⇥J <lb/>where J represents the number of joints and d is the subspace dimension <lb/>learnt on the training data. We set m = d, while m represents the truncation <lb/>parameter of observation. <lb/>In our LTBSVM approach, we train a linear SVM on our LTB represen-<lb/>tations of points on the Grassmann manifold. We use a multi-class SVM <lb/>classifier from LibSVM library [57], where the penalty parameter C is tuned <lb/>using a 5-fold cross-validation on the training dataset. <lb/>We evaluate the performance of our approach for action recognition and <lb/>explore the latency on recognition by evaluating the trade-o↵ between accu-<lb/>racy and latency over varying number of actions. To allow a better evalua-<lb/>tion of our approach, we conducted experiments respecting those made in the <lb/>state-of-the-art approaches. We note here that other interesting datasets are <lb/>available, like TUM kitchen dataset [43] which presents challenging short and <lb/>complex actions. In our experiments we concentrated on three other datasets <lb/></body>

			<page>26 <lb/></page>

			<body>from depth sensors (such as kinect), chosen according to the challenges they <lb/>contain, as occlusion, change of view and possibility to compare the latency. <lb/>Details of the experiments are presented in the following sections. <lb/>5.2. MSR-Action 3D dataset <lb/>MSR-Action 3D [28] is a public dataset of 3D action captured by a depth <lb/>camera. It consists of a set of temporally segmented actions where subjects <lb/>are facing the camera and they are advised to use their right arm or leg if <lb/>an action is performed by a single limb. The background is pre-processed <lb/>clearing discontinuities and there is no interaction with objects in performed <lb/>actions. Despite of all of these facilities, it is also a challenging dataset <lb/>since many activities appear very similar due to small inter-class variation. <lb/>Several works have already been conducted on this dataset. Table 2 shows <lb/>the accuracy of our approach compared to the state-of-the-art methods. We <lb/>followed the same experimental setup as in Oreifej et al. [34] and Jiang et <lb/>al. [45], where first five actors are used for training and the rest for testing. <lb/>Our results obtained in this table correspond to four learning methods: <lb/>simple Karcher Mean (KM), one Tangent SVM (TSVM), Truncated Wrapped <lb/>Gaussian (TWG) and Local Tangent Bundle SVM (LTBSVM). Our approach <lb/>using LTBSVM achieves an accuracy of 91.21%, exceeding the best method <lb/>from the state-of-the-art proposed by Oreifej et al. [34]. We note that our <lb/>approach is based on only skeletal joint coordinates as motion features, com-<lb/>pared to other approaches, such as Oreifej et al. [34] and Wang et al. [32] <lb/>which use the depth map or depth information around joint locations. <lb/>To evaluate the e↵ect of the changing of the subspace dimensions, we <lb/>conduct several tests on MSR-Action 3D dataset with di↵erent dimensions <lb/></body>

			<page>27 <lb/></page>

			<body>Method <lb/>accuracy % <lb/>Histograms of 3D Joints [58] <lb/>78.97 <lb/>Eigen Joints [36] <lb/>82.33 <lb/>DMM-HOG [33] <lb/>85.52 <lb/>HON4D [34] <lb/>85.80 <lb/>Random Occupancy patterns [32] <lb/>86.50 <lb/>Actionlet Ensemble [45] <lb/>88.20 <lb/>HOH4D + D disc [34] <lb/>88.89 <lb/>TSVM on one tangent space <lb/>74.32 <lb/>KM <lb/>77.02 <lb/>TWG <lb/>84.45 <lb/>LTBSVM <lb/>91.21 <lb/>Table 2: Recognition accuracy (in %) for the MSR-Action 3D dataset using our approach <lb/>compared to the previous approaches. <lb/>of subspaces. Figure 5 shows the variation of recognition performances with <lb/>the change of the subspace dimension. We remark that until dimension 12, <lb/>the recognition rate generally increase with the increase of the size of the <lb/>subspaces dimensions. This is expected, since a small dimension causes a <lb/>lack of information but also a big dimension of the subspace keeps noise and <lb/>brings confusion between inter-classes. We also compare in this figure, our <lb/>new introduced learning algorithm LBTSVM to TWG and KM. <lb/>To better understand the behavior of our approach according to the action <lb/>type, the confusion matrix is illustrated in Figure 6. For most actions, about <lb/>11 classes of actions, video sequences are 100% correctly classified. <lb/>The classification error occurs if two actions are very similar, such as <lb/>&apos;horizontal arm wave&apos; and &apos;high arm wave&apos;. Besides, one of most problematic <lb/>action to classify is &apos;hammer&apos; action which is frequently confused with &apos;draw <lb/>X&apos;. The particularity of these two actions is that they start in the same <lb/>way but one finishes before the other. If we show only the first part of <lb/>&apos;draw X&apos; action and the whole sequence of &apos;hammer&apos; action we can see that <lb/></body>

			<page>28 <lb/></page>

			<body>0 <lb/>2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>12 <lb/>14 <lb/>16 <lb/>0 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>100 <lb/>Dimension of subspaces <lb/>Accuracy (%) <lb/>GJD (KM) <lb/>GJD (TWG) <lb/>GJD (LTBSVM) <lb/>91.21 <lb/>Figure 5: Recognition rate variation with learning approach and subspace dimension. <lb/>they are very similar. The same for &apos;hand catch&apos; action which is confused <lb/>with &apos;draw circle&apos;. It is important to note that &apos;hammer&apos; action is completely <lb/>misclassified with the approach presented by Oreifej et al. [34] which presents <lb/>the second better recognition rate after our approach. <lb/>While the focus of this paper is mainly on action recognition and latency <lb/>reduction, some applications need to perform training step with a reduced <lb/>amount of data. To study the e↵ect of the amount of training dataset, we <lb/>measured how the accuracy changed as we iteratively reduced the number of <lb/>actions per class in the training dataset. Table 3 shows obtained accuracy <lb/>results with di↵erent size of training dataset. <lb/>These results show that, in contrast to approaches that use HMM which <lb/>require a large number of training data, our approach reveals robustness and <lb/>e ciency. This robustness is due to the fact that the Control Tangents, which <lb/></body>

			<page>29 <lb/></page>

			<body>93.33 33.33 <lb/>60.00 <lb/>6.67 <lb/>7.14 <lb/>40.00 <lb/>73.33 <lb/>92.86 <lb/>7.14 92.86 <lb/>6.67 <lb/>46.67 <lb/>7.14 92.86 6.67 <lb/>6.67 6.67 <lb/>93.33 <lb/>6.67 <lb/>13.33 <lb/>100 <lb/>100 <lb/>100 <lb/>100 <lb/>100 <lb/>13.33 <lb/>100 <lb/>100 <lb/>100 <lb/>6.67 <lb/>100 <lb/>100 <lb/>100 <lb/>86.67 <lb/>High arm wave <lb/>H ig h a r m <lb/>w a v e <lb/>Horizontal arm wave <lb/>H o r iz o n t a l a r m w a v e <lb/>Hammer <lb/>H a m m e r <lb/>Hand catch <lb/>H a n d c a t c h <lb/>Forward punch <lb/>F o r w a r d p u n c h <lb/>High throw <lb/>H ig h t h r o w <lb/>Draw X <lb/>D r a w X <lb/>Draw tick <lb/>D r a w t ic k <lb/>Draw circle <lb/>D r a w c ir c le <lb/>Hand clap <lb/>H a n d c la p <lb/>Two hand wave <lb/>T o w h a n d w a v e <lb/>Side−boxing <lb/>S id e − b o x in g <lb/>Bend <lb/>B e n d <lb/>Forward kick <lb/>F o r w a r d k ic k <lb/>Side kick <lb/>S id e k ic k <lb/>Jogging <lb/>J o g g in g <lb/>Tennis swing <lb/>T e n n is s w in g <lb/>Tennis serve <lb/>T e n n is s e r v e <lb/>Golf swing <lb/>G lo f s w in g <lb/>Pick up &amp; throw <lb/>P ic k u p &amp; t h r o w <lb/>Figure 6: The confusion matrix for the proposed approach on MSR-Action 3D dataset. <lb/>It is recommended to view the Figure on the screen. <lb/>play an important role in learning process, can be computed e ciently using <lb/>small number of action points per class on the manifold. <lb/>5.3. UT-Kinect dataset <lb/>Sequences of this dataset are taken using one depth camera (kinect) in <lb/>indoor settings and their length vary from 5 to 120 frames. We use this <lb/>dataset because it contains several challenges: <lb/>• View change, where actions are taken from di↵erent views: right view, <lb/>frontal view or back view. <lb/></body>

			<page>30 <lb/></page>

			<body>Actions <lb/>per class <lb/>Training dataset % Accuracy % <lb/>5 <lb/>37.17 <lb/>73.36 <lb/>6 <lb/>44.23 <lb/>77.64 <lb/>7 <lb/>51.13 <lb/>83.10 <lb/>8 <lb/>58.36 <lb/>84.79 <lb/>9 <lb/>65.54 <lb/>88.51 <lb/>10 <lb/>72.49 <lb/>89.18 <lb/>11 <lb/>79.95 <lb/>87.83 <lb/>12 <lb/>86.24 <lb/>88.85 <lb/>13 <lb/>91.07 <lb/>90.20 <lb/>14 <lb/>95.91 <lb/>90.54 <lb/>15 <lb/>100 <lb/>91.21 <lb/>Table 3: Recognition accuracy, obtained by our approach using LTBSVM on MSR-Action <lb/>3D dataset, with di↵erent size of training dataset. <lb/>• Significant variation in the realization of the same action: same action <lb/>is done with one hand or two hands can be used to describe the &apos;pick <lb/>up&apos; action. <lb/>• Variation in duration of actions: the mean and standard-deviation are <lb/>respectively for the whole actions 31.1 and 11.61 frames at 30 fps. <lb/>To compare our results with state-of-the-art approaches, we follow experi-<lb/>ment protocol proposed by Xia et al. [35]. The protocol is leave-one-out <lb/>cross-validation. In Table 4, we show comparison between the recognition <lb/>accuracy produced by our approach and the approach presented by Xia et <lb/>al. [35]. <lb/>This table shows the accuracy of the five least-recognized actions in UT-<lb/>kinect dataset and the five best-recognized actions. Our system performs <lb/>the worst when the action represents an interaction with an object: &apos;throw&apos;, <lb/>&apos;push&apos;, &apos;sit down&apos; and &apos;pick up&apos;. However, for the best five recognized actions, <lb/>our approach improves the recognition rate reaching 100%. These actions <lb/></body>

			<page>31 <lb/></page>

			<body>Action <lb/>Acc % Xia et al. [35] Acc % LTBSVM <lb/>Walk <lb/>96.5 <lb/>100 <lb/>Stand up <lb/>91.5 <lb/>100 <lb/>Pick up <lb/>97.5 <lb/>100 <lb/>Carry <lb/>97.5 <lb/>100 <lb/>Wave <lb/>100 <lb/>100 <lb/>Throw <lb/>59 <lb/>60 <lb/>Push <lb/>81.5 <lb/>65 <lb/>Sit down <lb/>91.5 <lb/>80 <lb/>Pull <lb/>92.5 <lb/>Clap hands <lb/>100 <lb/>95 <lb/>Overall <lb/>90.92 <lb/>88.5 <lb/>Table 4: Recognition accuracy (per action) for the UT-kinect dataset obtained by our <lb/>approach using LTBSVM compared to Xia et al. [35]. <lb/>contain variations in view point and realization of the same action. This <lb/>means that our approach is view-invariant and it is robust to change in action <lb/>types thanks to the used learning approach. The overall accuracy of Xia et al. <lb/>[35] is better than our recognition rate. However on MSR Action3D database, <lb/>the recognition rate obtained by this approach gives only 78.97%. This can <lb/>be explained by the fact that this approach requires a large training dataset. <lb/>Especially for complex actions which a↵ect adversely the HMM classification <lb/>in case of small samples of training. <lb/>5.4. UCF-kinect dataset <lb/>In this experiment, our approach is evaluated in terms of latency, i.e. <lb/>the ability for a rapid (low-latency) action recognition. The goal here is to <lb/>automatically determine when a su cient number of frames are observed to <lb/>permit a reliable recognition of the occurring action. For many applications, <lb/>a real challenge is to define a good compromise between &quot;making forced de-<lb/>cision&quot; on partial available frames (but potentially unreliable) and &quot;waiting&quot; <lb/></body>

			<page>32 <lb/></page>

			<body>for the entire video sequence. <lb/>To evaluate the performance of our approach in reducing latency, we con-<lb/>ducted our experiments on UCF-kinect dataset [41]. The skeletal joint loca-<lb/>tions (15 joints) over sequences of this dataset are estimated using Microsoft <lb/>Kinect sensor and the PrimeSense NiTE. The same experimental setup as <lb/>in Ellis et al. [41] is followed. For a total of 1280 action samples contained <lb/>in this dataset, a 70% and 30% split is used for respectively training and <lb/>testing datasets. From the original dataset, new subsequences were created <lb/>by varying a parameter corresponding to the K first frames. Each new sub-<lb/>sequence was created by selecting only the first K frames from the video. For <lb/>videos shorter than K frames, the entire video is used. We compare the re-<lb/>sult obtained by our approach to those obtained by Latency Aware Learning <lb/>(LAL) method proposed by Ellis et al. [41] and other baseline algorithms: <lb/>Bag-of-Words (BoW) and Linear Chain Conditional Random Field (CRF), <lb/>also reported by Ellis et al. [41]. <lb/>As shown in Figure 7, our approach using LTBSVM clearly achieves im-<lb/>proved latency performance compared to all other baseline approaches. Anal-<lb/>ysis of these curves shows that, accuracy rates for all other approaches are <lb/>close when using small number of frames (less than 10) or a large number of <lb/>frames (more than 40). However, the di↵erence increases significantly in the <lb/>middle range. The table joint to Figure 7 shows numerical results at several <lb/>points along the curves in the figure. Thus, given only 20 frames of input, <lb/>our system achieves 74.37%, while BOW, CRF recognition rate below 50% <lb/>and LAL achieves 61.45%. <lb/>It is also interesting to notice the improvement of accuracy of 92.08% <lb/></body>

			<page>33 <lb/></page>

			<body>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>70 <lb/>80 <lb/>90 <lb/>100 <lb/>Maximum frames <lb/>Accuracy (%) <lb/>LTBSVM <lb/>LAL <lb/>TWG <lb/>CRF <lb/>BoW <lb/>Approach/frames <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>40 <lb/>60 <lb/>LTBSVM <lb/>21.87 <lb/>49.37 <lb/>74.37 <lb/>86.87 <lb/>92.08 <lb/>97.29 <lb/>97.91 <lb/>TWG <lb/>18.95 <lb/>40.62 <lb/>61.45 <lb/>74.79 <lb/>82.7 <lb/>92.29 <lb/>95.62 <lb/>LAL [41] <lb/>13.91 <lb/>36.95 <lb/>64.77 <lb/>81.56 <lb/>90.55 <lb/>95.16 <lb/>95.94 <lb/>CRF [41] <lb/>14.53 <lb/>25.46 <lb/>46.88 <lb/>67.27 <lb/>80.70 <lb/>91.41 <lb/>94.06 <lb/>BOW [41] <lb/>10.7 <lb/>21.17 <lb/>43.52 <lb/>67.58 <lb/>83.20 <lb/>91.88 <lb/>94.06 <lb/>Figure 7: Accuracy vs. state-of-the-art approaches over videos truncated at varying maxi-<lb/>mum lengths. Each point of this curve shows the accuracy achieved by the classifier given <lb/>only the number of frames shown in the x-axis. <lb/>obtained by LTBSVM compared to 82.7% obtained by TWG, with maximum <lb/> frame number equal to 30. For a large number of frames, all of the methods <lb/>perform globally a good accuracy with an improvement of the ours (97.91% <lb/>comparing to 95.94% obtained by LAL proposed in Ellis et al. [41]). These <lb/>results show that our approach can recognize actions at the desired accuracy <lb/>with reducing latency. <lb/>Finally, the detail of recognition rates, when using the totality of frames <lb/>in the sequence, are shown through the confusion matrix in Figure 8. Unlike <lb/>what gives LAL, we can observe that the &apos;twist left&apos;, &apos;twist right&apos; actions are <lb/></body>

			<page>34 <lb/></page>

			<body>not confused with each others. All classes of actions are classified with a rate <lb/>more than 93.33% which gives a lot of confidence to our proposed learning <lb/>approach. <lb/>100 <lb/>100 <lb/>93.33 <lb/>3.33 <lb/>100 3.33 <lb/>93.33 <lb/>100 <lb/>6.67 <lb/>3.33 <lb/>96.67 <lb/>100 <lb/>3.33 <lb/>100 <lb/>93.33 <lb/>3.33 100 <lb/>100 <lb/>3.33 <lb/>100 <lb/>100 3.33 <lb/>93.33 <lb/>3.33 <lb/>96.67 <lb/>balance <lb/>b a la n c e <lb/>climbladder <lb/>c li m b la d d e r <lb/>climbup <lb/>c li m b u p <lb/>duck <lb/>d u c k <lb/>hop <lb/>h o p <lb/>kick <lb/>k ic k <lb/>leap <lb/>le a p <lb/>punch <lb/>p u n c h <lb/>run <lb/>r u n <lb/>stepback <lb/>s t e p b a c k <lb/>stepfront <lb/>s t e p f r o n t <lb/>stepleft <lb/>s t e p le f t <lb/>stepright <lb/>s t e p r ig h t <lb/>twistleft <lb/>t w is t le f t <lb/>twistright <lb/>t w is t r ig h t <lb/>vault <lb/>v a u lt <lb/>Figure 8: The confusion matrix for the proposed method on UCF-kinect dataset. Overall <lb/>accuracy achieved 97.91%. It is recommended to view the figure on the screen. <lb/>5.5. Discussion <lb/>Manifold representation and learning. Data representation is one of the most <lb/>important factors in the recognition approach, on which we must take a lot <lb/>of consideration. Our data representation, like many state-of-the-art man-<lb/>ifold techniques [19, 14, 21], consider the geometric space and incorporates <lb/>the intrinsic nature of the data. In our framework, which is 3D joint-based, <lb/>both geometric appearance and dynamic of human body are captured simul-<lb/></body>

			<page>35 <lb/></page>

			<body>taneously. Furthermore, unlike the manifold approaches using silhouettes <lb/>[14, 15, 18], or directly raw pixels [22, 19], our approach use informative <lb/>geometric features, which capture useful knowledge to understand the in-<lb/>trinsic motion structure. Thanks to recent release of depth sensor, these <lb/>features are extracted and tracked along the action sequence, while classical <lb/>pixel-based manifold approaches relying on a good action localization, or on <lb/>tedious feature extraction from 2D videos like silhouettes. <lb/>In terms of learning method, we generalized a learning algorithm to work <lb/>with data points which are geometrically lying to a Grassmann manifold. <lb/>Other approaches are tested in the learning process on the manifold: one <lb/>tangent space (TSVM) and class-specific tangent spaces (TWG). In the first <lb/>one, recognition rate is low. In fact, the computation of the mean of all <lb/>actions from all classes can be inaccurate. Besides, projections on this plane <lb/>can lead to big deformations. A better solution is to operate on each class by <lb/>computing its proper tangent space, as in TWG [56] which improve TSVM <lb/>results (see Table 2). In our approach (LTBSVM), both Control Tangent <lb/>and statistics on the manifold are used. The purpose was to formulate our <lb/>learning algorithm using a discriminative parametrization which incorporate <lb/>class separation properties. The particularity of our learning model is the <lb/>incorporation of proximities relative to all Control Tangent spaces represent-<lb/>ing class clusters, instead of classifying using a function of local distances. <lb/>The results in Table 2 demonstrate that the proposed algorithm is more e -<lb/>cient in action recognition scenario when inter-variation classes is present as <lb/>a challenge. <lb/>Furthermore, the analysis of the impact of reducing the number of actions <lb/></body>

			<page>36 <lb/></page>

			<body>in the training set on the accuracy of the classifier show robustness. Even <lb/>with a small number of actions in the training data recognition rates remain <lb/>good as demonstrated in Table 3. However it is a limitation especially for <lb/>approaches using an HMM learning because they require a large number of <lb/>training dataset. Such as Xia et al. approach [35], which gives only 78.97% <lb/>of recognition rate while performing cross subject test on MSR dataset. <lb/>Latency and Time computation. The evaluations in terms of latency have <lb/>clearly revealed the e ciency of our approach for a rapid recognition. It <lb/>is possible to recognize actions up to 95% using only 40 frames which is <lb/>a good performance comparing to state-of-the-art approaches presented in <lb/>[41]. Thus, our approach can be used for interactive systems. Particularly, <lb/>in entertainment applications to resolve the problem of lag and improve some <lb/>motion-based games. <lb/>Since the proposed approach is based on only skeletal joint coordinates, <lb/>it is simple to calculate and it needs only a small computation time. In fact, <lb/>with our current implementation written in C++, the whole recognition time <lb/>takes 0.26 sec to recognize a sequence of 60 frames. The joint extraction and <lb/>normalisation take 0.0001 sec, the Grassmann and the LTB representation <lb/>take 0.0108 sec and the prediction on SVM takes 0.251 sec. These computa-<lb/>tion time are reported on UCF dataset, with Grassmann manifold dimension <lb/>n = 540 and d = 12. We also reported the computation time needed to <lb/>recognize actions while incorporating latency on UCF dataset. Figure 9 il-<lb/>lustrates inline time recognition with time progression, after only 40 frames <lb/>the recognition is given at the 0.94 sec within 97.29% of correctness rate. <lb/>After 60 frames, in 1.3 sec the algorithm recognize correctly the action with <lb/></body>

			<page>37 <lb/></page>

			<body>97.91%. All the computation time experiments are lunched on a PC having <lb/>Intel Core i5-3350P (3.1 GHz) CPU, 4GB RAM and a PrimeSense camera <lb/>for skeleton extraction giving about 60 skeleton/sec. <lb/>20 <lb/>40 <lb/>60 <lb/>Frame number <lb/>… <lb/>… <lb/>… <lb/>0 <lb/>0.66 <lb/>0.94 <lb/>1.31 <lb/>Inline me recogni on <lb/>(secondes) <lb/>Recogni on me: 0.26 sec <lb/>Figure 9: The computation time to perform 20 frames actions sequences is 0.26 sec by <lb/>using our approach. The computation time is given for each actions frames sequences (e.g. <lb/>0.94 sec for 40 frames). <lb/>Limitations. Our proposed approach is a 3D joint-based framework derives <lb/>a human action recognition from skeletal joint sequences. In the case of <lb/>presence of object interaction in human actions, our approach do not provides <lb/>any relevant information about objects and thus, action with and without <lb/>objects are confused. This limitation can be leveraged in future by the use <lb/>of additional features, which can be extracted from depth or color images <lb/>associated to 3D joint locations. <lb/></body>

			<page>38 <lb/></page>

			<body>6. Conclusion <lb/>In this paper, an e↵ective framework for modelling and recognizing hu-<lb/>man motion in the 3D skeletal joint space is proposed. In this framework, <lb/>sequence features are modeled temporally as subspaces lying to a Grassman-<lb/>nian manifold. A new learning algorithm on this manifold is then introduced. <lb/>It embeds each action, presented as a point on the manifold, in higher dimen-<lb/>sional representation providing natural separation directions. We formulated <lb/>our learning algorithm using the notion of local tangent bundles on class clus-<lb/>ters on the Grassmann manifold. The empirical results and the analysis of <lb/>the performance of our proposed approach show promising results with high <lb/>accuracies superior to 88% on three di↵erent datasets. The evaluation of <lb/>our approach in terms of accuracy/latency reveals an important ability for <lb/>a low-latency action recognition system. Obtained results show that with <lb/>minimum number of frames, it provides the highest recognition rate. <lb/>We would encourage future works to extend our approach to investigate <lb/>more challenging problems like human behaviour recognition. Finally, we <lb/>plan to use additional features from depth or color images associated to 3D <lb/>joint locations to solve the problem of human-object interaction. <lb/></body>

			<listBibl>References <lb/>[1] S. Fothergill, H. Mentis, P. Kohli, S. Nowozin, Instructing people for <lb/>training gestural interactive systems, in: CHI Conference on Human <lb/>Factors in Computing Systems, New York, NY, USA, 2012, pp. 1737-<lb/>1746. <lb/></listBibl>

			<page>39 <lb/></page>

			<listBibl>[2] W. Lao, J. Han, P. de With, Automatic video-based human motion <lb/>analyzer for consumer surveillance system, in: IEEE Transactions on <lb/>Consumer Electronics, Vol. 55, 2009, pp. 591-598. <lb/>[3] A. Jalal, M. Uddin, T. S. Kim, Depth video-based human activity recog-<lb/>nition system using translation and scaling invariant features for life log-<lb/>ging at smart home, in: IEEE Transactions on Consumer Electronics, <lb/>Vol. 58, 2012, pp. 863-871. <lb/>[4] R. Poppe, A survey on vision-based human action recognition, in: Image <lb/>and Vision Computing, Vol. 28, 2010, pp. 976-990. <lb/>[5] P. Turaga, R. Chellappa, V. S. Subrahmanian, O. Udrea, Machine recog-<lb/>nition of human activities: A survey, in: IEEE Transactions on Circuits <lb/>and Systems for Video Technology, Vol. 18, Piscataway, NJ, USA, 2008, <lb/>pp. 1473-1488. <lb/>[6] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, <lb/>A. Kipman, A. Blake, Real-time human pose recognition in parts from <lb/>single depth images, in: Machine Learning for Computer Vision, Vol. <lb/>411, 2013, pp. 119-135. <lb/>[7] C.-S. Lee, A. M. Elgammal, Modeling view and posture manifolds for <lb/>tracking, in: IEEE International Conference on Computer Vision, 2007, <lb/>pp. 1-8. <lb/>[8] Y. M. Lui, Advances in matrix manifolds for computer vision, in: Image <lb/>and Vision Computing, Vol. 30, 2012, pp. 380 -388. <lb/></listBibl>

			<page>40 <lb/></page>

			<listBibl>[9] M. T. Harandi, C. Sanderson, S. Shirazi, B. C. Lovell, Kernel analysis <lb/>on grassmann manifolds for action recognition, in: Pattern Recognition <lb/>Letters, Vol. 34, 2013, pp. 1906 -1915. <lb/>[10] M. Bregonzio, T. Xiang, S. Gong, Fusing appearance and distribution <lb/>information of interest points for action recognition, in: Pattern Recog-<lb/>nition, Vol. 45, 2012, pp. 1220 -1234. <lb/>[11] S. O&apos;Hara, Y. M. Lui, B. A. Draper, Using a product manifold distance <lb/>for unsupervised action recognition, in: Image and Vision Computing, <lb/>Vol. 30, 2012, pp. 206 -216. <lb/>[12] J. Aggarwal, M. Ryoo, Human activity analysis: A review, in: ACM <lb/>Computing Surveys, Vol. 43, 2011, pp. 1-43. <lb/>[13] D. Weinland, R. Ronfard, E. Boyer, A survey of vision-based methods <lb/>for action representation, segmentation and recognition, in: Computer <lb/>Vision and Image Understanding, Vol. 115, 2011, pp. 224-241. <lb/>[14] A. Veeraraghavan, A. Roy-Chowdhury, R. Chellappa, Matching shape <lb/>sequences in video with applications in human movement analysis, <lb/>in: IEEE Transactions on Pattern Analysis and Machine Intelligence, <lb/>Vol. 27, 2005, pp. 1896-1909. <lb/>[15] M. F. Abdelkader, W. Abd-Almageed, A. Srivastava, R. Chellappa, <lb/>Silhouette-based gesture and action recognition via modeling trajecto-<lb/>ries on riemannian shape manifolds, in: Computer Vision and Image <lb/>Understanding, Vol. 115, 2011, pp. 439 -455. <lb/></listBibl>

			<page>41 <lb/></page>

			<listBibl>[16] D. Gong, G. Medioni, Dynamic manifold warping for view invariant <lb/>action recognition, in: IEEE International Conference on Computer Vi-<lb/>sion, Barcelona, Spain, 2011, pp. 571-578. <lb/>[17] D. Gong, G. Medioni, X. Zhao, Structured time series analysis for human <lb/>action segmentation and recognition, in: IEEE Transactions on Pattern <lb/>Analysis and Machine Intelligence, Vol. PP, 2014, pp. 1-1. <lb/>[18] P. Turaga, A. Veeraraghavan, A. Srivastava, R. Chellappa, Statistical <lb/>computations on grassmann and stiefel manifolds for image and video-<lb/>based recognition, in: IEEE Transactions on Pattern Analysis and Ma-<lb/>chine Intelligence, Vol. 33, 2011, pp. 2273-2286. <lb/>[19] P. Turaga, R. Chellappa, Locally time-invariant models of human ac-<lb/>tivities using trajectories on the grassmannian, in: IEEE Conference on <lb/>Computer Vision and Pattern Recognition, 2009, pp. 2435-2441. <lb/>[20] K. Guo, P. Ishwar, J. Konrad, Action recognition in video by sparse <lb/>representation on covariance manifolds of silhouette tunnels, in: Recog-<lb/>nizing Patterns in Signals, Speech, Images and Videos, Vol. 6388, 2010, <lb/>pp. 294-305. <lb/>[21] Y. M. Lui, J. R. Beveridge, Tangent bundle for human action recogni-<lb/>tion, in: IEEE International Conference on Automatic Face and Gesture <lb/>Recognition, 2011, pp. 97-102. <lb/>[22] Y. M. Lui, Tangent bundles on special manifolds for action recognition, <lb/>in: IEEE Transactions on Circuits and Systems for Video Technology, <lb/>Vol. 22, 2012, pp. 930-942. <lb/></listBibl>

			<page>42 <lb/></page>

			<listBibl>[23] S. Shirazi, M. T. Har, C. S, A. Alavi, B. C. Lovell, Clustering on grass-<lb/>mann manifolds via kernel embedding with application to action anal-<lb/>ysis, in: International Conference on Image Processing, 2012, pp. 781-<lb/>784. <lb/>[24] M. T. Harandi, C. Sanderson, S. Shirazi, B. C. Lovell, Kernel analysis <lb/>on grassmann manifolds for action recognition, in: Pattern Recognition <lb/>Letters, Vol. 34, 2013, pp. 1906 -1915. <lb/>[25] J. Gall, A. Yao, L. Van Gool, 2D action recognition serves 3d human <lb/>pose estimation, in: European Conference on Computer Vision, Vol. <lb/>6313, 2010, pp. 425-438. <lb/>[26] L. Chen, H. Wei, J. Ferryman, A survey of human motion analysis using <lb/>depth imagery, in: Pattern Recognition Letters, Vol. 34, 2013, pp. 1995 <lb/>-2006. <lb/>[27] M. Ye, Q. Zhang, L. Wang, J. Zhu, R. Yang, J. Gall, A survey on human <lb/>motion analysis from depth data, in: Time-of-Flight and Depth Imaging. <lb/>Sensors, Algorithms, and Applications, Vol. 8200, 2013, pp. 149-187. <lb/>[28] W. Li, Z. Zhang, Z. Liu, Action recognition based on a bag of 3D <lb/>points, in: IEEE Conference on Computer Vision and Pattern Recogni-<lb/>tion Workshops, 2010, pp. 9-14. <lb/>[29] B. Ni, G. Wang, P. Moulin, Rgbd-hudaact: A color-depth video database <lb/>for human daily activity recognition, in: International Conference on <lb/>Computer Vision Workshops, 2011, pp. 1147-1153. <lb/></listBibl>

			<page>43 <lb/></page>

			<listBibl>[30] L. Xia, J. Aggarwal, Spatio-temporal depth cuboid similarity feature <lb/>for activity recognition using depth camera, in: IEEE Conference on <lb/>Computer Vision and Pattern Recognition, 2013, pp. 2834-2841. <lb/>[31] A. W. Vieira, E. R. Nascimento, G. L. Oliveira, Z. Liu, M. F. Campos, <lb/>STOP: Space-time occupancy patterns for 3D action recognition from <lb/>depth map sequences, in: Progress in Pattern Recognition, Image Anal-<lb/>ysis, Computer Vision, and Applications, Vol. 7441, 2012, pp. 252-259. <lb/>[32] J. Wang, Z. Liu, J. Chorowski, Z. Chen, Y. Wu, Robust 3D action <lb/>recognition with random occupancy patterns, in: European Conference <lb/>on Computer Vision, 2012, pp. 872-885. <lb/>[33] X. Yang, C. Zhang, Y. Tian, Recognizing actions using depth motion <lb/>maps-based histograms of oriented gradients, in: international confer-<lb/>ence on ACM Multimedia, New York, NY, USA, 2012, pp. 1057-1060. <lb/>[34] O. Oreifej, Z. Liu, Hon4d: Histogram of oriented 4d normals for activity <lb/>recognition from depth sequences, in: IEEE Conference on Computer <lb/>Vision and Pattern Recognition, Washington, DC, USA, 2013, pp. 716-<lb/>723. <lb/>[35] L. Xia, C.-C. Chen, J. K. Aggarwal, View invariant human action recog-<lb/>nition using histograms of 3D joints, in: Computer Vision and Pattern <lb/>Recognition Workshops, 2012, pp. 20-27. <lb/>[36] X. Yang, Y. Tian, Eigenjoints based action recognition using naive bayes <lb/>nearest neighbor, in: Computer Vision and Pattern Recognition Work-<lb/>shops, 2012, pp. 14-19. <lb/></listBibl>

			<page>44 <lb/></page>

			<listBibl>[37] T. Giorgino, Computing and visualizing dynamic time warping align-<lb/>ments in R: The dtw package, in: Journal of Statistical Softwar, Vol. 31, <lb/>2009, p. 1-24. <lb/>[38] M. Reyes, G. Dominguez, S. Escalera, Featureweighting in dynamic <lb/>timewarping for gesture recognition in depth data, in: IEEE Interna-<lb/>tional Conference on Computer Vision Workshops, 2011, pp. 1182-1188. <lb/>[39] S. Sempena, N. Maulidevi, P. Aryan, Human action recognition using <lb/>Dynamic Time Warping, in: International Conference on Electrical En-<lb/>gineering and Informatics, 2011, pp. 1-5. <lb/>[40] M. Bautista, A. Hernndez-Vela, V. Ponce, X. Perez-Sala, X. Bar, O. Pu-<lb/>jol, C. Angulo, S. Escalera, Probability-based dynamic time warping for <lb/>gesture recognition on RGB-D data, in: Advances in Depth Image Anal-<lb/>ysis and Applications, Vol. 7854, 2013, pp. 126-135. <lb/>[41] C. Ellis, S. Z. Masood, M. F. Tappen, J. J. Laviola, Jr., R. Sukthankar, <lb/>Exploring the trade-o↵ between accuracy and observational latency in <lb/>action recognition, in: International Journal of Computer Vision, Vol. <lb/>101, 2013, pp. 420-436. <lb/>[42] M. Barnachon, S. Bouakaz, B. Boufama, E. Guillou, Ongoing human ac-<lb/>tion recognition with motion capture, in: Pattern Recognition, Vol. 47, <lb/>2014, pp. 238 -247. <lb/>[43] M. Tenorth, J. Bandouch, M. Beetz, The TUM kitchen data set of every-<lb/>day manipulation activities for motion tracking and action recognition, <lb/></listBibl>

			<page>45 <lb/></page>

			<listBibl>in: International Conference on Computer Vision Workshops, 2009, pp. <lb/>1089-1096. <lb/>[44] S. Azary, A. Savakis, A spatiotemporal descriptor based on radial dis-<lb/>tances and 3D joint tracking for action classification, in: IEEE Interna-<lb/>tional Conference on Image Processing, 2012, pp. 769-772. <lb/>[45] J. Wang, Z. Liu, Y. Wu, J. Yuan, Mining actionlet ensemble for action <lb/>recognition with depth cameras, in: IEEE Conference on Computer <lb/>Vision and Pattern Recognition, 2012, pp. 1290-1297. <lb/>[46] S. Althloothi, M. H. Mahoor, X. Zhang, R. M. Voyles, Human activity <lb/>recognition using multi-features and multiple kernel learning, in: Pat-<lb/>tern Recognition, Vol. 47, 2014, pp. 1800 -1812. <lb/>[47] M. Devanne, H. Wannous, S. Berretti, P. Pala, M. Daoudi, <lb/>A. Del Bimbo, Space-time pose representation for 3D human action <lb/>recognition, in: Workshop on Social Behaviour Analysis ICIAP, Vol. <lb/>8158, 2013, pp. 456-464. <lb/>[48] S. Azary, A. Savakis, Grassmannian sparse representations and motion <lb/>depth surfaces for 3D action recognition, in: IEEE Conference on Com-<lb/>puter Vision and Pattern Recognition Workshops, 2013, pp. 492-499. <lb/>[49] X. Zhang, Y. Yang, L. Jiao, F. Dong, Manifold-constrained coding and <lb/>sparse representation for human action recognition, in: Pattern Recog-<lb/>nition, Vol. 46, 2013, pp. 1819 -1831. <lb/>[50] R. Li, P. Turaga, A. Srivastava, R. Chellappa, Di↵erential geometric <lb/></listBibl>

			<page>46 <lb/></page>

			<listBibl>representations and algorithms for some pattern recognition and com-<lb/>puter vision problems, in: Pattern Recognition Letters, Vol. 43, 2014, <lb/>pp. -16. <lb/>[51] H. Wang, C. Yuan, G. Luo, W. Hu, C. Sun, Action recognition using <lb/>linear dynamic systems, in: Pattern Recognition, Vol. 46, 2013, pp. <lb/>-1718. <lb/>[52] G. Doretto, A. Chiuso, Y. N. Wu, S. Soatto, Dynamic textures, in: <lb/>International Journal of Computer Vision, Vol. 51, 2003, pp. 91-109. <lb/>[53] A. Bissacco, A. Chiuso, Y. Ma, S. Soatto, Recognition of human gaits, <lb/>in: IEEE Conference on Computer Vision and Pattern Recognition, <lb/>Vol. 2, 2001, pp. 52-57. <lb/>[54] A. Edelman, T. A. Arias, S. T. Smith, The geometry of algorithms with <lb/>orthogonality constraints, in: SIAM Journal on Matrix Analysis and <lb/>Applications, Vol. 20, 1998, pp. 303-353. <lb/>[55] A. Srivastava, E. Klassen, S. Joshi, I. Jermyn, Shape analysis of elastic <lb/>curves in euclidean spaces, in: IEEE Transactions on Pattern Analysis <lb/>and Machine Intelligence, Vol. 33, 2011, pp. 1415-1428. <lb/>[56] S. Kurtek, A. Srivastava, E. Klassen, Z. Ding, Statistical modeling of <lb/>curves using shapes and related features, in: Journal of the American <lb/>Statistical Association, Vol. 107, 2012, pp. 1152-1165. <lb/>[57] C.-C. Chang, C.-J. Lin, LIBSVM: A library for support vector machines, <lb/>in: ACM Transactions on Intelligent Systems and Technology, Vol. 2, <lb/>2011, pp. 1-27. <lb/></listBibl>

			<page>47 <lb/></page>

			<listBibl>[58] L. Xia, C.-C. Chen, J. K. Aggarwal, View invariant human action recog-<lb/>nition using histograms of 3D joints., in: IEEE Conference on Computer <lb/>Vision and Pattern Recognition Workshops, 2012, pp. 20-27. <lb/></listBibl>

			<page>48 </page>


	</text>
</tei>
