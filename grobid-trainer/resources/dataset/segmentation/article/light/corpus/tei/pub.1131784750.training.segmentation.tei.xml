<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
		<front>R E S E A R C H A R T I C L E <lb/>Predicting alcohol dependence from multi-site brain structural <lb/>measures <lb/>Sage Hahn 1 <lb/>| Scott Mackey 1 | Janna Cousijn 2 | John J. Foxe 3 | <lb/>Andreas Heinz 4 | Robert Hester 5 | Kent Hutchinson 6 | Falk Kiefer 7 | <lb/>Ozlem Korucuoglu 8 | Tristram Lett 4 | Chiang-Shan R. Li 9 <lb/>| Edythe London 10 | <lb/>Valentina Lorenzetti 11,12,13 | Luijten Maartje 14 | Reza Momenan 15 | <lb/>Catherine Orr 1 | Martin Paulus 16,17 | Lianne Schmaal 18,19 | Rajita Sinha 9 | <lb/>Zsuzsika Sjoerds 20,21 | Dan J. Stein 22 | Elliot Stein 23 | Ruth J. van Holst 24 | <lb/>Dick Veltman 25 | Henrik Walter 4 | Reinout W. Wiers 2 | Murat Yucel 10,26 | <lb/>Paul M. Thompson 27 | Patricia Conrod 28 | Nicholas Allgaier 1 | Hugh Garavan 1 <lb/>1 Department of Psychiatry, University of Vermont College of Medicine, Burlington, Vermont <lb/>2 Department of Psychology, University of Amsterdam, Amsterdam, the Netherlands <lb/>3 Department of Neuroscience &amp; The Ernest J. Del Monte Institute for Neuroscience, University of Rochester School of Medicine and Dentistry, Rochester, New York <lb/>4 Department of Psychiatry and Psychotherapy, Charité-Universitätsmedizin Berlin, Berlin, Germany <lb/>5 Melbourne School of Psychological Sciences, University of Melbourne, Melbourne, Australia <lb/>6 Department of Psychology and Neuroscience, University of Colorado, Boulder, Colorado <lb/>7 Department of Addictive Behaviour and Addiction Medicine, Central Institute of Mental Health, Heidelberg University, Mannheim, Germany <lb/>8 Department of Psychiatry, Washington University School of Medicine, St. Louis, Missouri <lb/>9 Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut <lb/>10 David Geffen School of Medicine, University of California at Los Angeles, Los Angeles, California <lb/>11 Monash Institute of Cognitive and Clinical Neurosciences &amp; School of Psychological Sciences, Monash University, Melbourne, Australia <lb/>12 School of Psychology, Faculty of Health Sciences, Australian Catholic University, Melbourne, Australia <lb/>13 Department of Psychological Sciences, the University of Liverpool, Liverpool, UK <lb/>14 Behavioural Science Institute, Radboud University, Nijmegen, the Netherlands <lb/>15 Clinical NeuroImaging Research Core, Division of Intramural Clinical and Biological Research, National Institute on Alcohol Abuse and Alcoholism, Bethesda, <lb/>Maryland <lb/>16 VA San Diego Healthcare System and Department of Psychiatry, University of California San Diego, La Jolla, California <lb/>17 Laureate Institute for Brain Research, Tulsa, Oklahoma <lb/>18 Orygen, The National Centre of Excellence in Youth Mental Health, Parkville, Australia <lb/>19 Centre for Youth Mental Health, The University of Melbourne, Melbourne, Australia <lb/>20 Department of Neurology, Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany <lb/>21 Institute of Psychology, Cognitive Psychology Unit &amp; Leiden Institute for Brain and Cognition, Leiden University, Leiden, the Netherlands <lb/>22 SA MRC Unit on Risk &amp; Resilience in Mental Disorders, Department of Psychiatry &amp; Neuroscience Institute, University of Cape Town, Cape Town, South Africa <lb/>23 Neuroimaging Research Branch, Intramural Research Program, National Institute on Drug Abuse, Baltimore, Maryland <lb/>24 Department of Psychiatry, Amsterdam UMC, Location AMC, University of Amsterdam, Amsterdam, the Netherlands <lb/>25 Department of Psychiatry, VU University Medical Center, Amsterdam, the Netherlands <lb/>26 Melbourne Neuropsychiatry Centre, Department of Psychiatry, The University of Melbourne and Melbourne Health, Melbourne, Australia <lb/>Received: 12 June 2020 <lb/>Revised: 21 September 2020 <lb/>Accepted: 6 October 2020 <lb/>DOI: 10.1002/hbm.25248 <lb/>This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, <lb/>provided the original work is properly cited. <lb/>© 2020 The Authors. Human Brain Mapping published by Wiley Periodicals LLC. <lb/>Hum Brain Mapp. 2022;43:555-565. <lb/>wileyonlinelibrary.com/journal/hbm <lb/>555 <lb/>27 Imaging Genetics Center, Stevens Institute for Neuroimaging &amp; Informatics, Keck School of Medicine, University of Southern California, California <lb/>28 Department of Psychiatry, Université de Montreal, CHU Ste Justine Hospital, Montreal, Quebec, Canada <lb/>Correspondence <lb/>Sage Hahn, Department of Psychiatry, <lb/>University of Vermont College of Medicine, <lb/>Burlington, VT. <lb/>Email: sahahn@uvm.edu <lb/>Funding information <lb/>Division of Advanced Cyberinfrastructure, <lb/>Grant/Award Number: OAC-1827314; <lb/>National Institute of Mental Health, Grant/ <lb/>Award Number: R01 DA018307; National <lb/>Institute on Alcohol Abuse and Alcoholism, <lb/>Grant/Award Numbers: R01-AA013892, ZIA <lb/>AA000125-04 DICB; National Institute on <lb/>Drug Abuse, Grant/Award Numbers: <lb/>PL30-1DA024859-01, R01-DA014100, <lb/>R01-DA020726, R01DA047119, <lb/>T32DA043593, UL1-RR24925-01; National <lb/>Institutes of Health, Grant/Award Number: <lb/>U54 EB020403; Nederlandse Organisatie voor <lb/>Wetenschappelijk Onderzoek, Grant/Award <lb/>Numbers: VICI grant 453.08.01, VIDI grant <lb/>016.08.322, ZonMW grant 31160003, <lb/>ZonMW grant 31160004, ZonMW grant <lb/>31180002, ZonMW grant 91676084; Philip <lb/>Morris International <lb/>Abstract <lb/>To identify neuroimaging biomarkers of alcohol dependence (AD) from structural <lb/>magnetic resonance imaging, it may be useful to develop classification models that <lb/>are explicitly generalizable to unseen sites and populations. This problem was <lb/>explored in a mega-analysis of previously published datasets from 2,034 AD and <lb/>comparison participants spanning 27 sites curated by the ENIGMA Addiction Work-<lb/>ing Group. Data were grouped into a training set used for internal validation including <lb/>1,652 participants (692 AD, 24 sites), and a test set used for external validation with <lb/>382 participants (146 AD, 3 sites). An exploratory data analysis was first conducted, <lb/>followed by an evolutionary search based feature selection to site generalizable and <lb/>high performing subsets of brain measurements. Exploratory data analysis revealed <lb/>that inclusion of case-and control-only sites led to the inadvertent learning of site-<lb/>effects. Cross validation methods that do not properly account for site can drastically <lb/>overestimate results. Evolutionary-based feature selection leveraging leave-one-site-<lb/>out cross-validation, to combat unintentional learning, identified cortical thickness in <lb/>the left superior frontal gyrus and right lateral orbitofrontal cortex, cortical surface <lb/>area in the right transverse temporal gyrus, and left putamen volume as final features. <lb/>Ridge regression restricted to these features yielded a test-set area under the <lb/>receiver operating characteristic curve of 0.768. These findings evaluate strategies <lb/>for handling multi-site data with varied underlying class distributions and identify <lb/>potential biomarkers for individuals with current AD. <lb/>K E Y W O R D S <lb/>addiction, alcohol dependence, genetic algorithm, machine learning, multi-site, prediction, <lb/>structural MRI <lb/></front>

		<body>1 | INTRODUCTION <lb/>While the evidence associating alcohol dependence (AD) with struc-<lb/>tural brain differences is strong (Ewing, Sakhardande, &amp; <lb/>Blakemore, 2014; Fein et al., 2002; Yang et al., 2016), there is consid-<lb/>erable merit in establishing robust and generalizable neuroimaging-<lb/>based AD biomarkers (Mackey et al., 2019; Yip, Kiluk, &amp; <lb/>Scheinost, 2020). These biomarkers would have objective utility for <lb/>diagnosis and may ultimately help in identifying youth at risk for AD <lb/>and for tracking recovery and treatment efficacy in abstinence, includ-<lb/>ing relapse potential. While these types of clinical applications have <lb/>not yet been realized with neuroimaging, current diagnostic practices <lb/>are far from perfect: The inter-observer reliability of AD, as diagnosed <lb/>by the DSM-IV, was calculated with Cohen&apos;s kappa as 0.66 (0.54, <lb/>0.77, n = 171; Pierucci-Lagha et al., 2007). More immediately, neuro-<lb/>biological markers of AD can give clues to potential etiological <lb/>mechanisms. <lb/>Here, we apply a supervised learning approach, in which a func-<lb/>tion is trained to map brain structural measures to AD diagnosis, and <lb/>then evaluated on unseen data. Prior approaches to developing <lb/>machine learning classifiers for AD include a similar binary machine <lb/>learning classification approach discriminating between AD and sub-<lb/>stance naive controls (Guggenmos et al., 2018). Their analysis made <lb/>use of 296 participants, case and control, and reported a leave-one-<lb/>out cross-validated (CV) balanced accuracy of 74%. A further example <lb/>of recent work includes that by Adeli et al. on distinguishing AD from <lb/>controls (among other phenotypes), on a larger sample of 421, yielding <lb/>a balanced accuracy across 10-fold CV of 70.1% (Adeli, 2019). In both <lb/>examples, volumetric brain measures were extracted and used to train <lb/>and evaluate proposed machine learning (ML) algorithms. The present <lb/>study differs from prior work in both its sample size (n = 2,034) and <lb/>complex case to control distribution across a large number of sites. <lb/>Mackey et al. (2019) developed a support vector machine (SVM) clas-<lb/>sifier that obtained an average area under the receiver characteristic <lb/></body>

			<page>556 <lb/></page>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<front>10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></front>

			<body>operator curve (AUC) of 0.76 on a subset of the training data pres-<lb/>ented within this work. Our present study expands on this previous <lb/>work by exploring new classification methods and additional samples <lb/>with a focus on how to optimize cross-validation consistent with gen-<lb/>eralization to new unseen sites. It is worth noting that the results from <lb/>this previous work are not intended to be directly compared with the <lb/>current work as the previous data were residualized (against pertinent <lb/>factors including site) and only results from a split-half analysis were <lb/>computed (wherein each fold, by design, included participants from <lb/>each site). <lb/>An important consideration for any large multi-site neuroimaging <lb/>study, particularly relevant in developing classifiers, is properly han-<lb/>dling data from multiple sites (Pearlson, 2009). More generally and <lb/>within the broader field of ML, the task of creating &quot;fair&quot; or otherwise <lb/>unbiased classifiers has received a great deal of attention (Noriega-<lb/>Campero, Bakker, Garcia-Bulle, &amp; Pentland, 2019). We argue that in <lb/>order for a classifier or biomarker to have utility, it must explicitly gen-<lb/>eralize to new data, possibly from a different scanner or country. Fur-<lb/>ther, any information gleaned from a classifier that fails to generalize <lb/>to new data is unlikely to represent the actual effect of interest. In our <lb/>study, the imbalance between numbers of cases and controls across <lb/>different sites is a significant challenge, as unrelated, coincidental <lb/>scanner or site effects may easily be exploited by multivariate classi-<lb/>fiers, leading to spurious or misleading results. We show that when <lb/>datasets include sites containing only cases or only controls this can <lb/>be a serious problem. <lb/>A related consideration is how one should interpret the neurobio-<lb/>logical significance of features that contribute most to a successful <lb/>classifier. We propose a multi-objective genetic algorithm (GA) based <lb/>feature selection search to both isolate meaningful brain measures <lb/>and tackle the complexities of handling differing class distributions <lb/>across sites. GA are considered a subset of evolutionary search opti-<lb/>mization algorithms. A sizable body of research has been conducted <lb/>into the usage of multi-objective genetic algorithms, introducing a <lb/>number of effective and general techniques to navigate high dimen-<lb/>sional search spaces, including, various optimization and mutation <lb/>strategies. (Coello, Lamont, &amp; Veldhuizen, 2007; Deb, Pratap, <lb/>Agarwal, &amp; Meyarivan, 2002; Gen &amp; Lin, 2007). Our proposed GA is <lb/>designed to select a set of features both useful for predicting AD and <lb/>generalizable to new sites. By selecting not just predictable, but <lb/>explicitly generalizable and predictable features, we hope to identify <lb/>features with true neurobiological relevance. We draw motivation <lb/>from a large body of existing work that has successfully applied GAs <lb/>to feature selection in varied machine learning contexts (Dong, Li, <lb/>Ding, &amp; Sun, 2018; Yang &amp; Honavar, 1998). <lb/>This study represents a continuation of work by Mackey <lb/>et al. (2019) and the Enhancing Neuro-Imaging Genetics through <lb/>Meta-Analysis (ENIGMA) Addiction Working Group (http:// <lb/>enigmaaddiction.com), in which neuroimaging data were collected and <lb/>pooled across multiple laboratories to investigate dependence on mul-<lb/>tiple substances. Here, we focus on a more exhaustive exploration of <lb/>machine learning to distinguish AD from nondependent individuals, <lb/>spanning 27 different sites. Notably, individual sites are highly imbal-<lb/>anced, with most sites containing only participants with AD or only <lb/>controls (see Figure 1). Due to the unavoidable presence of site-<lb/>related scanner and demographic differences, ML classifiers can <lb/>appear to distinguish participants with AD, but are actually exploiting <lb/>site-related effects. In this context, we evaluate how different cross-<lb/>validation (CV) strategies can either reveal or hide this phenomenon, <lb/>in addition to how choices around which sites to include <lb/>F I G U R E 1 The distribution of both training (Sites 1-24) and testing (25-27) datasets is shown, and further broken down by AD to case ratio <lb/>per site, as well as split by category (e.g., balanced vs. control-only) <lb/></body>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<page>557 <lb/></page>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>(e.g., removing control-only sites) can impact estimates of perfor-<lb/>mance. We then introduce a GA based feature selection strategy and <lb/>show how it can be used to address the unique concerns present in <lb/>complex multi-site data with varied underlying class distributions. <lb/>Finally, we present classification results for a left-out testing set sou-<lb/>rced from three unseen sites, as a measure of classifier <lb/>generalizability. <lb/>2 | METHODS <lb/>2.1 | Dataset <lb/>Informed consent was obtained from all participants and data collec-<lb/>tion was performed in compliance with the Declaration of Helsinki. <lb/>Individuals were excluded if they had a lifetime history of any neuro-<lb/>logical disease, a current DSM-IV axis I diagnosis other than depres-<lb/>sive and anxiety disorders, or any contraindication for MRI. A variety <lb/>of diagnostic instruments were used to assess alcohol dependence <lb/>(Mackey et al., 2019). See Supporting Information for more specific <lb/>details on the included studies. <lb/>Participants&apos; structural T1 weighted brain MRI scans were first <lb/>analyzed using FreeSurfer 5.3 which automatically segments 7 bilateral <lb/>subcortical regions of interest (ROIs) and parcellates the cortex into <lb/>34 bilateral ROIs according to the Desikan parcellation. In total, we <lb/>employ 150 different measurements representing cortical mean thick-<lb/>ness (n = 68) and surface area (n = 68) along with subcortical volume <lb/>(n = 14; Dale, Fischl, &amp; Sereno, 1999; Desikan et al., 2006). <lb/>Quality control of the FreeSurfer output including visual inspec-<lb/>tion of the ROI parcellations was performed at each site according to <lb/>the ENIGMA protocols for multi-site studies, available at http:// <lb/>enigma.ini.usc.edu/protocols/imaging-protocols/. In addition, a ran-<lb/>dom sample from each site was examined at the University of Ver-<lb/>mont to ensure consistent quality control across sites. All individuals <lb/>with missing volumetric or surface ROIs were excluded from analyses. <lb/>In total, 2,034 participants from 27 different sites met all inclusion <lb/>criteria. Further, data were separated into a training set (used in an <lb/>exploratory data analysis and to train a final model) composed of <lb/>1,652 participants (692 with AD), from 24 sites with the remaining <lb/>382 participants (146 with AD) from three sites isolated as a test set <lb/>(used as a final left-aside test of generalizability). The testing set rep-<lb/>resents a collection of new data submitted to the consortium that was <lb/>not included in the most recent working group publication (Mackey <lb/>et al., 2019). Table 1 presents basic demographic information on train-<lb/>ing and test splits. Within the training set, three sites contained only <lb/>cases, 14 sites included only controls, and five sites contained a bal-<lb/>anced mix in the number of cases and controls. Figure 1 shows the <lb/>distribution by site, broken down by AD versus control. A more <lb/>detailed breakdown of the dataset by study and collection site is pro-<lb/>vided within the supplemental materials. <lb/>2.2 | Exploratory data analysis <lb/>In this section, we describe an exploratory analysis investigating dif-<lb/>ferent choices of training data, classification algorithms, and cross-<lb/>validation strategy. This step serves as an initial validation to ensure <lb/>that the classification model of interest is actually learning to distin-<lb/>guish AD versus control versus exploiting an unintended effect. Fur-<lb/>ther, this step allows us to explore how different choices of classifier <lb/>and data affect performance, as the ultimate goal is to build as predic-<lb/>tive a classifier as possible. A final framework for training is deter-<lb/>mined from this exploration, and its implementation and evaluation <lb/>are covered in the following sections. <lb/>We explored classifier performance first on a base training <lb/>dataset (Figure 1, Sites 1-5), composed of the five sites containing a <lb/>balance of both case and control participants. The same experimental <lb/>evaluation was then repeated with two augmented versions of the <lb/>dataset, first adding in participants from case-only sites (Figure 1, Sites <lb/>6-8), and then adding further additional participants from 16 control-<lb/>only sites (Figure 1, Sites 9-24). The top row of Figure 2 outlines <lb/>these three combinations within the context of our experimental <lb/>design. <lb/>Three machine learning algorithms suitable for binary classifica-<lb/>tion (Figure 2, middle row) were implemented within the python <lb/>library Scikit-learn (Pedregosa et al., 2011). Feature normalization was <lb/>conducted in all cases with Scikit-learn&apos;s StandardScaler. Most simply, <lb/>we considered a regularized ridge logistic regression classifier (l2 loss) <lb/>with regularization parameter values chosen through an internal <lb/>CV. Another variant of regularized logistic regression optimized with <lb/>stochastic gradient descent (SGD) was implemented with an elastic <lb/>net loss (l1 and l2). A nested random parameter search was con-<lb/>ducted, across 100 values, determining the choice of loss function and <lb/>regularization values (Zou &amp; Hastie, 2005). Finally, we considered a <lb/>SVM with radial basis function (rbf) kernel, which allowed the classi-<lb/>fier to learn nonlinear interactions between features (Suykens &amp; <lb/>Vandewalle, 1999). Similar to the hyperparameter optimization strat-<lb/>egy employed for the SGD logistic regression, a random search over <lb/>100 SVM parameter combinations, with differing values for the regu-<lb/>larization and kernel coefficients, was employed with nested CV for <lb/>parameter selection. Exact parameter distributions and training details <lb/>are provided within the supplemental materials. <lb/>Proper CV is of the utmost importance in machine learning appli-<lb/>cations. It is well known that-if improperly cross-validated-classifiers <lb/>can overfit onto validation sets, and even with more sophisticated CV <lb/>T A B L E 1 Sex and age, across the full collected dataset from 27 <lb/>sites as split further into training and withheld testing set, and by <lb/>alcohol use disorder (AD) versus control <lb/>Split-AD <lb/>Participants <lb/>Male (%) <lb/>Mean age (SD) <lb/>Train-AD <lb/>692 <lb/>423 (61) <lb/>33.36 ± 9.96 <lb/>Train-Control <lb/>960 <lb/>554 (57) <lb/>28.54 ± 9.56 <lb/>Test-AD <lb/>146 <lb/>79 (54) <lb/>44.72 ± 10.55 <lb/>Test-Control <lb/>236 <lb/>99 (42) <lb/>42.33 ± 12.31 <lb/></body>

			<page>558 <lb/></page>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>techniques can overestimate expected generalization (Santos, Soares, <lb/>Abreu, Araujo, &amp; Santos, 2018). Within this work, we employed a ran-<lb/>dom 50 repeated three-fold CV stratified on AD status, where an indi-<lb/>cation of classifier performance is given by its averaged performance <lb/>when trained on one portion of the data and tested on a left-out por-<lb/>tion, across different random partitions (Burman, 1989). We also <lb/>made use of a leave-site-out (or leave-one-site-out) CV scheme across <lb/>the five sites that include both cases and controls (see Figure 1). Per-<lb/>formance for this leave-site-out CV is computed as the average score <lb/>from each site when that site is left out, this is, the average of 5 scores. <lb/>These options are shown in the bottom row in Figure 2. We com-<lb/>puted metrics according to both schemes for all considered classifiers <lb/>on the training dataset. The area under the Receiver Operating Char-<lb/>acteristic curve (AUC) was used as a base performance metric insensi-<lb/>tive to class imbalance (DeLong, DeLong, &amp; Clarke-Pearson, 1988). <lb/>2.3 | Final analytic pipeline <lb/>Based on the intermediate results from the previous Exploratory Data <lb/>Analysis, we implemented a GA designed to select sets of features <lb/>most useful in training a site generalizable classifier. We operate <lb/>under the assumption in this stage that if a classifier can be restricted <lb/>to only features relevant to distinguishing AD versus control, and <lb/>explicitly not those useful in exploiting site effects, we can create a <lb/>more robust and generalizable classifier. Toward this goal, the GA <lb/>repeatedly trained and evaluated a regularized logistic regression clas-<lb/>sifier on initially random subsets of brain features. The regularized <lb/>logistic classifier is chosen here as it is quick to train, and the initial <lb/>exploratory analysis revealed little difference in performance between <lb/>different classifiers. These feature subsets were then optimized for <lb/>high AUC scores as determined by the leave-site-out CV on the five <lb/>sites that included both cases and controls. Multi-objective optimiza-<lb/>tion was conducted with the aid of a number of successful GA <lb/>strategies, and these include: random tournament selection <lb/>(Eremeev, 2018), feature set mutations, repeated runs with isolated <lb/>populations, a sparsity objective similar in function to &quot;Age-fitness <lb/>Pareto optimization&quot; (Schmidt &amp; Lipson, 2011), among others. An <lb/>introduction to GA and a complete description of our design decisions <lb/>regarding the algorithm are provided in the supplemental material. <lb/>The algorithm was run across six different variants of hyper-<lb/>parameters, as shown in Figure 3. We explored choices related to size <lb/>(number of subsets of features considered in each round) and scope <lb/>(how many optimization rounds the search is run for) in addition to <lb/>objective functions. The results from each of the six search variants <lb/>represent thousands (exact number dependent on hyper-parameters <lb/>of that variant) of subsets of features, each with an associated perfor-<lb/>mance score. We restricted the output from each search to the top <lb/>200-and therefore to high performing-feature subsets. All of these <lb/>final feature subsets (1,200 total) were ultimately pooled together and <lb/>considered in a feature importance meta-analysis. In determining fea-<lb/>ture importance, the following considerations were used: each sub-<lb/>set&apos;s individual performance (higher performance weighted higher) <lb/>and the number of features (subsets with more features were penal-<lb/>ized). A final measure of feature importance was calculated as the <lb/>average feature importance from each of the six search variants com-<lb/>puted separately. Within each search variant, an individual feature&apos;s <lb/>importance was defined as the sum of a feature set&apos;s fitness scores, <lb/>further divided by the number of total features in that set, across all <lb/>F I G U R E 3 A simplified view of the final pipeline, where the full <lb/>training dataset is employed in an evolutionary feature search <lb/>designed to produce optimal subsets of high performing features. <lb/>From this collection of feature subsets a meta analysis for <lb/>determining feature importance is conducted and a subset of &quot;best&quot; <lb/>features are selected. Next, a logistic regression classifier is trained <lb/>and evaluated on the testing dataset, with access to only the &quot;best&quot; <lb/>subset of features <lb/>F I G U R E 2 The different permutations of analyses conducted <lb/>internally on the training set, with differing input dataset options (top <lb/>row), classifiers (middle row), and computed CV scoring metrics <lb/>(bottom row) <lb/></body>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<page>559 <lb/></page>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>of the top 200 sets in which that feature appeared. Importances per <lb/>set were then normalized, such that intuitively a feature present in all <lb/>of the top 200 feature sets would have a value of 1, and if present in <lb/>none, 0. Each feature&apos;s final score represents that feature&apos;s averaged <lb/>score (between 0 and 1) as derived from each separate search. We <lb/>were interested at this stage in identifying a relative ranking of brain <lb/>features, as, intuitively, some features should be more helpful in clas-<lb/>sifying AD, and features that are useful toward classification are can-<lb/>didates to be related to the underlying AD neurobiology. <lb/>As referenced in Figure 3, we selected a &quot;best&quot; subset of features <lb/>with which to train and evaluate a final regularized logistic regression <lb/>classifier on the withheld testing set. We determined the &quot;best&quot; subset <lb/>of features to be those which obtained a final feature importance <lb/>score above a user-defined threshold. Ideally, this threshold would be <lb/>determined analytically on an additional independent validation sam-<lb/>ple, but with limited access to data from case-control balanced sites <lb/>we employed only internal CV. Posthoc analyses were conducted with <lb/>differing thresholds, providing an estimate as to how important this <lb/>step may prove in future analyses. <lb/>3 | RESULTS <lb/>3.1 | Exploratory data analysis <lb/>The complete exploratory training set results are shown in Table 2. <lb/>The base dataset composed of only the five balanced sites across clas-<lb/>sifiers obtained an AUC of 0.723 to 0.724 under three-fold CV versus <lb/>0.623-0.663 under leave-site-out CV. Regularized logistic regression <lb/>on the base dataset with the addition of extra case-only subjects <lb/>yielded an AUC of 0.907 ± 0.022 (standard deviation across folds) <lb/>under random three-fold CV versus 0.560 ± 0.189 under leave-site-<lb/>out and with added controls an AUC of 0.917 ± 0.010 with random <lb/>three-fold CV and 0.636 ± 0.169 with leave-site-out. The choice of <lb/>classifier produced only minor differences in performance (±.02), <lb/>regardless of the CV method. The full dataset (including additional <lb/>control participants and case-only participants) yielded a small boost <lb/>to random three-fold CV scores (.003-.023), and a more noticeable <lb/>gain to leave-site-out CV scores (.053-.091). The CV strategy <lb/>(Random vs. Leave-site-out) produced the largest discrepancy in <lb/>scores when either case-only or both case-only and control-only par-<lb/>ticipants were included (.267-.347) with the former yielding inflated <lb/>results. <lb/>3.2 | Feature importance <lb/>The top 15 features as determined by average weighted feature <lb/>importance, from all six searches (i.e., base training dataset only and <lb/>base plus control-only datasets, by three machine-learning algorithms; <lb/>see Figure 2), are presented in Figure 4. Four features emerged with <lb/>an importance score greater than 0.8 (where an importance score of <lb/>1 represents a feature present in every top feature set and 0 in none), <lb/>followed by a slightly sharper decline and, not shown, a continuing <lb/>decline. Also shown are the cortical surface area and thickness fea-<lb/>tures as projected onto the fsaverage cortical surface space. The left <lb/>putamen (0.816) and left pallidum (0.210) volumes were the only sub-<lb/>cortical features with feature importance scores over 0.05 (not <lb/>shown). <lb/>3.3 | Testing set evaluation <lb/>Further internal nested validation on the training set selected a <lb/>threshold of 0.8 weighted feature importance and above, which corre-<lb/>sponds to the top four features only (Figure 4). The final model, <lb/>trained on only this &quot;best&quot; subset of four features, achieved an AUC <lb/>of 0.768 on the testing set. The ROC curve for this classifier on the <lb/>testing set is shown in Figure 5. We further conducted a number of <lb/>posthoc analyses on the testing dataset. To confirm the predictive <lb/>T A B L E 2 The results for each of the three considered classifiers with just the base dataset, the base dataset with added case-only sites and <lb/>lastly the full dataset with control-only sites (see Figure 1 for information on which sites are balanced vs. control or case-only) across both cross <lb/>validation (CV) strategies, as highlighted in Figure 2 <lb/>Dataset <lb/>Classifier <lb/>Random three-fold CV AUC (± STD) <lb/>Leave-site-out CV (5 sites) AUC (± STD) <lb/>Base <lb/>Logistic regression <lb/>0.723 ± 0.042 <lb/>0.644 ± 0.125 <lb/>Base <lb/>SGD <lb/>0.731 ± 0.034 <lb/>0.663 ± 0.139 <lb/>Base <lb/>SVM <lb/>0.724 ± 0.038 <lb/>0.623 ± 0.096 <lb/>Base ± case-only <lb/>Logistic regression <lb/>0.907 ± 0.022 <lb/>0.560 ± 0.189 <lb/>Base ± case-only <lb/>SGD <lb/>0.896 ± 0.012 <lb/>0.561 ± 0.183 <lb/>Base ± case-only <lb/>SVM <lb/>0.912 ± 0.011 <lb/>0.578 ± 0.111 <lb/>Full (case ± control) <lb/>Logistic regression <lb/>0.917 ± 0.012 <lb/>0.636 ± 0.169 <lb/>Full (case ± control) <lb/>SGD <lb/>0.919 ± 0.009 <lb/>0.652 ± 0.132 <lb/>Full (case ± control) <lb/>SVM <lb/>0.915 ± 0.014 <lb/>0.631 ± 0.139 <lb/>Note: Standard deviation in area under the receiver characteristic operator curve (AUC) across cross-validated folds is provided, as an estimate of confi-<lb/>dence. Random three-fold CV was stratified according to AD status and was repeated 50 times with different random splits. <lb/></body>

			<page>560 <lb/></page>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>F I G U R E 4 (a) The top 15 features (threshold chosen for readability), as ranked by average weighted feature importance (where 0 indicates a <lb/>feature appeared in none of the GA final models, and 1 represents a feature appeared in all) are shown. (b) The cortical thickness and (c) cortical <lb/>average surface area feature importance scores, above an a priori selected threshold of 0.1, are shown as projected onto the fsaverage surface <lb/>space <lb/>F I G U R E 5 The ROC curve for the final logistic regression model on the testing set, as restricted to only the &quot;best&quot; subset of four features <lb/></body>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<page>561 <lb/></page>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>utility of GA feature selection, a regularized logistic regression model <lb/>and SVM model with access to all features were both trained on the <lb/>full training dataset and evaluated on the testing set. The logistic <lb/>regression scored 0.697 AUC and the SVM 0.673 AUC. Similarly, reg-<lb/>ularized logistic regression and SVM models were trained on all fea-<lb/>tures, but without the inclusion of additional control-only sites, and <lb/>scored, respectively 0.647 and 0.609 AUC. The final model was better <lb/>than both the logistic regression model with all features and subjects <lb/>(p = .0098) and without control subjects (p = 3.5 × 10 -5 ). We then <lb/>trained on just the five balanced sites, where logistic regression <lb/>scored 0.717 AUC and the SVM 0.700 AUC. We further investigated <lb/>the choice of user-defined threshold in selecting the number of top <lb/>features by testing the inclusion of the top 2 to 15 features. Some <lb/>notable differences can be seen in performance, for example: .782 <lb/>AUC with top three, .737 AUC with top five and 0.741 AUC with <lb/>top 10. <lb/>4 | DISCUSSION <lb/>We used multi-site neuroimaging data to identify structural brain <lb/>features that classify new participants, from new sites, as having an <lb/>AD with high accuracy. In doing so, we highlighted the importance of <lb/>carefully chosen metrics in accurately estimating ML classifier perfor-<lb/>mance in the context of multi-site imbalanced neuroimaging <lb/>datasets. We further explored a number of techniques, ranging from <lb/>analytical methods to more general approaches, and their merit <lb/>toward improving classifier performance and generalizability. Our <lb/>proposed GA-derived feature importance measure, in addition to <lb/>aiding classification, might help in identifying neurobiologically mean-<lb/>ingful effects. <lb/>A clear discrepancy arose between random repeated CV <lb/>(i.e., participants randomly selected from across sites) and leave-site-<lb/>out CV results (Table 2). We suspect that the random repeated CV <lb/>overestimates performance due to covert site effects. The classifiers <lb/>appeared to memorize some set of attributes, unrelated to AD, within <lb/>the case-and control-only sites and therefore were able to accurately <lb/>predict AD only if participants from a given site were present in both <lb/>training and validation folds. This is exemplified by the change in per-<lb/>formance seen when case-only subjects are added, where repeated <lb/>three-fold CV goes up 0.18 AUC, but leave-site-out CV drops 0.08 <lb/>AUC. Performance on leave-site-out CV, in contrast to random <lb/>repeated CV, better estimates classifier generalizability to new unseen <lb/>sites, especially when the dataset contains data from any case-only or <lb/>control-only sites. This is validated by post hoc analyses in which a <lb/>logistic regression trained on all features obtained a test set AUC <lb/>(0.697) far closer to its training set leave-site-out CV score (0.636 <lb/>± .119) then its random repeated CV score on the full training set <lb/>(0.917). While this observation must be interpreted within the scope <lb/>of our presented imbalanced dataset, these results stress the impor-<lb/>tance of choosing an appropriate performance metric, and further <lb/>highlight the magnitude of error that can be introduced when this <lb/>metric is chosen incorrectly. <lb/>In addition to performing model and parameter selection based <lb/>on a more accurate internal metric, the addition of control-only partic-<lb/>ipants relative to when just case-only subjects are included proved <lb/>beneficial to classifier performance (0.053-0.091 gain in leave-site-<lb/>out AUC). This effect can be noted within our exploratory data analy-<lb/>sis results (Table 2) comparing leave-site-out CV results between the <lb/>base dataset plus case-only subjects and the full dataset. When extra <lb/>control participants are added performance increased up to 0.09 <lb/>AUC. Posthoc analysis revealed a similar performance gain on the <lb/>testing set from adding control participants; logistic regression plus <lb/>0.05 AUC and SVM plus 0.06 AUC. This boost likely reflects a combi-<lb/>nation of two circumstances. In the first, the underlying ML algorithm <lb/>is aided by both more data points to learn from and a more balanced <lb/>case to control distribution, which have both been shown to aid <lb/>binary classification performance (Jordan &amp; Mitchell, 2015). The sec-<lb/>ond reflects a resistance to the learning of site-related effects which, <lb/>as noted above, can lead to the algorithm detrimentally learning <lb/>covert site effects. By including data from more sites and scanners, it <lb/>is possible the unintentional learning of specific site effects (as a proxy <lb/>for AD) is made more difficult. More generally, as neuroimaging data <lb/>banks continue to grow, the potential arises for supplementing ana-<lb/>lyses with seemingly unrelated external datasets. <lb/>Between-site variance, leading ML classifiers to exploit trivial site <lb/>differences, is a pernicious, but not wholly unexpected problem. One <lb/>source of this variance is likely related to scanning differences, that is, <lb/>manufacturer, acquisition parameters, field inhomogeneities and other <lb/>well-known differences (Chen et al., 2014; Jovicich et al., 2006; <lb/>Martinez-Murcia et al., 2017). Data pooled from studies around the <lb/>world also introduce sociodemographic differences between sites. <lb/>Importantly, the clinical measure of interest is also often variable (see <lb/>Supporting Information for more information on the different diag-<lb/>nostic instruments used in our sample) (Yip et al., 2020). Especially <lb/>when pooling studies, it is difficult to fully control or correct for all of <lb/>these sources of variances, as different studies will use a range of dif-<lb/>ferent scanning protocols and collect nonoverlapping phenotypic <lb/>measures. Despite a potential host of differences, pooled data from <lb/>multiple sites may actually provide a regularizing effect. For example, <lb/>if only a single diagnostic instrument were employed a classifier may <lb/>obtain strong within-study results, but be unlikely to generalize well <lb/>to new studies utilizing alternative instruments. <lb/>Our proposed GA-based feature selection, with the inclusion of <lb/>leave-site-out criteria, proved to be useful in improving classifier gen-<lb/>eralizability. This is highlighted by a 0.071 boost to AUC score in a <lb/>model trained on only the top identified four features in contrast to a <lb/>model trained with all the available features. We believe the observed <lb/>performance boost to be a result of only allowing the classifier to <lb/>learn from features previously determined to be useful toward site <lb/>generalizable classification. In this way, the final classifier is able to <lb/>avoid adverse site effects through a lack of exposure to brain mea-<lb/>surements highly linked to specific sites. We note also that our final <lb/>proposed classifier compares favorably to the other posthoc compari-<lb/>sons conducted. Specifically, we see a 0.095 boost relative to an SVM <lb/>trained on the full dataset, a 0.121 and 0.159 boost relative to <lb/></body>

			<page>562 <lb/></page>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

		<body>regularized logistic regression and SVM models trained on the base <lb/>dataset with added cases (or full dataset minus extra controls), and <lb/>lastly a 0.051 and 0.068 gain relative to just the base dataset. Further <lb/>posthoc results indicate even higher performance with just the top <lb/>three features (+.014 vs. selected top four feature model) and a slight <lb/>decrease with the addition of more features. In future work, an addi-<lb/>tional validation set might prove useful in selecting between different <lb/>final models and thresholds, in addition to careful comparisons <lb/>between different feature selection methods. <lb/>A persistent issue in typical interpretation from ML models is the <lb/>issue of shared variance between different features. The features a <lb/>single model selects may very well have suitable surrogate features <lb/>within the remaining dataset. In contrast, our feature importance met-<lb/>ric is derived from thousands of models, providing the chance for <lb/>equivalent features, with shared variance, to achieve similar impor-<lb/>tance scores. A natural distinction nevertheless exists between predic-<lb/>tive features and those that emerge from univariate testing as <lb/>significant. Specifically, the absence of a feature within our final <lb/>model, (i.e., the unimportance of a feature by our metric), does not <lb/>necessarily imply a lack of association between that feature and <lb/>AD. An absence could alternatively indicate that a different feature <lb/>better captures some overlapping predictive utility, which is different <lb/>conceptually from sharing variance in that in this case one feature is <lb/>consistently more useful for prediction. The redundant feature might <lb/>not appear as important despite an association with AD when consid-<lb/>ered in isolation. On the other hand, a feature with a relatively weak <lb/>association could emerge with consistently high feature importance if <lb/>it proves uniquely beneficial to prediction. Above and beyond univari-<lb/>ate significance, if a given feature does have predictive utility, it <lb/>strongly suggests that a real association exists. Our selected top fea-<lb/>tures were both identified as consistently useful features within the <lb/>training set and experimentally confirmed as site generalizable on the <lb/>testing set. <lb/>The top four features as identified by our metric of feature impor-<lb/>tance were the average cortical thickness of the left superior frontal <lb/>gyrus and right lateral orbitofrontal cortex (OFC), the left putamen <lb/>volume and the average surface area of the right transverse temporal <lb/>gyrus. Specifically, cortical thinning, volume and surface area reduc-<lb/>tion across these regions prompt the trained model toward an AD <lb/>prediction. Thinning, within the left superior frontal gyrus and right <lb/>lateral OFC, agrees broadly with the literature which has consistently <lb/>shown frontal lobe regions to be most vulnerable to alcohol conse-<lb/>quences (Oscar-Berman &amp; Marinkovi c, 2007). Prefrontal cortical thin-<lb/>ning and reduced volume in the left putamen seem to further indicate <lb/>specific involvement of the mesocorticolimbic system. This dopami-<lb/>nergic brain pathway has been consistently linked with alcohol depen-<lb/>dence and addiction in general (Ewing et al., 2014; Filbey et al., 2008). <lb/>Likewise, a recent voxel-based meta-analysis showed a significant <lb/>association between lifetime alcohol consumption and decreased vol-<lb/>ume in left putamen and left middle frontal gyrus (Yang et al., 2016). <lb/>Comparing the four selected regions in the present study with <lb/>those determined to be significant by univariate testing on an over-<lb/>lapping dataset from Mackey et al., 2019, we find three regions in <lb/>common (the exception being right transverse temporal gyrus surface, <lb/>as surface area was not considered in that analysis). Further, left supe-<lb/>rior frontal and putamen appeared as two of the top 20 features in <lb/>both folds of an SVM classifier trained and tested on split halves in <lb/>the Mackey paper (right lateral orbital frontal only appeared in one <lb/>fold). Of the existing alcohol classifiers mentioned in the introduction <lb/>by Guggenmos et al. (2018) and Adeli, Zahr, Pfefferbaum, Sullivan, <lb/>and Pohl (2019), only Adeli reported overlapping AD-associated <lb/>regions with our top four: lateral orbitofrontal thickness and superior <lb/>frontal volume. <lb/>In interpreting the performance of a classifier linking brain mea-<lb/>surements to an external phenotype of interest, we also need to con-<lb/>sider how reliably the phenotype can be measured. The exact <lb/>relationship between interobserver variability of a phenotype or spe-<lb/>cific diagnosis and ease of predictability or upper bound of predictabil-<lb/>ity is unknown, but it seems plausible that they would be related. This <lb/>proves pertinent in any case where the presented ground truth labels, <lb/>those used to generate performance metrics, are noisy. We believe <lb/>further study quantifying these relationships will be an important next <lb/>step toward interpreting the results of neuroimaging-based classifica-<lb/>tion, as even if a classifier capable of perfectly predicting between <lb/>case and control existed, it would be bound by our current diagnostic <lb/>standard. A potential route toward establishing a robust understand-<lb/>ing of brain changes associated with AD might involve some combina-<lb/>tion of standard diagnostic practices with objective measures or <lb/>indices gleaned from brain-based classifiers. Relating classifiers <lb/>directly with specific treatment outcomes (potential index for recov-<lb/>ery), or within a longitudinal screening context (potential index for <lb/>risk) represent further exciting and useful applications. <lb/>We have drawn attention to the impact on model generalizability <lb/>of case distribution by site within large multi-site neuroimaging stud-<lb/>ies. In particular, we have shown that CV methods that do not prop-<lb/>erly account for site can drastically overestimate results, and <lb/>presented a leave-site-out CV scheme as a better framework to esti-<lb/>mate model generalization. We further presented an evolutionary-<lb/>based feature selection method aimed at extracting usable informa-<lb/>tion from case-and control-only sites, and showed how this method <lb/>can produce more interpretable, generalizable and high-performing <lb/>AD classifiers. Finally, a measure of feature importance was used to <lb/>determine relevant predictive features, and we discussed their poten-<lb/>tial contribution to our understanding of AD neurobiology. <lb/></body>

		<div type="acknowledgment">ACKNOWLEDGMENTS <lb/>This work was made possible by NIDA grant R01DA047119 awarded <lb/>to Dr. Garavan. Sage Hahn was supported by NIDA grant <lb/>T32DA043593. Data collection was made possible through the fol-<lb/>lowing grants: Dr. Korucuoglu received support for the Neuro-ADAPT <lb/>study from VICI grant 453.08.01 from the Netherlands Organization <lb/>for Scientific Research (NWO), awarded to Reinout W. Wiers. Drs. <lb/>Schmaal and Veltman received funding from Netherlands Organiza-<lb/>tion for Health Research and Development (ZonMW) grant 31160003 <lb/>from NWO. Drs. Sjoerds and Veltman received funding from ZonMW <lb/>grant 31160004 from NWO. Dr. van Holst received funding from <lb/></div>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<page>563 <lb/></page>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<div type="acknowledgment">ZonMW grant 91676084 from NWO. Dr. Luijten and Veltman <lb/>received funding from VIDI grant 016.08.322 from NWO, awarded to <lb/>Ingmar H.A. Dr. Cousijn received funding for the Cannabis Prospec-<lb/>tive study from ZonMW grant 31180002 from NWO. Drs. Garavan <lb/>and Foxe received funds from NIDA grant R01-DA014100. <lb/>Dr. London was supported by NIDA grant R01 DA020726, the <lb/>Thomas P. and Katherine K. Pike Chair in Addiction Studies, the <lb/>Endowment From the Marjorie Greene Family Trust, and UCLA con-<lb/>tract 20063287 with Philip Morris USA. Data collection by <lb/>Dr. Momenan was supported by the Intramural Clinical and Biological <lb/>Research Program funding ZIA AA000125-04 DICB (Clinical NeuroIm-<lb/>aging Research Core to RM) of the National Institute on Alcohol <lb/>Abuse and Alcoholism (NIAAA). Dr. Paulus received funding from <lb/>National Institute of Mental Health grant R01 DA018307. Dr. Stein <lb/>was supported by the Intramural Research Program of NIDA and NIH. <lb/>Dr. Sinha received funds from NIDA (PL30-1DA024859-01), the NIH <lb/>National Center for Research Resources (UL1-RR24925-01), and <lb/>NIAAA (R01-AA013892). Prof. Yücel was supported by the National <lb/>Health and Medical Research Council Fellowship 1117188 and the <lb/>David Winston Turner Endowment Fund. Dr. Thompson was <lb/>supported in part by NIH grant U54 EB020403. Computations were <lb/>performed on the Vermont Advanced Computing Core supported in <lb/>part by NSF award No. OAC-1827314. <lb/></div>

			<div type="availability">DATA AVAILABILITY STATEMENT <lb/>Data Availability Statement: Data was gathered by the Enigma Addic-<lb/>tion Consortium (https://www.enigmaaddictionconsortium.com/). <lb/>Sharing data publically is not possible due to privacy concerns around <lb/>protected information, but interested researchers should contact the <lb/>Enigma Addiction Consortium for more information at enigma.addic-<lb/>tion.consortium@gmail.com. Code Availability Statement: Code used <lb/>in the evolutionary search, along with most postanalysis code is pro-<lb/>vided at https://github.com/sahahn/Alc_Dep. Note: If anyone is inter-<lb/>ested in directly replicating the methods used they should contact <lb/>sahahn@uvm.edu, who if there is interest is willing to provide a more <lb/>user friendly (and less bound to running on a cluster) version of the <lb/>methods used. <lb/></div>

			<front>ORCID <lb/>Sage Hahn <lb/>https://orcid.org/0000-0003-3560-2952 <lb/>Chiang-Shan R. Li <lb/>https://orcid.org/0000-0002-9393-1212 <lb/></front>

			<listBibl>REFERENCES <lb/>Adeli, E., Zahr, N. M., Pfefferbaum, A., Sullivan, E. V., &amp; Pohl, K. M. (2019). <lb/>Novel machine learning identifies brain patterns distinguishing diag-<lb/>nostic membership of human immunodeficiency virus, alcoholism, and <lb/>their comorbidity of individuals. Biological Psychiatry: Cognitive Neuro-<lb/>science and Neuroimaging, 4(6), 589-599. <lb/>Burman, P. (1989). A comparative study of ordinary cross-validation, v-fold <lb/>cross-validation and the repeated learning-testing methods. Bio-<lb/>metrika, 76(3), 503-514. <lb/>Chen, J., Liu, J., Calhoun, V. D., Arias-Vasquez, A., Zwiers, M. P., <lb/>Gupta, C. N., … Turner, J. A. (2014). Exploration of scanning effects in <lb/>multi-site structural MRI studies. Journal of Neuroscience Methods, 230, <lb/>37-50. <lb/>Coello, C. A. C., Lamont, G. B., &amp; Van Veldhuizen, D. A. (2007). Evolutionary <lb/>algorithms for solving multi-objective problems (Vol. 5, pp. 79-104). <lb/>New York, NY: Springer. <lb/>Dale, A. M., Fischl, B., &amp; Sereno, M. I. (1999). Cortical surface-based analysis: <lb/>I. Segmentation and Surface Reconstruction. Neuroimage, 9(2), 179-194. <lb/>Deb, K., Pratap, A., Agarwal, S., &amp; Meyarivan, T. A. M. T. (2002). A fast and <lb/>elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on <lb/>Evolutionary Computation, 6(2), 182-197. <lb/>DeLong, E. R., DeLong, D. M., &amp; Clarke-Pearson, D. L. (1988). Comparing <lb/>the areas under two or more correlated receiver operating characteris-<lb/>tic curves: A nonparametric approach. Biometrics, 44, 837-845. <lb/>Desikan, R. S., Ségonne, F., Fischl, B., Quinn, B. T., Dickerson, B. C., <lb/>Blacker, D., … Albert, M. S. (2006). An automated labeling system for <lb/>subdividing the human cerebral cortex on MRI scans into gyral based <lb/>regions of interest. NeuroImage, 31(3), 968-980. <lb/>Dong, H., Li, T., Ding, R., &amp; Sun, J. (2018). A novel hybrid genetic algorithm <lb/>with granular information for feature selection and optimization. <lb/>Applied Soft Computing, 65, 33-46. <lb/>Eremeev, A. V. (2018). On proportions of fit individuals in population of <lb/>mutation-based evolutionary algorithm with tournament selection. <lb/>Evolutionary Computation, 26(2), 269-297. <lb/>Ewing, S. W. F., Sakhardande, A., &amp; Blakemore, S. J. (2014). The effect of <lb/>alcohol consumption on the adolescent brain: A systematic review of <lb/>MRI and fMRI studies of alcohol-using youth. NeuroImage: Clinical, 5, <lb/>420-437. <lb/>Fein, G., Di Sclafani, V., Cardenas, V. A., Goldmann, H., Tolou-Shams, M., &amp; <lb/>Meyerhoff, D. J. (2002). Cortical gray matter loss in treatment-naive <lb/>alcohol dependent individuals. Alcoholism: Clinical and Experimental <lb/>Research, 26(4), 558-564. <lb/>Filbey, F. M., Claus, E., Audette, A. R., Niculescu, M., Banich, M. T., <lb/>Tanabe, J., … Hutchison, K. E. (2008). Exposure to the taste of alcohol <lb/>elicits activation of the mesocorticolimbic neurocircuitry. Neuropsych <lb/>opharmacology, 33(6), 1391-1401. <lb/>Gen, M., &amp; Lin, L. (2007). Genetic algorithms. In Wiley encyclopedia of com-<lb/>puter science and engineering (pp. 1-15). New York, NY: Wiley-<lb/>Interscience. <lb/>Guggenmos, M., Scheel, M., Sekutowicz, M., Garbusow, M., Sebold, M., <lb/>Sommer, C., … Smolka, M. N. (2018). Decoding diagnosis and lifetime <lb/>consumption in alcohol dependence from grey-matter pattern infor-<lb/>mation. Acta Psychiatrica Scandinavica, 137(3), 252-262. <lb/>Jordan, M. I., &amp; Mitchell, T. M. (2015). Machine learning: Trends, perspec-<lb/>tives, and prospects. Science, 349(6245), 255-260. <lb/>Jovicich, J., Czanner, S., Greve, D., Haley, E., van Der Kouwe, A., Gollub, R., <lb/>… Fischl, B. (2006). Reliability in multi-site structural MRI studies: <lb/>Effects of gradient non-linearity correction on phantom and human <lb/>data. NeuroImage, 30(2), 436-443. <lb/>Mackey, S., Allgaier, N., Chaarani, B., Spechler, P., Orr, C., Bunn, J., … <lb/>Brooks, S. (2019). Mega-analysis of gray matter volume in substance <lb/>dependence: General and substance-specific regional effects. American <lb/>Journal of Psychiatry, 176(2), 119-128. <lb/>Martinez-Murcia, F. J., Lai, M. C., Gorriz, J. M., Ramírez, J., Young, A. M., <lb/>Deoni, S. C., … Murphy, D. G. (2017). On the brain structure heteroge-<lb/>neity of autism: Parsing out acquisition site effects with significance-<lb/>weighted principal component analysis. Human Brain Mapping, 38(3), <lb/>1208-1223. <lb/>Noriega-Campero, A., Bakker, M. A., Garcia-Bulle, B., &amp; Pentland, A. S. <lb/>(2019). Active fairness in algorithmic decision making. In Proceedings <lb/>of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. <lb/>pp. 77-83. <lb/>Oscar-Berman, M., &amp; Marinkovi c, K. (2007). Alcohol: Effects on neuro-<lb/>behavioral functions and the brain. Neuropsychology Review, 17(3), <lb/>239-257. <lb/>Pearlson, G. (2009). Multisite collaborations and large databases in psychi-<lb/>atric neuroimaging: Advantages, problems, and challenges. Schizophre-<lb/>nia Bulletin, 17(3), 239-257. <lb/></listBibl>

			<page>564 <lb/></page>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<listBibl>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., <lb/>Grisel, O., … Vanderplas, J. (2011). Scikit-learn: Machine learning in <lb/>python. The Journal of Machine Learning Research, 12, 2825-2830. <lb/>Pierucci-Lagha, A., Gelernter, J., Chan, G., Arias, A., Cubells, J. F., <lb/>Farrer, L., &amp; Kranzler, H. R. (2007). Reliability of DSM-IV diagnostic <lb/>criteria using the semi-structured assessment for drug dependence <lb/>and alcoholism (SSADDA). Drug and Alcohol Dependence, 91(1), <lb/>85-90. <lb/>Santos, M. S., Soares, J. P., Abreu, P. H., Araujo, H., &amp; Santos, J. (2018). <lb/>Cross-validation for imbalanced datasets: Avoiding overoptimistic and <lb/>overfitting approaches [research frontier]. IEEE Computational Intelli-<lb/>gence Magazine, 13(4), 59-76. <lb/>Schmidt, M., &amp; Lipson, H. (2011). Age-fitness pareto optimization. In <lb/>Genetic programming theory and practice VIII (pp. 129-146). New York, <lb/>NY: Springer. <lb/>Suykens, J. A., &amp; Vandewalle, J. (1999). Least squares support vector <lb/>machine classifiers. Neural Processing Letters, 9(3), 293-300. <lb/>Yang, J., &amp; Honavar, V. (1998). Feature subset selection using a genetic <lb/>algorithm. In Feature extraction, construction and selection <lb/>(pp. 117-136). Boston, MA: Springer. <lb/>Yang, X., Tian, F., Zhang, H., Zeng, J., Chen, T., Wang, S., … Gong, Q. <lb/>(2016). Cortical and subcortical gray matter shrinkage in alcohol-use <lb/>disorders: A voxel-based meta-analysis. Neuroscience &amp; Biobehavioral <lb/>Reviews, 66, 92-103. <lb/>Yip, S. W., Kiluk, B., &amp; Scheinost, D. (2020). Toward addiction prediction: <lb/>An overview of cross-validated predictive modeling findings and con-<lb/>siderations for future neuroimaging research. Biological Psychiatry: <lb/>Cognitive Neuroscience and Neuroimaging, 5(8), 748-758. <lb/>Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the <lb/>elastic net. Journal of the Royal Statistical Society: Series B (Statistical <lb/>Methodology), 67(2), 301-320. <lb/></listBibl>

			<div type="annex">SUPPORTING INFORMATION <lb/>Additional supporting information may be found online in the <lb/>Supporting Information section at the end of this article. <lb/>How to cite this article: Hahn S, Mackey S, Cousijn J, et al. <lb/>Predicting alcohol dependence from multi-site brain structural <lb/>measures. Hum Brain Mapp. 2022;43:555-565. https://doi. <lb/>org/10.1002/hbm.25248 <lb/></div>

			<note place="headnote">HAHN ET AL. <lb/></note>

			<page>565 <lb/></page>

			<note place="footnote">10970193, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/hbm.25248 by Cochrane France, Wiley Online Library on [30/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </note>


	</text>
</tei>
