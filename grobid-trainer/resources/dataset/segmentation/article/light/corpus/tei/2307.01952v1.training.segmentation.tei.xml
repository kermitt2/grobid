<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>SDXL: Improving Latent Diffusion Models for <lb/>High-Resolution Image Synthesis <lb/>Dustin Podell <lb/>Zion English <lb/>Kyle Lacey <lb/>Andreas Blattmann <lb/>Tim Dockhorn <lb/>Jonas Müller <lb/>Joe Penna <lb/>Robin Rombach <lb/>Stability AI, Applied Research <lb/>Code: https://github.com/Stability-AI/generative-models Model weights: https://huggingface.co/stabilityai/ <lb/>Abstract <lb/>We present SDXL, a latent diffusion model for text-to-image synthesis. Compared <lb/>to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet <lb/>backbone: The increase of model parameters is mainly due to more attention <lb/>blocks and a larger cross-attention context as SDXL uses a second text encoder. We <lb/>design multiple novel conditioning schemes and train SDXL on multiple aspect <lb/>ratios. We also introduce a refinement model which is used to improve the visual <lb/>fidelity of samples generated by SDXL using a post-hoc image-to-image technique. <lb/>We demonstrate that SDXL shows drastically improved performance compared to <lb/>previous versions of Stable Diffusion and achieves results competitive with those <lb/>of black-box state-of-the-art image generators. In the spirit of promoting open <lb/>research and fostering transparency in large model training and evaluation, we <lb/>provide access to code and model weights. <lb/>arXiv:2307.01952v1 [cs.CV] 4 Jul 2023 <lb/></front>

			<body>1 Introduction <lb/>The last year has brought enormous leaps in deep generative modeling across various data domains, <lb/>such as natural language [50], audio [17], and visual media [38, 37, 40, 44, 15, 3, 7]. In this report, <lb/>we focus on the latter and unveil SDXL, a drastically improved version of Stable Diffusion. Stable <lb/>Diffusion is a latent text-to-image diffusion model (DM) which serves as the foundation for an <lb/>array of recent advancements in, e.g., 3D classification [43], controllable image editing [54], image <lb/>personalization [10], synthetic data augmentation [48], graphical user interface prototyping [51], etc. <lb/>Remarkably, the scope of applications has been extraordinarily extensive, encompassing fields as <lb/>diverse as music generation [9] and reconstructing images from fMRI brain scans [49]. <lb/>User studies demonstrate that SDXL consistently surpasses all previous versions of Stable Diffusion <lb/>by a significant margin (see Fig. 1). In this report, we present the design choices which lead to this <lb/>boost in performance encompassing i) a 3× larger UNet-backbone compared to previous Stable <lb/>Diffusion models (Sec. 2.1), ii) two simple yet effective additional conditioning techniques (Sec. 2.2) <lb/>which do not require any form of additional supervision, and iii) a separate diffusion-based refinement <lb/>model which applies a noising-denoising process [28] to the latents produced by SDXL to improve <lb/>the visual quality of its samples (Sec. 2.5). <lb/>A major concern in the field of visual media creation is that while black-box-models are often <lb/>recognized as state-of-the-art, the opacity of their architecture prevents faithfully assessing and <lb/>validating their performance. This lack of transparency hampers reproducibility, stifles innovation, <lb/>and prevents the community from building upon these models to further the progress of science and <lb/>art. Moreover, these closed-source strategies make it challenging to assess the biases and limitations <lb/>of these models in an impartial and objective way, which is crucial for their responsible and ethical <lb/>deployment. With SDXL we are releasing an open model that achieves competitive performance with <lb/>black-box image generation models (see Fig. 10 &amp; Fig. 11). <lb/>2 Improving Stable Diffusion <lb/>In this section we present our improvements for the Stable Diffusion architecture. These are modular, <lb/>and can be used individually or together to extend any model. Although the following strategies are <lb/>implemented as extensions to latent diffusion models (LDMs) [38], most of them are also applicable <lb/>to their pixel-space counterparts. <lb/>Figure 1: Left: Comparing user preferences between SDXL and Stable Diffusion 1.5 &amp; 2.1. While SDXL already <lb/>clearly outperforms Stable Diffusion 1.5 &amp; 2.1, adding the additional refinement stage boosts performance. Right: <lb/>Visualization of the two-stage pipeline: We generate initial latents of size 128 × 128 using SDXL. Afterwards, <lb/>we utilize a specialized high-resolution refinement model and apply SDEdit [28] on the latents generated in the <lb/>first step, using the same prompt. SDXL and the refinement model use the same autoencoder. <lb/>2.1 Architecture &amp; Scale <lb/>Starting with the seminal works Ho et al. [14] and Song et al. [47], which demonstrated that DMs <lb/>are powerful generative models for image synthesis, the convolutional UNet [39] architecture has <lb/>been the dominant architecture for diffusion-based image synthesis. However, with the development <lb/></body>

			<page>2 <lb/></page>

			<body>Table 1: Comparison of SDXL and older Stable Diffusion models. <lb/>Model <lb/>SDXL <lb/>SD 1.4/1.5 <lb/>SD 2.0/2.1 <lb/># of UNet params <lb/>2.6B <lb/>860M <lb/>865M <lb/>Transformer blocks <lb/>[0, 2, 10] <lb/>[1, 1, 1, 1] <lb/>[1, 1, 1, 1] <lb/>Channel mult. <lb/>[1, 2, 4] <lb/>[1, 2, 4, 4] <lb/>[1, 2, 4, 4] <lb/>Text encoder <lb/>CLIP ViT-L &amp; OpenCLIP ViT-bigG CLIP ViT-L OpenCLIP ViT-H <lb/>Context dim. <lb/>2048 <lb/>768 <lb/>1024 <lb/>Pooled text emb. <lb/>OpenCLIP ViT-bigG <lb/>N/A <lb/>N/A <lb/>of foundational DMs [40, 37, 38], the underlying architecture has constantly evolved: from adding <lb/>self-attention and improved upscaling layers [5], over cross-attention for text-to-image synthesis [38], <lb/>to pure transformer-based architectures [33]. <lb/>We follow this trend and, following Hoogeboom et al. [16], shift the bulk of the transformer com-<lb/>putation to lower-level features in the UNet. In particular, and in contrast to the original Stable <lb/>Diffusion architecture, we use a heterogeneous distribution of transformer blocks within the UNet: <lb/>For efficiency reasons, we omit the transformer block at the highest feature level, use 2 and 10 <lb/>blocks at the lower levels, and remove the lowest level (8× downsampling) in the UNet altogether <lb/>-see Tab. 1 for a comparison between the architectures of Stable Diffusion 1.x &amp; 2.x and SDXL. <lb/>We opt for a more powerful pre-trained text encoder that we use for text conditioning. Specifically, <lb/>we use OpenCLIP ViT-bigG [19] in combination with CLIP ViT-L [34], where we concatenate the <lb/>penultimate text encoder outputs along the channel-axis [1]. Besides using cross-attention layers to <lb/>condition the model on the text-input, we follow [30] and additionally condition the model on the <lb/>pooled text embedding from the OpenCLIP model. These changes result in a model size of 2.6B <lb/>parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters. <lb/>2.2 Micro-Conditioning <lb/>Figure 2: <lb/>Height-vs-Width distribution of our <lb/>pre-training dataset. Without the proposed size-<lb/>conditioning, 39% of the data would be discarded due <lb/>to edge lengths smaller than 256 pixels as visualized <lb/>by the dashed black lines. Color intensity in each visu-<lb/>alized cell is proportional to the number of samples. <lb/>Conditioning the Model on Image Size A no-<lb/>torious shortcoming of the LDM paradigm [38] is <lb/>the fact that training a model requires a minimal <lb/>image size, due to its two-stage architecture. The <lb/>two main approaches to tackle this problem are ei-<lb/>ther to discard all training images below a certain <lb/>minimal resolution (for example, Stable Diffu-<lb/>sion 1.4/1.5 discarded all images with any size <lb/>below 512 pixels), or, alternatively, upscale im-<lb/>ages that are too small. However, depending on <lb/>the desired image resolution, the former method <lb/>can lead to significant portions of the training <lb/>data being discarded, what will likely lead to a <lb/>loss in performance and hurt generalization. We <lb/>visualize such effects in Fig. 2 for the dataset on <lb/>which SDXL was pretrained. For this particular <lb/>choice of data, discarding all samples below our <lb/>pretraining resolution of 256 2 pixels would lead <lb/>to a significant 39% of discarded data. The second method, on the other hand, usually introduces <lb/>upscaling artifacts which may leak into the final model outputs, causing, for example, blurry samples. <lb/>Instead, we propose to condition the UNet model on the original image resolution, which is trivially <lb/>available during training. In particular, we provide the original (i.e., before any rescaling) height <lb/>and width of the images as an additional conditioning to the model c size = (h original , w original ). <lb/>Each component is independently embedded using a Fourier feature encoding, and these encodings <lb/>are concatenated into a single vector that we feed into the model by adding it to the timestep <lb/>embedding [5]. <lb/>At inference time, a user can then set the desired apparent resolution of the image via this size-<lb/>conditioning. Evidently (see Fig. 3), the model has learned to associate the conditioning c size with <lb/></body>

			<page>3 <lb/></page>

			<body>c size = (64, 64) <lb/>c size = (128, 128), <lb/>c size = (256, 236), <lb/>c size = (512, 512), <lb/>&apos;A robot painted as graffiti on a brick wall. a sidewalk is in front of the wall, and grass is growing out of cracks in the concrete.&apos; <lb/>&apos;Panda mad scientist mixing sparkling chemicals, artstation.&apos; <lb/>Figure 3: The effects of varying the size-conditioning: We show draw 4 samples with the same random seed from <lb/>SDXL and vary the size-conditioning as depicted above each column. The image quality clearly increases when <lb/>conditioning on larger image sizes. Samples from the 512 2 model, see Sec. 2.5. Note: For this visualization, we <lb/>use the 512 × 512 pixel base model (see Sec. 2.5), since the effect of size conditioning is more clearly visible <lb/>before 1024 × 1024 finetuning. Best viewed zoomed in. <lb/>resolution-dependent image features, which can be leveraged to modify the appearance of an output <lb/>corresponding to a given prompt. Note that for the visualization shown in Fig. 3, we visualize samples <lb/>generated by the 512 × 512 model (see Sec. 2.5 for details), since the effects of the size conditioning <lb/>are less clearly visible after the subsequent multi-aspect (ratio) finetuning which we use for our final <lb/>SDXL model. <lb/>Table 2: Conditioning on the original spatial <lb/>size of the training examples improves per-<lb/>formance on class-conditional ImageNet [4] <lb/>on 512 2 resolution. <lb/>model <lb/>FID-5k ↓ IS-5k ↑ <lb/>CIN-512-only <lb/>43.84 <lb/>110.64 <lb/>CIN-nocond <lb/>39.76 <lb/>211.50 <lb/>CIN-size-cond <lb/>36.53 <lb/>215.34 <lb/>We quantitatively assess the effects of this simple but ef-<lb/>fective conditioning technique by training and evaluating <lb/>three LDMs on class conditional ImageNet [4] at spatial <lb/>size 512 2 : For the first model (CIN-512-only) we discard <lb/>all training examples with at least one edge smaller than <lb/>512 pixels what results in a train dataset of only 70k im-<lb/>ages. For CIN-nocond we use all training examples but <lb/>without size conditioning. This additional conditioning is <lb/>only used for CIN-size-cond. After training we generate <lb/>5k samples with 50 DDIM steps [46] and (classifier-free) <lb/>guidance scale of 5 [13] for every model and compute IS [42] and FID [12] (against the full val-<lb/>idation set). For CIN-size-cond we generate samples always conditioned on c size = (512, 512). <lb/>Tab. 2 summarizes the results and verifies that CIN-size-cond improves upon the baseline models in <lb/>both metrics. We attribute the degraded performance of CIN-512-only to bad generalization due to <lb/>overfitting on the small training dataset while the effects of a mode of blurry samples in the sample <lb/>distribution of CIN-nocond result in a reduced FID score. Note that, although we find these classical <lb/>quantitative scores not to be suitable for evaluating the performance of foundational (text-to-image) <lb/>DMs [40, 37, 38] (see App. F), they remain reasonable metrics on ImageNet as the neural backbones <lb/>of FID and IS have been trained on ImageNet itself. <lb/>Conditioning the Model on Cropping Parameters The first two rows of Fig. 4 illustrate a typical <lb/>failure mode of previous SD models: Synthesized objects can be cropped, such as the cut-off head of <lb/>the cat in the left examples for SD 1-5 and SD 2-1. An intuitive explanation for this behavior is the <lb/>use of random cropping during training of the model: As collating a batch in DL frameworks such as <lb/></body>

			<page>4 <lb/></page>

			<body>&apos;A propaganda poster depicting a cat dressed as french <lb/>emperor napoleon holding a piece of cheese.&apos; <lb/>&apos;a close-up of a fire spitting dragon, <lb/>cinematic shot.&apos; <lb/>SD 1-5 <lb/>SD 2-1 <lb/>SDXL <lb/>Figure 4: Comparison of the output of SDXL with previous versions of Stable Diffusion. For each prompt, we <lb/>show 3 random samples of the respective model for 50 steps of the DDIM sampler [46] and cfg-scale 8.0 [13]. <lb/>Additional samples in Fig. 14. <lb/>PyTorch [32] requires tensors of the same size, a typical processing pipeline is to (i) resize an image <lb/>such that the shortest size matches the desired target size, followed by (ii) randomly cropping the <lb/>image along the longer axis. While random cropping is a natural form of data augmentation, it can <lb/>leak into the generated samples, causing the malicious effects shown above. <lb/>To fix this problem, we propose another simple yet effective conditioning method: During dataloading, <lb/>we uniformly sample crop coordinates c top and c left (integers specifying the amount of pixels cropped <lb/>from the top-left corner along the height and width axes, respectively) and feed them into the model <lb/>as conditioning parameters via Fourier feature embeddings, similar to the size conditioning described <lb/>above. The concatenated embedding c crop is then used as an additional conditioning parameter. <lb/>We emphasize that this technique is not limited to LDMs and could be used for any DM. Note that <lb/>crop-and size-conditioning can be readily combined. In such a case, we concatenate the feature <lb/>embedding along the channel dimension, before adding it to the timestep embedding in the UNet. <lb/>Alg. 1 illustrates how we sample c crop and c size during training if such a combination is applied. <lb/>Given that in our experience large scale datasets are, on average, object-centric, we set (c top , c left ) = <lb/>(0, 0) during inference and thereby obtain object-centered samples from the trained model. <lb/>See Fig. 5 for an illustration: By tuning (c top , c left ), we can successfully simulate the amount of <lb/>cropping during inference. This is a form of conditioning-augmentation, and has been used in various <lb/>forms with autoregressive [20] models, and more recently with diffusion models [21]. <lb/>While other methods like data bucketing [31] successfully tackle the same task, we still benefit from <lb/>cropping-induced data augmentation, while making sure that it does not leak into the generation <lb/>process -we actually use it to our advantage to gain more control over the image synthesis process. <lb/>Furthermore, it is easy to implement and can be applied in an online fashion during training, without <lb/>additional data preprocessing. <lb/>2.3 Multi-Aspect Training <lb/>Real-world datasets include images of widely varying sizes and aspect-ratios (c.f. fig. 2) While the <lb/>common output resolutions for text-to-image models are square images of 512 × 512 or 1024 × 1024 <lb/>pixels, we argue that this is a rather unnatural choice, given the widespread distribution and use of <lb/>landscape (e.g., 16:9) or portrait format screens. <lb/>Motivated by this, we finetune our model to handle multiple aspect-ratios simultaneously: We follow <lb/>common practice [31] and partition the data into buckets of different aspect ratios, where we keep the <lb/>pixel count as close to 1024 2 pixels as possibly, varying height and width accordingly in multiples <lb/></body>

			<page>5 <lb/></page>

			<body>Algorithm 1 Conditioning pipeline for size-and crop-conditioning <lb/>Require: Training dataset of images D, target image size for training s = (h tgt , w tgt ) <lb/>Require: Resizing function R, cropping function function C <lb/>Require: Model train step T <lb/>converged ← False <lb/>while not converged do <lb/>x ∼ D <lb/>w original ← width(x) <lb/>h original ← height(x) <lb/>c size ← (h original , w original ) <lb/>x ← R(x, s) <lb/>▷ resize smaller image size to target size s <lb/>if h original ≤ w original then <lb/>c left ∼ U (0, width(x) -s w ) <lb/>▷ sample c left from discrete uniform distribution <lb/>c top = 0 <lb/>else if h original &gt; w original then <lb/>c top ∼ U (0, height(x) -s h ) <lb/>▷ sample c top from discrete uniform distribution <lb/>c left = 0 <lb/>end if <lb/>c crop ← (c top , c left ) <lb/>x ← C(x, s, c crop ) <lb/>▷ crop image to size s with top-left coordinate (c top , c left ) <lb/>converged ← T (x, c size , c crop ) <lb/>▷ train model conditioned on c size and c crop <lb/>end while <lb/>c crop = (0, 0) <lb/>c crop = (0, 256), <lb/>c crop = (256, 0), <lb/>c crop = (512, 512), <lb/>&apos;An astronaut riding a pig, highly realistic dslr photo, cinematic shot.&apos; <lb/>&apos;A capybara made of lego sitting in a realistic, natural field.&apos; <lb/>Figure 5: Varying the crop conditioning as discussed in Sec. 2.2. See Fig. 4 and Fig. 14 for samples from SD 1.5 <lb/>and SD 2.1 which provide no explicit control of this parameter and thus introduce cropping artifacts. Samples <lb/>from the 512 2 model, see Sec. 2.5. <lb/></body>

			<page>6 <lb/></page>

			<body>of 64. A full list of all aspect ratios used for training is provided in App. I. During optimization, <lb/>a training batch is composed of images from the same bucket, and we alternate between bucket <lb/>sizes for each training step. Additionally, the model receives the bucket size (or, target size) as a <lb/>conditioning, represented as a tuple of integers c ar = (h tgt , w tgt ) which are embedded into a Fourier <lb/>space in analogy to the size-and crop-conditionings described above. <lb/>In practice, we apply multi-aspect training as a finetuning stage after pretraining the model at a <lb/>fixed aspect-ratio and resolution and combine it with the conditioning techniques introduced in <lb/>Sec. 2.2 via concatenation along the channel axis. Fig. 16 in App. J provides python-code for this <lb/>operation. Note that crop-conditioning and multi-aspect training are complementary operations, and <lb/>crop-conditioning then only works within the bucket boundaries (usually 64 pixels). For ease of <lb/>implementation, however, we opt to keep this control parameter for multi-aspect models. <lb/>2.4 Improved Autoencoder <lb/>Table 3: Autoencoder reconstruction performance on <lb/>the COCO2017 [26] validation split, images of size <lb/>256 × 256 pixels. Note: Stable Diffusion 2.x uses an <lb/>improved version of Stable Diffusion 1.x&apos;s autoencoder, <lb/>where the decoder was finetuned with a reduced weight <lb/>on the perceptual loss [55], and used more compute. <lb/>Note that our new autoencoder is trained from scratch. <lb/>model <lb/>PNSR ↑ SSIM ↑ LPIPS ↓ rFID ↓ <lb/>SDXL-VAE <lb/>24.7 <lb/>0.73 <lb/>0.88 <lb/>4.4 <lb/>SD-VAE 1.x <lb/>23.4 <lb/>0.69 <lb/>0.96 <lb/>5.0 <lb/>SD-VAE 2.x <lb/>24.5 <lb/>0.71 <lb/>0.92 <lb/>4.7 <lb/>Stable Diffusion is a LDM, operating in a pre-<lb/>trained, learned (and fixed) latent space of an <lb/>autoencoder. While the bulk of the semantic <lb/>composition is done by the LDM [38], we can <lb/>improve local, high-frequency details in gener-<lb/>ated images by improving the autoencoder. To <lb/>this end, we train the same autoencoder archi-<lb/>tecture used for the original Stable Diffusion at <lb/>a larger batch-size (256 vs 9) and additionally <lb/>track the weights with an exponential moving <lb/>average. The resulting autoencoder outperforms the original model in all evaluated reconstruction <lb/>metrics, see Tab. 3. We use this autoencoder for all of our experiments. <lb/>2.5 Putting Everything Together <lb/>We train the final model, SDXL, in a multi-stage procedure. SDXL uses the autoencoder from Sec. 2.4 <lb/>and a discrete-time diffusion schedule [14, 45] with 1000 steps. First, we pretrain a base model <lb/>(see Tab. 1) on an internal dataset whose height-and width-distribution is visualized in Fig. 2 for <lb/>600 000 optimization steps at a resolution of 256 × 256 pixels and a batch-size of 2048, using size-<lb/>and crop-conditioning as described in Sec. 2.2. We continue training on 512 × 512 pixel images for <lb/>another 200 000 optimization steps, and finally utilize multi-aspect training (Sec. 2.3) in combination <lb/>with an offset-noise [11, 25] level of 0.05 to train the model on different aspect ratios (Sec. 2.3, <lb/>App. I) of ∼ 1024 × 1024 pixel area. <lb/>Refinement Stage Empirically, we find that the resulting model sometimes yields samples of low <lb/>local quality, see Fig. 6. To improve sample quality, we train a separate LDM in the same latent space, <lb/>which is specialized on high-quality, high resolution data and employ a noising-denoising process as <lb/>introduced by SDEdit [28] on the samples from the base model. We follow [1] and specialize this <lb/>refinement model on the first 200 (discrete) noise scales. During inference, we render latents from <lb/>the base SDXL, and directly diffuse and denoise them in latent space with the refinement model (see <lb/>Fig. 1), using the same text input. We note that this step is optional, but improves sample quality for <lb/>detailed backgrounds and human faces, as demonstrated in Fig. 6 and Fig. 13. <lb/>To assess the performance of our model (with and without refinement stage), we conduct a user <lb/>study, and let users pick their favorite generation from the following four models: SDXL, SDXL <lb/>(with refiner), Stable Diffusion 1.5 and Stable Diffusion 2.1. The results demonstrate the SDXL with <lb/>the refinement stage is the highest rated choice, and outperforms Stable Diffusion 1.5 &amp; 2.1 by a <lb/>significant margin (win rates: SDXL w/ refinement: 48.44%, SDXL base: 36.93%, Stable Diffusion <lb/>1.5: 7.91%, Stable Diffusion 2.1: 6.71%). See Fig. 1, which also provides an overview of the full <lb/>pipeline. However, when using classical performance metrics such as FID and CLIP scores the <lb/>improvements of SDXL over previous methods are not reflected as shown in Fig. 12 and discussed in <lb/>App. F. This aligns with and further backs the findings of Kirstain et al. [23]. <lb/></body>

			<page>7 <lb/></page>

			<body>Figure 6: 1024 2 samples (with zoom-ins) from SDXL without (left) and with (right) the refinement model <lb/>discussed. Prompt: &quot;Epic long distance cityscape photo of New York City flooded by the ocean and overgrown <lb/>buildings and jungle ruins in rainforest, at sunset, cinematic shot, highly detailed, 8k, golden light&quot;. See Fig. 13 <lb/>for additional samples. <lb/>3 Future Work <lb/>This report presents a preliminary analysis of improvements to the foundation model Stable Diffusion <lb/>for text-to-image synthesis. While we achieve significant improvements in synthesized image quality, <lb/>prompt adherence and composition, in the following, we discuss a few aspects for which we believe <lb/>the model may be improved further: <lb/>• Single stage: Currently, we generate the best samples from SDXL using a two-stage approach <lb/>with an additional refinement model. This results in having to load two large models into <lb/>memory, hampering accessibility and sampling speed. Future work should investigate ways <lb/>to provide a single stage of equal or better quality. <lb/>• Text synthesis: While the scale and the larger text encoder (OpenCLIP ViT-bigG [19]) <lb/>help to improve the text rendering capabilities over previous versions of Stable Diffusion, <lb/>incorporating byte-level tokenizers [52, 27] or simply scaling the model to larger sizes [53, <lb/>40] may further improve text synthesis. <lb/>• Architecture: During the exploration stage of this work, we briefly experimented with <lb/>transformer-based architectures such as UViT [16] and DiT [33], but found no immediate <lb/>benefit. We remain, however, optimistic that a careful hyperparameter study will eventually <lb/>enable scaling to much larger transformer-dominated architectures. <lb/>• Distillation: While our improvements over the original Stable Diffusion model are significant, <lb/>they come at the price of increased inference cost (both in VRAM and sampling speed). <lb/>Future work will thus focus on decreasing the compute needed for inference, and increased <lb/>sampling speed, for example through guidance-[29], knowledge-[6, 22, 24] and progressive <lb/>distillation [41, 2, 29]. <lb/>• Our model is trained in the discrete-time formulation of [14], and requires offset-noise [11, <lb/>25] for aesthetically pleasing results. The EDM-framework of Karras et al. [21] is a <lb/>promising candidate for future model training, as its formulation in continuous time allows <lb/>for increased sampling flexibility and does not require noise-schedule corrections. <lb/></body>

			<page>8 <lb/></page>

		<div type="annex">Appendix <lb/></div>

		<div type="acknowledgment">A Acknowledgements <lb/>We thank all the folks at StabilityAI who worked on comparisons, code, etc, in particular: Alex <lb/>Goodwin, Benjamin Aubin, Bill Cusick, Dennis Nitrosocke Niedworok, Dominik Lorenz, Harry <lb/>Saini, Ian Johnson, Ju Huo, Katie May, Mohamad Diab, Peter Baylies, Rahim Entezari, Yam Levi, <lb/>Yannik Marek, Yizhou Zheng. We also thank ChatGPT for providing writing assistance. <lb/></div>

		<div type="annex">B Limitations <lb/>&apos;A close up of a handpalm <lb/>with leaves growing from it.&apos; <lb/>&apos;An empty fireplace with a television above it. <lb/>The TV shows a lion hugging a giraffe.&apos; <lb/>&apos;A grand piano with a white bench.&apos; <lb/>&apos;Three quarters view of a rusty old red pickup <lb/>truck with white doors and a smashed windshield.&apos; <lb/>Figure 7: Failure cases of SDXL despite large improvements compared to previous versions of Stable Diffusion, <lb/>the model sometimes still struggles with very complex prompts involving detailed spatial arrangements and <lb/>detailed descriptions (e.g. top left example). Moreover, hands are not yet always correctly generated (e.g. top <lb/>left) and the model sometimes suffers from two concepts bleeding into one another (e.g. bottom right example). <lb/>All examples are random samples generated with 50 steps of the DDIM sampler [46] and cfg-scale 8.0 [13]. <lb/>While our model has demonstrated impressive capabilities in generating realistic images and synthe-<lb/>sizing complex scenes, it is important to acknowledge its inherent limitations. Understanding these <lb/>limitations is crucial for further improvements and ensuring responsible use of the technology. <lb/>Firstly, the model may encounter challenges when synthesizing intricate structures, such as human <lb/>hands (see Fig. 7, top left). Although it has been trained on a diverse range of data, the complexity of <lb/>human anatomy poses a difficulty in achieving accurate representations consistently. This limitation <lb/>suggests the need for further scaling and training techniques specifically targeting the synthesis of <lb/>fine-grained details. A reason for this occurring might be that hands and similar objects appear with <lb/>very high variance in photographs and it is hard for the model to extract the knowledge of the real 3D <lb/>shape and physical limitations in that case. <lb/>Secondly, while the model achieves a remarkable level of realism in its generated images, it is <lb/>important to note that it does not attain perfect photorealism. Certain nuances, such as subtle <lb/>lighting effects or minute texture variations, may still be absent or less faithfully represented in the <lb/>generated images. This limitation implies that caution should be exercised when relying solely on <lb/>model-generated visuals for applications that require a high degree of visual fidelity. <lb/>Furthermore, the model&apos;s training process heavily relies on large-scale datasets, which can inadver-<lb/>tently introduce social and racial biases. As a result, the model may inadvertently exacerbate these <lb/>biases when generating images or inferring visual attributes. <lb/>In certain cases where samples contain multiple objects or subjects, the model may exhibit a phe-<lb/>nomenon known as &quot;concept bleeding&quot;. This issue manifests as the unintended merging or overlap of <lb/>distinct visual elements. For instance, in Fig. 14, an orange sunglass is observed, which indicates <lb/>an instance of concept bleeding from the orange sweater. Another case of this can be seen in Fig. 8, <lb/>the penguin is supposed to have a &quot;blue hat&quot; and &quot;red gloves&quot;, but is instead generated with blue <lb/></div>

		<page>9 <lb/></page>

			<div type="annex">gloves and a red hat. Recognizing and addressing such occurrences is essential for refining the <lb/>model&apos;s ability to accurately separate and represent individual objects within complex scenes. The <lb/>root cause of this may lie in the used pretrained text-encoders: firstly, they are trained to compress all <lb/>information into a single token, so they may fail at binding only the right attributes and objects, Feng <lb/>et al. [8] mitigate this issue by explicitly encoding word relationships into the encoding. Secondly, <lb/>the contrastive loss may also contribute to this, since negative examples with a different binding are <lb/>needed within the same batch [35]. <lb/>Additionally, while our model represents a significant advancement over previous iterations of SD, <lb/>it still encounters difficulties when rendering long, legible text. Occasionally, the generated text <lb/>may contain random characters or exhibit inconsistencies, as illustrated in Fig. 8. Overcoming this <lb/>limitation requires further investigation and development of techniques that enhance the model&apos;s text <lb/>generation capabilities, particularly for extended textual content -see for example the work of Liu <lb/>et al. [27], who propose to enhance text rendering capabilities via character-level text tokenizers. <lb/>Alternatively, scaling the model does further improve text synthesis [53, 40]. <lb/>In conclusion, our model exhibits notable strengths in image synthesis, but it is not exempt from <lb/>certain limitations. The challenges associated with synthesizing intricate structures, achieving perfect <lb/>photorealism, further addressing biases, mitigating concept bleeding, and improving text rendering <lb/>highlight avenues for future research and optimization. <lb/></div>

			<page>10 <lb/></page>

			<div type="annex">C Diffusion Models <lb/>In this section, we give a concise summary of DMs. We consider the continuous-time DM frame-<lb/>work [47] and follow the presentation of Karras et al. [21]. Let p data (x 0 ) denote the data distribution <lb/>and let p(x; σ) be the distribution obtained by adding i.i.d. σ 2 -variance Gaussian noise to the data. For <lb/>sufficiently large σ max , p(x; σ max 2 ) is almost indistinguishable from σ 2 <lb/>max -variance Gaussian noise. <lb/>Capitalizing on this observation, DMs sample high variance Gaussian noise x M ∼ N (0, σ max 2 ) and <lb/>sequentially denoise x M into x i ∼ p(x i ; σ i ), i ∈ {0, . . . , M }, with σ i &lt; σ i+1 and σ M = σ max . For <lb/>a well-trained DM and σ 0 = 0 the resulting x 0 is distributed according to the data. <lb/>Sampling. In practice, this iterative denoising process explained above can be implemented through <lb/>the numerical simulation of the Probability Flow ordinary differential equation (ODE) [47] <lb/>dx = -σ(t)σ(t)∇ x log p(x; σ(t)) dt, <lb/>(1) <lb/>where ∇ x log p(x; σ) is the score function [18]. The schedule σ(t): [0, 1] → R + is user-specified and <lb/>σ(t) denotes the time derivative of σ(t). Alternatively, we may also numerically simulate a stochastic <lb/>differential equation (SDE) [47, 21]: <lb/>dx = -σ(t)σ(t)∇ x log p(x; σ(t)) dt <lb/>Probability Flow ODE; see Eq. (1) <lb/>-β(t)σ 2 (t)∇ x log p(x; σ(t)) dt + 2β(t)σ(t) dω t <lb/>Langevin diffusion component <lb/>, (2) <lb/>where dω t is the standard Wiener process. In principle, simulating either the Probability Flow ODE <lb/>or the SDE above results in samples from the same distribution. <lb/>Training. DM training reduces to learning a model s θ (x; σ) for the score function ∇ x log p(x; σ). <lb/>The model can, for example, be parameterized as ∇ x log p(x; σ) ≈ s θ (x; σ) = (D θ (x; σ) -<lb/>x)/σ 2 [21], where D θ is a learnable denoiser that, given a noisy data point x 0 + n, x 0 ∼ p data (x 0 ), <lb/>n ∼ N 0, σ 2 I d , and conditioned on the noise level σ, tries to predict the clean x 0 . The denoiser <lb/>D θ (or equivalently the score model) can be trained via denoising score matching (DSM) <lb/>E (x0,c)∼p data (x0,c),(σ,n)∼p(σ,n) λ σ ∥D θ (x 0 + n; σ, c) -x 0 ∥ 2 <lb/>2 , <lb/>(3) <lb/>where p(σ, n) = p(σ) N n; 0, σ 2 , p(σ) is a distribution over noise levels σ, λ σ : R + → R + is a <lb/>weighting function, and c is an arbitrary conditioning signal, e.g., a class label, a text prompt, or a <lb/>combination thereof. In this work, we choose p(σ) to be a discrete distributions over 1000 noise <lb/>levels and set λ σ = σ -2 similar to prior works [14, 38, 45]. <lb/>Classifier-free guidance. Classifier-free guidance [13] is a technique to guide the iterative sampling <lb/>process of a DM towards a conditioning signal c by mixing the predictions of a conditional and an <lb/>unconditional model <lb/>D w (x; σ, c) = (1 + w)D(x; σ, c) -wD(x; σ), <lb/>(4) <lb/>where w ≥ 0 is the guidance strength. In practice, the unconditional model can be trained jointly <lb/>alongside the conditional model in a single network by randomly replacing the conditional signal c <lb/>with a null embedding in Eq. (3), e.g., 10% of the time [13]. Classifier-free guidance is widely used <lb/>to improve the sampling quality, trading for diversity, of text-to-image DMs [30, 38]. <lb/></div>

			<page>11 <lb/></page>

			<div type="annex">D Comparison to the State of the Art <lb/>Figure 8: Qualitative comparison of SDXL with DeepFloyd IF, DALLE-2, Bing Image Creator, and Midjourney <lb/>v5.2. To mitigate any bias arising from cherry-picking, Parti (P2) prompts were randomly selected. Seed 3 <lb/>was uniformly applied across all models in which such a parameter could be designated. For models without a <lb/>seed-setting feature, the first generated image is included. <lb/></div>

			<page>12 <lb/></page>

			<div type="annex">E Comparison to Midjourney v5.1 <lb/>E.1 Overall Votes <lb/>To asses the generation quality of SDXL we perform a user study against the state of the art text-to-<lb/>image generation platform Midjourney 1 . As the source for image captions we use the PartiPrompts <lb/>(P2) benchmark [53], that was introduced to compare large text-to-image model on various challeng-<lb/>ing prompts. <lb/>For our study, we choose five random prompts from each category, and generate four 1024 × 1024 <lb/>images by both Midjourney (v5.1, with a set seed of 2) and SDXL for each prompt. These images <lb/>were then presented to the AWS GroundTruth taskforce, who voted based on adherence to the prompt. <lb/>The results of these votes are illustrated in Fig. 9. Overall, there is a slight preferance for SDXL over <lb/>Midjourney in terms of prompt adherence. <lb/>Vanilla <lb/>0.0 <lb/>0.1 <lb/>0.2 <lb/>0.3 <lb/>0.4 <lb/>0.5 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>Frequency → <lb/>Figure 9: Results from 17,153 user preference comparisons between SDXL v0.9 and Midjourney v5.1, which <lb/>was the latest version available at the time. The comparisons span all &quot;categories&quot; and &quot;challenges&quot; in the <lb/>PartiPrompts (P2) benchmark. Notably, SDXL was favored 54.9% of the time over Midjourney V5.1. Preliminary <lb/>testing indicates that the recently-released Midjourney V5.2 has lower prompt comprehension than its predecessor, <lb/>but the laborious process of generating multiple prompts hampers the speed of conducting broader tests. <lb/>E.2 Category &amp; challenge comparisons on PartiPrompts (P2) <lb/>Each prompt from the P2 benchmark is organized into a category and a challenge, each focus on <lb/>different difficult aspects of the generation process. We show the comparisons for each category <lb/>(Fig. 10) and challenge (Fig. 11) of P2 below. In four out of six categories SDXL outperforms <lb/>Midjourney, and in seven out of ten challenges there is no significant difference between both models <lb/>or SDXL outperforms Midjourney. <lb/>Food &amp; Beverage <lb/>Animals <lb/>Artifacts <lb/>Arts <lb/>Illustrations <lb/>Abstract <lb/>0.0 <lb/>0.1 <lb/>0.2 <lb/>0.3 <lb/>0.4 <lb/>0.5 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>Frequency → <lb/>Figure 10: User preference comparison of SDXL (without refinement model) and Midjourney V5.1 across <lb/>particular text categories. SDXL outperforms Midjourney V5.1 in all but two categories. <lb/></div>

			<note place="footnote">1 We compare against v5.1 since that was the best version available at that time. <lb/></note>

			<page>13 <lb/></page>

			<div type="annex">Imagination <lb/>Writing &amp; Symbols <lb/>Quantity <lb/>Complex <lb/>Fine-grained Detail <lb/>Perspective <lb/>Style &amp; Format <lb/>Simple Detail <lb/>Linguistic Structures <lb/>Properties &amp; Positioning <lb/>0.0 <lb/>0.1 <lb/>0.2 <lb/>0.3 <lb/>0.4 <lb/>0.5 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>Frequency → <lb/>Figure 11: Preference comparisons of SDXL (with refinement model) to Midjourney V5.1 on complex prompts. <lb/>SDXL either outperforms or is statistically equal to Midjourney V5.1 in 7 out of 10 categories. <lb/>F On FID Assessment of Generative Text-Image Foundation Models <lb/>Figure 12: Plotting FID vs CLIP score for different cfg scales. SDXL shows only slightly improved text-<lb/>alignment, as measured by CLIP-score, compared to previous versions that do not align with the judgement of <lb/>human evaluators. Even further and similar as in [23], FID are worse than for both SD-1.5 and SD-2.1, while <lb/>human evaluators clearly prefer the generations of SD-XL over those of these previous models. <lb/>Throughout the last years it has been common practice for generative text-to-image models to assess <lb/>FID-[12] and CLIP-scores [34, 36] in a zero-shot setting on complex, small-scale text-image datasets <lb/>of natural images such as COCO [26]. However, with the advent of foundational text-to-image <lb/>models [40, 37, 38, 1], which are not only targeting visual compositionality, but also at other difficult <lb/>tasks such as deep text understanding, fine-grained distinction between unique artistic styles and <lb/>especially a pronounced sense of visual aesthetics, this particular form of model evaluation has <lb/>become more and more questionable. Kirstain et al. [23] demonstrates that COCO zero-shot FID <lb/>is negatively correlated with visual aesthetics, and such measuring the generative performance of <lb/>such models should be rather done by human evaluators. We investigate this for SDXL and visualize <lb/>FID-vs-CLIP curves in Fig. 12 for 10k text-image pairs from COCO [26]. Despite its drastically <lb/>improved performance as measured quantitatively by asking human assessors (see Fig. 1) as well as <lb/>qualitatively (see Fig. 4 and Fig. 14), SDXL does not achieve better FID scores than the previous SD <lb/>versions. Contrarily, FID for SDXL is the worst of all three compared models while only showing <lb/>slightly improved CLIP-scores (measured with OpenClip ViT g-14). Thus, our results back the <lb/>findings of Kirstain et al. [23] and further emphasize the need for additional quantitative performance <lb/>scores, specifically for text-to-image foundation models. All scores have been evaluated based on <lb/>10k generated examples. <lb/></div>

			<page>14 <lb/></page>

			<div type="annex">G Additional Comparison between Single-and Two-Stage SDXL pipeline <lb/>Figure 13: SDXL samples (with zoom-ins) without (left) and with (right) the refinement model discussed. <lb/>Prompt: (top) &quot;close up headshot, futuristic young woman, wild hair sly smile in front of gigantic UFO, dslr, <lb/>sharp focus, dynamic composition&quot; (bottom) &quot;Three people having dinner at a table at new years eve, cinematic <lb/>shot, 8k&quot;. Zoom-in for details. <lb/></div>

			<page>15 <lb/></page>

			<div type="annex">H Comparison between SD 1.5 vs. SD 2.1 vs. SDXL <lb/>&apos;Vibrant portrait painting of Salvador Dalí <lb/>with a robotic half face.&apos; <lb/>&apos;A capybara made of voxels <lb/>sitting in a field.&apos; <lb/>SD 1-5 <lb/>SD 2-1 <lb/>SDXL <lb/>&apos;Cute adorable little goat, unreal engine, <lb/>cozy interior lighting, art station, detailed&apos; <lb/>digital painting, cinematic, octane rendering.&apos; <lb/>&apos;A portrait photo of a kangaroo wearing an orange hoodie <lb/>and blue sunglasses standing on the grass in front of the Sydney <lb/>Opera House holding a sign on the chest that says &quot;SDXL&quot;!.&apos; <lb/>SD 1-5 <lb/>SD 2-1 <lb/>SDXL <lb/>Figure 14: Additional results for the comparison of the output of SDXL with previous versions of Stable <lb/>Diffusion. For each prompt, we show 3 random samples of the respective model for 50 steps of the DDIM <lb/>sampler [46] and cfg-scale 8.0 [13] <lb/></div>

			<page>16 <lb/></page>

			<div type="annex">&apos;Monster Baba yaga house with in a forest, <lb/>dark horror style, black and white.&apos; <lb/>&apos;A young badger delicately sniffing a <lb/>yellow rose, richly textured oil painting.&apos; <lb/>SD 1-5 <lb/>SD 2-1 <lb/>SDXL <lb/>Figure 15: Additional results for the comparison of the output of SDXL with previous versions of Stable <lb/>Diffusion. For each prompt, we show 3 random samples of the respective model for 50 steps of the DDIM <lb/>sampler [46] and cfg-scale 8.0 [13]. <lb/>I Multi-Aspect Training Hyperparameters <lb/>We use the following image resolutions for mixed-aspect ratio finetuning as described in Sec. 2.3. <lb/>Height Width Aspect Ratio <lb/>512 <lb/>2048 <lb/>0.25 <lb/>512 <lb/>1984 <lb/>0.26 <lb/>512 <lb/>1920 <lb/>0.27 <lb/>512 <lb/>1856 <lb/>0.28 <lb/>576 <lb/>1792 <lb/>0.32 <lb/>576 <lb/>1728 <lb/>0.33 <lb/>576 <lb/>1664 <lb/>0.35 <lb/>640 <lb/>1600 <lb/>0.4 <lb/>640 <lb/>1536 <lb/>0.42 <lb/>704 <lb/>1472 <lb/>0.48 <lb/>704 <lb/>1408 <lb/>0.5 <lb/>704 <lb/>1344 <lb/>0.52 <lb/>768 <lb/>1344 <lb/>0.57 <lb/>768 <lb/>1280 <lb/>0.6 <lb/>832 <lb/>1216 <lb/>0.68 <lb/>832 <lb/>1152 <lb/>0.72 <lb/>896 <lb/>1152 <lb/>0.78 <lb/>896 <lb/>1088 <lb/>0.82 <lb/>960 <lb/>1088 <lb/>0.88 <lb/>960 <lb/>1024 <lb/>0.94 <lb/>Height Width Aspect Ratio <lb/>1024 <lb/>1024 <lb/>1.0 <lb/>1024 <lb/>960 <lb/>1.07 <lb/>1088 <lb/>960 <lb/>1.13 <lb/>1088 <lb/>896 <lb/>1.21 <lb/>1152 <lb/>896 <lb/>1.29 <lb/>1152 <lb/>832 <lb/>1.38 <lb/>1216 <lb/>832 <lb/>1.46 <lb/>1280 <lb/>768 <lb/>1.67 <lb/>1344 <lb/>768 <lb/>1.75 <lb/>1408 <lb/>704 <lb/>2.0 <lb/>1472 <lb/>704 <lb/>2.09 <lb/>1536 <lb/>640 <lb/>2.4 <lb/>1600 <lb/>640 <lb/>2.5 <lb/>1664 <lb/>576 <lb/>2.89 <lb/>1728 <lb/>576 <lb/>3.0 <lb/>1792 <lb/>576 <lb/>3.11 <lb/>1856 <lb/>512 <lb/>3.62 <lb/>1920 <lb/>512 <lb/>3.75 <lb/>1984 <lb/>512 <lb/>3.88 <lb/>2048 <lb/>512 <lb/>4.0 <lb/></div>

			<page>17 <lb/></page>

			<div type="annex">J Pseudo-code for Conditioning Concatenation along the Channel Axis <lb/>1 from einops import rearrange <lb/>2 import torch <lb/>3 <lb/>4 batch_size =16 <lb/>5 # channel dimension of pooled output of text encoder ( s ) <lb/>6 pooled_dim = 512 <lb/>7 <lb/>8 def fourier_embe dding ( inputs , outdim =256 , max_period =10000) : <lb/></div>

			<page>9 <lb/></page>

			<div type="annex">&quot;&quot;&quot; <lb/>10 <lb/>Classical sinusoidal timestep embedding <lb/>11 <lb/>as commonly used in diffusion models <lb/>12 <lb/>: param inputs : batch of integer scalars shape [b ,] <lb/>13 <lb/>: param outdim : embedding dimension <lb/>14 <lb/>: param max_period : max freq added <lb/>15 <lb/>: return : batch of embeddings of shape [b , outdim ] <lb/>16 <lb/>&quot;&quot;&quot; <lb/>17 <lb/>... <lb/>18 <lb/>19 def c a t_ a l o n g _ c h a n n e l_ d im ( <lb/>20 <lb/>x : torch . Tensor ,) -&gt; torch . Tensor : <lb/>21 <lb/>if x . ndim == 1: <lb/>22 <lb/>x = x [... , None ] <lb/>23 <lb/>assert x . ndim == 2 <lb/>24 <lb/>b , d_in = x . shape <lb/>25 <lb/>x = rearrange (x , &quot; b din -&gt; ( b din ) &quot; ) <lb/>26 <lb/># fourier fn adds additional dimension <lb/>27 <lb/>emb = fourie r_embedding ( x ) <lb/>28 <lb/>d_f = emb . shape [ -1] <lb/>29 <lb/>emb = rearrange ( emb , &quot; ( b din ) df -&gt; b ( din df ) &quot; , <lb/>30 <lb/>b =b , din = d_in , df = d_f ) <lb/>31 <lb/>return emb <lb/>32 <lb/>33 def conc at_embed dings ( <lb/>34 <lb/># batch of size and crop conditioning cf . Sec . 3.2 <lb/>35 <lb/>c_size : torch . Tensor , <lb/>36 <lb/>c_crop : torch . Tensor , <lb/>37 <lb/># batch of aspect ratio conditioning cf . Sec . 3.3 <lb/>38 <lb/>c_ar : torch . Tensor , <lb/>39 <lb/># final output of text encoders after pooling cf . Sec . 3.1 <lb/>40 <lb/>c_pooled_txt : torch . Tensor , ) -&gt; torch . Tensor : <lb/>41 <lb/># fourier feature for size conditioning <lb/>42 <lb/>c_size_emb = c a t_ along _chann el_dim ( c_size ) <lb/>43 <lb/># fourier feature for size conditioning <lb/>44 <lb/>c_crop_emb = c a t_ along _chann el_dim ( c_crop ) <lb/>45 <lb/># fourier feature for size conditioning <lb/>46 <lb/>c_ar_emb = c at _ al on g_chan nel_d im ( c_ar ) <lb/>47 <lb/># the concatenated output is mapped to the same <lb/>48 <lb/># channel dimension than the noise level conditioning <lb/>49 <lb/># and added to that conditioning before being fed to the unet <lb/>50 <lb/>return torch . cat ([ c_pooled_txt , <lb/>51 <lb/>c_size_emb , <lb/>52 <lb/>c_crop_emb , <lb/>53 <lb/>c_ar_emb ] , dim =1) <lb/>54 <lb/>55 # simulating c_size and c_crop as in Sec . 3.2 <lb/>56 c_size = torch . zeros (( batch_size , 2) ) . long () <lb/>57 c_crop = torch . zeros (( batch_size , 2) ) . long () <lb/>58 # simulating c_ar and pooled text encoder output as in Sec . 3.3 <lb/>59 c_ar = torch . zeros (( batch_size , 2) ) . long () <lb/>60 c_pooled = torch . zeros (( batch_size , pooled_dim ) ) . long () <lb/>61 <lb/>62 # get concatenated embedding <lb/>63 c_concat = conc at_embeddings ( c_size , c_crop , c_ar , c_pooled ) <lb/>Figure 16: Python code for concatenating the additional conditionings introduced in Secs. 2.1 to 2.3 along the <lb/>channel dimension. <lb/></div>

			<page>18 <lb/></page>

			<listBibl>References <lb/>[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo <lb/>Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. eDiff-I: Text-to-Image Diffusion <lb/>Models with an Ensemble of Expert Denoisers. arXiv:2211.01324, 2022. <lb/>[2] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, <lb/>Walter Talbot, and Eric Gu. TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation. <lb/>arXiv:2303.04248, 2023. <lb/>[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and <lb/>Karsten Kreis. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. <lb/>arXiv:2304.08818, 2023. <lb/>[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical <lb/>image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. <lb/>Ieee, 2009. <lb/>[5] Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. arXiv:2105.05233, <lb/>2021. <lb/>[6] Tim Dockhorn, Robin Rombach, Andreas Blattmann, and Yaoliang Yu. Distilling the Knowledge in <lb/>Diffusion Models. CVPR Workshop on Generative Models for Computer Vision, 2023. <lb/>[7] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. <lb/>Structure and content-guided video synthesis with diffusion models, 2023. <lb/>[8] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, <lb/>Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional <lb/>text-to-image synthesis. arXiv:2212.05032, 2023. <lb/>[9] Seth Forsgren and Hayk Martiros. Riffusion -Stable diffusion for real-time music generation, 2022. URL <lb/>https://riffusion.com/about. <lb/>[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel <lb/>Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. <lb/>arXiv:2208.01618, 2022. <lb/>[11] Nicholas Guttenberg and CrossLabs. Diffusion with offset noise, 2023. URL https://www.crosslabs. <lb/>org/blog/diffusion-with-offset-noise. <lb/>[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs <lb/>Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. arXiv:1706.08500, <lb/>2017. <lb/>[13] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. arXiv:2207.12598, 2022. <lb/>[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. arXiv preprint <lb/>arXiv:2006.11239, 2020. <lb/>[15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P <lb/>Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, and Tim Salimans. Imagen Video: High Definition <lb/>Video Generation with Diffusion Models. arXiv:2210.02303, 2022. <lb/>[16] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high <lb/>resolution images. arXiv preprint arXiv:2301.11093, 2023. <lb/>[17] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, <lb/>Xiang Yin, and Zhou Zhao. Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion <lb/>Models. arXiv:2301.12661, 2023. <lb/>[18] Aapo Hyvärinen and Peter Dayan. Estimation of Non-Normalized Statistical Models by Score Matching. <lb/>Journal of Machine Learning Research, 6(4), 2005. <lb/>[19] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal <lb/>Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig <lb/>Schmidt. OpenCLIP, July 2021. URL https://doi.org/10.5281/zenodo.5143773. <lb/>[20] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, and Ilya Sutskever. <lb/>Distribution Augmentation for Generative Modeling. In International Conference on Machine Learning, <lb/>pages 5006-5019. PMLR, 2020. <lb/>[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-Based <lb/>Generative Models. arXiv:2206.00364, 2022. <lb/>[22] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On Architectural Compression <lb/>of Text-to-Image Diffusion Models. arXiv:2305.15798, 2023. <lb/></listBibl>

			<page>19 <lb/></page>

			<listBibl>[23] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: <lb/>An open dataset of user preferences for text-to-image generation. arXiv:2305.01569, 2023. <lb/>[24] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, <lb/>and Jian Ren. SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. <lb/>arXiv:2306.00980, 2023. <lb/>[25] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common Diffusion Noise Schedules and Sample <lb/>Steps are Flawed. arXiv:2305.08891, 2023. <lb/>[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro <lb/>Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in <lb/>context, 2015. <lb/>[27] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, <lb/>RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual text rendering, <lb/>2023. <lb/>[28] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided <lb/>Image Synthesis and Editing with Stochastic Differential Equations. arXiv:2108.01073, 2021. <lb/>[29] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim <lb/>Salimans. On distillation of guided diffusion models, 2023. <lb/>[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya <lb/>Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-<lb/>Guided Diffusion Models. arXiv:2112.10741, 2021. <lb/>[31] NovelAI. Novelai improvements on stable diffusion, 2023. URL https://blog.novelai.net/ <lb/>novelai-improvements-on-stable-diffusion-e10d38db82ac. <lb/>[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor <lb/>Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, <lb/>Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, <lb/>and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. <lb/>[33] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. arXiv:2212.09748, 2022. <lb/>[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish <lb/>Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning <lb/>Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020, 2021. <lb/>[35] Aditya Ramesh. How dall•e 2 works, 2022. URL http://adityaramesh.com/posts/dalle2/dalle2. <lb/>html. <lb/>[36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and <lb/>Ilya Sutskever. Zero-shot text-to-image generation, 2021. <lb/>[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional <lb/>Image Generation with CLIP Latents. arXiv:2204.06125, 2022. <lb/>[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-Resolution <lb/>Image Synthesis with Latent Diffusion Models. arXiv preprint arXiv:2112.10752, 2021. <lb/>[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical <lb/>Image Segmentation. arXiv:1505.04597, 2015. <lb/>[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar <lb/>Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan <lb/>Ho, David J Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep <lb/>Language Understanding. arXiv:2205.11487, 2022. <lb/>[41] Tim Salimans and Jonathan Ho. Progressive Distillation for Fast Sampling of Diffusion Models. arXiv <lb/>preprint arXiv:2202.00512, 2022. <lb/>[42] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved <lb/>Techniques for Training GANs. arXiv:1606.03498, 2016. <lb/>[43] Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, and Xinxiao Wu. DiffCLIP: Leveraging Stable Diffusion <lb/>for Language Grounded 3D Classification. arXiv:2305.15957, 2023. <lb/>[44] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, <lb/>Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-A-Video: Text-to-Video <lb/>Generation without Text-Video Data. arXiv:2209.14792, 2022. <lb/>[45] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised <lb/>Learning using Nonequilibrium Thermodynamics. arXiv:1503.03585, 2015. <lb/></listBibl>

			<page>20 <lb/></page>

			<listBibl>[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, <lb/>2020. <lb/>[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. <lb/>Score-Based Generative Modeling through Stochastic Differential Equations. arXiv:2011.13456, 2020. <lb/>[48] Andreas Stöckl. Evaluating a synthetic image dataset generated with stable diffusion. arXiv:2211.01777, <lb/>2022. <lb/>[49] Yu Takagi and Shinji Nishimoto. High-Resolution Image Reconstruction With Latent Diffusion Models <lb/>From Human Brain Activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern <lb/>Recognition, pages 14453-14463, 2023. <lb/>[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, <lb/>Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, <lb/>Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. <lb/>arXiv:2302.13971, 2023. <lb/>[51] Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, and Gérard Dray. <lb/>Boosting gui prototyping with diffusion models. arXiv preprint arXiv:2306.06233, 2023. <lb/>[52] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, <lb/>and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models, 2022. <lb/>[53] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, <lb/>Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han <lb/>Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image <lb/>generation, 2022. <lb/>[54] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. <lb/>arXiv:2302.05543, 2023. <lb/>[55] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable <lb/>effectiveness of deep features as a perceptual metric, 2018. <lb/></listBibl>

			<page>21 </page>


	</text>
</tei>
