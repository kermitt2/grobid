<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="__W12-4305"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 37–46, <lb/>Jeju, Republic of Korea, 12 July 2012. c <lb/> 2012 Association for Computational Linguistics <lb/> A Three-Way Perspective on Scientific Discourse Annotation for <lb/>Knowledge Extraction <lb/> Maria Liakata <lb/> Aberystwyth University, UK / <lb/>EMBL-EBI, UK <lb/> liakata@ebi.ac.uk <lb/> Paul Thompson <lb/> University of Manchester, UK <lb/> paul.thompson@manchester. <lb/>ac.uk <lb/> Anita de Waard <lb/> Elsevier Labs, USA / <lb/>UiL-OTS, Universiteit Utrecht, NL <lb/> a.dewaard@elsevier.com <lb/> Raheel Nawaz <lb/> University of Manchester, UK <lb/> raheel.nawaz@cs.man. <lb/>ac.uk <lb/> Henk Pander Maat <lb/> UiL-OTS, Universiteit Utrecht, NL <lb/> h.l.w.pandermaat@uu.nl <lb/> Sophia Ananiadou <lb/> University of Manchester, UK <lb/> sophia.ananiadou@manchester. <lb/>ac.uk <lb/> Abstract <lb/> This paper presents a three-way perspective on <lb/>the annotation of discourse in scientific <lb/>literature. We use three different schemes, each <lb/>of which focusses on different aspects of <lb/>discourse in scientific articles, to annotate a <lb/>corpus of three full-text papers, and compare <lb/>the results. One scheme seeks to identify the <lb/>core components of scientific investigations at <lb/>the sentence level, a second annotates meta-<lb/>knowledge pertaining to bio-events and a third <lb/>considers how epistemic knowledge is <lb/>conveyed at the clause level. We present our <lb/>analysis of the comparison, and a discussion of <lb/>the contributions of each scheme. <lb/></front>

			<body> 1 Introduction <lb/> The literature boom in the life sciences over the <lb/>past few years has sparked increasing interest into <lb/>text mining tools, which facilitate the automatic <lb/>extraction of useful knowledge from text <lb/>(Ananiadou et al., 2006; Ananiadou <lb/>&amp; <lb/>McNaught, 2006; Zweigenbaum et al., 2007; <lb/>Cohen &amp; Hunter, 2008). Most of these tools have <lb/>focussed on entity recognition and relation <lb/>extraction and with few exceptions, e.g., (Hyland, <lb/>1996; Light et al., 2004; Sándor, 2007; Vincze et <lb/>al., 2008), do not take into account the discourse <lb/>context of the knowledge extracted. However, <lb/>failure to take this context into account results in <lb/>the loss of information vital for the correct <lb/>interpretation of extracted knowledge, e.g. the <lb/>scope of the relations, or the level of certainty with <lb/>which they are expressed. A particular piece of <lb/>knowledge may represent, e.g., an accepted fact, <lb/>hypothesis, results of an experiment, analysis <lb/>based on experimental results, factual or <lb/>speculative statements etc. Furthermore, this <lb/>knowledge may represent the author&apos;s current <lb/>work, or work reported elsewhere. The ability to <lb/>recognise <lb/>different <lb/>discourse <lb/>elements <lb/>automatically provides crucial information for the <lb/>correct interpretation of extracted knowledge, <lb/>allowing scientific claims to be linked to <lb/>experimental evidence, or newly reported <lb/>experimental knowledge to be isolated. The <lb/>importance of categorising such knowledge <lb/>becomes more pronounced as analysis moves from <lb/>abstracts to full papers, where the content is richer <lb/>and linguistic constructions are more complex <lb/>(Cohen et al., 2010). Analysis of full papers is <lb/>extremely important, since less than 8% of <lb/>scientific claims occur in abstracts (Blake, 2010). <lb/> Various different schemes for annotating <lb/>discourse elements in scientific texts have been <lb/>proposed. The schemes vary along several axes, <lb/>including perspective, motivation, complexity and <lb/>the granularity of the units of text to which the <lb/>scheme is applied. Faced with such variety, it is <lb/>important to be able to select the best scheme(s) <lb/>for the purpose at hand. Answers to questions such <lb/>as the following can help in the selection process: <lb/>1. What are the relative merits of the different <lb/>schemes? <lb/>2. What are the similarities and differences <lb/>between schemes? <lb/>3. Can annotation according to multiple schemes <lb/>provide enhanced information? <lb/>

			<page> 37 <lb/></page>

			Category <lb/> Description <lb/> Hypothesis <lb/> An unconfirmed statement which is a stepping stone of the investigation <lb/> Motivation <lb/> The reasons behind an investigation <lb/> Background <lb/> Generally accepted background knowledge and previous work <lb/> Goal <lb/> A target state of the investigation where intended discoveries are made <lb/> Object-New <lb/> An entity which is a product or main theme of the investigation <lb/> Object-New-Advantage <lb/> Advantage of an object <lb/> Object-New-Disadvantage <lb/> Disadvantage of an object <lb/> Method-New <lb/> Means by which authors seek to achieve a goal of the investigation <lb/> Method-New-Advantage <lb/> Advantage of a Method <lb/> Method-New-Disadvantage <lb/> Disadvantage of a Method <lb/> Method-Old <lb/> A method mentioned pertaining to previous work <lb/> Method-Old-Advantage <lb/> Advantage of a Method <lb/> Method-Old-Disadvantage <lb/> Disadvantage of a Method <lb/> Experiment <lb/> An experimental method <lb/> Model <lb/> A statement about a theoretical model or framework <lb/> Observation <lb/> The data/phenomena recorded in an investigation <lb/> Result <lb/> Factual statements about the outputs, interpretation of observations <lb/> Conclusion <lb/> Statements inferred from observations &amp; results <lb/>

			Table 1. The CoreSC Annotation scheme: layers 1 &amp; 2 <lb/> 4. Is there any advantage in merging annotation <lb/>schemes or is it better to allow complementary <lb/>and different dimensions of scientific discourse <lb/>annotation? <lb/>As a starting point to addressing such questions, <lb/>we provide a comparison of three different <lb/>schemes for the annotation of discourse elements <lb/>within scientific papers. Each scheme has a <lb/>different perspective and motivation:, one is <lb/>content-driven, seeking to identify the main <lb/>components of a scientific investigation, another is <lb/>driven by the need to describe events of biomedical <lb/>relevance and the third focusses on how epistemic <lb/>knowledge is conveyed in discourse. <lb/>These different viewpoints mean that the <lb/>schemes vary in both the type and complexity of <lb/>the discourse elements identified, as well as the <lb/>types of units to which the annotation is applied, <lb/>i.e. complete sentences, segments of sentences, or <lb/>specific relations/events occurring within these <lb/>sentences. To facilitate the comparison, we have <lb/>annotated three full papers according to each of the <lb/>schemes. The analysis resulting from this three-<lb/>way annotation considers mappings between <lb/>schemes, their relative merits, and how the <lb/>information annotated by the different schemes can <lb/>complement each other to provide enriched details <lb/>about knowledge extracted from the texts. <lb/> In the following sections, we firstly provide a <lb/>description of the three schemes, and then explain <lb/>how they have been used in our corpus annotation. <lb/>Finally we discuss the results from the comparison, <lb/>and the features of each scheme. <lb/> 2 Sentence annotation: CoreSC scheme <lb/> The reasoning behind this scheme is that a paper is <lb/>the human-readable representation of a scientific <lb/>investigation. Therefore, the goal of the annotation <lb/>is to retrieve the content model of scientific <lb/>investigations as reflected within scientific <lb/>discourse. The hypothesis is that there is a set of <lb/>core scientific concepts (CoreSC), which constitute <lb/>the key components of a scientific investigation. <lb/>CoreSCs consist of 11 concepts originating from <lb/>the CISP (Core Information about Scientific <lb/>Papers) meta-data (Soldatova &amp; Liakata, 2007), <lb/>which are a subset of classes from the EXPO <lb/>ontology for the description of scientific <lb/>experiments (Soldatova &amp; King, 2006). The <lb/>CoreSCs <lb/>are: Motivation, Goal, Object, <lb/>Background, <lb/>Hypothesis, <lb/>Method, <lb/>Model, <lb/>Experiment, Observation, Result and Conclusion. <lb/>

			<page> 38 <lb/></page>

			Figure 1. Bio-Event Representation <lb/> The CoreSC scheme (Liakata et al., 2010; <lb/>Liakata et al., 2012) implements the above-<lb/>mentioned concepts as a 3-layered sentence-based <lb/>annotation scheme. This means that each sentence <lb/>in a document is assigned one of the 11 CoreSC <lb/>concepts. The scheme also considers a layer <lb/>designated to properties of the concepts (e.g. New <lb/>Method vs Old Method) as well as identifiers <lb/>which link instances of the same concept across <lb/>sentences. A short definition of CoreSC categories <lb/>and their properties can be found in Table 1. <lb/>The CoreSC scheme is accompanied by 47-page <lb/>annotation guidelines, and has been used by 16 <lb/>domain experts to annotate a corpus of 265 full <lb/>papers from physical chemistry &amp; biochemistry <lb/>(Liakata &amp; Soldatova, 2009; Liakata et al., 2010). <lb/>This corpus consists of 40,000 sentences, <lb/>containing over 1 million words and was <lb/>developed in three phases (for details see Liakata <lb/>et al. (2012)). Inter-annotator agreement between <lb/>experts was measured in terms of Cohen&apos;s kappa <lb/>(Cohen, 1960) on 41 papers and ranged between <lb/>0.5 and 0.7. Machine learning classifiers have been <lb/>trained on the CoreSC corpus, achieving &gt; 51% <lb/>accuracy across the eleven categories. The most <lb/>accurately predicted category is Experiment, the <lb/>category describing experimental methods (Liakata <lb/>et al., 2012). Classifiers trained on 1000 Biology <lb/>abstracts annotated with CoreSC have obtained an <lb/>accuracy of over 80% (Guo et al., 2010). Models <lb/>trained on the CoreSC corpus papers have been <lb/>used to create automatic summaries of the papers, <lb/>which have been evaluated in a question answering <lb/>task (Liakata et al., 2012). Lastly, the CoreSC <lb/>scheme was used to annotate 50 papers from <lb/>Pubmed Central pertaining to Cancer Risk <lb/>Assessment. A web tool (SAPIENTA 1 ) allows <lb/>users to annotate their full papers with Core <lb/>Scientific concepts, and can be combined with <lb/>manual annotation. A UIMA framework 2 <lb/> implementation of this code for large-scale <lb/>annotation of CoreSC concepts is in progress. <lb/> 3 Event annotation: Meta-knowledge for <lb/>bio-events <lb/> The motivation for this annotation scheme is to <lb/>allow the training of more sophisticated event-<lb/> 
			
			<note place="footnote"> 1 http://www.sapientaproject.com/software <lb/></note> 
			
			<note place="footnote">2 http://uima.apache.org/ <lb/></note>

			 based information extraction systems. In contrast <lb/>to the sentence-based scheme described in section <lb/>2, this scheme is applied at the level of events <lb/> (Ananiadou et al., 2010), of which there may be <lb/>several within a single sentence. <lb/> 3.1 Bio-Events <lb/> Events are template-like, structured representations <lb/>of pieces of knowledge contained within sentences. <lb/>Normally, events are &quot; anchored &quot; to a trigger <lb/> (typically a verb or noun) around which the <lb/>knowledge expressed is organised. Each event has <lb/>one of more participants, which describe different <lb/>aspects of the event. Participants can correspond to <lb/>entities or other events, and are often labelled with <lb/>semantic <lb/>roles, <lb/>e.g., <lb/>CAUSE, <lb/>THEME, <lb/>LOCATION, etc. The work described here <lb/>focusses specifically on bio-events, which are <lb/>complex structured relations representing fine-<lb/>grained relations between bio-entities and their <lb/>modifiers. Figure 1 provides some examples of <lb/>bio-events. Event extraction systems (Björne et al., <lb/>2009; Miwa et al., 2010; Miwa et al., 2012; Quirk <lb/>et al., 2011) are typically trained on text corpora, in <lb/>which events and their participants have been <lb/>manually annotated by domain experts. Research <lb/>into bio-event extraction has been boosted by the <lb/>two recent shared tasks at BioNLP 2009/2011 <lb/>(Kim et al., 2011; Pyysalo et al., In Press). Several <lb/>gold standard event annotated corpora exist; <lb/>examples include the GENIA Event Corpus (Kim <lb/>et al., 2008), GREC (Thompson et al., 2009) and <lb/>BioInfer (Pyysalo et al., 2007), in addition to the <lb/>corpora produced for the shared tasks. <lb/> 3.2 Meta-knowledge Annotation <lb/> Until recently, the only attempts to recognise <lb/>information relating to the correct interpretation of <lb/>events were restricted to sparse details regarding <lb/>negation and speculation (Kim et al., 2011). <lb/>

			<page> 39 <lb/></page>

			In order to address this problem, a multi-<lb/>dimensional annotation scheme especially tailored <lb/>to bio-events was developed (Nawaz et al., 2010; <lb/>Thompson et al., 2011). The scheme identifies and <lb/>categorises several different types of contextual <lb/>details regarding events (termed meta-knowledge), <lb/> including discourse information. Different types of <lb/>meta-knowledge are encoded through five distinct <lb/>dimensions (Figure 2). The advantage of using <lb/>multiple dimensions is that the interplay between <lb/>the assigned values in each dimension can reveal <lb/>both subtle and substantial differences in the types <lb/>of meta-knowledge expressed. <lb/>In the majority of cases, meta-knowledge is <lb/>expressed through the presence of particular &quot; clue &quot; <lb/>words or phrases, although other features can also <lb/>come into play, such as the tense of the event <lb/>trigger, or the relative position within the text. <lb/>Figure 2: Meta-knowledge annotation <lb/> The annotation task consists of assigning an <lb/>appropriate value from a fixed set for each <lb/>dimension, as well as marking the textual evidence <lb/>for this assignment. The five meta-knowledge <lb/>dimensions and their values are as follows: <lb/> Knowledge Type (KT): Captures the general <lb/>information content of the event. Each event is <lb/>classified as one of: Investigation (enquiries and <lb/>examinations, <lb/>etc.), <lb/> Observation <lb/> (direct <lb/>experimental observations), Analysis (inferences, <lb/>interpretations and conjectures, etc.), Fact (known <lb/>facts), Method (methods) or Other (general events <lb/>that provide incomplete information or do not fit <lb/>into any other category). <lb/> Certainty Level (CL): Encodes the confidence or <lb/>certainty level ascribed to the event in the given <lb/>text. The epistemic scale is partitioned into three <lb/>distinct levels: L3 (no expression of uncertainty), <lb/> L2 (high confidence or slight speculation) and L1 <lb/> (low confidence or considerable speculation). <lb/> Polarity: Identifies negated events. Negation is <lb/>defined as the absence or non-existence of an <lb/>entity or a process. <lb/> Manner: Captures information about the rate, <lb/>level, strength or intensity of the event, using three <lb/>values: High, Low, or Neutral (no indication of <lb/>rate/intensity). <lb/> Source: Encodes the source of the knowledge <lb/>being expressed by the event as Current (the <lb/>current study) or Other (any other source). <lb/>Of these five dimensions, only KT, CL and <lb/> Source were considered during the comparison <lb/>with the other two schemes, since they are directly <lb/>related to discourse analysis. <lb/>The GENIA event corpus, consisting of 1000 <lb/>abstracts with 36,115 events (Kim et al., 2008) has <lb/>been annotated with meta-knowledge by 2 <lb/>annotators, supported by 64-page annotation <lb/>guidelines 3 (Thompson et al., 2011). Inter-<lb/>annotator agreement rates ranged between 0.84– <lb/>0.93 (Cohen&apos;s Kappa). Research has been carried <lb/>out into the automatic assignment of Manner <lb/>values to events (Nawaz et al., In Press). In <lb/>addition, the EventMine-MK service (Miwa et al., <lb/>In Press), based on EventMine (Miwa et al., 2010) <lb/>facilitates automatic extraction of biomedical <lb/>events with meta-knowledge assigned. The <lb/>performance of EventMine-MK in assigning <lb/>different meta-knowledge values to events ranges <lb/>between 57% and 87% (macro-averaged F-Score) <lb/>on the BioNLP&apos;09 Shared Task corpus (Kim et al, <lb/>2011). EventMine-MK is available as a component <lb/>of the U-Compare interoperable text mining <lb/>system 4 (Kano et al., 2011). <lb/> 4 Clause annotation: Segments for <lb/>epistemic knowledge <lb/> The third scheme we consider uses a Discourse <lb/>Segment Type classification of segments at, <lb/>roughly, a clause level, i.e., each segment has a <lb/>main verb. This means that the level of granularity <lb/>of argumentational elements in this scheme lies <lb/>between the other two schemes, i.e. it is usually <lb/>more granular than CoreSC, but sometimes less <lb/>granular <lb/>than <lb/>the <lb/>event-based <lb/>scheme. <lb/>

			<note place="footnote"> 3 http://www.nactem.ac.uk/meta-knowledge/ <lb/></note> 
			
			<note place="footnote">4 http://www.nactem.ac.uk/ucompare/ <lb/></note>

			<page> 40 <lb/></page>

			Table 2: Discourse Segment Types <lb/> The segment annotation scheme identifies a <lb/>taxonomy of discourse segment types that seem to <lb/>be exclusive and useful (de Waard &amp; Pander Maat, <lb/>2009). Three classes of segment types are defined: <lb/> − Basic segment types: segments referring <lb/>directly to the topic of study – see Table 2. <lb/> − &apos;Other&apos;-segment types: segments referring to <lb/>conceptual or experimental work in other <lb/>research papers than the current one <lb/> − Regulatory segment types: &apos;regulatory&apos; clauses <lb/>that control and introduce other segments. <lb/>A list of segment types is presented in Table 2; <lb/>further details, including a list of all segment types <lb/>and correlations with verb tense can be found in de <lb/>Waard &amp; Pander Maat (2009). The focus of this <lb/>work is to identify linguistic features that <lb/>characterise these discourse segment types, <lb/>according to three aspects: <lb/> − Verb tense, aspect, mood and voice <lb/> − Semantic verb class <lb/> − Epistemic modality markers <lb/>So far, 6 full-text papers (comprising about 2300 <lb/>segments) have been manually annotated with <lb/>segment types and correlated with the above <lb/>features. A first automated validation was <lb/>promising (de Waard, Buitelaar and Eigener, <lb/>2009). The need for parsing at a clause level is <lb/>especially prominent in biological text, since <lb/>specific semantic roles are played by particular <lb/>clause types. We give four examples of typical <lb/>clause constructions that play a specific rhetorical <lb/>role: firstly, reporting clauses are often sentence-<lb/>initial &apos;that&apos; matrix clauses (1a): <lb/> 1. a. This suggests that <lb/>1.b. miR-372 and miR-373 caused the observed <lb/>selective growth advantage. <lb/> Secondly, descriptions confirming certain <lb/>accepted characteristics of biological entities are <lb/>often given as nonrestrictive relative clauses (2b): <lb/> 2.a. We also generated BJ/ET cells expressing the <lb/>RASV12-ERTAM chimera gene, <lb/>2. b. which is only active when tamoxifen is <lb/>added <lb/> Thirdly, a subordinate gerund clause is often <lb/>used to describe a method (3a), with a main (finite) <lb/>clause describing a result (3b) and fourthly, <lb/>experimental goals are often given as a (mostly <lb/>sentence-initial) clause with a to-infinitive (4a) <lb/>often preceding a past-tense methods clause (4b). <lb/> 3. a. Using fluorescence microscopy and luciferase <lb/>assays, <lb/>b. we observed potent and specific miRNA activity <lb/>expressed from each miR-Vec (Figure S2). <lb/>4. a. To identify miRNAs that can interfere with <lb/>this process <lb/>4. b. we transduced BJ/ET fibroblasts with miR-Lib <lb/> However, the lack of simple robust clause <lb/>parsers has prevented the automated identification <lb/>of semantic roles at the clause level. Therefore, this <lb/>scheme has so far only been manually <lb/> Segment <lb/> Description <lb/>Examples <lb/> Fact <lb/>knowledge accepted to be <lb/>true, a known fact. <lb/> mature miR-373 is a homolog of miR-372, <lb/> Hypothesis <lb/>a proposed idea, not <lb/>supported by evidence <lb/> This could for instance be a result of high mdm2 levels <lb/> Problem <lb/>unresolved, contradictory, <lb/>or unclear issue <lb/> However, further investigation is required to demonstrate the exact <lb/>mechanism of LATS2 action <lb/> Goal <lb/>research goal <lb/> To identify novel functions of miRNAs, <lb/> Method <lb/>experimental method <lb/> Using fluorescence microscopy and luciferase assays, <lb/> Result <lb/>a restatement of the <lb/>outcome of an experiment <lb/> all constructs yielded high expression levels of mature miRNAs <lb/> Implication <lb/>an interpretation of the <lb/>results, in light of data <lb/> our procedure is sensitive enough to detect mild growth differences <lb/> Other-<lb/>Hypothesis <lb/>an idea proposed by <lb/>others <lb/> [It is generally believed that] transcription factors are the final <lb/>common pathway driving differentiation] <lb/> Regulatory-<lb/>Hypothesis <lb/>a matrix clause <lb/>introducing a hypothesis <lb/> It is generally believed that [transcription factors are the final <lb/>common pathway driving differentiation] <lb/>

			<page> 41 <lb/></page>

			implemented. Despite being less widely <lb/>implemented than the other two schemes, we <lb/>believe that the segment scheme offers some useful <lb/>pointers for linguistic features that can identify <lb/>particular rhetorical classes in the text, and <lb/>secondly, offers an interesting perspective on the <lb/>fact that in biological text, several rhetorical moves <lb/>are made within a single sentence. <lb/> 5 Data and methods <lb/> Three papers already annotated according to the <lb/>GENIA event annotation scheme (Kim et al., <lb/>2008), were further annotated according to the <lb/>three annotation schemes described above. We <lb/>obtained all corresponding CoreSCs, events and <lb/>segments per sentence. Each sentence has a single <lb/>CoreSC annotation and one or more segment <lb/>annotations (depending on the number of clauses). <lb/>Event annotations in a sentence may range from <lb/>zero to multiple, according to whether any relevant <lb/>biomedical events are described in the sentence. <lb/>Events within a sentence are mapped to <lb/>segments by identifying which segment contains <lb/>the trigger for a particular event. The three meta-<lb/>knowledge dimensions for events considered in <lb/>this comparison, i.e., KT, CL and Source, result in <lb/>16 different combinations of values encountered in <lb/>the three papers. The numbers for CoreSC and <lb/>Segment labels encountered were 12 and 22, <lb/>respectively. Confusion matrices were obtained for <lb/>each paper and for each pair of annotation <lb/>schemes. Note that, as bio-events are largely <lb/>unconcerned with describing methodology, the <lb/> Methods sections of these papers do not contain <lb/>event annotation or meta-knowledge annotation. <lb/>The pairwise confusion matrices from each paper <lb/>were combined, resulting in three matrices (Tables <lb/>3, 4 and 5), which describe the associations <lb/>between the annotation schemes in the three papers <lb/>examined. We have highlighted the highest <lb/>frequencies per row and where appropriate also the <lb/>highest values per column. The use of two <lb/>different colours aims to facilitate readability. <lb/> 6 Results and Discussion <lb/> We present the results from analysing the pairwise <lb/>confusion matrices for the three schemes and <lb/>discuss the merits of each scheme. <lb/> 6.1 Event Meta-knowledge v. CoreSC <lb/> In Tables 3 (and 5), the meta-knowledge categories <lb/>combine KT, CL and Source ((O)ther) values. <lb/>Table 3 shows some straightforward and expected <lb/>mappings, e.g.,Method (Met,L3) events are almost <lb/>always found within CoreSC Experiment or <lb/>Method sentences, whilst Investigation events <lb/>(Inv,L3) occur most frequently within CoreSC <lb/>Goal or Motivation sentences. <lb/>For other categories, information from the two <lb/>schemes can complement each other in different <lb/>ways. For example, KT and Source information <lb/>about events can help to distinguish different types <lb/>of information within CoreSC Background <lb/>sentences (top left corner of Table 3). Such <lb/>information mainly corresponds to facts, <lb/>observations from previous studies, or analyses of <lb/>information. Conversely, information from the <lb/>CoreSC scheme can help to further classify the <lb/>interpretation of events. For example, events with <lb/>an analytical interpretation (Ana,L1,L2,L3) may <lb/>occur as background information to a study (Bac), <lb/>as hypotheses (Hyp), as part of observations <lb/>(Obs), when reporting the results of the current <lb/>study (Res) or when making concluding remarks <lb/>about the study (Con). CoreSCs can also help to <lb/>further refine events relating to outcomes (Obs,L3) <lb/>according <lb/>to <lb/>whether <lb/>they <lb/>pertain <lb/>to <lb/>(Obs)ervations, (Res)ults or (Con)clusions. <lb/>CoreSC Conclusion, Result and Observation <lb/>sentences contain mainly Observation events <lb/>concerned with the current study. However, such <lb/>sentences often also include an analytical part, with <lb/>varying levels of certainty, which event <lb/>information can help to isolate. The CL annotated <lb/>for events is also useful in helping to determine the <lb/>confidence with which information is stated in <lb/>CoreSC Conclusion and Hypothesis sentences. <lb/>Due to the nature of bio-event annotation, only a <lb/>small number of events correspond to methods. <lb/>Thus, CoreSC provides a more detailed <lb/>characterisation of method-related sentences, i.e., <lb/>Experiment, Method_New, Model and Object. <lb/> 6.2 Discourse Segments v. CoreSC <lb/> In most cases, there seems to be natural mapping <lb/>between the two schemes (See Table 4). CoreSC <lb/>Observation maps to Result, CoreSC Method and <lb/>Experiment map to Method, CoreSC Hypothesis <lb/>maps to Hypothesis, CoreSC Goal maps to Goal, <lb/>

			<page> 42 <lb/></page>

			CoreSC Conclusion maps to Implication and <lb/> Hypothesis, CoreSC Result maps to Implication <lb/> and Result, and Problem is equivalent to CoreSC <lb/>Motivation. The bulk of CoreSC Background maps <lb/>to Fact and Other-Implication, but the &quot; Other &quot; <lb/>Segment categories provide a substantial <lb/>refinement of the CoreSC Background category. <lb/> Table 3. Event Meta-knowledge vs CoreSC <lb/> On the other hand, CoreSC refines Method, <lb/>Result and Implication segments. CoreSC Result <lb/>may include both Fact and Method clauses, which <lb/>can be captured by the Segment scheme, since <lb/>annotation is performed at the clause level. CoreSC <lb/>Conclusion maps to both Implication and <lb/> Hypothesis segments, suggesting that there may be <lb/>differences in the certainty levels of these <lb/>conclusions. This is supported by preliminary <lb/>classification experiments (paper in progress). <lb/> 6.3 Discourse Segments v. Event Meta-<lb/>Knowledge <lb/> Some straightforward mappings exist between <lb/>segment and event meta-knowledge categories <lb/>(Table 5). For example, Investigation events (Inv, <lb/>L3) are generally found within Goal and Problem <lb/> segments; Method events (Met,L3) are normally <lb/>found within Method segments, Observation events <lb/>(Obs,L3) are found mainly within Result, Fact and <lb/> Implication segments and (Ana,L1,L2) events <lb/>correspond mainly to Hypotheses and Implications. <lb/>Whilst these are similar findings to the <lb/>comparison between event meta-knowledge and <lb/>CoreSCs, the variance of the distribution is often <lb/>smaller when mapping from Events to Segments. <lb/>This is to be expected – the information encoded <lb/>by many events has the scope of roughly a clause, <lb/>which corresponds closely to the scope of <lb/>discourse segments. This could permit cleaner one-<lb/>to-one mappings between categories. <lb/> Table 4: Segments vs CoreSC <lb/> Hypothesis and Implication segments mainly <lb/>contain (Ana)lysis events. The differing certainty <lb/>levels of events can help to refine information <lb/>about the statements made within these segments. <lb/>Likewise, these segment types could help to refine <lb/>the nature of the analysis described by the event. <lb/>Similarly to the CoreSC scheme, the results <lb/>suggest that Result segments could be refined by <lb/>the meta-knowledge scheme to distinguish <lb/>between results emerging from direct experimental <lb/>observations, and those obtained through analysis <lb/>of experimental observations. Another interesting <lb/>result is that Fact segments can contain Fact, <lb/>(Ana)lysis or (Obs)ervation events. This may <lb/>suggest that Fact segments are actually a rather <lb/>general category, containing a range of different <lb/>information. Few events occur within the <lb/> Regulatory segments, as these mainly introduce <lb/>content-bearing segments. <lb/>The majority of Method segments and a <lb/>significant number of the Result segments do not <lb/>correspond to events, as none of the methods <lb/>sections have been annotated with event <lb/>information, for reasons explained previously. <lb/> Table 5: Segments vs Event Meta-Knowledge <lb/> Sheet1 <lb/> Page 1 <lb/> Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs Res <lb/>0 <lb/> 42 <lb/> 24 49 <lb/>7 <lb/>7 <lb/>25 <lb/>1 <lb/>13 <lb/>6 <lb/>7 47 54 <lb/> Obs,L3,O 166 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>3 <lb/>0 12 <lb/>0 <lb/>0 <lb/>2 <lb/> Ana,L3,O <lb/> 33 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> Ana,L2,O <lb/> 3 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> Fact,L3,O <lb/> 7 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> Fact,L3 <lb/> 24 <lb/>1 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>5 <lb/>3 <lb/>0 <lb/>2 <lb/> Oth,L3 <lb/> 125 <lb/>30 <lb/>0 <lb/>8 16 <lb/>5 <lb/>3 <lb/>2 <lb/>8 <lb/>3 <lb/>9 42 <lb/> Ana,L1 <lb/> 2 <lb/>10 <lb/>0 <lb/>0 <lb/>6 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>6 <lb/> Ana,L2 <lb/> 30 <lb/>15 <lb/>0 <lb/>1 14 <lb/>0 <lb/>0 <lb/>2 <lb/>1 <lb/>0 <lb/>8 33 <lb/> Ana,L3 <lb/> 11 <lb/>11 <lb/>0 <lb/>0 <lb/>2 <lb/>1 <lb/>2 <lb/>0 <lb/>3 <lb/>0 14 28 <lb/> Met,L3 <lb/> 4 <lb/>1 15 <lb/>1 <lb/>0 <lb/>5 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>6 <lb/> Inv,L2 <lb/> 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>1 <lb/> Inv,L3 <lb/> 5 <lb/>3 <lb/>1 <lb/>6 <lb/>2 <lb/>4 <lb/>3 <lb/>0 <lb/>8 <lb/>0 <lb/>1 <lb/>1 <lb/> Inv,L3,O <lb/> 0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> Obs,L1 <lb/> 1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> Obs,L2 <lb/> 1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/> Obs,L3 <lb/> 31 <lb/>34 <lb/>3 <lb/>1 10 <lb/>3 <lb/>0 <lb/>2 <lb/>7 <lb/>1 59 87 <lb/> Sheet1 <lb/> Page 1 <lb/> Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs Res <lb/>Fact <lb/> 118 <lb/> 3 <lb/>0 <lb/>3 <lb/>7 <lb/>0 <lb/>0 <lb/>1 15 <lb/>7 <lb/>5 34 <lb/> OtherFact <lb/> 70 <lb/>4 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>3 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/> OtherGoal <lb/> 2 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> OtherHypothesis <lb/> 14 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> OtherImplication <lb/> 124 <lb/>1 <lb/>0 <lb/>0 <lb/>3 <lb/>0 <lb/>0 <lb/>1 <lb/>5 <lb/>0 <lb/>0 <lb/>1 <lb/> OtherMethod <lb/> 5 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>3 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/> OtherProblem <lb/> 1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> OtherResult <lb/> 64 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>6 <lb/>0 <lb/>0 <lb/>3 <lb/>0 <lb/>9 <lb/> RegFact <lb/> 1 <lb/>3 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/> Implication <lb/> 13 58 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>0 <lb/>3 <lb/>1 <lb/>0 <lb/>3 80 <lb/> RegImplication <lb/> 5 <lb/>6 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 10 <lb/> Method <lb/> 6 <lb/>2 54 <lb/>2 <lb/>2 <lb/>32 <lb/>0 <lb/>6 <lb/>1 <lb/>0 <lb/>8 13 <lb/> Goal <lb/> 2 <lb/>0 <lb/>5 12 <lb/>6 <lb/>9 <lb/>2 <lb/>2 <lb/>4 <lb/>0 <lb/>0 <lb/>5 <lb/> RegGoal <lb/> 0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> Hypothesis <lb/> 24 31 <lb/>0 <lb/>5 34 <lb/>1 <lb/>0 <lb/>5 <lb/>0 <lb/>0 <lb/>0 12 <lb/> RegHypothesis <lb/> 6 <lb/>4 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/> Problem <lb/> 7 <lb/>6 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>0 11 <lb/>0 <lb/>0 <lb/>2 <lb/> RegProblem <lb/> 0 <lb/>3 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/> Result <lb/> 13 <lb/>6 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>0 <lb/>2 <lb/>8 <lb/>0 112 75 <lb/> RegResult <lb/> 1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/> Intertextual <lb/> 4 <lb/>0 <lb/>7 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>3 <lb/> Intratextual <lb/> 2 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>0 <lb/>8 <lb/>4 <lb/> Sheet1 <lb/> 0 Ana Ana Ana Ana Ana Fact Fact Met Oth Inv Inv Inv Obs Obs Obs Obs <lb/>L1 L2 L2,O L3 L3,O L3 L3,O L3 L3 L2 L3 L3,O L1 L2 L3 L3,O <lb/>Hypothesis <lb/> 8 18 26 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 39 0 4 <lb/>1 0 0 14 <lb/>0 <lb/> Implication <lb/> 22 2 30 <lb/>0 34 <lb/>2 <lb/>2 <lb/>0 <lb/>0 38 2 1 <lb/>0 0 0 27 <lb/>0 <lb/> OtherHypothesis <lb/> 0 0 3 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>9 0 1 <lb/>0 0 0 0 <lb/>0 <lb/> OtherImplication <lb/> 8 1 6 <lb/>1 <lb/>4 28 <lb/>0 <lb/>3 <lb/>3 27 0 2 <lb/>0 1 0 5 46 <lb/> RegImplication <lb/> 11 0 2 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>5 0 1 <lb/>0 0 0 3 <lb/>0 <lb/> RegHypothesis <lb/> 1 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>6 0 1 <lb/>0 0 0 7 <lb/>0 <lb/> Fact <lb/> 15 0 18 <lb/>0 <lb/>6 <lb/>0 28 <lb/>0 <lb/>0 55 0 1 <lb/>0 0 1 44 25 <lb/> RegFact <lb/> 0 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 0 0 <lb/>0 0 0 5 <lb/>0 <lb/> OtherGoal <lb/> 1 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 0 0 <lb/>0 0 0 0 <lb/>0 <lb/> OtherProblem <lb/> 0 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 0 0 <lb/>0 0 0 0 <lb/>0 <lb/> Method <lb/> 80 0 1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 23 <lb/>9 0 2 <lb/>0 0 0 8 <lb/>3 <lb/> OtherMethod <lb/> 7 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 0 2 <lb/>1 0 0 0 <lb/>0 <lb/> Goal <lb/> 13 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>0 18 0 11 <lb/>1 0 0 3 <lb/>0 <lb/> RegGoal <lb/> 1 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 0 0 <lb/>0 0 0 0 <lb/>0 <lb/> Problem <lb/> 9 4 0 <lb/>0 <lb/>2 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>5 0 8 <lb/>0 0 0 0 <lb/>0 <lb/> RegProblem <lb/> 3 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 0 0 <lb/>0 0 0 0 <lb/>0 <lb/> Result <lb/> 51 0 14 <lb/>0 20 <lb/>0 <lb/>0 <lb/>0 <lb/>6 18 0 0 <lb/>0 0 1 103 <lb/>7 <lb/> OtherResult <lb/> 11 0 1 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>1 <lb/>0 10 0 0 <lb/>0 0 0 12 47 <lb/> OtherFact <lb/> 4 0 1 <lb/>0 <lb/>0 <lb/>2 <lb/>5 <lb/>3 <lb/>0 <lb/>7 0 0 <lb/>0 0 0 2 54 <lb/> RegResult <lb/> 5 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 0 0 <lb/>0 0 0 0 <lb/>0 <lb/> Intertextual <lb/> 13 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 0 0 <lb/>0 0 0 1 <lb/>0 <lb/> Intratextual <lb/> 17 0 0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 0 0 <lb/>0 0 0 0 <lb/>0 <lb/> 280 4 35 <lb/>0 28 <lb/>3 34 <lb/>4 29 127 0 24 <lb/>2 0 2 178 136 <lb/> 

			<page>43 <lb/></page>

			7 Related Work <lb/> A number of schemes for annotating scientific <lb/> discourse elements at the sentence level have been <lb/>proposed. Certain schemes have been aimed at <lb/>abstracts, e.g., (McKnight &amp; Srinivasan, 2003; <lb/>Ruch et al., 2007; Hirohata et al., 2008; Björne et <lb/>al., 2009). The work of Hirohata et al. (2009) has <lb/>been integrated with the MEDIE service 5 (Miyao et <lb/>al., 2006), allowing the user to query facts using <lb/>conclusions, results, etc. For full papers, the most <lb/>notable work has focussed on argumentative <lb/>zoning (AZ) (Teufel et al., 1999; Teufel &amp; <lb/>Moens, 2002; Teufel et al., 2009; Teufel, 2010). <lb/>An important aspect of AZ involves capturing the <lb/>attribution of knowledge claims and citation <lb/>function, and the scheme has been tested on <lb/>information extraction and summarisation tasks <lb/>with Computational Linguistics papers. AZ was <lb/>modified for the annotation of biology papers by <lb/>Mizuta et al. (2005) in order to facilitate <lb/>information extraction, and more recently Teufel et <lb/>al. (2009) extended the AZ scheme to better <lb/>accommodate the life sciences and chemistry in <lb/>particular, producing AZ-II. <lb/>Scientific discourse annotation has also targeted <lb/>the retrieval of speculative text to help improve <lb/>curation. For a recent overview see de Waard and <lb/>Pander Maat (2012). Modality and negation in text <lb/>have also been the focus of recent workshops <lb/>(Farkas et al (2010), Morante &amp; Sporleder (2012)). <lb/>Finally, Shatkay et al (2008) define a multi-<lb/>dimensional scheme, which combines several of <lb/>the above-mentioned aspects. <lb/>Recent work has compared schemes to discover <lb/>mappings and relative merits. Liakata et al. (2010) <lb/>compared AZ-II and CoreSC on 36 papers <lb/>annotated with both schemes and found that <lb/>CoreSC provides finer granularity in distinguishing <lb/>content categories (e.g. methods, goals and <lb/>outcomes) while the strength of AZ-II lies in <lb/>detecting the attribution of knowledge claims and <lb/>identifying the different functions of background <lb/>information. Guo et al. (2010) compared three <lb/>schemes for the identification of discourse <lb/>structure in scientific abstracts from cancer <lb/>research assessment articles. The work showed a <lb/>subsumption relation between the scheme of <lb/>Hirohata et al. (2008), a cut-down version of the <lb/>

			<note place="footnote"> 5 http://www.nactem.ac.uk/medie/ <lb/></note> 
			
			scheme proposed by Teufel et al. (2009) and <lb/>CoreSC (1 st layer), from general to specific. <lb/> 8 Conclusion <lb/> We have compared three different schemes, each <lb/>taking a different perspective to the annotation of <lb/>scientific discourse. The comparison shows that <lb/>the three schemes are complementary, with <lb/>different strengths and points of focus. CoreSC <lb/>offers a fine-grained characterisation of methods, <lb/>outcomes and objectives. It has been used to <lb/>annotate a collection of 265 full papers, and <lb/>subsequently CoreSC recognition has been fully <lb/>automated, creating the online SAPIENTA tool. <lb/>The discourse segment annotation scheme can help <lb/>to provide a finer-grained characterisation of <lb/>background work, and could also help to split <lb/>multi-clause CoreSC sentences into appropriate <lb/>segments. Recognition of event meta-knowledge <lb/>has been fully automated in the U-Compare <lb/>framework, and the KT values of the scheme can <lb/>help to provide a finer-grained analysis of certain <lb/>segment and sentence types. The CL dimension <lb/>also allows confidence values to be ascribed to the <lb/>Conclusion, Result, Implication and Hypothesis <lb/>categories of the other two schemes. <lb/>Future work will focus on annotating texts with <lb/>several discourse perspectives to investigate the <lb/>advantages of the schemes. Ideally we would like <lb/>to propose a unified approach for scientific <lb/>discourse annotation, but recognize that choices <lb/>such as the unit of annotation are often task-<lb/>oriented, and that users should be able to mix and <lb/>match discourse segments as required. This said, <lb/>the analysis in this paper paves the way for <lb/>potential harmonisation, revealing points of union <lb/>and intersection between the schemes. <lb/>
		
		</body>

		<back>

			<div type="acknowledgement"> Acknowledgements <lb/> This work has been supported through funding for <lb/>Maria Liakata by JISC, the Leverhulme Trust and <lb/>EBI-EMBL. It has also been supported by the <lb/>BBSRC through grant number BB/G013160/1UK <lb/>(Automated Biological Event Extraction from the <lb/>Literature for Drug Discovery), the MetaNet4U <lb/>project (ICT PSP Programme, Grant Agreement: <lb/>No. 270893) and the JISC-funded ISHER project. <lb/></div>

			<page> 44 <lb/></page>

			<listBibl> References <lb/> Ananiadou, S., Kell, D.B. and Tsujii, J. (2006). Text <lb/>mining and its potential applications in systems <lb/>biology. Trends Biotechnol, 24(12): 571-9. <lb/>Ananiadou, S. and McNaught, J., Eds. (2006). Text <lb/>Mining for Biology and Biomedicine. Boston / <lb/>London, Artech House. <lb/>Ananiadou, S., Pyysalo, S., Tsujii, J. and Kell, D.B. <lb/>(2010). Event extraction for systems biology <lb/>by text mining the literature. Trends <lb/>Biotechnol, 28(7): 381-90. <lb/>Björne, J., Heimonen, J., Ginter, F., Airola, A., <lb/>Pahikkala, T. and Salakoski, T. (2009). <lb/>Extracting Complex Biological Events with <lb/>Rich <lb/>Graph-Based <lb/>Feature <lb/>Sets. <lb/>In <lb/> Proceedings of the BioNLP 2009 Workshop <lb/>Companion Volume for Shared Task, pp. 10-<lb/>18. <lb/>Blake, C. (2010). Beyond genes, proteins, and abstracts: <lb/>Identifying scientific claims from full-text <lb/>biomedical articles. Journal of Biomedical <lb/>Informatics, 43(2): 173-189. <lb/>Cohen, J. (1960). A coefficient of agreement for <lb/>nominal scales. Educational and psychological <lb/>measurement, 20: 37-46. <lb/>Cohen, K.B. and Hunter, L. (2008). Getting started in <lb/>text mining. PLoS Comput Biol, 4(1): e20. <lb/>Cohen, K.B., Johnson, H.L., Verspoor, K., Roeder, C. <lb/>and Hunter, L.E. (2010). The structural and <lb/>content aspects of abstracts versus bodies of <lb/>full text journal articles are different. BMC <lb/>Bioinformatics, 11: 492. <lb/>de Waard, A., Buitelaar, P., Eigner, T. (2009). <lb/> Identifying the epistemic value of discourse <lb/>segments in biology texts. Proceedings of the <lb/>Eighth <lb/>International <lb/>Conference <lb/>on <lb/>Computational Semantics, pp. 351-354 <lb/>de Waard, A. and Pander Maat, H. (2009). Categorizing <lb/>Epistemic Segment Types in Biology Research <lb/>Articles. In Proceedings of the Workshop on <lb/>Linguistic and Psycholinguistic Approaches to <lb/>Text Structuring (LPTS 2009) <lb/> de Waard, A. and Pander Maat, H. (2012). Knowledge <lb/>Attribution in Scientific Discourse: A <lb/>Taxonomy of Types and Overview of Features, <lb/>In Proceedings of the Workshop on Detecting <lb/>Structure in Scholarly Discourse (DSDD), <lb/> ACL 2012. <lb/>Farkas, R.	<lb/> Vincze, V., Móra, G., Csirik, J. and Szarvas, <lb/>G. 2010. The CoNLL-2010 Shared Task: <lb/>Learning to Detect Hedges and their Scope in <lb/>Natural Language Text. In Proceedings of the <lb/>Fourteenth Conference on Computational <lb/>Natural Language Learning, Uppsala, Sweden. <lb/>Association for Computational Linguistics, pp. <lb/>1-12. <lb/>Guo, Y., Korhonen, A., Liakata, M., Silins, I., LiSun, L. <lb/>and Stenius, U. (2010). Identifying the <lb/>information structure of scientific abstracts: An <lb/>investigation of three different schemes. In <lb/> Proceedings of BioNLP 2010, pp. 99-107. <lb/>Hirohata, K., Okazaki, N., Ananiadou, S. and Ishizuka, <lb/>M. (2008). Identifying Sections in Scientific <lb/>Abstracts using Conditional Random Fields. In <lb/> Proceedings of the 3rd International Joint <lb/>Conference on Natural Language Processing, <lb/> pp. 381-388. <lb/>Hyland, K. (1996). Writing without conviction? <lb/>Hedging in science research articles. Applied <lb/>Linguistics, 17(4): 433-454. <lb/>Kano, Y., Miwa, M., Cohen, K.B., Hunter, L.E., <lb/>Ananiadou, S. and Tsujii, J. (2011). U-<lb/>Compare: A modular NLP workflow <lb/>construction and evaluation system. IBM <lb/>Journal of Research and Development, 55(3): <lb/>11:1-11:10. <lb/>Kilicoglu, H. and Bergler, S. (2008). Recognizing <lb/>speculative language in biomedical research <lb/>articles: a linguistically motivated perspective. <lb/> BMC Bioinformatics, 9(Suppl 11): S10. <lb/>Kim, J.-D., Ohta, T. and Tsujii, J. (2008). Corpus <lb/>annotation for mining biomedical events from <lb/>literature. BMC Bioinformatics, 9(10). <lb/>Kim, J.D., Ohta, T., Pyysalo, S., Kano, Y. and Tsujii, J. <lb/>(2011). Extracting Bio-Molecular Events from <lb/>Literature -The BioNLP&apos;09 Shared Task. <lb/> Computational Intelligence, 27(4): 513-540. <lb/>Liakata, M., Saha, S., Dobnik, S., Batchelor, C. and <lb/>Rebholz-Schuhmann, D. (2012). Automatic <lb/>recognition of conceptualisation zones in <lb/>scientific articles and two life science <lb/>applications. Bioinformatics, 28 (7). <lb/>Liakata, M. and Soldatova, L.N. (2009). The ART <lb/>corpus. <lb/>Technical <lb/>Report. <lb/>Aberystwth <lb/>University. <lb/>Liakata, M., Teufel, S., Siddharthan, A. and Batchelor, <lb/>C. (2010). Corpora for the conceptualisation <lb/>and zoning of scientific papers. In Proceedings <lb/>of LREC, pp. 2054-2061. <lb/>Light, M., Qiu, X.Y. and Srinivasan, P. (2004). The <lb/>language of bioscience: Facts, speculations, <lb/>and statements in between. In Proceedings of <lb/>the BioLink 2004 Workshop at HLT/NAACL, <lb/> pp. 17–24. <lb/>McKnight, L. and Srinivasan, P. (2003). Categorization <lb/>of sentence types in medical abstracts. In AMIA <lb/>Annu Symp Proc, pp. 440-4. <lb/>Miwa, M., Saetre, R., Kim, J.D. and Tsujii, J. (2010). <lb/>Event extraction with complex event <lb/></listBibl>

			<page> 45 <lb/></page>

			<listBibl> classification using rich features. J Bioinform <lb/>Comput Biol, 8(1): 131-46. <lb/>Miwa, M., Thompson, P. and Ananiadou, S. (2012). <lb/>Boosting automatic event extraction from the <lb/>literature using domain adaptation and <lb/>coreference resolution. Bioinformatics. <lb/> Miwa, M., Thompson, P., McNaught, J, Kell, D.B and <lb/>Ananiadou, S. (In Press). Extracting <lb/>semantically enriched events from biomedical <lb/>literature. BMC Bioinformatics. <lb/> Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., <lb/>Yoshida, K., Ninomiya, T. and Tsujii, J. <lb/>(2006). Semantic Retrieval for the Accurate <lb/>Identification of Relational Concepts in <lb/>Massive Textbases. In Proceedings of ACL, pp. <lb/>1017-1024. <lb/> Mizuta, Y., Korhonen, A., Mullen, T. and Collier, N. <lb/>(2005). Zone Analysis in Biology Articles as a <lb/>Basis for Information Extraction. International <lb/> Journal of Medical Informatics,75(6): 468-487. <lb/> Morante R., and Sporleder C, (2012). Modality and <lb/>negation: An introduction to the special issue. <lb/> Computational Linguistics, 38(2): 1–38. <lb/>Nawaz, R., Thompson, P. and Ananiadou, S. (In Press). <lb/>Identification of Manner in Bio-Events. <lb/> Proceedings of the Eighth International <lb/>Conference on Language Resources and <lb/>Evaluation (LREC 2012). <lb/> Nawaz, R., Thompson, P., McNaught, J. and <lb/>Ananiadou, S. (2010). Meta-Knowledge <lb/>Annotation of Bio-Events. In Proceedings of <lb/>LREC 2010, pp. 2498-2507. <lb/>Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J., <lb/>Boberg, J., Jarvinen, J. and Salakoski, T. <lb/>(2007). BioInfer: a corpus for information <lb/>extraction in the biomedical domain. BMC <lb/>Bioinformatics, 8: 50. <lb/>Pyysalo, S., Ohta, T., Rak, R., Sullivan, D., Mao, C., <lb/>Wang, C., Sobral, B., Tsujii, J. and Ananiadou, <lb/>S. (In Press). Overview of the ID, EPI and REL <lb/>tasks of BioNLP Shared Task 2011. BMC <lb/>Bioinformatics. <lb/> Quirk, C., Choudhury, P., Gamon, M. and <lb/>Vanderwende, L. (2011). MSR-NLP Entry in <lb/>BioNLP Shared Task 2011. In Proceedings of <lb/>BioNLP Shared Task 2011 Workshop, pp. <lb/>155-163. <lb/>Ruch, P., Boyer, C., Chichester, C., Tbahriti, I., <lb/>Geissbuhler, A., Fabry, P., Gobeill, J., Pillet, <lb/>V., Rebholz-Schuhmann, D., Lovis, C. and <lb/>Veuthey, A.L. (2007). Using argumentation to <lb/>extract key sentences from biomedical <lb/>abstracts. Int J Med Inform, 76(2-3): 195-200. <lb/>Sándor, Á. (2007). Modeling metadiscourse conveying <lb/>the author&apos;s rhetorical strategy in biomedical <lb/>research abstracts. Revue Française de <lb/>Linguistique Appliquée, 200(2): 97-109. <lb/>Shatkay, H., Pan, F., Rzhetsky, A. and Wilbur, W.J. <lb/>(2008). Multi-dimensional classification of <lb/>biomedical text: toward automated, practical <lb/>provision of high-utility text to diverse users. <lb/> Bioinformatics, 24(18): 2086-2093. <lb/>Soldatova, L.N. and King, R.D. (2006). An ontology of <lb/>scientific experiments. Journal of the Royal <lb/>Society Interface, 3(11): 795-803. <lb/>Soldatova, L.N. and Liakata, M. (2007). An ontology <lb/>methodology and cisp-the proposed core <lb/>information <lb/>about <lb/>scientific <lb/>papers., <lb/>Aberystwyth University. Technical Report <lb/>JISC Project Report. <lb/>Teufel, S. (2010). The Structure of Scientific Articles: <lb/>Applications to Citation Indexing and <lb/>Summarization. <lb/> Stanford, <lb/>CA, <lb/>CSLI <lb/>Publications. <lb/>Teufel, S., Carletta, J. and Moens, M. (1999). An <lb/>annotation <lb/>scheme <lb/>for <lb/>discourse-level <lb/>argumentation in research articles. In <lb/> Proceedings of EACL, pp. 110-117. <lb/>Teufel, S. and Moens, M. (2002). Summarizing <lb/>scientific articles: experiments with relevance <lb/>and <lb/>rhetorical <lb/>status. <lb/> Computational <lb/>Linguistics, 28(4): 409-445. <lb/>Teufel, S., Siddharthan, A. and Batchelor, C. (2009). <lb/>Towards discipline-independent argumentative <lb/>zoning: Evidence from chemistry and <lb/>computational linguistics. In Proceedings of <lb/>EMNLP 2009, pp. 1493-1502. <lb/>Thompson, P., Iqbal, S.A., McNaught, J. and <lb/>Ananiadou, S. (2009). Construction of an <lb/>annotated corpus to support biomedical <lb/>information extraction. BMC Bioinformatics, <lb/> 10: 349. <lb/>Thompson, P., Nawaz, R., McNaught, J. and <lb/>Ananiadou, S. (2011). Enriching a biomedical <lb/>event corpus with meta-knowledge annotation. <lb/> BMC Bioinformatics, 12: 393. <lb/>Vincze, V., Szarvas, G., Farkas, R., Mora, G. and <lb/>Csirik, J. (2008). The BioScope corpus: <lb/>biomedical texts annotated for uncertainty, <lb/>negation <lb/>and <lb/>their <lb/>scopes. <lb/> BMC <lb/>Bioinformatics, 9(Suppl 11): S9. <lb/>Zweigenbaum, P., Demner-Fushman, D., Yu, H. and <lb/>Cohen, K.B. (2007). Frontiers of biomedical <lb/>text mining: current progress. Briefings in <lb/>Bioinformatics, 8(5): 358-375. <lb/></listBibl>

			<page> 46 </page>

		</back>
	</text>
</tei>
