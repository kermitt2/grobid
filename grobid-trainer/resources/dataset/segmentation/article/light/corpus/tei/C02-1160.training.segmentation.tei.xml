<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="__C02-1160"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Modular MT with a learned bilingual dictionary: rapid deployment <lb/>of a new language pair <lb/> Jessie Pinkham Martine Smets <lb/>Microsoft Research <lb/>One Microsoft Way <lb/>Redmond, WA 98052 <lb/>{jessiep martines}@microsoft.com <lb/> Abstract <lb/> The MT system described in this paper <lb/>combines hand-built analysis and generation <lb/>components with automatically learned <lb/>example-based transfer patterns. Up to now, <lb/>the transfer component used a traditional <lb/>bilingual dictionary to seed the transfer <lb/>pattern learning process and to provide <lb/>fallback translations at runtime. This paper <lb/>describes an improvement to the system by <lb/>which the bilingual dictionary used for these <lb/>purposes is instead learned automatically <lb/>from aligned bilingual corpora, making the <lb/>system&apos;s transfer knowledge entirely <lb/>derivable from corpora. We show that this <lb/>system with a fully automated transfer <lb/>process performs better than the system <lb/>with a hand-crafted bilingual dictionary. <lb/>More importantly, this has enabled us to <lb/>create in less than one day a new language <lb/>pair, French-Spanish, which, for a technical <lb/>domain, surpasses the quality bar of the <lb/>commercial system chosen for comparison. <lb/></front>

			<body> 1 <lb/> Introduction <lb/> The phrase &quot; MT in a day &quot; is strongly associated <lb/>with research in statistical MT. In this paper we <lb/>demonstrate that &quot; MT in a day &quot; is possible with <lb/>a non-statistical MT system provided that the <lb/>transfer component is learned from aligned <lb/>bilingual corpora (bi-texts), and does not rely on <lb/>any large hand-crafted bilingual resource. We <lb/>propose instead to use a bilingual dictionary <lb/>learned only from the same bi-texts. Section 4.2 <lb/>describes the creation of the new language pair, <lb/>French-Spanish, and gives evaluation results. <lb/>Section 4.1 examines the impact of the learned <lb/>dictionary on our existing French-English <lb/>system. <lb/> 2 <lb/> Previous work <lb/> Commercial systems and other large-scale <lb/>systems have traditionally relied heavily on the <lb/>knowledge encoded in their bilingual <lb/>dictionaries. Gerber &amp; Yang (1997) clearly <lb/>state that Systran&apos;s translation capabilities are <lb/>dependent on &quot; large, carefully encoded, high-<lb/>quality dictionaries &quot; . With the advent of bi-<lb/>texts, efforts to derive bilingual lexicons have <lb/>led to substantial research (Melamed 1996, <lb/>Moore 2001 for discussion), including resources <lb/>for semi-automatic creation of bilingual lexica <lb/>such as SABLE (Melamed 1997), used for <lb/>instance in Palmer et al. (1998). Statistical MT <lb/>systems have relied on bi-texts to automatically <lb/>create word-alignments; in many statistical MT <lb/>
			
			systems however, the authors state that use of a <lb/>conventional bilingual dictionary enhances the <lb/>performance of the system (Al-Onaizan et al. <lb/>1999, Koehn &amp; Knight 2001). We find then, <lb/>that in spite of the movement to create bilingual <lb/>dictionaries automatically, there is still a heavy <lb/>reliance on hand-crafted and hand-edited <lb/>resources. We found no full-scale MT system <lb/>that relied only on learned bilingual dictionaries <lb/>and certainly none that was found better in <lb/>performance for doing so. <lb/>Rapid deployment of a new language pair <lb/>has been one of the strong features of statistical <lb/>MT systems. For example, &quot; MT in a day &quot; was a <lb/>stated goal of the workshop on statistical MT <lb/>(Al-Onaizan et al. 1999). The system deployed <lb/>was of low quality, in part because of the small <lb/>size of the corpus used, and the difficulty of the <lb/>language pair chosen (Chinese to English). We <lb/>have chosen French-Spanish, because we are <lb/>constrained by the availability of well-<lb/>developed analysis and generation components <lb/>in our experiment. Those, needless to say, were <lb/> not created in one day, nor were the large size <lb/>monolingual dictionaries that they rely on. But <lb/>given the assumption that these modules are <lb/>available and of good quality, we demonstrate <lb/>that training the transfer dictionary 1 and <lb/>example base on bi-texts is sufficient to create a <lb/>new language pair which is of comparable <lb/>quality to others based on the same source <lb/>language. This, to our knowledge, has not been <lb/>done before in the context of a large hybrid MT <lb/>system <lb/> 3 <lb/> System overview <lb/> The MT system discussed here uses a source <lb/>language broad coverage analyzer, a large multi-<lb/>purpose source language dictionary, an <lb/>application-independent <lb/>natural <lb/>language <lb/>generation component which can access a full <lb/>monolingual dictionary for the target language, <lb/>and a transfer component. The transfer <lb/>component, described in detail in Menezes <lb/>(2001), consists of high-quality transfer patterns <lb/>automatically acquired from sentence-aligned <lb/>bilingual corpora. <lb/>The innovation of this work is the use of an <lb/>unedited, automatically created dictionary which <lb/>contains translation pairs and parts of speech, <lb/>without any use of a broad domain, general <lb/>purpose hand-crafted dictionary resource. The <lb/>architecture of the MT system as described <lb/>elsewhere (Richardson et al. 2001) used both a <lb/>traditional bilingual dictionary and an <lb/>automatically derived word-association file at <lb/>training time, but it used only the traditional <lb/>bilingual dictionary at runtime. We refer to this <lb/>below as the HanC system, because it uses a <lb/>Hand-crafted Dictionary 2 . We changed this so <lb/>that a learned dictionary consisting of word-<lb/>associations (Moore 2001) with parts of speech <lb/>and a function word only bilingual dictionary <lb/>(prepositions, conjunctions and pronouns) <lb/>replaces the previous combination both at <lb/>training and at runtime 3 . We refer to this as the <lb/>  
			
			<note place="footnote">1 In both French-English and French-Spanish, we use <lb/> a hand-crafted bilingual function word dictionary of <lb/>about 500 entries. It includes conjunctions, <lb/>prepositions and pronouns; see section 4.1.4. <lb/> </note>

			<note place="footnote">2 The dictionaries are automatically converted from <lb/>electronic dictionaries acquired from publishers, and <lb/>are updated by hand over time. <lb/></note> 

			<note place="footnote">3 The same statistical techniques identify certain <lb/>multi-word terms for parsing and transfer. This <lb/> </note>
			
			LeaD system (Learned Dictionary). <lb/>We <lb/>demonstrate that this change improves sentences <lb/>that differ between both systems, and show that <lb/>we can now adapt quickly to new language pairs <lb/>with excellent results. <lb/>Analysis of the consequences of removing <lb/>the standard hand-crafted bilingual dictionary <lb/>from the system (and having no dictionary as a <lb/>fallback at all) are provided in Pinkham &amp; <lb/>Smets (2002). It proved important to have a <lb/>dictionary containing parts of speech to use as a <lb/>fallback, motivating the work described here. <lb/> 4 <lb/> Experiments <lb/> We conducted two experiments. In the first one, <lb/>we compared the performance of the HanC <lb/>(Hand-Crafted dictionary) MT system to the <lb/>performance of our LeaD (Learned Dictionary) <lb/>system. The French-English system is trained <lb/>on 200,000 sentences in the computer domain, <lb/>and tested on unseen sentences from the same <lb/>domain. <lb/>In the second experiment, we created a new <lb/>language pair, French-Spanish, in less than 8 <lb/>hours. The French-Spanish system was trained <lb/>on 220,000 sentences from the same computer <lb/>domain, and also tested on unseen computer <lb/>domain data. <lb/> 4.1 French-English translation <lb/>with a learned bilingual <lb/>dictionary <lb/> 4.1.1 Comparing HanC to LeaD <lb/> In this first experiment, we compare the <lb/>performance of the HanC system and the LeaD <lb/>system for French-English versus the same <lb/>competitor. <lb/>Translations produced by the two versions of <lb/>our system differ in 30% of the cases. Out of <lb/>the 2000 sentences in our test set, only 595 were <lb/>translated differently. In about half of these <lb/>cases, there was an overt difference in the word <lb/>chosen as a fallback translation at runtime. In <lb/>the other half, the translation example-base <lb/>patterns were different. <lb/> 
			
			<note place="footnote">learned dictionary stays constant during the French-<lb/>English experiments. <lb/></note> 
			
			We evaluated 400 of the 595 &quot; diff &quot; sentences <lb/>mentioned. A complete description of the <lb/>evaluation method is given in Richardson <lb/>(2001), and repeated in Appendix A. Evaluation <lb/>for each version of the system was conducted <lb/>against the competitor system, which we use as <lb/>a benchmark of quality. Our current benchmark <lb/>for French-English is Systran 4 , which uses <lb/>relevant dictionaries available but has not been <lb/>otherwise customized to the domain in any way. <lb/> Scores <lb/>Signif. <lb/>Size <lb/>HanC system <lb/>(diffs only) <lb/>-.1777 +/-.087 <lb/>&gt; .999 <lb/>400 <lb/>LeaD system <lb/>(diffs only) <lb/>-.0735 +/-.182 <lb/>.97 <lb/>400 <lb/>French-English <lb/>HanC system <lb/>+.2626 +/-.103 <lb/>&gt; .999 <lb/>400 <lb/>French-English <lb/>LeaD system <lb/>+.2804 +/-.115 <lb/>&gt; .999 <lb/>400 <lb/> Table 1: LeaD vs. HanC for FE <lb/> We also evaluated a set of 400 sentences <lb/>taken randomly from the 2000 test sentence set. <lb/>They were translated with both the HanC system <lb/>and the LeaD system, and evaluated against the <lb/>same competitor, Systran. <lb/> 4.1.2 Results <lb/> The random test has a score representative of <lb/>the quality of the system (December 2001 <lb/>system), and is significantly better than the <lb/>competitor given the score of +0.2804 (0 means <lb/>the systems are the same, -1 the competitor is <lb/>better, 1 the competitor is worse). See Table 1. <lb/>Sentences whose translations differ between <lb/>the HanC and LeaD versions of our system are <lb/>less well translated overall. <lb/>Through <lb/>examination of the data, we have found that <lb/>reliance on the fallback translation at runtime <lb/>tends to indicate a failure to learn or apply <lb/>transfer patterns from the example-base, both of <lb/>which are often due to faulty analysis of the <lb/>source sentence. There are also cases where <lb/> 
			
			<note place="footnote">4 Systran was chosen on the basis of its ranking as <lb/>the best FE system in the IDC report (Flanagan &amp; <lb/>McClure, 2000) <lb/></note> 
			
			translations are not learned because of sparse <lb/>data, but these tend to be rare in our technical <lb/>corpus. <lb/>More importantly, we see that the LeaD <lb/>version of the system has a significantly higher <lb/>score than the HanC version (p=0.002 in a one-<lb/>tailed t-test). <lb/>Replacing the conventional <lb/>bilingual dictionary with the learned bilingual <lb/>dictionary combined with the small function <lb/>word dictionary has led to significant <lb/>improvement in quality when measured on <lb/> &quot; diff &quot; sentences, i.e. cases where all the <lb/>sentences are different. However, when we take <lb/>400 random sentences, the difference between <lb/>the two versions only affects 30% of the <lb/>sentences (133 or thereabouts) and therefore <lb/>does not result in a significant difference <lb/>(p=0.13 in a one tailed t-test). <lb/> 4.1.3 Translation examples <lb/> In this section, we give examples of translation <lb/>with both versions of our system, and compared <lb/>to Systran. The LeaD version of our system <lb/>uses the correct translation of &quot; casiers &quot; , in this <lb/>specific context, while both our HanC version of <lb/>the system and Systran use terms inappropriate <lb/>for this domain. By using a learned dictionary, <lb/>the LeaD system is better suited to the domain. <lb/> Source <lb/>Le finisseur est traité comme trois <lb/>casiers individuels, <lb/>Reference The Finisher is addressed as three <lb/>individual bins <lb/> LeaD <lb/>The finisher is processed like three <lb/>individual bins. <lb/> HanC <lb/>The finisher is processed like three <lb/>individual pigeonholes. <lb/> Systran <lb/>The finisher is treated like three <lb/>individual racks, <lb/> 4.1.4 Creation of the learned bilingual <lb/>dictionary <lb/> The learned dictionary with parts of speech was <lb/>created by the same method (Moore, 2001) as <lb/>the previously used word-association file, with <lb/>the exception that parts of speech were <lb/>appended to lemmas in the first step of the <lb/>process. We are easily able to modify the input <lb/>this way, because we use the output of the <lb/>analysis of the training data to create the file that <lb/>is the input to the word alignment process. <lb/>Appending the part of speech disambiguates <lb/>homographs such as &quot; use &quot; , causing them to be <lb/> treated as separate entities in the word-<lb/>association process: <lb/>use^^Verb <lb/>use^^Noun <lb/>The word-association process assigns scores <lb/>to each pair of words. We have established a <lb/>threshold below which the pairs are discarded. <lb/>Here are the top word pairs in the learned <lb/>dictionary for this domain: <lb/>utiliser^^Verb <lb/>use^^Verb <lb/>fichier^^Noun <lb/>file^^Noun <lb/>serveur^^Noun <lb/>server^^Noun <lb/>Because the input to the learning process is <lb/>derived from Logical Forms (the output of our <lb/>analysis systems), and because this format no <lb/>longer includes lemmas for function words, <lb/>there are no function words in the learned <lb/>dictionaries. This is the primary reason why we <lb/>complemented the learned dictionary with a <lb/>function word dictionary. See the future work <lb/>section for ideas on learning the function words <lb/>as well. <lb/>Both the French-English and the French-<lb/>Spanish were arbitrarily cut off at the same <lb/>threshold, and were not edited in any way, <lb/>resulting in a file with 24,000 translation pairs <lb/>for French-English and 28,000 translation pairs <lb/>for French-Spanish. The dictionary for function <lb/>words contains about 500 word pairs. The <lb/>traditional French-English dictionary had <lb/>approximately 40,000 entries. <lb/> 4.2  French-Spanish <lb/> 4.2.1 Creating French-Spanish <lb/> Our group currently has both a French-English <lb/>system and an English-Spanish system. In <lb/>choosing the new language pair to develop, we <lb/>were constrained by the availability of good <lb/>quality analysis and generation systems. This is <lb/>a limiting factor, but will become less so once <lb/>we have more generation modules available for <lb/>use 5 , as we currently have seven fully developed <lb/>analysis modules. We were fortunate to have <lb/>220,000 aligned sentences for French-Spanish <lb/>from the technical domain (manuals, help files), <lb/> 
			
			<note place="footnote">5 Members of our group (Corston-Oliver et al.) are <lb/>developing an automatic generation component. <lb/>This could speed up the development of generation <lb/>modules, giving us a potential of 42 different <lb/>language-pairs trainable on bi-texts. <lb/></note>
			
			 which enabled the construction of the learned <lb/>bilingual dictionaries, and the automatic <lb/>creation of the transfer pattern example base. <lb/>For reasons explained above, our first <lb/>learned dictionary made no attempt to learn <lb/>function word translations. <lb/>We needed, <lb/>therefore, to complement the learned French-<lb/>Spanish dictionary with a French-Spanish <lb/>function word bilingual dictionary, which was <lb/>bootstrapped from our French-English and <lb/>English-Spanish bilingual dictionaries. All the <lb/>translations for prepositions, conjunctions and <lb/>pronouns were created using both of these, and <lb/>hand-edited by a lexicographer bilingual in <lb/>French and Spanish. <lb/>The creation process, including the hand-<lb/>editing work, took less than 8 hours. <lb/> 4.2.2 Results <lb/> The test was conducted on 250 test sentences <lb/>from the same technical domain as the training <lb/>corpus, using the methodology described in <lb/> Appendix A. All test data is distinct from <lb/>training data and unseen by developers. The <lb/>Sail Labs French-Spanish system is the <lb/>benchmark used as comparison. The technical <lb/>domain dictionary on the website was applied to <lb/>the Sail Labs translation, but it was not <lb/>otherwise customized to the domain. <lb/>The Sail Labs translation included brackets <lb/>around unfound words, which were thought to <lb/>interfere with the raters&apos; ability to compare the <lb/>sentences; the brackets were removed for the <lb/>evaluation. <lb/> Condition <lb/>Scores <lb/>Signif <lb/>Size <lb/>FS LeaD <lb/>+.2278 <lb/>+/-.117 <lb/>&gt; .999 <lb/>250 <lb/>French-English +.2804 <lb/>+/-.114 <lb/>&gt; .999 <lb/>400 <lb/> Table 2: French Spanish results <lb/> As seen in Table 2, where the French-<lb/>Spanish system is ranked at +0.228, it is <lb/>significantly better than the Sail Labs French-<lb/>Spanish system in this technical domain. The <lb/>score is very similar to the French-English score <lb/>as measured against Systran (+.2804). Since <lb/>these are being compared against different <lb/> competitors, we also wanted to measure their <lb/>absolute quality. On a scale of 1 to 4, where 4 is <lb/>the best, we found that both Systran and Sail <lb/>Labs were comparable in quality, and that our <lb/>system scored slightly higher in both cases, but <lb/>not significantly so, if one considers the <lb/>confidence measures (Table 3). The details of <lb/>the scoring for absolute evaluations are given in <lb/>Appendix B. As a brief illustration, the LeaD <lb/>French-English translation in 4.1.3 has a score <lb/>of 3, while the LeaD French-Spanish translation <lb/>in 4.2.3 received a score of 2.5. <lb/> Absolute score <lb/>FS LeaD <lb/>2.676 +/-.329 <lb/>250 <lb/>FS Sail Labs <lb/>2.444 +/-.339 <lb/>250 <lb/>French-English <lb/>2.321 +/-.21 <lb/>400 <lb/>FE Systran <lb/>2.259 +/-.291 <lb/>250 <lb/> Table 3: Absolute scores FS and FE <lb/> 4.2.3 Translation Example for French-<lb/>Spanish <lb/> This section gives examples of translation from <lb/>French into Spanish. The LeaD translation has <lb/>the correct translation for domain specific terms <lb/>such as &quot; hardware &quot; and &quot; casilla de <lb/>verificación &quot; , while Sails Labs translation does <lb/>not in spite of the use of a domain bilingual <lb/>dictionary. <lb/> Source <lb/>Si la case à cocher Supprimer de ce <lb/>profil matériel est activée, le <lb/>périphérique est supprimé du profil <lb/>matériel. <lb/>Reference Si la casilla de verificación Quitar este <lb/>perfil de hardware está activada, se ha <lb/>quitado el dispositivo del perfil de <lb/>hardware. <lb/>LeaD <lb/>Si se activa la casilla de verificación <lb/>Eliminar de este perfil de hardware, el <lb/>dispositivo se quita del perfil de <lb/>hardware. <lb/>Sails Labs Si la coloca a marcar Suprimir de este <lb/>perfil material es activada, el periférico <lb/>se suprime del perfil material. <lb/> 5 <lb/> Future Work <lb/> We are planning to experiment with lowering <lb/>the threshold for the cutoff of information in the <lb/>learned bilingual dictionary, in an attempt to <lb/>include more word pairs (some words remain <lb/>untranslated). <lb/>To further validate the Learned Dictionary <lb/>approach, we are experimenting with other <lb/>domains. One might assume, for instance, that <lb/>as the domain becomes broader, learned <lb/>dictionaries would be less effective due to <lb/>sparse data. We have preliminary experiments <lb/>on Hansard French-English data which indicate <lb/>that this is not the case. <lb/> 6 <lb/> Conclusion <lb/> We have demonstrated that we can replace the <lb/>traditional bilingual dictionary with a <lb/>combination of a small bilingual function word <lb/>dictionary and a bilingual dictionary learned <lb/>from bi-texts. This removes the reliance on <lb/>acquired or hand-built bilingual dictionaries, <lb/>which can be expensive and time-consuming to <lb/>create. One can estimate that for any new <lb/>domain application, this could save as much as <lb/>1-2 person years of customization. This also <lb/>removes a major obstacle to quick deployment <lb/>of a new language pair. <lb/>We believe that high-quality linguistic <lb/>analysis is a necessary ingredient for successful <lb/>MT. In our system, it has enabled automation of <lb/>the transfer component, both in the learning of <lb/>the bilingual dictionary and in the creation of <lb/>example-based patterns. <lb/></body> 
			 
			 <div type="annex">Appendix A: Relative Evaluation Method <lb/> For each version of the system to be tested, <lb/>seven evaluators were asked to evaluate the <lb/>same set of blind test sentences. For each <lb/>sentence, raters were presented with a reference <lb/>sentence, the original English sentence from <lb/>which the human French translation was <lb/>derived. In order to maintain consistency among <lb/>raters who may have different levels of fluency <lb/>in the source language, raters were not shown <lb/>the original French sentence. Raters were also <lb/>shown two machine translations, one from the <lb/>system with the component being tested, and <lb/>one from the comparison system (Systran for <lb/>French-English, Sails Lab for French-Spanish). <lb/>Because the order of the two machine <lb/>translation sentences was randomized on each <lb/>sentence, evaluators could not determine which <lb/> sentence was from which system. The order of <lb/>presentation of sentences was also randomized <lb/>for each rater in order to eliminate any ordering <lb/>effect. <lb/>The raters were asked to make a three-way <lb/>choice. For each sentence, the raters were to <lb/>determine which of the two automatically <lb/>translated sentences was the better translation of <lb/>the (unseen) source sentence, assuming that the <lb/>reference sentence was a perfect translation, <lb/>with the option of choosing &quot; neither &quot; if the <lb/>differences were negligible. Raters were <lb/>instructed to use their best judgment about the <lb/>relative importance of fluency/style and <lb/>accuracy/content preservation. We chose to use <lb/>this simple three-way scale in order to avoid <lb/>making any a priori judgments about the relative <lb/>judgments of quality. The three-way scale also <lb/>allowed sentences to be rated on the same scale, <lb/>regardless of whether the differences between <lb/>output from system 1 and system 2 were <lb/>substantial or relatively small; and regardless of <lb/>whether either version of the system produced <lb/>an adequate translation. <lb/>The scoring system was similarly simple; <lb/>each judgment by a rater was represented as 1 <lb/>(sentence from our system judged better), 0 <lb/>(neither sentence judged better), or -1 (sentence <lb/>from Systran or Sails Labs judged better). The <lb/>score for each version of the system was the <lb/>mean of the scores of all sentences for all raters. <lb/>The significance of the scores was calculated in <lb/>two ways. First, we determined the range around <lb/>the mean which we could report with 95% <lb/>confidence (i.e. a confidence interval at .95), <lb/>taking into account both variations in the <lb/>sentences and variations across the raters&apos; <lb/>judgments. In order to determine the effects of <lb/>each stage of development on the overall quality <lb/>of the system, we calculated the significance of <lb/>the difference in the scores across the different <lb/>versions of the system to determine whether the <lb/>difference between them was statistically <lb/>meaningful. We used a one-tailed t-test, since <lb/>our a priori hypothesis was that the system with <lb/>more development would show improvement <lb/>(that is, a statistically meaningful change in <lb/>quality with respect to the competitor). <lb/></div> 
			 
			 <div type="annex">Appendix B: Absolute Evaluation <lb/>Method <lb/> At the same time as the relative evaluations are <lb/>made, all the raters enter scores from 1 to 4 <lb/>reflecting the absolute quality of the translation, <lb/>as compared to the reference translation given. <lb/>The grading is done according to these <lb/>guidelines: <lb/> 1 unacceptable: <lb/> Absolutely not comprehensible and/or little <lb/>or no information transferred accurately <lb/> 2 possibly acceptable: <lb/> Possibly comprehensible (given enough context <lb/>and/or time to work it out); some information <lb/>transferred accurately <lb/> 3 acceptable: <lb/> Not perfect (stylistically or grammatically odd), <lb/>but definitely comprehensible, AND with <lb/>accurate transfer of all important information <lb/> 4 ideal: <lb/> Not necessarily a perfect translation, but <lb/>grammatically correct, and with all information <lb/>accurately transferred <lb/></div> 
			 
			 <listBibl>References <lb/> Al-Onaizan, Y &amp; Curin, J. &amp; Jahr, M. &amp; Knight <lb/>K. &amp; Lafferty, J. &amp; Melamed, D. &amp; Och, F-J, <lb/>&amp; Purdy, D. &amp; Smith, N. A. &amp; Yarowsky, D. <lb/>(1999). Statistical Machine Translation: Final <lb/>Report, Johns Hopkins University 1999 <lb/>Summer <lb/>Workshop <lb/>on <lb/>Language <lb/>Engineering, Center for Speech and Language <lb/>Processing, Baltimore, MD. <lb/>Corston-Oliver, S., M. Gamon, E. Ringger, R. <lb/>Moore. 2002. An overview of Amalgam: A <lb/>machine-learned generation module. To <lb/>appear in Proceedings of the International <lb/>Natural Language Generation Conference. <lb/> New York, USA <lb/>Flanagan, M and McClure, S. (2000) Machine <lb/>Translation Engines: An Evaluation of Output <lb/>Quality, IDC publication 22722. <lb/>Gerber, L. &amp; Yang,J. (1997) Systran MT <lb/>Dictionary Development in the Proceedings <lb/>of the MT Summit V, San Diego. <lb/>Koehn, P. &amp; Knight, K. (2001) Knowledge <lb/>Sources for Word-Level Translation Models, <lb/> Proceedings of the conference on Empirical <lb/>Methods in Natural Language Processing <lb/> (EMNLP) <lb/>Melamed, D. (1998). Empirical Methods for MT <lb/>Lexicon Construction, in L. Gerber and D. <lb/>Farwell, Eds., Machine Translation and the <lb/>Information Soup, Springer-Verlag. <lb/>Melamed, D. (1997). A Scalable Architecture <lb/>for Bilingual Lexicography, Dept. of <lb/> Computer and Information Science Technical <lb/>Report #MS-CIS-91-01. <lb/>Melamed, D. (1996). Automatic Construction of <lb/>Clean Broad-Coverage Translation Lexicons, <lb/> Proceeding of the 2nd Conference of the <lb/>Association for Machine Translation in the <lb/>Americas (AMTA&apos;96), Montreal, Canada. <lb/>Menezes, A. &amp; Richardson, S. (2001). A Best-<lb/>First Alignment Algorithm for Automatic <lb/>Extraction of Transfer Mappings from <lb/>Bilingual Corpora. In Proceedings of the <lb/>Workshop <lb/>on <lb/>

			Data-Driven <lb/>Machine <lb/>Translation, ACL Conference, June 2001. <lb/>Moore, R.C. (2001). Towards a Simple and <lb/>Accurate Statistical Approach to Learning <lb/>Translation Relationships Between Words. In <lb/> Proceedings of the Workshop on Data-Driven <lb/>Machine Translation, ACL Conference, June <lb/>2001. <lb/>Pinkham, J &amp; Smets, M (2002) Machine <lb/>Translation without a bilingual dictionary <lb/>Proceedings of the TMI conference, Kyoto, <lb/>Japan. <lb/>Palmer, M. &amp; Rambow, O. &amp; Nasr, A. (1998). <lb/>Rapid Prototyping of Domain-Specific <lb/>Machine Translation Systems, in Proceedings <lb/>of the AMTA &apos;98. <lb/> Richardson, S. &amp; Dolan, W. &amp; Menezes, A. &amp; <lb/>Corston-Oliver, M. (2001). Overcoming the <lb/>Customisation Bottleneck Using Example-<lb/>Based MT. In Proceedings of the Workshop <lb/>on Data-Driven Machine Translation, ACL <lb/>Conference, June 2001. </listBibl>


	</text>
</tei>
