<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="__01198988"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Proceedings  of the 5th International Conference on <lb/> Neural Information Processing (ICONIP&apos;02)  ,  Vol. 4 <lb/> L i p  Wan&amp; J a p h  C  Rajapakre.  Kunihiko Fukushima <lb/> $cmYuuog  Lee.  and  Xm  Yao  (F~IO~SJ <lb/> FUSION ALONG TIME-DIMENSION TO INVESTIGATE ASPECTS <lb/>OF VISUAL PATTERN COMPLETION <lb/> Homayoun Navabi <lb/> University  of Tarbiat-Modem  Tehran <lb/> navabi @lvcos.com <lb/> ABSTRACT <lb/> Image fusion along time dimension is <lb/>investigated to explain aspects of multi-sensors <lb/>visual patterns and the related representation <lb/>tasks. We  address  image fusion problem by <lb/>describing a layered complex channels neural <lb/>model. In processing visual data conventionally <lb/>complex channels utilize both the linear and <lb/>nonlinear operations, and consist of two stages <lb/>filtering. The first filtering is sensitive to higher <lb/>spatial frequencies than the second, and they <lb/>suppose to explain a number of imagiag task <lb/>complexities that can not be explained by simple <lb/>channels. <lb/>The work reported here focuses on practical <lb/>applications and behavior modeling to examine <lb/>new perspectives for complex processes at the <lb/>sensory vision that begins with antagonistic <lb/>decomposition of input.  lhis  is then followed by <lb/>fusion of emergent time-sensitive  dita  to <lb/> promote a non-linear dynamic association.  Our <lb/> reported result suggests that time fusion  $of  data <lb/>in multi-staged complex channels of neural <lb/>processes exhibit the true adaptive behavior that <lb/>adjust itself to a changing environment. <lb/></front>
			
			<body>1. INTRODUCTION <lb/> -<lb/> According to the data in the literature, h i o n of <lb/>information for a visual object acquired by multi-<lb/> . . <lb/> sensors performing with different measuring <lb/>context  or  imaging sites  can  render to  a  more <lb/>successful image processing  [1][2].  This  is in <lb/>comparison to a series of separate pr0ces:;ing of <lb/>data possibilities acquired for the same image. <lb/>We explain  imagefision process as a process to <lb/> acquire a  higher  qualified image that  by <lb/> complementing local information componmts  of <lb/> multiform p a t t e m .  This is in effect an <lb/> extrapolation of the local information acquired <lb/>by various sensors performing over the exposure <lb/>time (t). <lb/> To clarify the ideas promoting image fusion, let <lb/> us first glance at few imaging samples. For <lb/> example, in remote sensing systems a wide <lb/>spectrum of data possibilities  can  be acquired <lb/>depending on the sensors are airborne  or  space <lb/>b e . In biomedical imaging, the magnetic <lb/>resonance imaging (MRI) and the computed <lb/>tomographic  (cr)  do not reveal identical details <lb/>

			of the  brain  tissues.  This  is while  C T  is suitable <lb/>for hard tissues the  M R  images are superior in <lb/>depicting the soft tissues in the brain. In either of <lb/>the above examples, different possibilities for <lb/>local data extraction  arise  from the different <lb/>imaging techniques that are complementary in <lb/>many ways but no one is sufficient in terms of <lb/>their local information content. <lb/>We may also refer to human vision, where <lb/>electrophysiologic, imaging, and biochemical <lb/>studies have provided us with much information <lb/>concerning the structural and functional <lb/>properties of visually responsive neurons and <lb/>their connections. However, crucial questions <lb/>concerning how local information for such <lb/>responsive cells are integrated (or fused), is still <lb/> unresolved. Another prime example relates to a <lb/> central issue in stereopsis that concerns the way <lb/> in which left and right eyes&apos; views are fused to <lb/>solve the correspondence problem and recover <lb/> three  dimensional scene structures. <lb/>There  are  numerous examples of the E n d and <lb/>the emerging superiority for fusion techniques to <lb/> render complemented images may appear partly <lb/>by redundant data as some regions are depicted <lb/> in  all  the possible image forms. Also partly by <lb/>complementary data  as  each different image <lb/>form highlight certain featnres that could be <lb/> absent in images of the other forms  [Z]. <lb/> Thus, in many applications the performances of <lb/>heterogeneous sensors are needed to provide the <lb/> additional source of information, where in such <lb/>cases data from an individual source are <lb/>incomplete. In this paper, we look at  three <lb/>
			
			<page> 1819 <lb/></page>

			different fusion applications to examine the <lb/>aspects of multiform images. <lb/>The  first  case relates to multi-frequency <lb/>fusion where data recorded by the same local <lb/> sensors operate in conditions with varying <lb/>frequency over the exposure time. <lb/>The second case relates to multi-data <lb/>fusion where data recorded by the same local <lb/> sensors operate in conditions with the scene <lb/>changes over the extent of exposure. <lb/>The third case relates to multi-images <lb/>fusion where data recorded by the same local <lb/> sensors operate in conditions with varying image <lb/>data over the exposure time. <lb/>The underlying idea for multi-sensors image <lb/>fusion is investigated within the fiamework of <lb/> biological-inspired complex channels neural <lb/>model. In all cases selected for our fusion <lb/>studies, we assume the multiform pattern fusion <lb/>is  performed  over the exposure time (t  &gt;  0). This <lb/>is  performed  by a grwp of fully interconnected <lb/> Sensors  scanning a Limited window of a scene. <lb/>Thus, in this context fusion process extends from <lb/>zero exposure time, i.e., spatial image fusion <lb/>with t  =  0, to the extended exposure time i.e., <lb/> temporal image fusion with t  &gt;  0. <lb/> The remainder of this paper is  ~ g a ~ ~ i z e d <lb/> into <lb/>four sections. Section 2 discusses the basic of <lb/>antagonistic temporal associative methodology <lb/>for a decomposed data space, a prior to fusion. <lb/>Section 3, we have a brief discussion on time <lb/>fusion and the neural formulation. Section  4, <lb/> gives the results of the selected samples. <lb/>Conclusions are  drawn  in Section 5. <lb/> 2. FUSION TECHNIQUES <lb/> In this section, our discussion centers on an <lb/>understanding of the underlying issues and <lb/>mechanisms for a self-adaptive scheme of image <lb/>fusion. The motivation behind the idea of <lb/> studying a self-adaptive fusion technique ensues <lb/>from its broad spectrum of engineering and <lb/>behavior modeling applications. <lb/> 2.1. The Challenge <lb/> We choose to study a computational model to <lb/>reconstruct the basic processing phenomena <lb/>relating to human sensory vision. In the course <lb/> of this shldy, we assume that  streams  of  image <lb/>data impinge the system sensors over the scene <lb/>exposure period  The associated  first  challenge <lb/>to such a multifaceted imaging protocol, <lb/>however, relates to sensors interactions in space <lb/>and time, i.e., response is a function of sensors <lb/>position and time. The challenging task can  he <lb/> put into perspective that given two spatial input <lb/>streams  or  patterns  a&quot;)  and  a&apos; &quot;  having general <lb/>form of  a@) =  (al(&apos;) ,aJk)  ,. . . . . . . . . . . .,  %(k)  )  impinge <lb/>the same local sensory area in temporal <lb/>succession, then the question is how does the <lb/>vision system know that spatial pattern  a(&apos;)  is <lb/>being changed to a different succeeding pattern <lb/> a&quot;) [3].  Moreover, how does the system generate <lb/>responses to differentiate space and time <lb/>attributed data within the self-adaptive processes <lb/> of a human like vision system? <lb/> Thus, in relating to a self-adaptive multi-sensors <lb/>fusion system the first and the foremost <lb/>confronting challenge,  however, relates  to  how <lb/> separate local regularities, i.e., i m g e pattem <lb/>modalities, are entwined Over space and time <lb/>without loss  of  structural integrity. <lb/> 2.2. Structural  &amp;  Time Constraints <lb/> In other words, we are looking at the <lb/>fundamental question of how space and time <lb/>interact over the exposure or fusion time to <lb/>determine the emergent properties of the higher <lb/>qualified fused image. <lb/>In general, time can be embedded in such a self-<lb/>adaptive system in two basic ways:  structural <lb/>constraints, and the presentation <lb/>time <lb/>constraints.  Structural constraints, may be <lb/> defined in terms  of cam&apos;ers regularities (or <lb/>background changes), syntactic regularities (or <lb/>pattem changes), and recurrent regularities (or <lb/> profile ordering changes).  Presentation time <lb/>constraints relates to exposure duration  [4], <lb/> which in a normal process control system can be <lb/>assumed to be directly linked to structural <lb/>complexities. <lb/>This implies that increase in structural <lb/>complexity changes is followed by increase in <lb/>presentation time. To study such an active <lb/>interposed relationship of input image time <lb/> components, we have developed a complex <lb/>channel neural model. The idea is to investigate a <lb/> new perspective for complex channels <lb/>processing  [SI  that by introducing a temporal <lb/>fusion mechanism. In essence, our multi-staged <lb/>processing begins with decomposition of sensory <lb/>streams impinging the local cell complexes in <lb/>the dual antagonistic pathways of a multi-layer <lb/>neural system. <lb/>In this system, responses of local cell complexes <lb/>in separated pathways are exclusively fused to <lb/>elicit <lb/>a <lb/>multi-staged <lb/>stimulus-response <lb/>transfomtion process forecasting an adaptive <lb/>

			<page> 1820 <lb/></page>

			sensory system that mediates data fusion. Thus, <lb/> our fusion-based neural scheme evolver from <lb/>time-emerged response association for multi <lb/>sensory inputs to promote a multiform image <lb/>completion.  Our  network result suggests an <lb/>overall performance shell to solve fundamental <lb/>imaging problems, i.e., decomposition and <lb/>association. <lb/> 2.3.  Temporal Decomposition  &amp; <lb/> Association <lb/> To understand the underlying problems and to <lb/>elaborate an explanation  on  image data <lb/>decomposition, temporal association, and image <lb/>fusion, let us take a brief background-view on <lb/>how time is interposed into the human ?;ensory <lb/> vision. To this end, we refer to active fixations <lb/>promoted by rapid eye movements, which are <lb/> necessary for perception of connected visual <lb/>world and the decomposition of input space. This <lb/>on-line action-response control the space-slicing <lb/>or eye .movement process which euab1.e~ the <lb/> - central -vision to quickly acquire images or <lb/>sample a  visual  scene. <lb/> An interesting information in such a serial image <lb/> scan  is that during each actual scanning episode <lb/>(eye movements) vision is in essence shut down <lb/> [6].  The existing data reveals the influeace of <lb/>such suppressive actions begin before the <lb/>sampling process and last after an episode:. Also, <lb/>it has been suggested this is due in part to image <lb/>motion and in part to temporal masking  ,effects. <lb/> ~. <lb/> ~ <lb/> there fore,^ while such  serial  imaging are <lb/>necessary to extract Scene data, it can have <lb/>diverse effects if imaging apparatus I d l a g too <lb/> much. <lb/> This  implies timing of the imaging events.  Thus, <lb/> a system is required to regulate the acqnisition <lb/>and p r o k i n g over the exposure  period.  This is <lb/>to promote a balance between image extraction <lb/>performance (hold episode) and presentation <lb/> ~ <lb/>~ <lb/>~ <lb/> timhg (shift episode). A model to understand the <lb/> ~. ~ ~ ~ <lb/> underlying complex circuitry for swh an <lb/>innicate~process of eye positioning is. far too <lb/>complex thus a subject of much research and-<lb/>debates (e.g.,  [6][7])  and beyond the scope of <lb/>present work. <lb/>-Here, to focus our discussion we concenmte  on <lb/> practical computational aspects of the multi-<lb/>Sampled data in the early vision. The idea is to <lb/>develop  a.  model with ability to promote active <lb/>processing for space-time properties of the time-<lb/>scanned pattems and to encode multiform <lb/>sampled images. <lb/> .~ <lb/> ~ <lb/> . . <lb/> -<lb/> . . <lb/> -<lb/> 3.  TIME  FUSION  &amp;  NEURAL <lb/> CIRCUIT <lb/> The above discussion on time promoted action-<lb/>response can easily reconfirm the ideas <lb/>pertaining to a fusion machine and justifies the <lb/>need to provide complementary data sources. <lb/>Hence, an understanding of scene sample fusion <lb/>can provide the needful visibility to the <lb/>underlying  processes  for stimulus-evoked neural <lb/>activation within the pre-attentive vision.  Our <lb/> model assumes, the process begins with input <lb/>space decomposition, i.e., active sampling and <lb/>resample of the scene during the object <lb/>presentation,  see  Figure  1. <lb/> In such a sequence of event-action scenario, it is <lb/> natural to believe that time plays its very crucial <lb/>role in regulating the convergence and <lb/>divergence of data in dual antagonistic pathways <lb/>to organize the input. The process is highlighted <lb/>with  first stage divergence to  ~  promote <lb/>antagonistic temporal association,  which <lb/>exhibits the fusion of structnral and <lb/>presentational attributes of sensory samples <lb/>(Figure 1). <lb/> The output of the first stage, i.e., divergence of <lb/>processes, is the input to the next adaptive <lb/> filtering stage, highlighted wiih convergence <lb/>process, which ispromoted by temporalfusion  of <lb/>the antagonistic attributed data. This is an <lb/>intermediate level of data fusion to mediate an <lb/>adaptive masking to control the short-term <lb/>memory  (STM)  components of antagonistic  local <lb/> cell groups. This in effect is time-sensitive <lb/>masking, which serves to control response <lb/> sensitizationldesensitization  of the network. The <lb/>delegated intermediate fusion data serves to <lb/>control sensitization and desensitization that <lb/>shunts the first-stage response and modulates the <lb/>signaling of the maximum advation changes for <lb/>either of the antagonistic pathways local <lb/>processes. The-signaling of maximum dual <lb/>antagonistic activation changes are assumed to <lb/>initiate a higher level processing that too control <lb/>the tempcaal decomposition of the input sensory <lb/>space (also  see  the scheme proposed in  [7]  for <lb/>comparison). In this paper, our work is limited to <lb/>a discussion on the first stage and primary fusion <lb/>for input transformation  (see  Figure 1). <lb/> .~ <lb/> 3.1. The  Neural  Activation Model <lb/> Various experimental result [ll] siiow that the <lb/>receptive field  (RF),  the area of visual space <lb/>within d c h the response of cell can be <lb/>

			<page> 1821 <lb/></page>

			influenced is inherently a space-time entity.  Our <lb/> Figure  1  gives an overall view of the developed <lb/>fusion-based neural mechanism suggests <lb/>neural scheme with unique fusion methodology <lb/>antagonistic response organization for local <lb/> formulated in its multi-staged architectural <lb/>groups of visual responsive cells in the joint <lb/>design that to derive components of cellular <lb/>activity in a complex channels processes. <lb/> *...... <lb/> domain of space and time. <lb/> ...... <lb/> /......... -<lb/> .. <lb/> .....--<lb/> e <lb/> . <lb/>. <lb/>. <lb/> . . <lb/> ..... <lb/> ..  ...e-<lb/> Input Space <lb/> . . <lb/>. . <lb/>. . <lb/> . . <lb/> . . <lb/>. . <lb/> Timd  gegulated  space <lb/> ~ <lb/> Serial Scanning <lb/> . . <lb/>. . <lb/> . . <lb/>. . <lb/>. . <lb/>. . <lb/>. . <lb/>. . <lb/>. . <lb/> . . <lb/>. . <lb/>. . <lb/> . . <lb/> . . <lb/>. . <lb/>. . <lb/> . . <lb/>. . <lb/>. . <lb/> . . <lb/> : : <lb/> . . <lb/>. . <lb/> m L  Time-Sliced Input Space <lb/> . . <lb/>. . <lb/> . . <lb/> . . <lb/> . . <lb/>. . <lb/>. . <lb/> . . <lb/>. . <lb/>. . <lb/> . . <lb/>. . <lb/>. . <lb/>. . <lb/> : : <lb/>: : <lb/> . . <lb/> . . <lb/> . . <lb/>. . <lb/> . . <lb/>. . <lb/> t r <lb/> Fusion <lb/> I-<lb/>I <lb/> The basics of our activation model have been <lb/>investigated in  [3],  however, we have revised a <lb/> time control factor for the instantaneous evoked <lb/>activation  equation  to promote time dependent <lb/>behavior of a cell over the fusion time. <lb/>In this  paper,  the network response protocol is <lb/>computed over a complete fusion cycle.  A <lb/> response vector covering the fusion time range 0, <lb/> to,  1  represent the output of the model, which is <lb/>to measure temporal frequency of maximum <lb/>activity changes.  Thus,  frequency of maximum <lb/> Figure 1, depicts an overall schematic of <lb/> data flow for a multi-staged processes that <lb/> begins with the sensory space slicing over <lb/>the duration of scene exposure. <lb/>The scheme is consistent with data on <lb/>transduction of sampled patterns into <lb/>pulse-coded neural response in a typical <lb/>form of complex channels processing  [5]. <lb/> In the stimulus-response protocol of our <lb/>sensory model input samples are organized <lb/>by the antagonistic local cell groups i.e., <lb/>fust stage filter, to provide  a  bedding to <lb/>trigger a second stage filtering process. We <lb/>assume the outcome is cell activity <lb/>enhancement and suppression for groups <lb/> of local interacting cells with known <lb/>receptive field  (RF)  organizations. These <lb/> series of temporal evoked-events derive <lb/>the sensory process to signal the image <lb/>interest points and conceal the background. <lb/> activity changes over the fusion time range  [O-I] <lb/> is computed for all the cells or group of local <lb/>cells participating in an antagonistic dual channel <lb/>operation. <lb/> To derive the spatial and temporal components <lb/>of cellular activity changes  at  any time instant, <lb/>the system submits the instantaneous evoked <lb/>activity of cell at the  shut  of an interval to <lb/>compute the new state activity gains  (or  changes) <lb/>for progress from  x:  to the new state  xi  . <lb/>

			<note place="headnote"> ,+I <lb/></note>

			<page> 1822 <lb/></page>

			The activation equation  (see  ref.  [3]) for  the <lb/>spatietemporal response behavior of a cell over <lb/>the temporal exposure sequence presented by <lb/>equation  1: <lb/> (1) <lb/> where the temporal adaptation coefficient is: <lb/>and  f(U)  characterizes the activation of cell <lb/> i  at any time step, and it is dehed  as: <lb/> ) Q <lb/> the choice <lb/> f ( w ) = [ : <lb/> : <lb/> ;  -p &lt; w  &lt; q <lb/> - P <lb/> if <lb/> W ( -P <lb/> of  p  a d  q  is arbitrary.  I  ! r + l l  Delinate the <lb/>extemal stimulus effects for a cell in either of the <lb/>dual channels cell population.  I  I,,*&apos;) Lknotes <lb/>local cell surrounding effects, which is imposed <lb/>Fixed weight <lb/>on the cell  i  . <lb/> between local cells for computational <lb/>conveniences, without loss of generality. The  M <lb/> parameter represents  number  of cells in the <lb/>active local region. <lb/>Panmeterh is the revised temporal factor <lb/>introdnced in this paper. This parameter in our <lb/>prior studies represented a ratio of active (cells to <lb/>total lccal cells. However,  h  factor complements <lb/>the adaptive discharge control factor Z, together <lb/>they function to regulate the spatietemporal <lb/>cellular sensitivities. In this paper, we have  h <lb/> revised as a time range factor that has slower <lb/> period  for the sensory stage and a faster  period <lb/> for the higher stages. <lb/> For  this purpose I  =  sin ( c y c l e m )  where  cycle <lb/> represent unfolding stimulus exposure time. The <lb/>term  - z  xi  is time step modulator with  z =  (1-<lb/> h) <lb/> [SI <lb/> to <lb/>control <lb/>temporal <lb/> sensitizatiddesensiitization  behaviors at a cell <lb/> level. <lb/>Parameter  r  denotes the cellular gating faotor and <lb/> as such it plays two important roles:  ( r  =-I) <lb/> inverts the cells&apos; transformation process; and (r <lb/> =  0)  act as a rectifier to discriminates the multi-<lb/>staged cell transformation for antagonistic dual <lb/>processing. <lb/>Finally, dual temporal response are put together <lb/>at the intermediate level to generate a fused <lb/>multiform map (see results). The map is formed <lb/>by the subtractive coupling of antagonistic <lb/>associated temporal data, given by the diflerence <lb/>equation 2: <lb/> W b = / ! ( M -l ) <lb/> =  [on  ( x i <lb/> I I t l I  -o f f  (Xj&apos;+&apos;))l <lb/> + <lb/> (2). <lb/> lI+l) <lb/> Y i <lb/> where  On(Xi  ).  and <lb/>are the <lb/>partial solutions to equation. (1). Here the <lb/>intermediate fused response is limited to upper <lb/>bound to act as a dynamic activation control <lb/>mechanism. <lb/> &lt;,+I) <lb/> 4. FUSION APPLICATIONSAND <lb/>SELECTED SAMPLES <lb/> Our  method assumes neither prior preprocessing <lb/>knowledge nor any other insertion of data, which <lb/>may pre-mediate a self-organized fusion process. <lb/>According to the data in the literature (e.g.  [9]), <lb/> to define self-organizing process,  tlpee  sub <lb/> processes-results in emergence of a self-<lb/>organized response map. This includes the <lb/>broadcasting of extemal events, the competition <lb/>among local stimulus-activated cells, and <lb/>adaptation in the neighborhood of competing <lb/>units, which seem to  be  sufficient. <lb/> 4.1. Experiment Methods <lb/> Depicted in Rgure 1, we have the local cells <lb/> RFs, which are organized into antagonistic <lb/>zones. The cellular organization is inflnenced by <lb/>the broadcast of temporal sampling events that <lb/>compete against the local components of fused <lb/>information organized into local intercomection <lb/>topologies. <lb/> Our goal is to self-organize different probable <lb/>representations of an incoming events, i.e., to <lb/>fuse multiform data for the input panern over the <lb/>presentation time.  This is to  provide  diferent <lb/>organiurrion maps  from  the  visual  object <lb/>attributes to evoke funher scene sampling (or the <lb/>re-sampling) events. <lb/> To elaborate the idea we have selected  three <lb/> categories of image fusion problems where we <lb/>have tested 3 sets of synthesized pattems to <lb/>demonstrate the system behaviors for the <lb/>duration of stimuli presentation. <lb/>The few selected test samples give a brief <lb/>account of system&apos;s general scope of image <lb/>fusion applications. The snapshots of system <lb/>response show fusion maps that capture the <lb/>stimulus-generated neural processing at the first <lb/>and intermediate stages of complex channels. For <lb/>clarity, the system outputs are  meen  captures, <lb/>and are demonstrated in the same manner as the <lb/>input pattems. <lb/> In all demonstrated input and outputs samples, <lb/>which show luminancdactivity dishibution <lb/>

			<page> 1823 <lb/></page>

			profiles are reconsbucted by using filled-in type <lb/>circles with varying size that represents the input <lb/>intensity distribution or the local cells activity <lb/>configurations. <lb/> 4.2.  Simulation  Results <lb/> In all the cases selected for our fusion studies, <lb/>we assume exposure time t  &gt;  0. The system <lb/>performance is tested for panem completion <lb/> .............. <lb/> Capture 1 <lb/> using multiform images over the fusion time by <lb/>interactions of fully interconnected sensors <lb/>scanning a limited input window. <lb/>The first case relates to multi-frequency image <lb/>fusion where data recorded by the same local <lb/>sensors operate in conditions with high varying <lb/>temporal frequency over the fusion time. <lb/> Dual  Channels <lb/> Resoonse <lb/> . . . . . . . . . . . . . . . . <lb/> . . . . <lb/> . ! ! p 4 ; ; ; ; <lb/> ;.;; <lb/> . . . . . . . . . . . . . . . . . . . <lb/> . . . . . . . . . . e . . . . <lb/> . . * <lb/> .&apos;  :*:a:  : :?:  :E: <lb/> . . . . . . . . . <lb/> . . .  .*,.L.  . . . <lb/> . . * . . <lb/> 0 .  &apos; <lb/> ..+. <lb/> + . : <lb/> .I. <lb/> . <lb/> . . . . <lb/> . . . . . . . . . . . . . -<lb/> .................... <lb/> . . . . . . . . . . . . . . . . <lb/> .  .a.  . . . . . . . . . . . . <lb/> . . . . . . . . . . . . . . <lb/> ...  .p <lb/> ..  .::,+:a;  :::::&apos;: <lb/> . . : : . . . <lb/> ... <lb/> .............. <lb/> ............... <lb/> ........... -. * . . <lb/> . . . . . . . . . . . . . . . <lb/> . . . . . . . . . . . . . . <lb/> .*. . . . . ._* ..... <lb/> . . . . . . . . . . . . .* <lb/> . . . . . . . . . . . . . . . . . . <lb/> Figure2 a)Two screen captures from input sequence <lb/> b)  Output Response <lb/> Figure 2 above gives the system response to <lb/>sequences of input at fusion time cycle 10. The <lb/>synthesized input pattems that are used for this <lb/>series of experimentation formed by using <lb/>varying frequency pattem aimed at a constantly <lb/>illuminated background. The background is <lb/> placed at gray-scale 90 and random varying <lb/>luminance pattem ranging from  0.250. <lb/> The demonstration shows the response in <lb/>antagonistic channels and the fused image <lb/>formed at the intermediate stage by the coupling <lb/>the STM components for local cells  witbin  the <lb/>dual pathways of complex channels. The higher <lb/>system responses depicted in Figure  1  are  not <lb/>presented here since the focus of discussion is on <lb/> a neural technique for fusion of separate <lb/>attributed images during  sensory  activities. <lb/>The second case relates to multi-data fusion <lb/>where data recorded by the same local sensors <lb/> operate in conditions with the scene changes <lb/>over the extent of exposure (or fusion time). In <lb/>this set of experimentation, the system sensors <lb/>are to capture response sequence to multiple <lb/>form input patterns. The sampled result gives the <lb/>intermediate stage of network response to such a <lb/>sequence of visual events by fusion of incoming <lb/>data (i.e., sequence of scene changes). In Figure <lb/> 3, we have the snapshot of different input <lb/>pattems from the incoming sequence. The <lb/>response formed in the antagonistic pathways <lb/>and the resulting fusion for the intermediate <lb/>stage is also presented. <lb/>The order of pattern presentation is depicted in <lb/>Figure 3.  This  order of sequences for sensory <lb/>events is repeated during the fusion  period  that a <lb/>light rectangle (intensity at 90) is moved against <lb/> a dark background (intensity level 50). <lb/> The third case relates to multi-images fusion <lb/>where data recorded by the same local sensors <lb/>operate in conditions with varying image  data <lb/> over the exposure time. <lb/>In this set of experimentation, Figure  4,  we have <lb/>two sets of pattem configurations with varying <lb/>resolution against the background. During the <lb/>course of experiment, background conditions are <lb/>kept constant and the pattem information for the <lb/>two synthesized images are fused over the <lb/>exposure time. Background intensity is kept at <lb/> 80 gray-scale and light pattem profiles at 120, <lb/> with dark figure intensity at 30. <lb/>

			<page> 1824 <lb/></page>

			................ <lb/> ............... <lb/> ..... ......... <lb/> .....  i f . . .  6  .... <lb/> .........*.+.r  --<lb/> ....+......... <lb/> .............. <lb/> r............. <lb/> .............. <lb/> .............. <lb/> .............. <lb/> .............. .............. ............. ................ <lb/> ............... <lb/> ................. <lb/> ............... <lb/> .............. <lb/> Dual  Channels <lb/> Reswnse <lb/> ........... ..,: ........... <lb/> . . . . . . . . . . . . . . . . . . . . <lb/> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .~ <lb/> ..  . o r . .  ..... *.. .. <lb/> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . <lb/> ............... <lb/> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . <lb/> ......... ....... <lb/> .. ........... <lb/> . . . . . . . . . . <lb/> . . <lb/> r . <lb/> ............... <lb/> ............... <lb/> .................... <lb/> ............... <lb/> . . <lb/> .............. <lb/> .......... <lb/> Fused <lb/> *.. <lb/> ......  f f f . . . . . <lb/> ............... <lb/> ....  r + r r . *  ....... <lb/> .............. <lb/> ..* .... ..... .;~. <lb/> . . . . . . . . . . . . . . . . . <lb/> *. .......*.... ........ <lb/> . . . . . . . . . . . . . . . . <lb/>

			 .*.*.... <lb/> ~. <lb/>. <lb/> ..*.*..*...... <lb/> . . . . . . . . . . . . . . <lb/> * I  .............. <lb/> . . . . . . . . . . . . . . . <lb/> .~ <lb/>. . <lb/> . . <lb/> ... ...*... ....... <lb/>..... * ......... <lb/> .....*...*.... . <lb/> ................. <lb/> .................. <lb/> Figure  3. a) Snapshot of input patterns <lb/> b) Fused response <lb/> Dual  Channels <lb/> Reswnse <lb/> Figure  4.  a) Input pattern sequence <lb/> -<lb/> The neural response generated in antagonistic <lb/>channels show assoclation of input stimuli and <lb/>the same effect were observed for the system <lb/>response to many other sample runs (also  :;e  the <lb/>results of our studies in [IO]). Thus, based  on  our <lb/>empirical observations we may make a strong <lb/>suggestion that  antagonistic organization and <lb/>temporal association within the pafhways  of <lb/> complex channels meditate fusion of mulri-fom <lb/> sensory data <lb/> b) Fused Response <lb/> 5.  CONCLUSION <lb/> The paper investigates the problem of image <lb/>fusion over a series of input images. We have <lb/>developed a neural scheme on the basics of <lb/> complex channels for multi-staged neural <lb/>processing. In our developed scheme, the first <lb/>stage of neural processing promotes antagonistic <lb/>responses to the incoming sequences of visual <lb/>

			<page> 1825 <lb/></page>

			events. The next  higher  level witness the <lb/>Masking, In Prccof IJCNN&apos;2001, Int. Joint <lb/>emergence of fused information originated <lb/>Conf. on Neural Networks, USA. <lb/>withim the segregated antagonistic pathways. The <lb/> [ I l l  G.C. Dehgelis, I. Ohzawa, and R.D. <lb/> STM  components of evoked neural activities <lb/>Freeman, receptive-field dynamics in the central <lb/>show particular sensitivity to timing of imaging <lb/>visual pathways, Trends In Nenrosc., 18, no.10, <lb/>events. The stimulus-evoked activities  at  the <lb/>pp. 451-458, 1995. <lb/>early stages are  then  coupled to form a dynamic <lb/>fusion-based server to control the time-<lb/>dependent responses of the network.  Our <lb/> reported result suggests  that  temporal association <lb/>for local cell activities can emerge from those <lb/>imaging attributes, which display complex <lb/>temporal configurations. <lb/></body>
			
			<back>
				
				<listBibl>6.  REFERENCES <lb/> [I] G. Simon, A. Farina, F.C. Morabito, S.B. <lb/>Serpico, L. Bruzzone, Image fusion techniques <lb/>for remote sensing applications, Information <lb/>Fusion,  3,  pp. 3-15.2002. <lb/>[2]  S.  Mukhopadhyay, and  B.  Chanda, Fusion of <lb/> 2D grayscale images using multiscale <lb/>morphology, Pattem Recognition, 34, pp. 1939-<lb/>1949,2001. <lb/> [3]  H. Navabi, and A. Aganval, adaptive <lb/>response organizer network for spacetime <lb/>pattems in low level vision, Neural Networks <lb/>(Pergamon), 11, No. 5, pp. 825-836, 1998. <lb/>[4] G. Rees, and R. Frackowiak, C.  Frith, Two <lb/>Modulatory Effects of Attention That Mediate <lb/>Object Categorization in Human Cortex, <lb/>Science, 275, pp. 835-837, 1997 <lb/>[SI N.  Graham,  and A. Sutter, Spatial <lb/>Summation in Simple (Fourier) and Complex <lb/>(Non-Fourier) Texture Channels, Vision <lb/>Research, 38.No.2, pp. 231-257,1998. <lb/> [6] J.D. Schall, D.P. Hanes, Neural mechanism <lb/> of selection and control of visually guided eye <lb/>movements, Neural Networks,  11,  pp. 1241-<lb/>1251,1998. <lb/>[7]  G.  Gancarz, and  S.  Grossherg, A neural <lb/>model of  the  saccade generator in the recticular <lb/>formation, Neural Networks,  11,  pp. 1159-1174, <lb/>1998. <lb/> [8]  H. Navabi,  ARO  A neural network to <lb/>investigate a time-delayed imposed organization <lb/>on first-order and second-order visual stimuli at <lb/>the early vision, In Proc.of IJCNN&apos;2000, Int. <lb/>Joint Conf.  on  Neural Networks, Como, Italy. <lb/>191 T. Kohonen and R. Hari, where the abstract <lb/>feature maps of the brain might come from TNS. <lb/>22, no.3, 135-139. 1999. <lb/> [IO] H. Navabi, Adaptive Response Organizer <lb/>Network-ARO for Time Sensitive Visual <lb/></listBibl>

				<page>1826 </page>

			</back>
	</text>
</tei>
