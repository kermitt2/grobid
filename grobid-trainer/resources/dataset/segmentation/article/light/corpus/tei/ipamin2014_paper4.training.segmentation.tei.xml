<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_ipamin2014_paper4"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Applications and Challenges of Text Mining with Patents <lb/> Hidir Aras, René Hackl-Sommer, Michael Schwantner and Mustafa Sofean <lb/> FIZ Karlsruhe <lb/>Hermann-von-Helmholtz-Platz 1, D-76344 Eggenstein-Leopoldshafen <lb/> firstname.lastname@fiz-karlsruhe.de <lb/> ABSTRACT <lb/> This paper gives insight into our current research on three <lb/>text mining tools for patents designed for information pro-<lb/>fessionals. The first tool identifies numeric properties in the <lb/>patent text and normalises them, the second extracts a list <lb/>of keywords that are relevant and reveal the invention in <lb/>the patent text, and the third tool attempts to segment the <lb/>patent&apos;s description into it&apos;s sections. Our tools are used in <lb/>the industry and could be applied in research as well. <lb/> </front>
			
			<body>1. INTRODUCTION <lb/> Patents are a very complex and difficult to analyse type of <lb/>text. As described in [10], their linguistic structure differs <lb/>very much from common language. Patents, as a corpus and <lb/>as a single document, are both very heterogeneous. They be-<lb/>long to subject areas as diverse as chemistry, pharmacology, <lb/>mining and all areas of engineering, with the consequence <lb/>that all kinds of terminology can be found in a patent cor-<lb/>pus. A patent corpus usually covers a long time span, often <lb/>from the 1950s to the present. Patents from the princi-<lb/>pal patent authorities amount to more than 70 million pub-<lb/>lications. Typographical errors are not uncommon, since <lb/>many patents in their machine-readable form are derived <lb/>from OCR-processing and machine-translation. Patents are <lb/>on the average two up to five times longer than scientific ar-<lb/>ticles. Their textual part is composed mainly of the detailed <lb/>description of the invention and the claims. The former is <lb/>often similar to scientific articles, whereas the latter is char-<lb/>acterised by a legal language. <lb/>Users of patent information usually are information profes-<lb/>sionals, who cooperate with the research departments or the <lb/>legal department of their companies. They have very high <lb/>requirements on the correctness and completeness of the <lb/>data, on the efficiency of the search interface, and on the <lb/>trustworthiness of the provider. The cause of their search <lb/>is normally business critical, the endeavour compares to a <lb/>search for a needle in a haystack. Their search strategy is by <lb/>far different from a typical Google search; it uses complex <lb/> </body>
			
			<front>Copyright l&apos; 2014 for the individual papers by the papers&apos; authors. Copy-<lb/>ing permitted for private and academic purposes. This volume is published <lb/>and copyrighted by its editors. Published at Ceur-ws.org Proceedings of <lb/>the First International Workshop on Patent Mining and Its Applications <lb/>(IPAMIN) 2014. Hildesheim. Oct. 7th. 2014. At KONVENS&apos;14, Octo-<lb/>ber 8-10, 2014, Hildesheim, Germany. <lb/> </front>
			
			<body>Boolean queries, the diligent usage of proximity operators, <lb/>and vast lists of synonyms. New functionality, which helps <lb/>them in searching and analysing the result set, is therefore <lb/>greatly appreciated. Tools and methods for ordinary docu-<lb/>ments are manifold, the challenge is to adapt or to re-design <lb/>them in such a manner that they work with patents. <lb/>In this paper, we introduce three text mining tools specifi-<lb/>cally designed for patent texts we have implemented or are <lb/>investigating on, respectively. Section 2 describes the nu-<lb/>meric property extraction, which allows for recognising num-<lb/>bers, measurements, and intervals. This feature enables the <lb/>user to integrate a search for numeric properties, e.g. for <lb/>temperature measurements ranging from 150K to 200K, into <lb/>his query to enhance the precision. Section 3 shows the chal-<lb/>lenges of automatic keyword extraction with focus on the <lb/>invention, giving the user the opportunity to get a quicker <lb/>overview of the content of a single document or an answer <lb/>set. Section 4 outlines the patent description segmentation, <lb/>a tool for identifying the several parts which constitute a <lb/>patent description. With that, the user can limit his search <lb/>to specific parts of the description, again for a higher preci-<lb/>sion. Finally, we conclude this work with our main findings <lb/>and future work. <lb/> 2. NUMERIC PROPERTY EXTRACTION <lb/> In many technical fields, key information is provided in the <lb/>form of figures and units of measurement. However, when <lb/>these data appear in full text, they are almost certainly lost <lb/>for search and retrieval purposes. The reason for this is that <lb/>full text is indexed in a way that makes it searchable with <lb/>strings. In that manner, only the string representation of a <lb/>numeric property would be searchable, which is, of course, <lb/>wholly unsatisfactory. <lb/> 2.1 Related Work <lb/> To date, some attempts have been made to extract such data <lb/>automatically from text. A tentative approach in GATE <lb/>where the identification of numeric properties from patents <lb/>was addressed as a sub-task is described in [1]. [4] exam-<lb/>ine the detection of units of measurement in English and <lb/>Croatian newspaper articles over a small sample of 1745 <lb/>articles per language using NooJ. [9] investigate the issue <lb/>from a Belarussian/Russian perspective with many unique <lb/>language-related challenges relying on NooJ, too. These ap-<lb/>proaches lack either the generalisability to an extensive cor-<lb/>pus or deal mainly with the Russian language. There is also <lb/> a commercial tool available from quantalyze  1  , however, this <lb/>tool appears to identify a much more limited variety of units <lb/>than ours and it also lacks the identification of enumerations, <lb/>which are abundant in patents and therefore indispensable. <lb/> 2.2 Requirements and Tasks <lb/> The following sections describe the requirements and rele-<lb/>vant tasks in numeric property extraction. <lb/> Identification of numbers <lb/> Clearly, a number consisting of digits only can be easily iden-<lb/>tified. For numbers with decimal points we have observed <lb/>that in our data both numbers following English as well as <lb/>German convention are present. Numbers do also appear in <lb/>scientific notation, and there is a range of characters that <lb/>is used to denote a multiplication or a exponentiation. We <lb/>also note the use of the HTML sup-tag indicating super-<lb/>script. Examples of valid expressions therefore include: <lb/> 1,300.5 (English convention); 1.300,5 (German); <lb/>3.6 x 10-4; 10^5; 4.5x10sup&quot;5; 8.44 x 10 sup* 10 <lb/> Frequently, in patents numbers are spelled-out, as in ten <lb/> mg instead of 10 mg. These instances are recognised and <lb/>converted into their respective numerical values. <lb/> Identification of units of measurements <lb/> This task, looking simple at first sight, requires some at-<lb/>tention with respect to spelling (in particular uppercase vs. <lb/>lowercase), spacing, and disambiguation. <lb/> • Upper-/lower case: There are some instances, in which <lb/>capital letters and small letters refer to different en-<lb/>tities, e.g. S stands for Siemens, a unit for electric <lb/>conductance, whereas s stands for second. <lb/> • Spacing: There is some diversity regarding blank char-<lb/>acters in spellings of units of measurement consisting <lb/>of more than one word, e.g. J per mol-K. Therefore, <lb/>the longest possible sequence in a series of tokens has <lb/>to be matched. <lb/> • Ambiguity: For a few units, their abbreviated spellings <lb/>might refer to different entities, e.g. C might stand for <lb/> Degrees Celsius or Coulomb; A might mean Ampere or <lb/>Angström (cf. Noise Reduction). <lb/>The vast majority of units appear after numbers; however, <lb/>there are some units that only appear before numbers, like <lb/>the pH value or the refractive index. <lb/> Unit normalisation <lb/> Many measurements of physical properties can be expressed <lb/>with various units. For example, 800W is equivalent to 800 <lb/>Joules/second, and 180  •  C to 453 degrees Kelvin. For the <lb/>measurement of pressure, the following non-exhaustive list <lb/>of units can be used: kg/m2; N/m2; Pa; Torr; atm; cm <lb/>Hg; ounces per square yard. Additionally, a great number <lb/>of prefixes like nano, µ, kilo, tera and their abbreviations <lb/>have to be considered. Hence, to get a hit with standard <lb/>indexing, a user would need to include all sorts of variations <lb/>in order to achieve even a modicum of accuracy and recall. <lb/>Clearly, a superior way to address these issues is to define <lb/> 
			
			<note place="footnote">1  https://www.quantalyze.com/ <lb/></note>
			
			a common base unit for all units which describe the same <lb/>physical property and convert all instances from the full text <lb/>to that base unit for indexing and searching. Therefore, <lb/>all instances of units from the full text are converted into <lb/>their corresponding base units as they are defined in the <lb/>International System of Units (SI). <lb/> Identification of Intervals <lb/> There are two main ways in which intervals can be con-<lb/>strued. One relies on context words, in which the words sur-<lb/>rounding numeric entities indicate an interval, e.g. between <lb/>12 and 100 Watts. Another way comprises the use of sym-<lb/>bols, e.g. 5–6 mg or &gt;12 hours. While there are only some <lb/>phrases that are often encountered which indicate intervals <lb/>with bounds on both sides, there are many more when it <lb/>comes to intervals unbounded on one side. The latter can <lb/>appear before or behind the numeric entities to which they <lb/>refer, e.g. more than 200ml or 200ml or more. Negated for-<lb/>mulations like not more than have to be taken into account <lb/>as well. Frequently, there are also adverbs present which <lb/>add no specific information to the context, but just need to <lb/>be filtered out, e.g. about, around, roughly. <lb/> Enumerations and Ratios <lb/> Enumerations of numbers or even intervals are very com-<lb/>mon in patents. They usually follow a comma-separated <lb/>pattern: a thickness of 1, 2, 3, 4, or 5 mm. The identifi-<lb/>cation of enumerations is rather straightforward as there is <lb/>only a small number of variations that together cover &gt;90% <lb/> of occurrences. <lb/>Ratios are used to describe the proportionate relationship <lb/>between two or more entities from a common physical di-<lb/>mension. A sample expression from an everyday background <lb/>might be make sure the ratio between sugar and flour is 1:3. <lb/> This being a simple example, the recognition of ratios is <lb/>actually a difficult endeavour. The reason is the immense <lb/>heterogeneity in which ratios can be expressed. Simple ra-<lb/>tio formulations are typically separated by colons or slashes. <lb/>They take general forms like &quot; Number:Number &quot; or &quot; Number-<lb/>to-Number &quot; . An approach relying solely on these patterns <lb/>will invariably locate many false positives. <lb/> Noise Reduction <lb/> The aim of noise reduction is to eliminate false positives. <lb/>This is a critical task especially for units of measurements <lb/>consisting of only one letter, the most frequent being the <lb/>aforementioned A and C. <lb/> 2.3 Implementation <lb/> We are using the Apache UIMA framework for the pre-<lb/>sented analysis of data. It provides a robust infrastructure <lb/>for developing modular components and deploying them in <lb/>a production environment. Finite State Automata (FSA) <lb/>are used throughout for pattern matching. They perform <lb/>much better than Java-patterns and regular expressions, and <lb/>even small improvements add up quickly when it comes to <lb/>processing data in the terabyte range. For the identifica-<lb/>tion of numbers, intervals, and enumerations valid sequences <lb/>of phrase parts and type-related placeholders (both config-<lb/>urable) are expressed in a FSA-based grammar. <lb/> Adapted to the English language, our system currently recog-<lb/>nises more than 15,000 unit variants belonging to 80 base <lb/>units. Included are all commonly used dimensions like time, <lb/>temperature, or weight, but also many dimensions that are <lb/>more relevant in professional use, e.g. dynamic viscosity, <lb/>solubility, or thermal conductivity. We are using a window-<lb/>ing technique for ratio recognition. From any occurrence of <lb/>the word ratio in the text, up to five words to the left and <lb/>15 words to the right are evaluated. While this approach <lb/>manages to identify many valid ratios, many cases still re-<lb/>main in which ratios are not recognised, like ratios for more <lb/>than two entities or ratios in alternative formulations (e.g. <lb/> 10 parts carbon black and 4 to 6 parts oil extender ). These <lb/>will be dealt with in future versions. <lb/>Conversion between units is a straightforward task. The <lb/>units, their variants and conversion rules are kept in a con-<lb/>figuration file. Three more configuration files are provided <lb/>for rules to recognise intervals and for the noise reduction, <lb/>respectively. By this means, changes or extensions can be <lb/>effected without the need to change the source code and re-<lb/>deploy the software. For the noise reduction task, two lists <lb/>have been defined. The first list applies to all units. It con-<lb/>tains terms like figure or example. If one of those global <lb/>terms precedes a numeric entity, that entity is judged as <lb/>noise and removed (examples: figure 1A or drawing 2C ). <lb/>The second list is specific to certain units only. If a term <lb/>contained therein follows a numeric entity, this text passage <lb/>will be ignored as well (e.g. 13C NMR). Extracted and con-<lb/>verted entities are added to our search engine indexes. <lb/>Regarding evaluation, we followed an iterative development <lb/>cycle with many intellectual assessments. In the process, we <lb/>have set up extensive JUnit-tests for software development <lb/>and continuous integration. When a test person or, later, <lb/>one of our customers found a specific piece of text that re-<lb/>quired improvement, we included it. As a result, given the <lb/>size of our data it has over time become increasingly difficult <lb/>to find text snippets that are not or faultily recognised. We <lb/>have not carried out extensive formal recall/precision evalu-<lb/>ations, because the effort required building a gold standard <lb/>with significant sample size and real world data (as opposed <lb/>to manually construed &quot; difficult &quot; data) is not offset by the <lb/>projected gains. All our customer feedback indicates that <lb/>our results are very good. <lb/> 3. KEYWORD EXTRACTION <lb/> Keywords extracted from a document are of great benefit for <lb/>search and content analysis. In the patent domain important <lb/>keywords can be utilised for searching as well as getting <lb/>an overview of the topics and the focus of a single patent <lb/>document or an answer set. In both cases they can avoid <lb/>unnecessary time-consuming and costly analysis e.g. in prior <lb/>art or freedom to operate scenarios. Existing methods for <lb/>keyword extraction – be it automatic or supervised – use <lb/>either statistical features for detecting keywords based on <lb/>the distribution of words, sub-words and multi-words, or <lb/>exploit linguistic information (e.g. part-of-speech) over a <lb/>lexical, syntactic or discourse analysis. Furthermore, hybrid <lb/>approaches exist, which try to combine the various types of <lb/>algorithms and apply additional heuristic rules, e.g. based <lb/>on position, length or layout. <lb/> 3.1 Related Work <lb/> [2] used term frequency, phrase frequency and the frequency <lb/>of the head noun for identifying the relevant keywords from <lb/>a candidate set. The phrase candidates are sorted according <lb/>to the head noun frequency. Afterwards additional statis-<lb/>tical filters are applied. [7] reported that technical terms <lb/>mainly consist of multi-words, e.g. noun phrases with a <lb/>noun, adjective and the preposition &quot; of &quot; in English texts. <lb/>Single words in general are less appropriate for represent-<lb/>ing terminology. Most word combinations describing termi-<lb/>nology are noun phrases with adjective-noun combinations. <lb/>Experiments also indicate the impact of the term position, <lb/>e.g. in title or a special section. It was also shown that <lb/>proper nouns rarely represent good keywords for represent-<lb/>ing terminology. <lb/> 3.2 Challenges and Tasks <lb/> One main challenge in keyword extraction is related to the <lb/>subjectivity of keywords for a particular user, whose exper-<lb/>tise, common knowledge about the regarded technical do-<lb/>mains and the focus of interest can vary with respect to man-<lb/>ifold aspects. Besides that, patent full texts describe general <lb/>aspects, state of the art that experts are familiar with and <lb/>make use of expressions and terms that are rarely used in <lb/>classic texts (neologisms). Hence, separating the wheat from <lb/>the chaff can be difficult. Moreover, as the description part <lb/>of a patent can be very heterogeneous, mixed with tables, <lb/>figures, examples, mathematical or chemical formulas, etc., <lb/>identifying relevant sections that contain keywords that are <lb/>directly related to the invention can be a tricky task as well. <lb/>All these challenges call for deeper analysis of the content, in <lb/>order to better understand patent texts and improve search-<lb/>ing specific aspects or entities in the patent texts. <lb/> Figure 1: Phrase pattern distribution of top key-<lb/>words from three experts (Analysis of EPO patents). <lb/> Analyses show that most of the relevant linguistic phrases in <lb/>patent texts are noun-sequences and noun-adjective combi-<lb/>nations (Figure 1). Despite this, depending on the domain <lb/>of interest, complex noun phrases that are used to describe, <lb/>e.g. a process, chemical entity or formula, and verbal phrases <lb/>can be observed. The role of the verbal phrases seems to be <lb/>debatable, as recent results [8] show. <lb/>Investigation of evaluation data from experts indicate that <lb/>extracting phrases of length ≤ 5 is reasonable in case of <lb/>linguistic technical terms, which might be different when <lb/>considering also domain-specific entities from the chemical, <lb/>bio-pharma, or other domains. Figure 2 shows the frequency <lb/>distribution of the phrase lengths up to 9 words in the an-<lb/> notated corpus. For example, in the descriptions part, the <lb/>experts annotated more than 350 times phrases consisting <lb/>of only two words. Focusing on automatic keyword extrac-<lb/>tion, a further prerequisite is to deal with similar phrases <lb/>with different morphological and syntactical structure. For <lb/>keyword search or for generating content overviews this syn-<lb/>tactic variations [5] must be normalised and mapped to one <lb/>canonical form. For example: circular or rectangular pat-<lb/>terns → circular pattern, rectangular pattern, method for <lb/>combating spam → spam combating method, etc. <lb/>Another important task that also concerns patent search <lb/>in general is semantic normalisation to aggregate semanti-<lb/>cally equivalent or similar phrases which can vary in wording <lb/>considerably. The recognition of specific entities – be it sim-<lb/>ple or complex forms, identifying taxonomic relations, syn-<lb/>onyms, chemical entities, enumerations, etc. represent other <lb/>challenges in the course of understanding a given patent text <lb/>beyond general linguistic phrases or terms. In classic key-<lb/>word extraction, keywords in title or abstract are automat-<lb/>ically regarded as important, while for patents a sophisti-<lb/>cated weighting scheme based on analysing keyword occur-<lb/>rence and co-occurrence with respect to different sections is <lb/>required. A further task is to decide how the final keyword <lb/>set is presented to the user. While in classic keyword extrac-<lb/>tion rarely more than 10 keywords are returned to the user, <lb/>in the patent domain information professionals indicate that <lb/>displaying 50, even 100 keywords would be desirable. <lb/> 3.3 Implementation and Evaluation <lb/> A proof-of-concept prototype based on linguistic and sta-<lb/>tistical analysis was implemented in order to evaluate some <lb/>of the described tasks. The general procedure comprised <lb/>the steps for linguistic and statistical pre-processing, noun <lb/>phrase extraction and analysis and phrase weighting based <lb/>on features such as length, position, TF-iDF weight or sec-<lb/>tion. A typical linguistic pre-processing includes sentence <lb/>detection, tokenisation, POS-tagging and noun phrase chunk-<lb/>ing. The noun phrase extraction allows to identify basic pat-<lb/>terns of important noun phrase chunks, while applying a fil-<lb/>tering method for removing irrelevant (stop-)words at start <lb/>and end. As many syntactic variations of the extracted key-<lb/>words may occur besides a syntactic normalisation method, <lb/>linguistic and statistical analysis must be applied in order <lb/>to reduce the candidate set for ranking. A candidate phrase <lb/>is evaluated by means of a scoring formula that takes into <lb/> Figure 2: Phrase length distribution of top keywords <lb/>for abstract, claims and descriptions. <lb/> account the respective parameters. In order to avoid loss of <lb/>information, a conservative method is preferred over utilising <lb/>harsh frequency thresholds. Rather, the overall ranking is <lb/>affected by an elaborated weighting scheme considering be-<lb/>sides intra-section features also field-based analysis for the <lb/>sections title, abstract, claims and the descriptions text. <lb/> 3.3.1 Dataset and Evaluation <lb/> The implemented approach was evaluated based on a corpus <lb/>with 20.000 documents from several domains, e.g. chemical, <lb/>bio-pharma as well as engineering, from the European patent <lb/>database comprising granted patent documents having ti-<lb/>tle, abstract, claims and descriptions text. An expert-based <lb/>study served to create a test corpus of 70 patent documents <lb/>annotated with keywords in the aforementioned main sec-<lb/>tions of the patent text. Therefore, the two participating <lb/>experts marked up to 20 most relevant keywords in a patent <lb/>document that characterise the topic and the focus of the de-<lb/>scribed invention. The main textual sections comprising the <lb/>combined title-abstract, claims and descriptions were eval-<lb/>uated separately, i.e. keywords sets were not mixed. The <lb/>created (annotated) datasets were used for evaluating the <lb/>keyword extraction. For evaluating the implemented base-<lb/>lines based on the TF-iDF weighting scheme, the rank-based <lb/>evaluation metrics precision@k, recall@k and F-Score have <lb/>been used. <lb/>For the field combination title-abstract, the exact keyword <lb/>match results for precision varied between 34% for the top <lb/>10 keywords and 20% up to 30% for the top 20. Looking <lb/>at recall considering a wider range of up to 50 keywords, a <lb/>score around 40% was calculated. As exact match does not <lb/>consider syntactic variations for the extracted key phrases, <lb/>a fuzzy matching method was applied as well. Depending <lb/>on the fuzziness parameter, false positives may also be re-<lb/>turned, which only can be detected by manual expert-based <lb/>inspection. The results after applying the fuzzy matching <lb/>method were much better for precision ( ˜ 75% for the top <lb/>10 keywords and 46% for the top 20 keywords) and recall <lb/>( ˜ 87%). For the claims the precision varied between 27% <lb/>and 30% for the top 20 keywords in case of exact match, <lb/>while again the recall for the extracted keywords increased <lb/>from 27% to approx. 46% when taking a wider range of <lb/>up to 50 keywords. For fuzzy matching, a precision score <lb/>above 75% for the top 10 keywords and 70% for the top 20 <lb/>was achieved. In claims, the recall for the top 50 keywords <lb/>was about 92%. Due to the heterogeneity and the amount <lb/>of text present in the descriptions part, the challenges seem <lb/>here much higher. For the TF-iDF baseline the exact match <lb/>results for precision varied between 14%-15%, while the re-<lb/>call for the top 10-50 keywords increases from 8% to 25%. <lb/>Applying fuzzy matching, the scores for precision were again <lb/>much better. Depending on the fuzziness parameter for the <lb/>matching similarity that varied between 0.5 (50% match) <lb/>and 0.9 (90% match), the precision score was between 80% <lb/>and 50% for the top 50 keywords for the regarded dataset. <lb/> 4. TEXT SEGMENTATION <lb/> Patent documents are lengthy, abundant, and full of de-<lb/>tails, such that it may hinder the topic analysis for humans <lb/>and for machines as well. One of the text mining techniques <lb/>which can ease these intricacies is text segmentation [3]. The <lb/>automatic structuring of patent texts into pre-defined sec-<lb/> Table 1: A list of sections in description text of the patent. <lb/> Section Types <lb/>Example <lb/> Detailed Description <lb/>Best Mode of the Invention, Embodiments of the Invention <lb/>Background <lb/>Background of Invention, Prior Art <lb/>Summary <lb/>Summary of the Invention, Objectives of the Invention, Disclosures <lb/>Methods <lb/>Procedures, Operations, Experiments <lb/>Drawing and Figures <lb/>Detailed Description of the Drawing <lb/>Applicability <lb/>Industrial Applications, Applications of the Invention <lb/>Technical Field <lb/>Technical Field of the Invention, Field of Technology <lb/>Examples <lb/>Embodiment Example, Experimental Example <lb/>Sequences <lb/>List of Sequences, Numerical Sequence <lb/>References <lb/>List of References, Literatures <lb/>Statements <lb/>Statement of Government Rights, Acknowledgement <lb/> tions will serve as a pre-processing step to patent retrieval <lb/>and information extraction, as well as enable the interested <lb/>people to understand easily the structure of a patent that <lb/>leads to fast, efficient, and easy access to specific information <lb/>which they are looking for. Furthermore, noun phrases of <lb/>important sections in the patent texts could be used as main <lb/>features for patent classification and clustering to achieve a <lb/>good performance. <lb/>The textual part of a patent contains title, abstract, claims, <lb/>and the detailed description (DetD) of the invention. The <lb/>latter includes the summary, embodiment, and the descrip-<lb/>tion of figures and drawings of the invention. As of the <lb/>amount of information in DetD, there is a need for auto-<lb/>mated tools, which can determine the document-level struc-<lb/>ture of the DetD, identify the different sections and map <lb/>them automatically to known section types. There has been <lb/>previous work which showed that the semantic of the patent <lb/>document structure is valuable in patent retrieval [6], but it <lb/>only focused on structured patent text which is labelled by <lb/>specific tags in the original text. The work in [1] presented <lb/>a rule-based information extraction system to automatically <lb/>annotate patents with relevant metadata including section <lb/>titles. In this section, we describe our text segmentation <lb/>method which is used to recognise the structure of the DetD. <lb/>There are many challenges that arise in patent text segmen-<lb/>tation, for example measuring the similarity between the <lb/>sentences is difficult to use because there are a lot of iden-<lb/>tical terms in the sentences. Another challenge is that the <lb/>patent contains a lot of new technical terminologies which <lb/>are hard to collect when using a term matching technique. <lb/>To meet these challenges, we currently develop a patent text <lb/>segmentation tool which automatically segments the patent <lb/>text into semantic sections by discovering the headers inside <lb/>the texts, identifying the text content which is related to <lb/>each header, and determining the meaning of the header. <lb/> 4.1 Dataset and Preprocessing <lb/> Our dataset consists of a random sample of 139,233 patents <lb/>from the European Patent Office (EPO) and converted by <lb/>FIZ Karlsruhe  2  into a proprietary XML format with tagged <lb/>paragraphs. Processing techniques have been applied to <lb/>understand the type, style, and format of headings inside <lb/>patent texts. We started by parsing XML files to get a list of <lb/>headings in the description part. The headers pass through <lb/>a cleansing process that is represented by removing unde-<lb/>sired tokens in each header (e.g.; numbers, special charac-<lb/>ters, words containing special symbols, words starting with <lb/> 2  http://www.fiz-karlsruhe.de <lb/>numbers, math equations, and formulas) via a tokenisation <lb/>process. Then, we created the positive-list which contains <lb/>terms that appear more than five times in all headings of the <lb/>dataset, and the first-token list which includes terms from <lb/>the headers which appear more than five times as the first <lb/>word of a header. <lb/> 4.2 Header Detection and Meaning <lb/> In cooperation with a patent expert, we identified segmen-<lb/>tation guidelines. These guidelines help us to understand <lb/>the section types (Table 1) in the DetD. In order to discover <lb/>the headers inside the DetD, we need to get the boundary <lb/>of the headers. i.e., the header&apos;s start and end. We call this <lb/>operation Header Detection. Then, we identify the text con-<lb/>tent which is related to each header. The header meaning <lb/>on the other hand is represented by assigning the header to <lb/>an appropriate section type (e.g.; summary, example, back-<lb/>ground, method, etc). Here, a rule-based approach is more <lb/>suitable because in the patent domain, there is no sufficient <lb/>training data for a machine learning algorithm to be success-<lb/>ful. To do so, we develop a rule-based algorithm to identify <lb/>headers and their boundaries. The output consists of all <lb/>headers and their positions inside the DetD. Our algorithm <lb/>works as follows: As input we take the DetD as a sequence of <lb/>paraghraphs. Then, we test the following features to decide <lb/>whether a paragraph is a header or not: <lb/>A. The number of words in the paragraph. <lb/>B. The number of characters in the paragraph. <lb/>C. True, if all letters in the current paragraph are in upper <lb/>case; false otherwise. <lb/>D. True, if all words in the paragraph start with an upper <lb/>case letter; false otherwise. <lb/>E. True, if the current paragraph contains words from the <lb/>positive-list, false otherwise. <lb/>F. True, if in the current paragraph more words start with <lb/>a capital letter than with a lowercase; false otherwise. <lb/>G. True, if the current paragraph starts with a bullet; <lb/>false otherwise. <lb/>H. True, if the previous or the next paragraph starts with <lb/>a bullet; false otherwise. <lb/>I. True, if the first token in the paragraph appears in the <lb/>first-token list; false otherwise. <lb/>J. True, if the current text paragraph contains a simple <lb/>chemical text; false otherwise. <lb/></body>

			<note place="headnote"> K. The average header length in the dataset&apos;s headers. <lb/></note>

			<body>L. The average number of words in the dataset&apos;s headers. <lb/>We use these features on each input paragraph of the DetD <lb/>to build decision rules for the header detection. Some of the <lb/>decision rules are listed below: <lb/>i. C is true and G is false and A≥1 and J is false <lb/>ii. D is true, E is true, A≥1, G is false, and J is false <lb/>iii. G is true, H is false, A&lt;L, J is false, B&lt;K, and A≥1 <lb/>iv. I is true, F is true, J is false, A≥1, and G is false. <lb/>After detecting the headers, we identify the start and end <lb/>position of each header in the DetD. The detection of the <lb/>text belonging to the header is performed by identifying the <lb/>paragraphs between the current header and the next header. <lb/>After the detection of headers and their boundaries, each <lb/>header should be assigned to one of the appropriate pre-<lb/>defined section types by using a prediction model from the <lb/>machine learning step. This task was modelled as a classifi-<lb/>cation task via constructing a training dataset by labelling <lb/>manually a representative sample of 1377 headers of sec-<lb/>tion types that are shown in the Table 1. The labelling <lb/>process is done by applying the guidelines created by the <lb/>patent expert. Pre-processing steps were performed to re-<lb/>move undesired tokens like numbers, special symbols, and <lb/>stopwords, as well as to compute the weight vector for the <lb/>training set. We used Support Vector Machines (SVMs) as <lb/>a multi-classification technique to train the dataset. The <lb/>evaluation was done by using 5-fold cross validation, and <lb/>the performance of the categorisation achieved up to 91%, <lb/>90%, and 91% of accuracy, recall, and precision respectively. <lb/> 5. CONCLUSION AND FUTURE WORK <lb/> In this paper we presented our research on three text min-<lb/>ing tools tailored to the singularities of patent documents. <lb/>Though patents are very different from normal texts in length, <lb/>structure, language, and terminology, though the require-<lb/>ments of patent information searchers are much more strict <lb/>than those of other users, and though no gold-standards for <lb/>these tasks are available, which reflect a realistic retrieval sit-<lb/>uation, we could show, that solutions exist which can cope <lb/>with these challenges. The results of our numeric entity <lb/>extractor are since long available to our clients and are well <lb/>accepted. When designing functionality like keyword extrac-<lb/>tion or description segmentation, we seek at an early stage <lb/>the feedback of our customers. For the numeric property <lb/>extraction, there are still some areas of potential for further <lb/>research. Disambiguation is one of them: the symbols A <lb/> and C were already mentioned; in might be a preposition <lb/>or denote inch. Other topics concern the extraction of rela-<lb/>tions. For instance, it might be useful to identify what kind <lb/>of a temperature is discussed in a text. Is it a melting point <lb/>or a boiling point? To which substance or process does the <lb/>temperature refer? Oftentimes in patents, whole recipe-like <lb/>paragraphs are available from which a lot of factual data <lb/>could be extracted. For keyword extraction, besides the <lb/>challenges discussed before, learning keywords by consider-<lb/>ing domain-specific knowledge from controlled vocabularies <lb/>is required to identify most relevant facts about an inven-<lb/>tion more precisely. It is also reasonable to extract keywords <lb/>rather on the basis of semantic information tailored for a <lb/>specific domain and use, e.g. treatment of diseases, medical <lb/>substances, etc. than in an isolated manner. Possible en-<lb/>hanced methods for keyword context analysis could rely on <lb/>semantic analysis based on the co-occurrence method, (la-<lb/>tent) semantic analysis or other dedicated semi-supervised <lb/>and unsupervised machine learning techniques. Further-<lb/>more, a more enhanced method for semantic segmentation of <lb/>patent text needs to deal with patents that do not have any <lb/>heading inside their texts and address the overlap problem <lb/>between section types. Our final goal is to develop a flexible, <lb/>scalable and automatic tool, which has the ability to facili-<lb/>tate the reading of a patent, keyword extraction, summary <lb/>extraction, and classification and clustering of patent texts. <lb/> </body>
			
		<back>	
			<listBibl>6. REFERENCES <lb/> [1] M. Agatonovic, N. Aswani, K. Bontcheva, <lb/>H. Cunningham, T. Heitz, Y. Li, I. Roberts, and <lb/>V. Tablan. Large-scale, Parallel Automatic Patent <lb/>Annotation. In Proceedings of the 1st ACM Workshop <lb/>on Patent Information Retrieval, PaIR &apos;08, pages 1–8, <lb/>New York, NY, USA, 2008. ACM. <lb/>[2] K. Barker and N. Cornacchia. Using Noun Phrase <lb/>Heads to Extract Document Keyphrases. In <lb/>H. Hamilton, editor, Advances in Artificial <lb/>Intelligence, volume 1822 of LNCS, pages 40–52. <lb/>Springer Berlin Heidelberg, 2000. <lb/>[3] D. Beeferman, A. Berger, and J. Lafferty. Statistical <lb/>Models for Text Segmentation. Machine Learning, <lb/> 34(1-3):177–210, Feb. 1999. <lb/>[4] B. Bekavac, Z. Agic, K. Sojat, and M. Tadic. Detecting <lb/>Measurement Expressions using NooJ. In Proceedings <lb/>of the Conference on NooJ, pages 121–127, 2009. <lb/>[5] R. Bhagat and E. H. Hovy. What Is a Paraphrase? <lb/> Computational Linguistics, 39(3):463–472, 2013. <lb/>[6] H.-Y. J. Jae-Ho Kim, Jin-Xia Huang and K.-S. Choi. <lb/>Patent Document Retrieval and Classification at <lb/>KAIST. &quot; Proceedings of NTCIR-5 Workshop Meeting, <lb/>December 6-9, Tokyo, Japan &quot; . <lb/>[7] J. S. Justeson and S. M. Katz. Technical Terminology: <lb/>Some Linguistic Properties and an Algorithm for <lb/>Identification in Text. Natural Language Engineering, <lb/> 1(1):9–27, 1995. <lb/>[8] J. M. Schulz, D. Becks, C. Womser-Hacker, and <lb/>T. Mandl. A Resource-light Approach to Phrase <lb/>Extraction for English and German Documents from <lb/>the Patent Domain and User Generated Content. In <lb/>N. Calzolari, K. Choukri, T. Declerck, M. U. Dogan, <lb/>B. Maegaard, J. Mariani, J. Odijk, and S. Piperidis, <lb/>editors, LREC, pages 538–543. European Language <lb/>Resources Association (ELRA), 2012. <lb/>[9] A. Skopinava and Y. Hetsevich. Identification of <lb/>Expressions with Units of Measurement in Scientific, <lb/>Technical &amp; Legal Texts in Belarusian and Russian. In <lb/> Proceedings of the Integrating IR technologies for <lb/>Professional Search Workshop, pages 26–34, 2013. <lb/>[10] J. M. Struss, D. Becks, T. Mandl, M. Schwantner, and <lb/>C. Womser-Hacker. Patent Retrieval und Patent <lb/>Mining: Sind die Anforderungen eingelöst? In 3. <lb/>DGI-Konferenz. Informationsqualität und <lb/>Wissensgenerierung., pages 25–36, 2014. </listBibl>

		</back>
	</text>
</tei>
