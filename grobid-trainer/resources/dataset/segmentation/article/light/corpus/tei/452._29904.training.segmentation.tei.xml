<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Cloud Classi cation Using Error-Correcting Output Codes <lb/>David W. Aha <lb/>Navy Center for Applied Research in Arti cial Intelligence <lb/>Naval Research Laboratory <lb/>Washington, DC 20375 <lb/>aha@aic.nrl.navy.mil <lb/>Richard L. Bankert <lb/>Marine Meteorology Division <lb/>Naval Research Laboratory <lb/>Monterey, CA 93943 <lb/>bankert@nrlmry.navy.mil <lb/>October 30, 1996 <lb/>Submission to AI Applications: Natural Resources, Agriculture, and Environmental Science. <lb/>Corresponding author: The rst author is the corresponding author for this submission. David&apos;s <lb/>phone number is (202) 767-9006 and FAX number is (202) 767-3172. <lb/>Suggested running head: \Cloud Classi cation Using Error-Correcting Output Codes&quot; <lb/>Available as NCARAI Technical Note AIC-96-024 <lb/></front>

			<page>1 <lb/></page>

			<body>Abstract <lb/>Novel arti cial intelligence methods are used to classify 16x16 pixel regions <lb/>(obtained from Advanced Very High Resolution Radiometer (AVHRR) images) in <lb/>terms of cloud type (e.g., stratus, cumulus, etc.). We previously reported that <lb/>intelligent feature selection methods, combined with nearest neighbor classi ers, can <lb/>dramatically improve classi cation accuracy on this task. Our subsequent analyses <lb/>of the confusion matrices revealed that a small number of confusable classes (e.g., <lb/>cirrus and cirrostratus) dominated the classi cation errors. We conjectured that, if <lb/>the class labels in the data were re-represented so that these cloud classes are more <lb/>easily distinguished, then additional accuracy gains might result. We explored this <lb/>hypothesis by replacing each class label with a set of error-correcting output codes, <lb/>a general technique applicable to any classi cation algorithm for tasks with at least <lb/>three classes. Our initial results are promising; error correcting codes signi cantly <lb/>increased classi cation accuracy compared with using standard representations for <lb/>class labels. To our knowledge, this is the rst successful integration of k-nearest <lb/>neighbor classi ers and error-correcting output codes (i.e., where k is, e ectively, <lb/>small). One conclusion is that environmental scientists should always select, for <lb/>their classi cation tasks, a classi er that reduces both variance and learning bias. <lb/>1 Introduction <lb/>Several types of algorithms have been evaluated for their utility on various cloud classi cation <lb/>tasks, including neural networks (Welch et al. 1992), Bayesian approaches (Bankert 1994), and <lb/>k-nearest neighbor algorithms (Aha and Bankert 1996). A frequent goal of classi cation <lb/>algorithms in the context of environmental science applications is to maximize accuracy when <lb/>predicting classi cations for previously unseen query samples. Thus, several innovative methods <lb/>have been evaluated for their ability to enhance accuracy (e.g., clustering hierarchical neural <lb/>networks (Welch 1996)). <lb/>One general way to increase accuracy involves re-representing samples so that their classes can be <lb/>more easily distinguished. For example, principle components analysis can be used to re-represent <lb/>numeric features used to describe samples; new features are combinations of the old ones, <lb/>carefully selected to increase the distance between samples of di erent classes. Several similar <lb/>methods, for use with either numeric or symbolic features, have been developed to automatically <lb/>modify the representations of samples (e.g., Mohri and Tanaka 1994, Rendell and Seshu 1990) <lb/>with the intent of increasing accuracy. This topic is often referred to as constructive induction by <lb/>machine learning researchers (Birnbaum and Collins 1991). <lb/>Although these methods re-represent samples, they do not re-represent the class labels associated <lb/>with them. Recently, Dietterich and Bakiri (1991, 1995) introduced a technique that re-represents <lb/>samples&apos; class labels using error-correcting output codes (ECOCs). Like successful constructive <lb/>induction techniques, ECOCs distinguish samples in di erent classes by making the class <lb/>encodings themselves more dissimilar. Unlike constructive induction techniques, ECOCs do not <lb/>require a carefully constrained search through the space of feature combinations, and thus are <lb/>easier to use. Dietterich and Bakiri showed that, for some multiclass (i.e., more than two classes) <lb/>classi cation tasks, ECOCs can signi cantly increase classi cation accuracy for both decision tree <lb/>induction algorithms and neural networks trained using the error backpropagation algorithm <lb/>(Rumelhart et al. 1986), although with substantially greater computational costs. These two <lb/>algorithms both generate nonlocal classi ers, which generate class predictions using all <lb/>information in the set of training samples rather than only information local to the query sample. <lb/>Kong and Dietterich (1995) explained why ECOCs work well for nonlocal learning algorithms: <lb/>they reduce the variance of learning algorithms and correct for errors caused by learning biases. <lb/>Thus, we surmised that they might work well for our cloud classi cation application. <lb/>However, we have been using local algorithms (i.e., variants of the nearest neighbor 1 classi er) for <lb/>our cloud classi cation task, which generate class predictions by using only local information (i.e., <lb/>one or more most similar samples) to the query. This causes a con ict: ECOCs usually do not <lb/>work well in conjunction with local classi ers because local learning algorithms cannot correct <lb/>errors caused by learning biases (i.e., their classi cation errors are correlated) (Kong and <lb/>Dietterich 1995). We hypothesized that selecting di erent feature subsets for each bit in an <lb/>ECOC code would allow ECOCs to perform well with a k-nearest neighbor classi er; this prevents <lb/>errors across output bits from being correlated because di erent sets of k neighbors will be <lb/>nearest for di erent output bits. This paper describes an empirical evaluation for this hypothesis. <lb/>The rest of this paper describes our application of error-correcting output code techniques to a <lb/>cloud classi cation task. Section 2 describes the cloud data set we used and summarizes our <lb/>previous ndings. Section 3 then introduces the subject of error-correcting output codes, while <lb/>Section 4 details the speci c ECOC technique we applied. Our initial evaluation of this approach <lb/>is summarized in Section 5. Section 6 discusses the implications of our ndings for environmental <lb/>scientists interested in applying AI techniques to classi cation tasks. We conclude and describe <lb/>future research issues in Section 7. <lb/>2 Task Description and Background <lb/>Our work was motivated by research conducted at the Marine Meteorology Division (MMD) of <lb/>the Naval Research Laboratory. One of their objectives is to create shipboard decision aid tools <lb/>for assisting US Navy personnel in various meteorological analysis and prediction tasks. Tools <lb/>developed or being developed at MMD that use AI include AESOP (an expert system for <lb/>shipboard obscuration prediction (Peak and Tag 1989)), SIAMES (a satellite image analysis <lb/>meteorological expert system (Fett et al. 1997)), ExperCAT (a clear air turbulence expert system <lb/>(Ellrod et al. 1993)), and MEDEX (an expert system for forecasting Mediterranean gale force <lb/>winds (Kuciauskus et al. 1996)), as well as classi ers of cloud types (Bankert 1994) and cloud <lb/>systems (Bankert and Tag 1996) in satellite imagery. <lb/>With satellite imagery being a primary (and sometimes only) source of observational data for <lb/>remote maritime regions, automated identi cation of cloud types within those images would be a <lb/>useful tool for the operational forecaster. Because of the number of years required to develop <lb/>satellite cloud analysis expertise, Bankert&apos;s motivation was to provide navy personnel with <lb/>automated cloud type classi cation similar to that displayed by the experts. Bankert (1994) <lb/>describes two steps towards this goal: development of (1) an expertly-labeled cloud database, and <lb/>(2) a cloud classi cation procedure that is trained using this database. Based on the work of <lb/>Crosiar (1993), Bankert assimilated a data set containing 1633 samples (16x16 pixel areas) of <lb/></body>

			<note place="footnote">1 A nearest neighbor classi er (Dasarathy 1991) generates a class prediction for an unlabeled query q by using <lb/>a real-valued distance function on all samples; it computes the query&apos;s distance between q and all stored training <lb/>samples, retrieving the stored sample xi whose distance is smallest. The class of xi is returned as q&apos;s predicted class. <lb/></note>

			<page>3 <lb/></page>

			<body>Figure 1: An AVHRR image with sample regions marked (visible channel, left; infrared channel, <lb/>right). <lb/>cloud types taken from the National Oceanic and Atmospheric Administration (NOAA) <lb/>Advanced Very High Resolution Radiometer (AVHRR) sensor aboard polar-orbiting satellites. <lb/>Two channels of an AVHRR image with sample regions (16x16 pixel) marked by boxes is shown <lb/>in Figure 1. Images were taken from various maritime regions throughout the Northern <lb/>Hemisphere during a one-year period beginning in October, 1988. Adding to an initial collection <lb/>of 610 samples labeled by experts at the Naval Postgraduate School, four experts independently <lb/>labeled samples (Crosiar 1993) as one of nine cloud types, \clear&quot; (see Table 1), or mixed clouds <lb/>using image-displaying software. 2 Bankert then selected the subset of these 3625 cloud type <lb/>samples for which there was a majority agreement (i.e., at least three of the four experts labelled <lb/>it with the same class), and discarded the remaining samples. The experts had a majority <lb/>agreement on only 2151 of the 3625 (59%) sample regions, which indicates that this task is <lb/>di cult even for experts. He further decreased this number from 2151 to 1023 so as to reduce the <lb/>number of samples from the \clear&quot; class and delete the samples from the mixed clouds class. <lb/>Combined with the 610 NPS samples, a data set of 1633 samples was generated. <lb/>To represent each of the 1633 samples for training and testing of the cloud type classi er, features <lb/>were computed from the calibrated and scaled AVHRR data. Ideally, a complete domain theory <lb/>would dictate which features to use to encode each image. Unfortunately, no such domain theory <lb/>exists. Bankert, instead, encoded each image using a variety of numeric functions, yielding a set <lb/>of 204 continuous spectral (e.g., maximum, minimum, mean pixel values), textural (i.e., spatial <lb/>distribution of pixel values, including contrast and homogeneity), physical (e.g., cloud top <lb/>temperature), and geographic (i.e., latitude) features. <lb/>There was little information on how, or even whether, these features relate to the ten classes. <lb/>They were primarily chosen based on educated guesses and experience with similar applications. <lb/></body>

			<note place="footnote">2 Experts labelled a sample as a particular cloud type only if they judged that at least 75% of the sample region <lb/>corresponded to that cloud type (50% for stratocumulus). <lb/></note>

			<page>4 <lb/></page>

			<body>Table 1: The ten cloud type classes in the cloud classi cation data set. <lb/>Name <lb/>Code <lb/>Cirrus <lb/>Ci <lb/>Cirrocumulus Cc <lb/>Cirrostratus <lb/>Cs <lb/>Altostratus <lb/>As <lb/>Nimbostratus Ns <lb/>Stratocumulus Sc <lb/>Stratus <lb/>St <lb/>Cumulus <lb/>Cu <lb/>Cumulonimbus Cb <lb/>Clear <lb/>Clr <lb/>Therefore, Bankert predicted that eliminating any irrelevant or redundant features would improve <lb/>the classi er&apos;s performance. Towards this goal, he used a feature selection algorithm (FSA) to <lb/>preprocess the training data. Given a (usually large) set of features, an FSA searches for subsets <lb/>that improve performance (e.g., classi cation accuracy) on a given task (Fu 1968). Since the <lb/>space of feature subsets (i.e., the power set of all features) for this task is huge (2 204 2:6 10 61 ), <lb/>Bankert (1994) selected a computationally inexpensive FSA named forward sequential selection <lb/>(FSS) (Devijver and Kittler 1982) to search for feature subsets. It starts by locating the singleton <lb/>subset of features that has highest accuracy and then adds the feature that maximizes accuracy. <lb/>This process iterates with additional features until no accuracy improvements occur during an <lb/>iteration. Bankert also selected an inexpensive function, the Bhattacharya class separability index <lb/>measure, to evaluate and rank feature subsets. It ranks subsets based on their ability to cluster <lb/>samples by their class. FSS, a deterministic algorithm in this context, selected only 15 of the 204 <lb/>features. Bankert tested both the entire feature set and the selected subset on this data set using <lb/>a probabilistic neural network (PNN) classi er (Specht 1990). PNN was chosen because it <lb/>performed well on similar tasks (Welch et al. 1992). It uses a standard three-layer network of <lb/>nodes and a Bayesian classi cation strategy to select the class with the highest value of the a <lb/>posteriori class probability density function. The leave-one-out cross validation (LOOCV) <lb/>accuracy 3 was 75.3% for the complete feature set; this accuracy improved to 79.8% when using <lb/>only the selected 15 features. Using only the 15 selected features also greatly reduced the variance <lb/>of the individual class&apos;s accuracy di erences, which can be just as important as the overall <lb/>accuracy when used operationally. <lb/>Although this FSA dramatically reduced the number of features used, reduced processing times <lb/>correspondingly, and increased classi cation accuracy by 4.5% (i.e., compared with using all 204 <lb/>features), it is not obvious whether enhanced performance can be obtained by using more <lb/>sophisticated FSAs. Thus, we evaluated (Aha and Bankert 1994, 1996) a novel beam-search <lb/>extension for sequential selection on this cloud classi cation task, coupled with a di erent <lb/>evaluation function for feature subsets. Our extension, named BEAM, is a nondeterministic <lb/></body>

			<note place="footnote">3 LOOCV is a process that yields the percentage of samples in a training set T r whose class is correctly predicted <lb/>when the classi cation for each x2T r is obtained by classifying it using only the other training samples, namely <lb/>T r ? fxg. <lb/></note>

			<page>5 <lb/></page>

			<body>algorithm that maintains a queue (size n) of feature subsets, ordered by nonincreasing 10-fold <lb/>classi cation accuracy. BEAM begins by evaluating m randomly-selected subsets of size s, <lb/>retaining the best one in its queue. Assuming that FSS is used to select features, it then <lb/>stochastically extracts a queue element and adds a randomly-selected feature (i.e., BEAM uses <lb/>constraints to avoid creating previously-evaluated feature subsets). The newly constructed subset <lb/>is evaluated and, if its accuracy is higher than at least one of the queue&apos;s subsets, the queue is <lb/>updated to include that subset. This select-and-evaluate process iterates until none of the <lb/>remaining subsets can improve on the \best&quot; subset found. The subset with the highest <lb/>classi cation accuracy is chosen. <lb/>We also changed the way in which feature subsets were evaluated. Rather than use a clustering <lb/>algorithm (e.g., the Bhattacharya index measure), we instead used the classi er itself to evaluate <lb/>feature subsets (John et al. 1994). In this way, the classi er can provide feedback to the feature <lb/>subset evaluation routine, and this practice can prevent a mismatch in the preference rankings <lb/>between the subset evaluator and the classi er. That is, if we used any algorithm other than the <lb/>classi er to evaluate feature subsets, then it might rank a feature subset di erently than would <lb/>the classi er. By using the same algorithm for both tasks, we avoid this con ict. <lb/>We used a variant of the nearest neighbor classi er, named IB1 (Aha et al. 1991), rather than <lb/>PNN to evaluate feature subsets because, unlike PNN (which has a user-de ned parameter) or <lb/>backpropagation (which has many tunable parameters), IB1 does not require users or automated <lb/>strategies to set parameter values. Given a query q and training set X, IB1 computes the <lb/>similarity(q; x) for each x2X, determines which stored case x is most similar to q, and then yields <lb/>the class of x as its prediction for q&apos;s class. The de nition of similarity we used, assuming each <lb/>case is de ned by a set F of features, is <lb/>similarity(q; x) = ?distance(q; x) <lb/>(1) <lb/>distance(q; x) = s X <lb/>i2F <lb/>(q i ? x i ) 2 <lb/>(2) <lb/>Nearest neighbor algorithms typically have a parameter k that determines how many of the <lb/>nearest neighbors are used to predict classi cations. In this study, we set k = 1. In Section 4, we <lb/>will use a di erent variant of IB1 where, e ectively, k &gt; 1. <lb/>IB1&apos;s 10-fold accuracy using all 204 features is 72.6%, which is similar to PNN&apos;s accuracy. <lb/>BEAM, using FSS to select feature subsets, located a subset of only eleven features that increase <lb/>IB1&apos;s accuracy to 87.0%. This accuracy was the highest we found while working with this data <lb/>set (Bankert and Aha 1996). 4 <lb/>An analysis of the remaining errors lead us to question whether further improvements were <lb/>possible. We found that three of the 45 pairs of classes were responsible for 42.3% of the <lb/>(remaining) classi cation errors. We hypothesized that, if the samples in these classes could <lb/>somehow be more easily distinguished, then classi cation accuracy will increase. This leads us to <lb/>investigate whether error-correcting output codes could be used, in conjunction with our feature <lb/>selection results, for this purpose. We introduce and describe how to use ECOCs in classi cation <lb/>tasks in Section 3. <lb/></body>

			<note place="footnote">4 We reported similar results for a cloud system (e.g., fronts, tropical cyclones) classi cation task (Aha and Bankert <lb/>1996), where a variant of BEAM located a feature subset that increased LOOCV from 62.3% to 95.7% while reducing <lb/>the set of features from 98 to 16. <lb/></note>

			<page>6 <lb/></page>

			<body>Table 2: Output representations for a hypothetical set of three classes <lb/>Class <lb/>Output Representation <lb/>Name <lb/>Monolithic <lb/>Distributed <lb/>One-Per-Class Error-Correcting (ECOC) <lb/>Earthling <lb/>1 <lb/>100 <lb/>00000 <lb/>Martian <lb/>2 <lb/>010 <lb/>11100 <lb/>American <lb/>3 <lb/>001 <lb/>00111 <lb/>3 Error-Correcting Output Codes <lb/>Multiclass classi cation tasks are classi cation tasks that involve at least three classes. For these <lb/>tasks, error-correcting output codes (ECOCs) can help distinguish class labels by making them <lb/>more dissimilar, which can increase classi cation accuracy. This section introduces ECOCs in the <lb/>context of methods for solving multiclass tasks, and summarizes the motivation for using them. <lb/>At least three general strategies exist for designing classi ers to solve multiclass classi cation <lb/>tasks. Table 2 exempli es the three types of class label representations we describe here, and <lb/>which we empirically compare in Section 5. <lb/>3.1 Monolithic output representations <lb/>The rst approach, which we call the monolithic strategy, uses a unique monolithic encoding for <lb/>each class label. 5 For example, this corresponds to using the label \1&quot; for the Earthling class in <lb/>Table 2. Learning algorithms that use this approach induce a single concept description that <lb/>distinguishes all class boundaries. For example, C4.5 (Quinlan 1993a) uses this approach; given a <lb/>training set, C4.5 induces a decision tree where classi cation judgements are made at the leaves. <lb/>The class prediction of a leaf is (usually) the majority class of all training samples at that leaf, <lb/>where each class is represented by a unique symbolic label. <lb/>3.2 Distributed output representations <lb/>One distinguishing, and limiting, capability of monolithic output representations is that there is <lb/>no useful distance measure de ned on monolithic class labels. That is, the monolithic labels for <lb/>two samples are either identical or di erent; no real-valued similarity function exists on <lb/>monolithic class labels that can express that a new class label is more similar to one than some <lb/>other existing class label. <lb/>In contrast, distributed output representations de ne a similarity function on class labels, and can <lb/>thus de ne a distance function on class labels. Distributed codes represent each class label with a <lb/>unique set of values called a codeword, typically in the form of a bit string. For example, in the <lb/></body>

			<note place="footnote">5 Dietterich and Bakiri (1991, 1995) refer to this as the direct multiclass representation because only a single <lb/>decision tree is required for classi cation decisions. <lb/></note>

			<page>7 <lb/></page>

			<body>rst entry of Table 2, this corresponds to using either the string \100&quot; or \00000&quot; to represent <lb/>cases in the Earthling class. Each bit in the output string corresponds to a di erent partition of <lb/>the classes and, thus, a di erent learning task. Given a query, classi ers with distributed output <lb/>representations generate predictions for each output bit, and the class whose codeword has <lb/>smallest Hamming distance (i.e., number of mismatching bit values) from the query&apos;s prediction is <lb/>predicted as the query&apos;s class. We describe two examples of distributed output representations: <lb/>one-per-class and error-correcting. <lb/>3.2.1 One-per-class output representations <lb/>The one-per-class 6 encoding is a simple distributed output representation where there is a <lb/>one-to-one correspondence between output bits and classes. That is, it involves learning a <lb/>separate binary function for each class. For example, this corresponds to using the bit string <lb/>\100&quot; for cases in the Earthling class in the rst entry of Table 2. For many learning algorithms, <lb/>such as those that induce decision tree (Quinlan 1993a) or k-nearest neighbor classi ers (Aha <lb/>1989), one-per-class encoding is done by generating one concept description per class. For <lb/>example, if C4.5 is used and there are n classes, samples for each tree T i (1 i n) are labelled <lb/>positive if they are members of class c i and negative otherwise, and each tree T i generates a <lb/>prediction for how con dent it is that the sample is member of class c i . This strategy classi es a <lb/>new sample according to the class of the tree with the highest con dence. For <lb/>backpropagation-trained networks (Rumelhart et al. 1986), this representation instead uses n <lb/>output nodes (i.e., one per class), and the classi er predicts the class corresponding to the output <lb/>node with highest activation (i.e., a winner-take-all network). Thus, all samples in class c i have a <lb/>(binary) encoding of 1 for output node o i and 0 for all other output nodes. <lb/>Hamming distance can be used to de ne the distance between codewords. Unfortunately, this <lb/>representation is severely limited; it de nes the Hamming distance between all codewords to be <lb/>exactly 2, and only n possible encodings (i.e., one of the class&apos;s codewords) can be generated in a <lb/>classi cation attempt. This representation leaves little room for classi cation error, such as when <lb/>one or more output bits are incorrectly predicted. <lb/>3.2.2 Error-correcting output representations <lb/>Distributed output codes do not need to have a one-to-one correspondence constraint between <lb/>output bits and classes. For example, they can instead assign a set of log n nodes to represent the <lb/>n classes, or use some other pre-determined number of bits to de ne codeword length (Sejnowski <lb/>and Rosenberg 1987). Class encodings can be carefully designed with speci c distinguishing <lb/>properties. <lb/>ECOCs are examples of this approach: they are not restricted to having exactly n output bits <lb/>and their codewords can have multiple output bits that are \on&quot; (corresponding to values of 1). <lb/>This lack of restriction supplies ECOC strategies with great freedom in how class encodings are <lb/>assigned. Furthermore, each class&apos;s encoding di ers, in Hamming distance, from all other class <lb/>encodings by (at least) a pre-determined amount (e.g., the ECOCs in Table 2, such as \11100&quot; for <lb/></body>

			<note place="footnote">6 Could also be called one-bit-per-class representations because each bit represents a di erent binary learning task. <lb/></note>

			<page>8 <lb/></page>

			<body>the Martian class entry, have a minimum Hamming distance of 3). This encoding gives it an <lb/>error-correcting capability. For example, suppose that the minimum Hamming distance between <lb/>all classes is ve. This example corresponds to an error-correcting capability of two. That is, if no <lb/>more than two of the predicted output bit values for a given sample are incorrect, then the <lb/>correct class&apos;s encoding is still the \closest&quot; to the predicted encoding, and will be predicted as <lb/>the class for the query. More generally, class codewords that are separated by a minimum <lb/>Hamming distance of d have an error-correcting capability of b(d ? 1)=2c. In contrast, the <lb/>monolithic and one-per-class output representations have no error-correcting capabilities, even <lb/>though the latter imposes a Hamming distance of 2 between each class&apos;s codeword (i.e., if one <lb/>output bit is incorrectly predicted, then more than one codeword may have the smallest <lb/>Hamming distance, and the classi cation prediction is unclear). <lb/>Dietterich and Bakiri (1991, 1995) introduced the use of ECOCs for multiclass classi cation tasks. <lb/>They detailed experiments showing that, versus other multiclass approaches, ECOCs can improve <lb/>the classi cation accuracy of C4.5 (Quinlan 1993a) and a multilayer connectionist network <lb/>trained by backpropagation (Rumelhart et al. 1986) on a set of multiclass tasks, although ECOCs <lb/>are slower because they involve learning a function for each output bit (i.e., as opposed to <lb/>learning a single, albeit multiclass, function). Dietterich and Bakiri also explore the robustness of <lb/>their techniques and describe methods for designing error-correcting codes. They advocate two <lb/>principles when designing ECOC codewords: <lb/>1. row separation: Each class&apos;s codeword should be separated by a pre-determined <lb/>(minimal) Hamming distance from the other codewords. <lb/>2. column separation: The function de ned by each output bit should be uncorrelated with <lb/>the other output nodes&apos; functions. Highly correlated bits are not useful for classi cation <lb/>because they enforce similar decision boundaries on the data. This also entails these <lb/>functions should not be correlated with the complement of other output bit functions. <lb/>Row separation is clearly crucial for distinguishing the class&apos;s codewords. Column separation is <lb/>crucial to ensure that the errors of the output bit predictions are uncorrelated. <lb/>Kong and Dietterich (1995) analyzed why ECOCs tend to increase classi cation accuracy on <lb/>multiclass classi cation tasks, why column separation is crucial for designing e ective <lb/>error-correcting codes, and why learning algorithms that generate nonlocal classi ers, including <lb/>decision trees and neural networks, should generally bene t from using ECOCs. In particular, <lb/>they showed that ECOCs work well because they they reduce the number of errors caused by <lb/>both the variance and bias of the learning algorithm. <lb/>Variance errors result from random variation and noise in the training set and from any random <lb/>behaviors of the learning algorithm itself. These errors can be reduced by averaging the <lb/>contributions of multiple predictions during a single prediction task, such as by voting among <lb/>multiple runs of the same algorithm (Perrone and Cooper 1993, Breiman 1994). This also occurs <lb/>when using ECOCs because a vote (among predictions for the di erent output bits) takes place <lb/>when selecting the nearest codeword, as computed by predicting the class whose codeword has the <lb/>smallest Hamming distance. <lb/>Bias errors instead refer to an algorithm&apos;s systematic errors. These errors can also be reduced by <lb/>a form of voting, but only when the individual predictions are uncorrelated. This can often be <lb/></body>

			<page>9 <lb/></page>

			<body>accomplished by using di erent algorithms applied to the same learning task (Zhang et al. 1992, <lb/>Quinlan 1993b). Alternatively, the same algorithm can be used multiple times, so long as the vote <lb/>is on di erent subproblems that cause the algorithm to generate di erent bias errors. For <lb/>example, C4.5 performed well when extended with ECOCs because each output bit function <lb/>creates di erent partitions on the classes. Because these partitions encode di erent decision <lb/>boundaries, C4.5 will yield di erent bias errors on each bit. This is the motivation for designing <lb/>error-correcting codes with good column separation. <lb/>More generally, Kong and Dietterich (1995) predict that \nonlocal&quot; learning algorithms (i.e., <lb/>those that induce compact classi ers) such as backpropagation and C4.5 should bene t from <lb/>using ECOCs, but not local learning algorithms (i.e., those that generate predictions based on <lb/>information near the query sample), including nearest-neighbor classi ers. Their reasoning is <lb/>that, when using only local information, the bias errors in di erent output bits will be correlated, <lb/>which will prevent the ECOCs from reducing bias errors. Empirical evidence exists to support <lb/>this claim (Wettschereck and Dietterich 1992). <lb/>The rest of this paper describes and evaluates a method for decorrelating the errors made by <lb/>nearest-neighbor classi ers so that they can bene t from using ECOCs. Our approach relies on <lb/>using feature selection for each output bit, so that similarity will be computed on di erent subsets <lb/>of features for each bit. This will cause di erent stored samples to be retrieved for di erent <lb/>output bits, and their class predictions should be independent. Thus, our strategy, while still <lb/>depending on using local information during classi cation predictions, uses di erent, yet local, <lb/>information for each output bit. The following section describes our algorithm, while Section 5 <lb/>details its application to the cloud classi cation task. <lb/>4 Local Learning with Error-Correcting Output Codes <lb/>This section describes the AI approach we tested on the cloud classi cation task. A feature <lb/>selection approach is integrated with error-correcting output codes for a k-nearest neighbor <lb/>classi er. Because ECOCs require that the errors for each of the output bits be uncorrelated, we <lb/>modi ed IB1 to use di erent features when computing distances for each of the ECOC output <lb/>bits. We also varied IB1 in other ways for use with ECOCs. <lb/>More speci cally, we used a variant of IB1 where, e ectively, more than one nearest neighbor was <lb/>used to generate classi cation predictions. But rather than optimizing for k during each feature <lb/>subset evaluation, we instead summed the similarity of the query q to all stored features, <lb/>weighting their similarity according to an exponentially decreasing function of their distance. <lb/>That is, for a given output bit j, we de ned <lb/>similarity(q; x; j) = exp ?s distance(q; x; j) <lb/>(3) <lb/>where <lb/>distance(q; x; j) = s X <lb/>i2F j <lb/>(q i ? x i ) 2 <lb/>(4) <lb/>F j is the set of features selected for bit j, and s is a scaling constant (set to 10 in our <lb/>experiments) that determines the slope of the exponential curve. 7 Then, for each class c2C, we <lb/></body>

			<note place="footnote">7 We found similar results when we set s = 100, making it behave more like 1-nearest neighbor. <lb/></note>

			<page>10 <lb/></page>

			<body>k-NN-ECOC(Tr; Te; Cl; p(); h) <lb/>||||||||||||| <lb/>Tr: Training set <lb/>Te: Test (evaluation) set <lb/>Cl: Set of classes <lb/>p(): Class partition function <lb/>h: Requested inter-class Hamming distance for codewords <lb/>M: Confusion matrix <lb/>O: Set of output bit functions (i.e., partitions on the classes) <lb/>Co: Set of ECOC class codewords <lb/>F: Matrix of feature subsets selected by FSS (one subset per output bit) <lb/>p: Classi cation prediction vector <lb/>e i : Test sample i <lb/>c: Class predicted by k-NN-ECOC for e i <lb/>count: Number of correct class predictions (initially 0) <lb/>||||||||||||| <lb/>M := create confusion matrix(IB1(),T r) <lb/>O := create output bits(Cl; p(); h; M) <lb/>Co := create codewords(O) <lb/>FOREACH output bit o i 2O DO: <lb/>F i := FSS(k-NN(),T r; Co) <lb/>FOREACH e i 2Te DO: <lb/>FOR EACH output bit o j 2O DO: <lb/>p j := IB1(T r; e i ; o j ) <lb/>IF class e i = minimum Hamming class(p; Co) <lb/>THEN count = count + 1 <lb/>OUTPUT count/jT ej <lb/>Figure 2: k-NN-ECOC: An extension of k-NN that uses error-correcting output codes. <lb/>summed the similarity of q to all training cases in c. Finally, the class with highest summed <lb/>similarity was the class predicted for q. <lb/>Figure 2 lists the pseudocode for our approach, which we name k-NN-ECOC (i.e., k-nearest <lb/>neighbor extended with ECOCs). Boldfaced function names are detailed in later gures. <lb/>k-NN-ECOC begins by generating a confusion matrix M, obtained by applying IB1 to the <lb/>training set using LOOCV. This matrix is then used to help generate the ECOC codewords Co, <lb/>for the classes Cl, so that their minimum Hamming distance is h. Function p() achieves this goal <lb/>by selecting the partition of classes to use for each output bit. k-NN-ECOC next computes the <lb/>set of features to use when predicting bit values for each output bit. We again chose FSS as the <lb/>FSA, and we chose the version of k-NN in Equation 4 to rank feature subsets (i.e., according to <lb/>LOOCV on the training sets). 8 Then, for each test sample e i , k-NN-ECOC predicts a value for <lb/>each output bit and compares the predicted output string with each codeword, yielding the most <lb/></body>

			<note place="footnote">8 This de nition of k-NN has two advantages: it is nonparametric, which means that we do not need to tune <lb/>parameters in each feature subset evaluation, and it is as cheap to execute as IB1. <lb/></note>

			<page>11 <lb/></page>

			<body>create output bits(Cl; p(); h; M) <lb/>||||||||||||| <lb/>Cl: Set of classes <lb/>p(): Class partition function <lb/>h: Requested inter-class Hamming distance for codewords <lb/>M: Confusion matrix <lb/>o: A partition on the set of classes, initially empty <lb/>O: A set of output bit partitions <lb/>||||||||||||| <lb/>REPEAT <lb/>o := create class partition(Cl; p(); M) <lb/>IF not redundant(o; O) <lb/>THEN O := O + fog <lb/>UNTIL distinguished(Cl; O; h) <lb/>RETURN Codes <lb/>Figure 3: Procedure for creating the output bit functions. <lb/>similar codeword&apos;s class as its prediction for e i &apos;s class. A count is maintained on the number of <lb/>correct predictions and the average classi cation accuracy is output. <lb/>The function create output bits, which creates the class partitions corresponding to the output bit <lb/>functions, is detailed in Figure 3. Each output bit is a binary partition function on the set of <lb/>classes (Cl). Thus, create output bits builds a set of partitions, one per output bit, until the set <lb/>of partitions distinguishes each pair of classes according to at least the requested Hamming <lb/>distance h. Bits that are redundant with previously-created bits in terms of how they partition <lb/>the classes are discarded. <lb/>Each partition (output bit) is created by create class partition, which is detailed in Figure 4. For <lb/>a given output bit o i , this procedure randomly selects two classes from Cl to initialize two subsets <lb/>of classes. It then iteratively uses the partition function p() to determine, for each of the <lb/>remaining classes in Cl, which subset they will join. Afterwards, the codeword for each class in <lb/>subset S j is assigned value j for o i . <lb/>Function create output bits constrains the partition function p to generate a di erent partition of <lb/>the classes for each output bit. We describe experiments with three di erent functions for p() in <lb/>Section 5, motivated by our interest in determining whether the confusion matrix can be <lb/>pro tably exploited to bias the bit-creation process. The rst (and simplest) de nition ignores <lb/>the confusion matrix and, instead, randomly assigns the remaining classes to each subset S i . This <lb/>corresponds to a top-level procedure call of <lb/>k-NN-ECOC(Tr; Te; Cl; random partition(); h) <lb/>where random partition is de ned in Figure 5. <lb/>The random partition function does not use information concerning how c relates to the classes <lb/>already in subsets S 0 and S 1 . The other two partition functions bias the code-generation process <lb/>by using the information in the confusion matrix M. The second de nition of p() that we will <lb/></body>

			<page>12 <lb/></page>

			<body>create class partition(Cl; p(); M) <lb/>||||||||||||| <lb/>Cl: Set of classes <lb/>p(): Class partition function <lb/>M: Confusion matrix <lb/>S 0 ; S 1 : Disjoint subsets of Cl <lb/>c: A class in Cl <lb/>||||||||||||| <lb/>S 0 := f random(Cl) g <lb/>S 1 := f random(Cl ? S 0 ) g <lb/>Cl := Cl ? S 0 ? S 1 <lb/>FOREACH c2Cl DO: <lb/>IF p(c; M; S 0 ; S 1 ) <lb/>THEN S 0 := S 0 fcg <lb/>ELSE S 1 := S 1 fcg <lb/>RETURN fS 0 ; S 1 g <lb/>Figure 4: Procedure for generating a class partition corresponding to an output bit. <lb/>random partition(c; M; S 0 ; S 1 ) <lb/>||||||||||||| <lb/>c: A class, in neither S 0 nor S 1 <lb/>M: Confusion matrix (not used here) <lb/>S 0 ; S 1 : Disjoint subsets of C (not used here) <lb/>||||||||||||| <lb/>IF random( 0; 1]) 0:5 <lb/>THEN RETURN TRUE <lb/>ELSE RETURN FALSE <lb/>Figure 5: Random partition function for partitioning classes. <lb/>evaluate is the min confusion function, displayed in Figure 6. It computes the mean class <lb/>confusion of the given class with the classes already in the two subsets, and stochastically favors <lb/>assigning c to the subset with lower mean class confusion. Thus, classes with lower confusion are <lb/>grouped together. <lb/>Our nal partition function, max confusion (Figure 7), is the reverse of min confusion and tries to <lb/>group maximally confusable classes together. <lb/>It is unclear how these three partition functions will compare. Our expectation is that the <lb/>individual output bit functions of min confusion will be di cult to learn because they distinguish <lb/>highly confusable classes. In contrast, the bits created by max confusion should be simpler to <lb/>learn, but will not be as informative. However, we have not found any comparisons of random <lb/>partition functions with those that bias the bit-creation process using class confusion tables. <lb/></body>

			<page>13 <lb/></page>

			<body>min confusion(c; M; S 0 ; S 1 ) <lb/>||||||||||||| <lb/>c: A class, in neither S 0 nor S 1 <lb/>M: Confusion matrix <lb/>S 0 ; S 1 : Disjoint subsets of Cl <lb/>||||||||||||| <lb/>C 0 := mean confusion(c; S 0 ) <lb/>C 1 := mean confusion(c; S 1 ) <lb/>IF random( 0; 1]) &lt; C 0 <lb/>C 0 +C 1 <lb/>THEN RETURN TRUE <lb/>ELSE RETURN FALSE <lb/>Figure 6: Min-confusion partition function for partitioning classes. <lb/>max confusion(c; M; S 0 ; S 1 ) <lb/>||||||||||||| <lb/>c: A class, in neither S 0 nor S 1 <lb/>M: Confusion matrix <lb/>S 0 ; S 1 : Disjoint subsets of Cl <lb/>||||||||||||| <lb/>C 0 := mean confusion(c; S 0 ) <lb/>C 1 := mean confusion(c; S 1 ) <lb/>IF random( 0; 1]) &lt; C 0 <lb/>C 0 +C 1 <lb/>THEN RETURN FALSE <lb/>ELSE RETURN TRUE <lb/>Figure 7: Max-confusion partition function for partitioning classes. <lb/>The following section describes our experiments with applying error-correcting codes and other <lb/>multiclass methods for the cloud classi cation task described in Section 2. <lb/>5 Evaluation <lb/>This section describes three experiments using ECOCs for our cloud classi cation task. First, we <lb/>compared the ECOC approach versus alternative multiclass classi cation approaches, where the <lb/>codewords were randomly generated (Figure 5) according to a given minimum Hamming distance <lb/>constraint. Second, we varied the minimum Hamming distance between classes, and predicted <lb/>that larger error-correcting capabilities would also have a positive e ect on accuracy. Finally, we <lb/>examined whether more sophisticated methods for generating error-correcting codewords <lb/>(Figures 6 and 7) could further decrease classi cation error. Our results support our rst two <lb/>hypotheses, but not our third. <lb/></body>

			<page>14 <lb/></page>

			<body>5.1 Empirical Methodology <lb/>All of the experiments followed the same empirical methodology. Due to the inherent complexity <lb/>of this task (i.e., an expensive feature selection process repeated for each of several output bits), <lb/>we did not work with the entire cloud classi cation data set, but with only subsets of it. That is, <lb/>we created three subsets of size 100, 250, and 500 samples, respectively. We then ran each <lb/>algorithm ten times on each data set, each time randomly splitting the data set into 70% training <lb/>and 30% testing partitions. All algorithms were trained and tested on identical training and test <lb/>sets. For the algorithms using ECOCs, the number of bits in the output code was determined by <lb/>the function create output bits (Figure 3) using one of the three functions for the partition <lb/>function p() described in Section 3.2.2 (i.e., random partition, min confusion, or max confusion). <lb/>Features were selected for each output bit; feature subsets were selected by FSS and were <lb/>evaluated by k-NN (Equation 4) on the complete training set. <lb/>5.2 Initial Comparison <lb/>In our rst experiment, we compared the three approaches for solving multi-classi cation tasks <lb/>described in Section 3, namely the monolithic, one-per-class, and error-correcting approaches. We <lb/>used a single class label for the monolithic strategy that can take on one of ten discrete values <lb/>corresponding to the ten cloud types in our data set. For the one-per-class strategy, we converted <lb/>monolithic class labels to a set of ten output class bits, and ran k-NN once per bit. That is, the <lb/>one-per-class strategy simply used the k-NN-ECOC (Figure 2) system where create output bits <lb/>was modi ed to create one bit per class and the create codewords created corresponding <lb/>one-per-class codewords. k-NN-ECOC was also used for the ECOC strategy using the <lb/>random partition function for p(). However, in this experiment, we set h (Figure 2), to 1, thus <lb/>requesting no error-correction capability. In response, create output bits distinguished the ten <lb/>cloud type classes by generating ten codewords that were only ve bits in length. The same <lb/>codewords were used for each of the ten runs in this experiment. <lb/>We quickly found that the monolithic strategy performed poorly when feature selection was <lb/>performed (i.e., its average accuracy for the subset of size 500 was only 21.0%). This poor <lb/>performance occurred because FSS stopped after selecting only one feature in each run. <lb/>Therefore, we abandoned feature selection when evaluating the monolithic strategy, which greatly <lb/>improved its performance. We will refer to this strategy as monolithic-all (i.e., monolithic <lb/>strategy using all features). In contrast, FSS performed adequately when using distributed output <lb/>codes. For example, it retrieved, on average, 3.8 features when using random confusion with <lb/>Hamming distance 1 on the smallest data set size, and doubled this for the largest data set size <lb/>(the average feature subset sizes were similar for other Hamming distance settings). Although this <lb/>subset is small compared with the entire set of 204 features, it was adequate to yield good results. <lb/>Learning curves summarizing these results are shown in Figure 8. We generated learning curves <lb/>rather than report a single classi cation result with the largest data set because they reveal more <lb/>detailed behavior for distinguishing the algorithms&apos; capabilities. We also display error bars <lb/>representing 1 standard deviation in this gure, but only for the top-performing algorithm at <lb/>each data subset size (showing all error bars yields a confusing graph). In this case, we see that <lb/>the monolithic-all approach attains higher accuracy than one-per-class, and that the degenerate <lb/>error-correcting encoding algorithm eventually surpasses both. According to a one-tailed t-test, <lb/></body>

			<page>15 <lb/></page>

			<body>Monolithic&apos; <lb/>One-Per-Class <lb/>ECOC (Random/1) <lb/>0 <lb/>100 <lb/>200 <lb/>300 <lb/>400 <lb/>500 <lb/>Data Subset Size <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>AVERAGE CLASSIFICATION ACCURACY <lb/>Figure 8: Comparing multi-classi cation approaches on the cloud classi cation task. <lb/>the higher accuracies recorded by this strategy for the data set of size 500 signi cantly di er from <lb/>those recorded by the monolithic-all (t = 2:7; p &lt; 0:025) and one-per-class (t = 2:9; p &lt; 0:01) <lb/>strategies. However, the bene t does not appear to exist for the smaller data set sizes for this <lb/>type of distributed output codes. The question remains whether error-correcting codes (i.e., with <lb/>inter-class Hamming distances of at least 3) can produce further bene ts. <lb/>5.3 Varying the error-correction capability <lb/>In our second experiment, we again used the random partition function (Figure 5), but this time <lb/>we varied the requested minimum Hamming distance between codewords. In three runs, we set h <lb/>to 1, 3, and 5, respectively. The results, shown in Figure 9, indicate that error-correcting codes <lb/>(i.e., Random/3 and Random/5) attain higher accuracies than when the distributed codes are not <lb/>error-correcting (i.e., Random/1). For the 500-size data set, the di erences between Random/3 <lb/>and Random/1 are signi cant at the p &lt; 0:025 level (t = 2:4), and the di erences between <lb/>Random/5 and Random/1 are even larger (t = 2:7). We also compared these size-500 results with <lb/>the previous multiclass and one-per-class results. Their di erences with the ECOC results were <lb/>all signi cant at the p &lt; 0:01 or p &lt; 0:005 levels. Thus, the ECOC codes signi cantly increase <lb/>classi cation accuracies on these subsets of the cloud classi cation data set. <lb/>5.4 Varying the partition function <lb/>In our nal experiment, we varied the way in which the error-correcting codes were generated. In <lb/>particular, we compared how the ECOC approach performs when p() is assigned the <lb/>random partition, min confusion, and max confusion strategies described in Section 3.2.2. <lb/>Whereas the random partition function generates output bits corresponding to arbitrary <lb/>partitions of the classes, the latter two instead generate bits that attempt to (respectively) <lb/>maximize or minimize the Hamming distances of confusable classes. <lb/></body>

			<page>16 <lb/></page>

			<body>ECOC (Random/1) <lb/>ECOC (Random/3) <lb/>ECOC (Random/5) <lb/>0 <lb/>100 <lb/>200 <lb/>300 <lb/>400 <lb/>500 <lb/>Data Subset Size <lb/>25 <lb/>35 <lb/>45 <lb/>55 <lb/>65 <lb/>75 <lb/>AVERAGE CLASSIFICATION ACCURACY <lb/>Figure 9: Comparing error-correcting codes versus distributed codes without error-correcting ca-<lb/>pabilities on the cloud classi cation task. <lb/>The latter two partition functions were chosen as foils to the random partition function. We <lb/>might expect max confusion to perform well because it tends to separate highly confusable <lb/>classes, and that min confusion will perform poorly because it groups highly confusable classes. <lb/>However, it is much more di cult to learn the individual output bit functions for min confusion <lb/>then max confusion because it is harder to learn functions where highly confusable classes are in <lb/>di erent halves of the partition. Thus, it is unclear how these methods will compare, and how <lb/>they will compare with the random partition function. <lb/>Table 3 summarizes the results of their comparison on the cloud classi cation data set. We see <lb/>that min confusion performs exceptionally well for the smallest-sized data set (i.e., for h = 1 and <lb/>h = 3), but neither of the partition functions that exploit confusion matrix information <lb/>outperforms the random partition function for the larger data set sizes. Thus, while the <lb/>error-correcting algorithms (i.e., distributed codes with h &gt; 1) outperform the monolithic-all and <lb/>one-per-class approaches, we found no strong bene t from these two simple methods for biasing <lb/>the selection of class codewords. <lb/>5.5 Confusion matrices <lb/>As previously mentioned, one of the goals of this study was to determine whether ECOCs could <lb/>reduce the number of errors among the most confusable classes. Our evidence con rms this <lb/>hypothesis. Table 4 displays the confusion matrix for one run of k-NN (i.e., without feature <lb/>selection, using the monolithic output representation). In this case, these results correspond to <lb/>the rst of the ten runs with 350 training and 150 test cases. Table 5 displays the confusion <lb/>matrix for the same run when using ECOCs generated by random partition (h = 5). In both <lb/>cases, the numbers shown refer to the number of times, among the 150 test cases, where a test <lb/>case according to the row was predicted to be in the class according to the column. In this <lb/>particular run, k-NN&apos;s accuracy was 54.7% while k-NN-ECOC&apos;s accuracy was 70.0%. <lb/></body>

			<page>17 <lb/></page>

			<body>Table 3: Average percentage accuracies and standard deviations of the multiclass approaches on <lb/>the cloud classi cation task, where the minimum Hamming distance between codewords (H) and <lb/>length of codewords (L) are shown for the distributed methods. <lb/>Algorithm <lb/>H L <lb/>Data Set Size <lb/>100 <lb/>250 <lb/>500 <lb/>Monolithic <lb/>1 1 16.3 8.8 15.6 6.8 21.1 4.9 <lb/>Monolithic-all <lb/>1 1 47.0 8.5 44.9 3.3 52.5 2.8 <lb/>One-per-class <lb/>2 10 30.7 8.6 40.3 4.5 48.3 6.4 <lb/>Random partition 1 5 32.0 10.0 49.6 6.7 64.3 2.6 <lb/>3 21 51.3 9.8 61.9 3.0 73.0 5.0 <lb/>5 32 58.7 9.8 65.6 5.7 74.3 4.2 <lb/>Max confusion <lb/>1 6 34.0 6.8 51.2 4.6 62.5 6.0 <lb/>3 20 48.0 12.1 59.2 3.6 70.6 4.3 <lb/>5 29 54.0 10.7 64.8 4.6 73.9 4.5 <lb/>Min confusion <lb/>1 8 42.3 14.3 53.1 5.5 57.3 5.9 <lb/>3 19 59.7 8.8 59.6 4.9 69.5 5.2 <lb/>5 32 58.7 10.8 63.9 4.6 74.6 4.5 <lb/>Table 4: Confusion matrix for one run of k-NN. <lb/>Classes Ci Cc Cs As Ns Sc St Cu Cb Clr <lb/>Ci <lb/>16 0 3 0 0 0 0 2 1 0 <lb/>Cc <lb/>0 4 2 1 2 0 0 0 0 0 <lb/>Cs <lb/>9 0 5 0 0 0 1 0 1 0 <lb/>As <lb/>1 0 1 10 0 4 4 0 0 0 <lb/>Ns <lb/>0 1 0 0 2 0 0 0 1 0 <lb/>Sc <lb/>0 0 0 2 0 15 4 1 0 0 <lb/>St <lb/>0 0 0 2 0 2 9 0 0 0 <lb/>Cu <lb/>0 0 0 0 0 2 0 9 0 0 <lb/>Cb <lb/>0 0 4 0 0 0 0 0 12 0 <lb/>Clr <lb/>0 0 0 0 0 1 16 0 0 0 <lb/>The pair of classes most frequently confused by k-NN are Clr and St. This result is somewhat <lb/>surprising in the sense that this confusion had not been seen in previous experiments, but <lb/>considering the similarities in terms of texture and temperature, one can see how this <lb/>misclassi cation could happen. This pair accounts for 16 of k-NN&apos;s 68 errors. The next most <lb/>frequently confused class pairing is, not surprisingly, Ci (Cirrus) and Cs (Cirrostratus), which <lb/>accounts for another 12 errors by k-NN. Together they represent 41.2% of k-NN&apos;s errors. Overall, <lb/>the top six pairs of most frequently confused classes account for 51 (75.0%) of k-NN&apos;s errors. <lb/>k-NN-ECOC reduced this number to 25 (56% of the total error), eliminating all the errors of the <lb/>most confusable pair of classes (St,Clr). Unfortunately, errors for the second most confusable pair, <lb/>Ci and Cs, were not reduced. In future research, we plan to determine whether more other error <lb/></body>

			<page>18 <lb/></page>

			<body>Table 5: Confusion matrix for one run of k-NN-ECOC (random partition, h = 5). <lb/>Classes Ci Cc Cs As Ns Sc St Cu Cb Clr <lb/>Ci <lb/>12 2 5 1 0 1 0 0 1 0 <lb/>Cc <lb/>0 7 2 0 0 0 0 0 0 0 <lb/>Cs <lb/>7 2 6 0 0 0 0 0 1 0 <lb/>As <lb/>1 0 0 13 0 2 3 0 1 0 <lb/>Ns <lb/>0 0 0 0 4 0 0 0 0 0 <lb/>Sc <lb/>0 0 0 1 0 18 3 0 0 0 <lb/>St <lb/>0 0 0 0 0 1 12 0 0 0 <lb/>Cu <lb/>0 0 0 0 0 6 0 5 0 0 <lb/>Cb <lb/>0 0 2 1 0 1 0 0 12 0 <lb/>Clr <lb/>0 0 0 0 0 1 0 0 0 16 <lb/>correcting codes, designed speci cally for this pair of classes, can reduce their confusion rate. <lb/>These confusion matrices are typical of the ones we found while comparing these algorithms. <lb/>While accuracy does not improve on all pairs of classes, the greatest accuracy increases typically <lb/>occur with the most confusable pair of classes. <lb/>6 Discussion and Future Work <lb/>Although the max confusion and min confusion partition algorithms did not improve performance <lb/>over random partition on our task, performance improvements might result from other \smart&quot; <lb/>codeword selection strategies. For example, smart strategies could focus on distinguishing only <lb/>the class pairs with highest confusions. However, these strategies might be elusive. While we <lb/>could train an output bit o i to focus speci cally on a given pair of highly confusable classes c j and <lb/>c j 0 (i.e., by training them on samples drawn only from these two classes), it is unclear how o i <lb/>could be used during testing, when we do not know whether a test sample is among these two <lb/>classes. Perhaps we could de ne an output bit o i 0 to distinguish samples in c j or c j 0 from all <lb/>others and then use o i 0 as a conditional lter before using o i . However, this would greatly <lb/>complicate the training and testing processes. Moreover, our preliminary attempts at using a <lb/>cascading approach have not been promising (Bankert 1994). <lb/>While ECOCs can increase classi cation accuracy, they have two primary drawbacks. First, <lb/>unlike the one-per-class approach, the output node functions no longer correspond to unique <lb/>classes. Instead, output bits are functions that partition classes into (almost) arbitrary groupings. <lb/>Thus, this limitation causes a loss of comprehensibility. Second, ECOCs increase computational <lb/>complexity because they typically require far more output nodes than the one-per-class approach <lb/>(e.g., 2x + 1 output nodes are required for an error-correcting capability of x for (only) two <lb/>classes). This can be expensive for some algorithms. For example, backpropagation&apos;s training <lb/>complexity is O(mn 2 ), where m is the number of training samples and n is the maximum number <lb/>of nodes at any layer (Bankman and Aha 1992). Thus, increases in the number of output nodes <lb/></body>

			<page>19 <lb/></page>

			<body>can cause greater than linear increases in the training time. As an example from these <lb/>experiments, the time required per run (on a Sun SparcStation Ultra 1) for data sets of size 500 <lb/>was only about 20 seconds for monolithic-all, but the time increased to about ve hours for runs <lb/>with the largest number of output bits. The amount of time required was roughly a linearly <lb/>increasing function of the number of output bits. For this investigation, we were willing to <lb/>sacri ce comprehensibility and time for accuracy, but this will not be an appropriate sacri ce for <lb/>many (e.g., time-critical) tasks. <lb/>As with other application areas, many environmental science tasks are posed as classi cation <lb/>problems, in part because several robust classi cation tools exist. We have an important message <lb/>for environmental scientists who are concerned about maximizing classi cation accuracy on their <lb/>tasks: select algorithms that are speci cally designed to reduce both variance and bias errors. As <lb/>Kong and Dietterich (1995) explain, several algorithms reduce only errors caused by variance. <lb/>These algorithms generate multiple hypotheses (e.g., by using di erent initial random weights in a <lb/>neural network or di erent training sets to induce multiple decision trees or rule sets) and then <lb/>vote among these hypotheses (Perrone 1994, Breiman 1994). They cannot reduce bias errors <lb/>because the predictions among the multiple hypotheses are correlated. However, both variance <lb/>and bias errors can be reduced by voting among multiple hypotheses produced by di erent <lb/>learning algorithms applied to the same problem, assuming the bias errors di er among the <lb/>di erent algorithms (Clemen 1989, Makridakis and Winkler 1983, Quinlan 1993b). Error <lb/>correcting output codes also reduce variance and bias errors, but with a di erent form of voting <lb/>(i.e., computing Hamming distance between the prediction and the codewords of each class). <lb/>They also reduce bias by applying an algorithm multiple times, where each application is for a <lb/>di erent learning task (i.e., output bit). Finally, local algorithms, such as the k-nearest neighbor <lb/>classi er, require that di erent local information is selected when making predictions for each bit, <lb/>which decorrelates the errors among the bit predictions. <lb/>In summary, environmental scientists should consider narrowing their choice of classi ers. Instead <lb/>of using one that appeared to work well, but for unknown reasons, on some previous task, we <lb/>instead encourage them to select (or design) an algorithm known to reduce both variance and bias <lb/>errors. The question then becomes one of selecting which, from among these many algorithms, <lb/>best matches their needs for other problem-solving aspects (e.g., training speed, space <lb/>requirements). <lb/>7 Conclusion <lb/>We described an application of error correcting output codes (ECOCs) for a multiclass cloud <lb/>classi cation task, where samples in the database were derived from AVHRR images. More <lb/>precisely, we modi ed a nearest neighbor classi er by replacing each sample&apos;s class label with a <lb/>set of \output&quot; functions (bits) that each partition the set of classes di erently. Furthermore, we <lb/>used a simple sequential feature selection algorithm (FSS) to select a di erent set of features for <lb/>each output function, implying that distances are computed di erently for each bit. Our <lb/>experiments showed that this method, which reduces both variance and bias errors, signi cantly <lb/>outperforms other multiclass classi cation approaches on our cloud classi cation task. It <lb/>performed especially well in increasing accuracy on the most confusable classes in the data set. <lb/>However, using ECOCs can be more computationally expensive than using monolithic class <lb/></body>

			<page>20 <lb/></page>

			<body>labels, and this cost increases with the number of bits used in the ECOC representation. <lb/>The primary contribution of this paper to environmental scientists is to provide additional <lb/>evidence that learning algorithms which reduce both variance and bias errors are excellent choices <lb/>for environmental science applications. The primary contribution for AI researchers is a useful <lb/>algorithm for integrating k-nearest neighbor classi ers with ECOCs and its empirical evaluation <lb/>on an interesting task. To our knowledge, this is the rst successful integration of ECOCs with a <lb/>local learning algorithm where, e ectively, k is small (i.e., due to the high slope imposed by s in <lb/>Equation 4). Selecting di erent features for each output bit is crucial to this integration&apos;s success. <lb/>Several avenues for future research remain. We plan to investigate the use of error-correcting <lb/>output codes on other environmental science applications, and to also determine whether certain <lb/>modi cations of this approach are promising. This future work includes designing smarter <lb/>partition functions for generating codewords, and using more sophisticated distributed <lb/>error-correcting codes (e.g., with symbolic representations, of varying lengths). <lb/></body>

			<div type="acknowledgement">Acknowledgements <lb/>Many thanks to professor Nick Flann of Utah State University, who generated the <lb/>error-correcting codes used in this study. Thanks also to Francesco Ricci, John Grefenstette, Paul <lb/>Tag, Jim Peak, and other colleagues at NRL/DC and NRL/Monterey for their continuing <lb/>encouragement of and support for our joint research e orts. <lb/></div>

			<listBibl>References <lb/>Aha, D. W. 1989. Incremental, instance-based learning of independent and graded concept <lb/>descriptions. Pages 387{391 in: Proceedings, Sixth International Workshop on Machine <lb/>Learning, Ithaca, New York, July 1989. Morgan Kaufmann, San Mateo, California. <lb/>Aha, D. W., and R. L. Bankert. 1994. Feature selection for case-based classi cation of cloud <lb/>types: An empirical comparison. Pages 106{112 in: Case-Based Reasoning: Papers from the <lb/>1994 Workshop (Technical Report WS-94-01), D. W. Aha, editor. AAAI Press, Menlo Park, <lb/>California. <lb/>Aha, D. W., and R. L. Bankert. 1996. A comparative evaluation of sequential feature selection <lb/>algorithms. To appear in: Arti cial Intelligence and Statistics V, D. Fisher and J.-H. Lenz, <lb/>editors. Springer-Verlag, New York. <lb/>Aha, D. W., D. Kibler, and M. K. Albert. 1991. Instance-based learning algorithms. Machine <lb/>Learning 6: 37{66. <lb/>Bankert, R. L. 1994. Cloud classi cation of AVHRR imagery in maritime regions using a <lb/>probabilistic neural network. Journal of Applied Meteorology 33: 909{918. <lb/>Bankert, R. L., and D. W. Aha. 1996. Improvement to a neural network cloud classi er. <lb/>Applied Meteorology 35: 2036{2039. <lb/></listBibl>

			<page>21 <lb/></page>

			<listBibl>Bankert, R. L. and P. M. Tag. 1996. Automated extraction and identi cation of cloud systems <lb/>in satellite imagery. Pages 373{376 in: Proceedings, Eighth Conference on Satellite <lb/>Meteorology and Oceanography. American Meteorological Society, Atlanta, Georgia. <lb/>Bankman, I. N., and D. W. Aha. 1992. Fast learning in feedforward neural networks by <lb/>migrating hidden unit outputs. Pages 179{184 in: Intelligent Engineering Systems Through <lb/>Arti cial Neural Networks, Volume II, C. H. Dagli, editor. ASME Press, St. Louis, Missouri. <lb/>Birnbaum, L. A., and G. C. Collins, editors. 1991. Proceedings, Eighth International Workshop <lb/>on Machine Learning, Evanston, Illinois, June 1991. Morgan Kaufmann, San Mateo, <lb/>California. <lb/>Breiman, L. 1994. Bagging predictors (Technical Report 421). University of California, <lb/>Department of Statistics, Berkeley, California. <lb/>Clemen, R. T. 1989. Combining forecasts: A review and annotated bibliography. International <lb/>Journal of Forecasting 5: 559{583. <lb/>Crosiar, C. L. 1993. An AVHRR cloud classi cation database typed by experts (Technical <lb/>Report NRL/MR/7531-93-7207). Naval Research Laboratory, Prediction Systems Branch, <lb/>Marine Meteorology Division, Monterey, California. <lb/>Devijver, P. A., and J. Kittler. 1982. Pattern recognition: A statistical approach. Prentice-Hall, <lb/>Englewood Cli s, New Jersey. <lb/>Dietterich, T. G., and G. Bakiri. 1991. Error-correcting output codes: A general method for <lb/>improving multiclass inductive learning programs. Pages 572{577 in: Proceedings, Ninth <lb/>National Conference on Arti cial Intelligence, Anaheim, California, August 1991. AAAI <lb/>Press, Menlo Park, California. <lb/>Dietterich, T. G., and G. Bakiri. 1995. Solving multiclass learning problems via error-correcting <lb/>output codes. Journal of Arti cial Intelligence Research 2: 263{286. <lb/>Ellrod, G. P., J. M. Peak, and R. L. Bankert. 1993. An expert system for the analysis of high <lb/>altitude turbulence. Pages 617{620 in: Proceedings, Thirteenth Conference on Weather <lb/>Analysis and Forecasting. American Meteorological Society, Vienna, Virginia. <lb/>Fett, R. W., M. E. White, J. E. Peak, S. Brand, and P. M. Tag. 1997. Application of <lb/>hypermedia and expert system technology to navy environmental satellite image analysis. <lb/>Accepted for publication in Bulletin of the American Meteorological Society. <lb/>Fu, K. S. 1968. Sequential methods in pattern recognition and machine learning. Academic <lb/>Press, New York. <lb/>John, G., R. Kohavi, and K. P eger. 1994. Irrelevant features and the subset selection problem. <lb/>Pages 121{129 in: Proceedings, Eleventh International Machine Learning Conference, New <lb/>Brunswick, New Jersey, August 1994. Morgan Kaufmann, San Mateo, California. <lb/>Kong, E. B., and T. G. Dietterich. 1995. Error-correcting output coding corrects bias and <lb/>variance. Pages 313{321 in: Proceedings, Twelfth International Conference on Machine <lb/>Learning, Tahoe City, California, July 1995. Morgan Kaufmann, San Mateo, California. <lb/></listBibl>

			<page>22 <lb/></page>

			<listBibl>Kuciauskas, A. P., L. R. Brody, R. L. Bankert, P. M. Tag, and M. Hadjimichael. 1996. <lb/>Automated forecasting of gale force winds in the mediterranean region. Pages 385{361 in: <lb/>Proceedings, Fifteenth Conference on Weather Analysis and Forecasting. American <lb/>Meteorological Society, Norfolk, Virginia. <lb/>Makridakis, S., and R. L. Winkler. 1983. Averages of forecasts: Some empirical results. <lb/>Management Science 29(9): 987{996. <lb/>Mohri, T., and H. Tanaka. 1994. An optimal weighting criterion of case indexing for both <lb/>numeric and symbolic attributes. Pages 123{127 in: Case-Based Reasoning: Papers from <lb/>the 1994 Workshop (Technical Report WS-94-01), D. W. Aha, editor. AAAI Press, Menlo <lb/>Park, California. <lb/>Peak, J. E., and P. M. Tag. 1989. An expert system approach for prediction of maritime <lb/>visibility obscuration. Monthly Weather Review 117: 2641{2653. <lb/>Perrone, M. P. 1994. Putting it all together: Methods for combining neural networks. Pages <lb/>1188{1190 in: Advances in Neural Information Processing Systems, 6, J. D. Cowan, <lb/>G. Tesauro, and J. Alspector, editors. Morgan Kaufmann, San Mateo, California. <lb/>Perrone, M. P., and L. N. Cooper. 1993. When networks disagree: Ensemble methods for hybrid <lb/>neural networks. In Neural Networks for Speech and Image Processing, R. J. Mammone, <lb/>editor. Chapman and Hall, Philadelphia, Pennsylvania. <lb/>Quinlan, J. R. 1993a. C4.5: Programs for machine learning. Morgan Kaufmann, San Mateo, <lb/>California. <lb/>Quinlan, J. R. 1993b. Combining instance-based learning and model-based learning. Pages <lb/>236{243 in: Proceedings, Tenth International Conference on Machine Learning, Amherst, <lb/>Massachusetts, July 1993. Morgan Kaufmann, San Mateo, California. <lb/>Rendell, L., and R. Seshu. 1990. Learning hard concepts through constructive induction: <lb/>Framework and rationale. Computational Intelligence 6: 247{270. <lb/>Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by <lb/>error propagation. Pages 318{362 in: Parallel Distributed Processing: Explorations in the <lb/>Microstructures of Cognition, D. E. Rumelhart and J. L. McClelland, editors. MIT Press, <lb/>Cambridge, Massachusetts. <lb/>Sejnowski, T. J., and C. R. Rosenberg. 1987. Parallel networks that learn to pronounce English <lb/>text. Complex Systems 1: 145{168. <lb/>Specht, D. F. 1990. Probabilistic neural networks. Neural Networks 3: 109{118. <lb/>Welch, R. M. 1996. Hierarchical neural networks with don&apos;t care nodes. Presentation at the <lb/>Arti cial Intelligence esearch in Environmental Sciences Workshop, Wake eld, assachusetts, <lb/>August 1996. <lb/>Welch, R. M., S. K. Sengupta, A. K. Goroch, P. Rabindra, N. Rangaraj, and M. S. Navar. 1992. <lb/>Polar cloud and surface classi cation using AVHRR imagery: An intercomparison of <lb/>methods. Journal of Applied Meteorology 31: 405{420. <lb/></listBibl>

			<page>23 <lb/></page>

			<listBibl>Wettschereck, D., and T. G. Dietterich. 1992. Improving the performance of radial basis <lb/>function networks by learning center locations. Pages 1133{1140 in: Neural Information <lb/>Processing Systems 4, J. Moody, S. Hanson, and R. Lippmann, editors. Morgan Kaufmann, <lb/>San Mateo, California. <lb/>Zhang, X., J. Mesirov, and D. Waltz. 1992. Hybrid system for protein structure prediction. <lb/>Journal of Molecular Biology 225: 1049{1063. <lb/></listBibl>

			<div type="annex">Biographical sketches <lb/>David W. Aha (UC Irvine 1990) joined the Naval Research Laboratory&apos;s Navy Center for Applied <lb/>Research in Arti cial Intelligence in 1993. His research interests lie at the intersection of machine <lb/>learning (ML) and case-based reasoning (CBR). He serves both communities as a frequent <lb/>conference program committee member, editing board member (Machine Learning, Journal of <lb/>Arti cial Intelligence Research, Applied Intelligence), workshop (co-)organizer, and WWW page <lb/>maintainer (http://www.aic.nrl.navy.mil/ aha). He also is an AIRIES committee member, and <lb/>recently edited a special issue for Arti cial Intelligence Review on Lazy Learning. <lb/>Richard L. Bankert (Penn State, 1988) joined the Marine Meteorology Division of the Naval <lb/>Research Laboratory in 1990 (then called the Naval Oceanographic and Atmospheric Research <lb/>Laboratory). His research interests include the application of a variety of arti cial intelligence <lb/>techniques to meteorological and other environmental science problems. He has previously served <lb/>on the AIRIES committee (1992) and is currently on the American Meteorological Society&apos;s <lb/>Science and Technological Activities Committee on AI Applications to Environmental Science. <lb/></div>

			<page>24 </page>


	</text>
</tei>
