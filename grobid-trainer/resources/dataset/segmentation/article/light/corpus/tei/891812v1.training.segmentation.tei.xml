<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Crowdsourced mapping of unexplored target space of kinase inhibitors <lb/>Anna Cichonska 1,2,3 *, Balaguru Ravikumar 1 *, Robert J Allaway 4 *, Sungjoon Park 5 , Fangping <lb/>Wan 6 , Olexandr Isayev 7 , Shuya Li 6 , Michael Mason 4 , Andrew Lamb 4 , Zia-ur-Rehman Tanoli 1 , <lb/>Minji Jeon 5 , Sunkyu Kim 5 , Mariya Popova 7 , Jianyang Zeng 6 , Kristen Dang 4 , Gregory Koytiger 8 , <lb/>Jaewoo Kang 5 , Carrow I. Wells 9 , Timothy M. Willson 9 , The IDG-DREAM Drug-Kinase Binding <lb/>Prediction Challenge Consortium , Tudor I. Oprea 10 , Avner Schlessinger 11 , David H. Drewry 9 , <lb/>Gustavo Stolovitzky 12 , Krister Wennerberg 13 , Justin Guinney 4 **, Tero Aittokallio 1,2,14,15,16 ** <lb/>*shared first authors <lb/>**shared last authors <lb/>Affiliations: <lb/>1 Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Helsinki, Finland <lb/>2 Department of Computer Science, Helsinki Institute for Information Technology (HIIT), Aalto <lb/>University, Espoo, Finland <lb/>3 Department of Future Technologies, University of Turku, Turku, Finland <lb/>4 Computational Oncology, Sage Bionetworks, Seattle, WA, USA <lb/>5 Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea <lb/>6 Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, 100084 <lb/>7 Department of Chemistry, Carnegie Mellon University, Pittsburgh, PA <lb/>8 Immuneering Corporation, Cambridge, MA, USA <lb/>9 Structural Genomics Consortium, UNC Eshelman School of Pharmacy, University of North <lb/>Carolina at Chapel Hill, Chapel Hill, North Carolina, USA <lb/>10 Translational Informatics Division, University of New Mexico School of Medicine, Albuquerque, <lb/>New Mexico, USA <lb/>11 Department of Pharmacological Sciences, Icahn School of Medicine at Mount Sinai, New York, <lb/>New York, USA <lb/>12 IBM T J Watson Research Center, IBM, Yorktown Heights, NY, USA <lb/>13 Biotech Research and Innovation Centre (BRIC), University of Copenhagen, Copenhagen, <lb/>Denmark <lb/>14 Department of Mathematics and Statistics, University of Turku, Turku, Finland <lb/>15 Institute for Cancer Research, Oslo University Hospital, Oslo, Norway <lb/>16 Oslo Centre for Biostatistics and Epidemiology (OCBE), University of Oslo, Oslo, Norway <lb/>Abstract <lb/>Despite decades of intensive search for compounds that modulate the activity of particular <lb/>proteins, there are currently small-molecule probes available only for a small proportion of the <lb/>human proteome. Effective approaches are therefore required to map the massive space of <lb/>unexplored compound-target interactions for novel and potent activities. Here, we carried out a <lb/>crowdsourced benchmarking of the accuracy of machine learning (ML) algorithms at predicting <lb/>kinase inhibitor potencies across multiple kinase families. A total of 268 ML predictions were <lb/>scored in unpublished bioactivity data sets. Top-performing algorithms used kernel learning, <lb/>gradient boosting and deep learning, with predictive accuracy exceeding that of target activity <lb/>assays. Subsequent experiments carried out based on the the top-performing model predictions <lb/>demonstrated that these models and their ensemble can improve the accuracy of experimental <lb/>mapping efforts, especially for so far under-studied kinases. The open-source ML algorithms <lb/>together with the novel dose-response data for 905 bioactivities between 95 compounds and <lb/>295 kinases provide a unique resource for extending the druggable kinome. <lb/></front>

			<body>Introduction <lb/>Despite many years of target-based drug discovery, chemical agents inhibiting single protein <lb/>targets are still rare. 1 For instance, most approved drugs have multiple targets, suggesting their <lb/>therapeutic efficacy as well as adverse side-effects originate from polypharmacological effects. 2 <lb/>Even if agents with narrow target profile often present with less toxic effects, multi-targeted <lb/>approaches may provide improved efficacy for treating complex diseases. Systematic mapping <lb/>of the target binding profiles is therefore critical not only to explore the therapeutic potential of <lb/>promiscuous agents, but also to better predict and manage their possible adverse effects prior <lb/>to further development and clinical trials (i.e., speeding-up and de-risking the drug development <lb/>process). Novel off-target potencies of approved drugs could also extend the therapeutic <lb/>application area of repurposed agents. However, the massive size of the chemical universe <lb/>makes experimental mapping of the full space of compound-target interactions infeasible, even <lb/>with automated high-throughput profiling assays. <lb/>To address this problem, we implemented the IDG-DREAM Drug-Kinase Binding Prediction <lb/>Challenge, a crowd-sourced competition that evaluated the power of machine learning (ML) <lb/>models as a systematic and cost-effective means for predicting novel compound-target <lb/>potencies that warrant experimental evaluation (i.e., target prioritization). The Challenge focused <lb/>on kinase inhibitors, since kinases are tractable in drug development and play a role in a wide <lb/>range of diseases, such as cardiovascular disorders and cancers. However, protein kinase <lb/>domains share structural and sequence similarity, and most kinase inhibitors bind to conserved <lb/>ATP-binding pockets, which leads to prevalent target promiscuity and polypharmacological <lb/>effects. 3-5 Such promiscuity requires effective target deconvolution approaches, including ML or <lb/>AI approaches, that can leverage the information extracted from similar kinases and compounds <lb/>to predict the activity of so far unexplored interactions. <lb/>The Challenge was implemented in a screening-based, pre-competitive drug discovery project in <lb/>collaboration with the NIH-supported Illuminating the Druggable Genome (IDG) program <lb/>( https://commonfund.nih.gov/idg ), with the common aim to establish kinome-wide target profiles <lb/>of small-molecule agents, and thereby to extend the druggability of the human kinome space by <lb/>providing activity information on under-studied proteins. The specific questions this Challenge <lb/>sought to address were: (i) What are the best computational modelling approaches for predicting <lb/>quantitative compound-target activity profiles?; (ii) What are the optimal molecular and chemical <lb/>descriptors for maximal prediction accuracy?; and (iii) What are the most predictive bioactivity <lb/>assays and publicly available datasets? The Challenge attracted 212 active participants, and a <lb/>total of 268 predictions were scored, covering a wide range of ML approaches, including deep <lb/>and kernel learning and gradient boosting decision trees. Here, we describe the benchmarking <lb/>results from the Challenge, and the use of top-performing models for identifying novel kinase <lb/>inhibitor activities. <lb/>Results <lb/>Challenge implementation <lb/>To develop their predictive models, the participants had access to a wide variety of bioactivity <lb/>data for model training and cross-validation through open databases such as ChEMBL 6 , <lb/>BindingDB 7 and IDG Pharos 8 (Fig. 1). For training data collection, integration, management and <lb/>harmonization, the Challenge made use of an open-data platform, DrugTargetCommons (DTC). 9 <lb/>DTC is a community platform that facilitates the annotation and curation of bioactivity data, and <lb/>provides a comprehensive and standardized interface to retrieve compound-target profiles and <lb/>related information to support predictive modelling (Suppl. Fig. 1). The Challenge infrastructure <lb/>was built on the Synapse collaborative science platform 10 , which supported receiving, validating <lb/>and scoring of the teams&apos; predictions as well as long-term management of the test bioactivity <lb/>data and submitted Challenge models as a benchmarking resource (Fig. 1). <lb/>Figure 1 . Overview of the IDG-DREAM Drug-Kinase Binding prediction Challenge . The <lb/>heatmap on the left is for illustrative purposes only (see Suppl. Fig. 2 for the actual test data <lb/>matrices, and Suppl. Fig. 3 for the Challenge timeline). <lb/>Challenge test datasets <lb/>Evaluation of the model predictions was based on unpublished target activity data generated by <lb/>the IDG Kinase Data and Resource Generation Center, conducted over a series of &quot;rounds&quot; <lb/>based on availability of validation datasets (Suppl. Fig. 3). Generation of the test data for Round <lb/>1 was based on a single-dose kinome scan of a library of multi-targeted compounds. 5,11 This <lb/>was followed by a dose-response determination of the dissociation constant (K d ) values for 430 <lb/>compound-kinase pairs between 70 inhibitors and 199 kinases that were not available in the <lb/>public domain (see Methods). An additional set of completely new K d data was generated for <lb/>Round 2, consisting of 394 multi-dose assays between 25 inhibitors and 207 kinases with <lb/>single-dose inhibition &gt;80%. Together, these 824 K d assays in the two Rounds spanned a total <lb/>of 95 compounds and 295 kinases (Fig. 2A-B), consisting of promiscuous compounds targeting <lb/>multiple kinases at low concentrations, compounds with narrow target profiles, as well as <lb/>compounds with no potent targets among the tested kinases (Suppl. Fig. 2). <lb/>Round 1 enabled the teams to carry out initial testing of various model classes and data <lb/>resources, whereas Round 2, implemented 6 months later, was used to score the final <lb/>prediction models and to select the top-performing teams. Round 1 and 2 test data had very <lb/>similar K d distributions (Fig. 2C), which provided comparable binding affinity outcome data to <lb/>monitor the improvements made by the teams between the two rounds. Compounds in the test <lb/>sets were mutually exclusive between rounds (Fig. 2A), with Round 2 including less selective <lb/>compounds with broader target profiles (Fig. 2D), and therefore fewer inactive compound-target <lb/>pairs (pK d =5). Round 1 and 2 kinase targets were partly overlapping, and covered all major <lb/>kinase families and groups (Fig. 2B,E). Taken together, these two test datasets provided a <lb/>standardized and sufficiently large quantitative bioactivity resource to evaluate the accuracy of <lb/>predicting on-and off-target activities. <lb/>Figure 2. Challenge test datasets. (A) The overlap between Round 1 and Round 2 test <lb/>compounds and kinases, and their distributions in the kinome tree (B) and across kinase groups <lb/>(E). (C) The quantitative dissociation constant (K d ) of compound-kinase activities was measured <lb/>in dose-response assays (see Methods), presented in the logarithmic scale as pK d = -log 10 (K d ). <lb/>The higher the pK d value, the higher the inhibitory ability of a compound against a protein kinase <lb/>(Suppl. Fig. 2 lists the compounds and kinases in Round 1 and Round 2). (D) The selectivity <lb/>index for compounds was calculated based on the single-dose activity assay (at 1000 nM) <lb/>across full compound-kinase matrices before the Challenge. The kinome tree figure was created <lb/>with KinMap, reproduced courtesy of Cell Signaling Technology, Inc. <lb/>Overall performance of the models <lb/>The competition challenged the participants to predict blinded K d profiles between 95 <lb/>compounds and 295 kinases. A recently published and experimentally validated kernel <lb/>regression approach for compound-kinase activity prediction was used as the &quot;baseline <lb/>model&quot; 12 . The accuracy of the predictions improved from Round 1 to Round 2 submissions as <lb/>measured by Spearman correlation ( two-sample Wilcoxon test , p&lt;0.005; Fig. 3A) and Root <lb/>
			Mean Square Error (RMSE, p&lt;10 -6 ; Fig. 3C). Comparison against the baseline model indicated <lb/>that the Round 2 dataset was marginally easier to predict (Suppl. Fig. 4), partly due to a smaller <lb/>proportion of inactive pairs in Round 2 (pK d = 5, Fig. 2C). To take into account this shift, we <lb/>compared the submissions against a set of random predictions . Using Spearman correlation, <lb/>we observed that 48% of the submission were better than random in Round 1, compared to 61% <lb/>in Round 2 (Fig 3B). Using RMSE, 71% of the submissions in Round 1 were better than random, <lb/>compared to 76% in Round 2 (Fig 3D). <lb/>The 20 teams that participated in both rounds improved their K d predictions (p&lt;0.05 and p&lt;0.001 <lb/>for Spearman and RMSE, paired Wilcoxon signed-rank test), but when comparing against the <lb/>baseline model, the overall improvements became insignificant (p&gt;0.05). However, there were <lb/>individual teams (like Zahraa Sobhy) that were able to improve their predictions considerably <lb/>between the two rounds. The practical upper bound of the model predictions was defined based <lb/>on experimental replicates of K d measurements (Fig. 3B,D). The predictive accuracy of the <lb/>top-performing models in Round 2 was relatively high based on both of the winning metrics, <lb/>Spearman correlation for rank predictions and RMSE for activity predictions; these metrics <lb/>showed less correlated performance over the less-accurate models in Round 2 (Fig. 3F). The tie <lb/>breaking metric, averaged area under the curve (AUC), provided complementary information on <lb/>prediction accuracies when compared to RMSE but not to Spearman correlation (Suppl. Fig. 5). <lb/>Figure 3. Overall performance of the submissions . (A, C) Performance of the submissions in <lb/>terms of the two winning metrics in Round 1 (n=169) and Round 2 (n=99). The colors mark the <lb/>baseline model and top-performing participants in Round 2. The empty circles mark the <lb/>submissions that did not differ from random predictions. The baseline model 12 remained the <lb/>same in both of the rounds. (B, D) Distribution of the random predictions (based on 10000 <lb/>permuted pK d values) and replicate distributions (based on 10000 subsamples with replacement <lb/>of overlapping pK d pairs between two large-scale target activity profiling studies 3,4 ) in Round 1 <lb/>(top panel) and Round 2 (bottom). The points correspond to the individual submissions. (E, F) <lb/>Relationship of the two winning metrics across the submissions in Round 1 and Round 2. T he <lb/>shape indicates submissions based on deep learning in Round 2 (F). For instance, team <lb/>DMIS_DK submitted predictions based on both random forest (RF) and deep learning (DL) <lb/>algorithms in Round 2, where the latter showed slightly better accuracy (triangle). Overall, DL <lb/>approaches did not perform better than the other learning approaches used in Round 2. <lb/>Analysis of the top-performing models <lb/>The top-performing models were selected in Round 2 based on 394 pK d predictions between 25 <lb/>compounds and 207 kinases. Only those participants who submitted their Dockerized models, <lb/>method write-ups and method surveys were qualified to win the two sub-challenges. T o select <lb/>the top-performers for the two winning metrics, Spearman correlation and RMSE, we conducted <lb/>a bootstrap analysis of each participant&apos;s best submission, and then calculated a Bayes factor <lb/>(K) relative to the bootstrapped overall best submission for each winning metric (Suppl. Fig. 6). <lb/>Considering Spearman correlation, the top-performer was team Q.E.D (K&lt;3; Fig. 4A). For the <lb/>RMSE metric, the top-performing teams were AI Winter is Coming (AIWIC) and DMIS_DK (K&lt;3; <lb/>Suppl. Fig. 6 ) , with AIWIC having a marginally better tie-breaking metric (average AUC of 0.773; <lb/>Fig. 4B). Only two non-qualifying participants ( Gregory Koytiger and Olivier Labayle ) showed a <lb/>comparable performance. Overall, these five teams performed the best when considering the 54 <lb/>teams in Round 2 (Suppl. Fig. 7). <lb/>Figure 4 . Top-performing models and their ensemble combination. (A) Spearman <lb/>correlation sub-challenge top-performer in Round 2, Q.E.D. (B) RMSE sub-challenge <lb/>top-performer in Round 2, AI Winter is Coming. (C) Ensemble model that combines the top four <lb/>models selected based on their Spearman correlation in Round 2. The points correspond to the <lb/>394 compound-kinase pairs between 25 inhibitors and 207 kinases in Round 2. (D) The mean <lb/>aggregation ensemble model was constructed by adding an increasing number of <lb/>top-performing models based on their Spearman correlation, until the ensemble correlation <lb/>dropped below 0.45. The peak performance was reached when aggregating four teams (marked <lb/>in the legend, see Suppl. Fig. 8 for names of all the teams). <lb/>Notably, the top-performing models were based on various ML approaches, including deep <lb/>learning, graph convolutional networks, gradient boosting decision trees, kernel learning and <lb/>regularized regression (Table 1). To study whether combining predictions from multiple ML <lb/>approaches could improve prediction accuracy, we constructed an ensemble model by simple <lb/>mean aggregation of an increasing number of top-performing models in Round 2. The <lb/>combination of the four best performing models resulted in the peak Spearman correlation (Fig. <lb/>4C), demonstrating complementary value of these predictions. After adding more models, the <lb/>ensemble prediction accuracy started to decrease rather rapidly, both in terms of Spearman <lb/>correlation and RMSE (Fig. 4D). However, an ensemble prediction from a total of 21 best teams <lb/>had a significantly better correlation than the best single model alone (K&gt;5; Suppl. Fig. 8). This <lb/>suggests that combination of various ML approaches using an ensemble model leads to <lb/>accurate and robust predictions of kinase inhibitor potencies across multiple kinase families. <lb/>A <lb/>Team Algorithm Type <lb/>Algorithm Names <lb/>Combined <lb/>Models <lb/>Training Strategy <lb/>DMIS_DK Deep learning <lb/>Graph Neural Networks <lb/>12 <lb/>Train test split <lb/>AI Winter is <lb/>Coming <lb/>Gradient boosting decision <lb/>trees <lb/>XGboost <lb/>5 per target <lb/>K-fold nested cross <lb/>validation, boosting <lb/>Q.E.D Kernel learning <lb/>CGKronRLS <lb/>440 <lb/>Boosting <lb/>Gregory Koytiger Deep learning <lb/>Not applicable <lb/>6 <lb/>Fixed hold out <lb/>Olivier Labayle Ridge regression <lb/>Not applicable <lb/>Not applicable <lb/>K-fold cross <lb/>validation <lb/>Baseline Kernel learning <lb/>CGKronRLS <lb/>1 <lb/>K-fold nested cross <lb/>validation <lb/>B <lb/>Team Training Data Sources <lb/>Compound-<lb/>Protein Pairs <lb/>Bioactivity Types <lb/>Protein <lb/>Representation <lb/>Chemical <lb/>Representation <lb/>DMIS_DK <lb/>DrugTargetCommons, <lb/>BindingDB <lb/>953521 <lb/>K d , K i , IC 50 <lb/>None <lb/>2D molecular <lb/>graphs <lb/>AI Winter is <lb/>Coming <lb/>DrugTargetCommons, <lb/>ChEMBL <lb/>600000 <lb/>K d , K i , IC 50 , EC 50 <lb/>None <lb/>ECFP5, ECFP7, <lb/>ECFP9, ECFP11 <lb/>Q.E.D <lb/>DrugTargetCommons, <lb/>ChEMBL, Uniprot <lb/>60462 <lb/>K d , K i , EC 50 <lb/>Amino acid <lb/>sequences <lb/>ECFP4, ECFP6 <lb/>Gregory <lb/>Koytiger ChEMBL <lb/>250000 <lb/>K d , K i , IC 50 <lb/>None <lb/>None <lb/>Olivier Labayle <lb/>DrugTargetCommons, <lb/>ChEMBL, Uniprot <lb/>18200 <lb/>K d <lb/>K-mer counting <lb/>ECFP <lb/>Baseline DrugTargetCommons <lb/>44186 <lb/>K d <lb/>Amino acid <lb/>sequences <lb/>Path-based <lb/>fingerprints <lb/>Table 1. Characteristics of the Round 2 top-performing methods and the baseline model 12 . <lb/>Comparison against single-dose activity <lb/>We next investigated how well the top-performing ML models compare against the single-dose <lb/>activity assays when predicting the pK d measurements. The practical problem of many target <lb/>screening studies is how to reduce the number of false positives and false negatives when <lb/>selecting most potent compound-target activities for more detailed, multi-dose K d profiling. For <lb/>this classification task, we defined the ground truth activity classes based on the measured K d <lb/>potencies, which provide a more direct prediction outcome, compared to the rank correlation <lb/>analyses that already demonstrated predictive rankings from the top-performing models (Fig. 4). <lb/>Using the activity cut-off of measured pK d = 6 and an single-dose inhibition cut-off of 80%, <lb/>similar to previous studies, 5,11,13 the positive predictive value (PPV) and the false discovery rate <lb/>(FDR) of the single-dose assay were PPV = 0.66 and FDR = 0.44 in the Round 2 dataset. When <lb/>using the mean aggregation ensemble of the predicted pK d values from the top-performing <lb/>models and the same cut-off of pK d &gt; 6 for both the predicted and measured activities, we <lb/>observed an improved precision of PPV = 0.76 and FDR = 0.24. <lb/>We further repeated the activity classification with multiple cut-off levels, and ranked the Round <lb/>2 pairs both using the model-predicted pK d values and the measured single-dose inhibition <lb/>assay values, and then compared these rankings against the measured dose-response assay <lb/>(pK d &gt; 6 indicates positive activity class). The ROC analyses demonstrated an improved activity <lb/>classification accuracy using the mean ensemble of the top-performing models (Fig. 5A), <lb/>especially when focusing on the most potent compound-target activities with the highest <lb/>specificity that are important in practice when prioritizing a subset of most potent activities for <lb/>multi-dose validation. This improvement in both sensitivity and specificity was achieved without <lb/>making any additional activity measurements, and it became even more pronounced with the <lb/>precision-recall analysis, which showed that the precision of the prediction models remained <lb/>above PPV=75% level even when the recall (sensitivity) level exceeded 75% (Fig. 5B). As <lb/>expected, the prediction accuracy decreased when using more stringent activity cut-off of pK d &gt; <lb/> 7 (Suppl. Fig. 9), since these extreme activities are more challenging to predict. <lb/> 
			Figure 5. Top-performing model predictions compared against single-dose assays. ( A) <lb/>Receiver operating characteristic (ROC) curves when ranking the 394 compound-kinase pairs <lb/>from Round 2 using both the ensemble of the top-performing models (average predicted pK d <lb/>from Q.E.D, DMIS_DK and AI Winter is Coming) and the experimental single-dose inhibition <lb/>assays ( the true positive activity class includes pairs with measured pK d &gt; 6). The area under <lb/>ROC curve values are shown in the parentheses and the diagonal dotted line shows the random <lb/>prediction accuracy of AU-ROC=0.50. (B) Precision-recall (PR) curves for the same activity <lb/>classification analysis as shown in panel A. The area under the PR curve values are shown in <lb/>parentheses and the horizontal dotted line indicates the precision of 0.75 level. <lb/>Sensitivity=Recall. Precision=PPV. <lb/>Since the Round 2 multi-dose K d measurements were pre-selected among all the 5100 <lb/>compound-kinase pairs to include mostly those pairs with single-dose inhibition&gt;80%, Round 2 <lb/>dataset enables systematic analysis of false positive predictions made based on single-dose <lb/>assays or model predictions. However, these 394 pairs selected for K d profiling were more <lb/>limited for a comprehensive analysis of false negative predictions (i.e., those with measured pK d <lb/>&gt; 6, but single-dose inhibition&lt;80% or predicted pK d &lt; 6). This means that the above comparison <lb/>between single-dose inhibition assays and model-predicted pK d is biased in the sense that the <lb/>inhibition values were used to select the pairs for the Round2 K d profiling, which therefore <lb/>misses the more challenging compound-kinase pairs that had lower single-dose inhibition. This <lb/>is why in the next section we carried out further experimental validations of the model-predicted <lb/>pK d profiles in a more unbiased manner to investigate false negative predictions from both <lb/>single-dose assays and ML models. <lb/>Model-based target predictions <lb/>To explore the compound-kinase pairs across the full spectrum of single-dose inhibition levels, <lb/>we experimentally profiled 81 additional pairs, which were not part of Round 1 or 2 datasets, <lb/>based solely on the pK d predictions from the three top-performing models. These follow-up <lb/>experiments were carried out in an unbiased manner, regardless of the compound class or <lb/>selectivity, kinase target families, or inhibition levels of the pairs, to investigate whether it is <lb/>possible to use predictive models to identify potent inhibitors of kinases showing less than 80% <lb/>single-dose inhibition; this activity cut-off is often used when selecting pairs for multi-dose K d <lb/>testing 5,11,13 . Most of the measured pK d values of these 81 pairs were distributed as expected, <lb/>according to the expected single-dose inhibition function (Fig. 6A, black trace). However, this <lb/>model-based approach also identified unexpected activities (pK d &gt; 6) that could not be predicted <lb/>based on the inhibition assay only; those with pK d &gt; 7 are discussed below. <lb/>As an example of a potent activity missed by the single-dose assays, the top-performing models <lb/>predicted PYK2 (PTK2B) as a high affinity target of a PLK inhibitor TPKI-30 (Fig. 6A). The new <lb/>multi-dose pK d measurements validated that TPKI-30 indeed has an activity against PYK2 close <lb/>to its potency towards PLK2 (Fig. 6B, left panel). This is rather surprising and novel result that a <lb/>PLK inhibitor targets PYK2, and with a somewhat lesser potency also its paralog, FAK (PTK2). <lb/>Neither PYK2 or FAK would have been predicted to be potent targets based on the single-dose <lb/>testing alone, which led to multiple false negatives (Fig. 6B, right panel). Similarly, the single <lb/>dose-testing had a relatively low predictivity of actual potencies for TPKI-30, since kinases other <lb/>than PLKs with high single-dose activity were reported as non-potent targets based on <lb/>dose-response K d testing, resulting in false positives. In contrast, the model predictions turned <lb/>out to be relatively accurate, except for a few receptor tyrosine kinases (Fig. 6B, left panel). <lb/>Another unexpected target activity was predicted for GSK1379763 that showed high potency <lb/>against DDR1 based on the K d assays, exceeding that of the AURKB (Fig. 6C, left panel). The <lb/>single-dose testing suggested that this compound would not have potency against DDR1 or <lb/>AURKB (Fig. 6C, right panel) , whereas multi-dose assays confirmed potency towards DDR1 at a <lb/>higher level as against the Round 2 highest affinity target MEK5 (MAP2K5). Notably, both of <lb/>these high affinity predicted targets, PYK2 and DDR1, are less-explored kinases with only a few <lb/>bioactivity data values available in DTC or ChEMBL. This suggests that the prediction models <lb/>can identify potent inhibitors for under-studied kinases that would have been missed when using <lb/>single-dose assays alone. The third high predicted activity between AKI00000050a and FLT1 <lb/>could have been predicted based on its relatively high single-dose activity (Fig. 6A). This <lb/>compound was confirmed to be a potent KDR (FLT2) inhibitor with quite similar potency as that <lb/>against FLT1. <lb/>
			Figure 6. Machine learning-based target predictions. (A) Comparison of single-dose <lb/>inhibition assay (at 1 µM) against multi-dose K d assay activities across 475 compound-target <lb/>pairs (394 Round 2 pairs and 81 additionally profiled pairs). The red points indicate false <lb/>negatives and blue points false positives when using cut-offs of pK d = 6 and inhibition=80% <lb/>among the Round 2 pairs ( including 75 pairs with inhibition&gt;80% but that showed no activity in <lb/>the dose-response assays, i.e, pK d = 5) . The green points indicate the new experimental <lb/>validations based solely on model predictions, regardless of inhibition levels. Both the <lb/>single-dose and dose-response assays were carried out as competitive binding assays similar to <lb/>previous studies. 11 The black trace indicates t he expected %inhibition rate based on measured <lb/>pK d &apos;s, estimated using the maximum ligand concentration of 1 µM both for the single-dose and <lb/>dose-response assays. (B) Multi-dose (left) and single-dose (right) assays for kinases tested <lb/>with TPKI-30. Green points indicate the new experimental validations based on model <lb/>predictions, whereas black points come from Round 2 data. Blue points indicate false positive <lb/>predictions based either on predictive models or single-dose testing. Single-dose testing <lb/>predicted TPKI-30 to be relatively potent PLK1/2/3 inhibitor, whereas the dose-response testing <lb/>confirmed it as PLK1 selective. (C) Multi-dose (left) and single-dose (right) assays for kinases <lb/>tested with GSK1379763. Green points indicate the new experimental validations based on <lb/>model predictions, whereas black points come from Round 2 data. Blue points indicate false <lb/>positive predictions based either on predictive models or single-dose testing. (D) Predictive <lb/>accuracy of the ensemble of top-performing models ( average predicted pK d ) and single-dose <lb/>assay (at 1 µM ) when classifying various subsets of 475 pairs into those with measured pK d less <lb/>or higher than 6. The y-axis indicates the area under the receiver operating characteristic curve <lb/>(AUC) as a function single-dose inhibition cut-off levels, x-axis indicates the pairs with <lb/>inhibition&gt;x%, and the dotted black curve the percentage of all pairs that passed that activity <lb/>cut-off threshold. The combined model trace corresponds to the average of measured and <lb/>expected inhibition values, where the latter was calculated based on the mean ensemble of the <lb/>top-performing model pK d predictions (Q.E.D., DMIS_DK and AI Winter is Coming). (E) Receiver <lb/>operating characteristic (ROC) curves (left) and precision-recall (PR) curves (right), when <lb/>ranking all the 475 pairs either using the top-performing model-predicted pK d values or the <lb/>measured single-dose inhibition assays (the true positive activity class includes pairs with <lb/>measured pK d &gt; 6). The AUC values are shown in parentheses, and the diagonal dotted line <lb/>indicates the random prediction accuracy of AU-ROC=0.50 (left), and the horizontal dotted line <lb/>indicates the precision level of 0.75 (right). Sensitivity=Recall. Precision=PPV. <lb/>Surprisingly, the single-dose inhibition assays and model-based pK d <lb/>predictions were almost <lb/>uncorrelated (Suppl. Fig. 10, Spearman correlation 0.24), and they showed opposite trends for <lb/>K d prediction accuracy when increasing the inhibition cut-off level (Fig. 6D). To combine these <lb/>two activity estimators, we calculated for each compound-kinase pair an average of the <lb/>measured and expected inhibition values based on the single-dose assay and the <lb/>
			top-performing models, respectively. This combined predictor showed improved activity <lb/>classifications beyond that of the model predictions alone, across various inhibition levels, and <lb/>identified a larger number of potent compound-target interactions with lower single-dose activity, <lb/>compared to the standard 80% cut-off (Fig. 6D, dotted line). The combined model improved both <lb/>the sensitivity and specificity of the pK d predictions among all the 475 pairs (Fig. 6E, left panel), <lb/>and especially the precision of the top-activity predictions that are prioritized for further <lb/>experimental validation (Fig. 6E, right panel). <lb/>Discussion <lb/>Experimental mapping of compound-target interactions is critical for understanding compounds&apos; <lb/>mode of action (MoA), but biochemical target activity profiling experiments are both time <lb/>consuming and costly. Moreover, the enormous size of the chemical universe, estimated to <lb/>consist of approximately 10 20 molecules exhibiting good pharmacological properties, 14,15 makes <lb/>experimental bioactivity mapping of the full compound and target space quickly infeasible in <lb/>practice. ML models are aimed at guiding data-driven decision making, and these models have <lb/>shown their potential to reduce failure rates and accelerate several phases of drug discovery <lb/>and development. 16 The IDG-DREAM Drug Kinase Binding Prediction Challenge sought to <lb/>benchmark state-of-the-art ML algorithms in the task of exploring the druggable kinome space <lb/>by combining predictive modelling with experimental target activity profiling. In particular, the <lb/>Challenge participants applied supervised ML models in the task of guiding biochemical <lb/>mapping efforts by systematic prioritization of the most potent compound-target activities for <lb/>further experimental evaluation. The ML model-guided approach has the potential to help both <lb/>(i) phenotype-based drug discovery (e.g. mapping the active target space of lead compounds), <lb/>and (ii) target-based drug discovery (e.g. identification of candidate compounds that selectively <lb/>inhibit a particular disease-related target). <lb/>Although previous work has already demonstrated the potential of ML algorithms for filling in the <lb/>gaps in existing drug-target interaction maps, 12 there are no systematic benchmarking <lb/>comparisons of the algorithms in blinded, comprehensive datasets. The participants were <lb/>therefore encouraged to explore various statistical and machine learning modelling approaches. <lb/>In the Round 2 results, no particular method class, training data source or bioactivity type stood <lb/>out. Rather, the top-performing teams used relatively different approaches (Table 1). Some of <lb/>the top-performing models used protein sequence as target feature, but no structural <lb/>information. Furthermore, none of the top-performing models require 3D or other detailed <lb/>chemical information, making the ML models rather straightforward to apply for many compound <lb/>and target classes. Recently, many advanced deep learning (DL) algorithms have been <lb/>proposed for compound-target interaction prediction, 22-24 but our results did not find DL <lb/>outperforming other learning approaches. The Spearman correlation sub-challenge top-<lb/>performer (Q.E.D) actually used the same modelling approach as the baseline model, 12 yet <lb/>showing markedly better performance (Fig. 3F), indicating that careful feature selection, method <lb/>implementation, or other domain knowledge, could result in marked performance improvement. <lb/>A number of other models also outperformed the baseline model in Round 2 (Suppl. Fig. 7). <lb/>To get a more global picture, at the end of the Challenge we asked all the teams to fill in survey <lb/>questionnaires to explore whether there would be any broad method classes or chemical or <lb/>target features shared among the models. Among the 31 teams that answered the surveys, <lb/>none of the method classes had a very strong contribution to the accuracy (Suppl. Fig. 11), <lb/>similarly as has been seen also in other DREAM challenges. 17-19 A rather surprising observation <lb/>from the survey was that the K d prediction accuracies could be somewhat improved by using <lb/>also other types of multi-dose bioactivity data (e.g. K i, IC 50 , EC 50 ), compared to using K d data <lb/>alone (Suppl. Fig. 11). This provides a further opportunity for ML models that often require <lb/>relatively large training datasets, as these bioactivity types are among the most common ones <lb/>so far used in multi-dose target profiling, and more common than K d in DTC database (Suppl. <lb/>Fig. 11G). Another observation was that the teams that used DTC alone as training bioactivity <lb/>data source tended to have somewhat decreased predictive accuracy, perhaps because of more <lb/>heterogeneous bioactivity data stored in DTC, compared to BindingDB 7 or ChEMBL. 6 This <lb/>suggests that further annotation and harmonization of the various types and sources of <lb/>bioactivity data will be needed to make the most of these data for predictive modelling, ideally in <lb/>the form of a crowdsourced community effort. <lb/>Many previous DREAM Challenges have demonstrated that &apos;wisdom of the crowds&apos; may also <lb/>improve the predictive power of the individual models through combining the models as <lb/>meta-predictors or ensemble models. 17-19 The ensemble model constructed in this Challenge <lb/>based on the Round 2 submissions showed that the critical point came rather quickly after which <lb/>adding more models led to rather rapid decrease in accuracy (Fig. 4D). This suggests that in <lb/>drug-target interaction prediction a better strategy may be to use merely the &apos;wisdom of the best <lb/>teams&apos;. The combination of the top-performing ML models improved both the sensitivity and <lb/>specificity, compared to single-dose target activity assays, without requiring any additional <lb/>experiments (Fig. 5). None of the top-performing models used single-dose inhibition assay data, <lb/>and we showed how by combining the inhibition measurements with ML models, one can reach <lb/>higher prediction accuracy than using either one alone, while identifying an increased number of <lb/>potent compound-kinase activities than when using the standard 80% inhibition cut-off (Fig. 6). <lb/>
			The best-performing models were not dependent on the number or type of available bioactivity <lb/>data, provided the training data have sufficient structural diversity for the kinase families being <lb/>predicted. Subsequent experiments carried out based on the the top-performing model <lb/>predictions demonstrated that these models can guide the experimental mapping efforts, 25 <lb/>especially for so far under-studied kinases (Fig. 6B,C). <lb/>To enable the community to apply the ML models benchmarked in the Challenge to various drug <lb/>development applications, we have made available the top-performing prediction models as <lb/>containerized source code. Such Docker models enable continuous validation of the model <lb/>predictions whenever new experimental kinase profiling data will become available, as well as <lb/>enable the researchers to run the best performing models on private data that would otherwise <lb/>remain closed and unavailable to the research community. 20 This Challenge will, therefore, <lb/>contribute to the further development and benchmarking of the current and future target activity <lb/>prediction models on a much larger scale and for various precision medicine applications. For <lb/>instance, for the prediction of selective inhibitors for new kinase targets, or off-target potency <lb/>predictions for new investigational compounds. All the prediction models, new bioactivity data, <lb/>and benchmarking infrastructure are openly available either on Synapse <lb/>(www.doi.org/10.7303/syn15667962) or via DTC open-data platform <lb/>(https://drugtargetcommons.fimm.fi/). We envision that the IDG-DREAM Challenge will provide a <lb/>continuously-updated resource for the chemical biology community to prioritize and <lb/>experimentally test new target activities toward accelerating many drug discovery and <lb/>repurposing applications. <lb/>Online Methods <lb/>Challenge infrastructure and timeline <lb/>The Challenge was organized and run on the collaborative science platform Synapse. All <lb/>prediction files were submitted using the Challenge feature of this platform to track which teams <lb/>and individuals submitted files, and to track the number of submissions per team. Challenge <lb/>infrastructure scripts including code for calculating the scoring metrics are available at <lb/>https://github.com/Sage-Bionetworks/IDG-DREAM-Drug-Kinase-Challenge . Teams were <lb/>permitted to submit three predictions for Round 1, and two predictions for Round 2 (Suppl. Fig. <lb/>3). For Rounds 1 and 2, we used a common workflow language-based challenge infrastructure <lb/>to perform the following tasks: (1) validate a prediction file to ensure that it conformed to the <lb/>correct file structure and had numeric pK d predictions and return an error email to participants if <lb/>invalid, (2) run a python script to calculate the performance metrics for a submitted prediction, <lb/>and (3) return the score to the Synapse platform. For Round 1b, in which we permitted 1 <lb/>submission a day for 60 days, we implemented a modified Ladderboot 21 protocol to prevent <lb/>model overfitting. This was done by modifying step (2) above as follows: the scoring <lb/>infrastructure receive a submitted prediction, check for a previous submission from the same <lb/>team, and run an R script to bootstrap the current and previous submission 10000 times, <lb/>calculate a Bayes factor (K) between the two submissions; the scoring harness would then only <lb/>return an updated score if it was substantially better (K &gt; 3) than the previous submission. <lb/>Bioactivity data for model testing <lb/>To generate unpublished test bioactivity data for scoring of predictions, we sent kinase inhibitors <lb/>to DiscoverX (Eurofins Corporation) for the generation of new dose-response dissociation <lb/>constant (K d ) values, as a measure of a binding affinity. In order to give a better sense of the <lb/>relative compound potencies, K d is represented in the logarithmic scale, as pK d = -log 10 (K d ), <lb/>where K d is given in molars [M]. The higher the pK d value, the higher the inhibitory ability of a <lb/>compound against a protein kinase. The 105 inhibitors used in the Challenge (70 for Round 1 <lb/>and 25 for Round 2) were a part of the kinase inhibitor collection at the SGC-UNC for which we <lb/>already had the single-dose inhibition screening done at DiscoverX across their large kinase <lb/>panel. This scan Max SM data (also called KINOMEScan) was collected at a screening <lb/>concentration of 1 µM. A two-step screening approach was adopted, similar to the previous <lb/>studies 3-5,5 , <lb/>using <lb/>the <lb/>DiscoverX <lb/>KINOME scan <lb/>standard <lb/>protocol <lb/>( https://www.discoverx.com/services/drug-discovery-development-services/kinase-profiling/kino <lb/>mescan ). The dose-response K d values were generated for a range of compound-kinase pairs <lb/>that had inhibition&gt;80% in the single-dose assay. The compounds were supplied as 10 mM <lb/>
			stocks in DMSO, and the top screening concentration was 10 mM. <lb/>25 of the axitinib-kinase pairs generated for Round 2 were already profiled in previous published <lb/>studies, 5,11 and were therefore excluded from the Round 2 dataset. The Spearman correlation <lb/>between these newly-measured pK d &apos;s and those available from DTC was 0.701 (Suppl. Fig. <lb/>12A), providing the experimental consistency of the K d measurements for axitinib. We note this <lb/>25 pK d &apos;s is a rather limited set for such analysis of consistency, and therefore we extracted a <lb/>larger set of 416 K d values that overlapped with the Round 2 kinases from two comprehensive <lb/>target profiling studies 3,4 . including 104 pairs where pK d = 5 in both of the studies. The Spearman <lb/>correlation of these replicate pK d measurements wa s 0.842 (Suppl. Fig. 12B), demonstrating a <lb/>good reproducibility of the pK d measurements. These replicate measurements were also used <lb/>when determining a practical upper limit for the predictive accuracy of the machine learning <lb/>models in the scoring of their predictions (see below). <lb/>To subsequently test the top-performing model predictions in additional compound-kinase pairs <lb/>that were not part of Round 1 or 2 datasets, we selected a set of 88 pairs that showed most <lb/>potency based on the average predicted pK d of the top-performing models (Q.E.D., DMIS-DK <lb/>and AIWIC), regardless of their single-dose inhibition levels. These 88 pairs were scattered <lb/>across the whole spectrum of single-dose inhibition levels, ranging from 0% to 78% <lb/>(Supplementary Fig. 10; note: pairs with inhibition &gt;80% were K d -profiled already in Round 2). <lb/>One of the compounds (TPKI-35) was not available from IDG, so the predicted 7 kinase targets <lb/>for that compound could not be tested experimentally, resulting in a dataset of total of 81 <lb/>compound-kinase pairs that were shipped to DiscoverX for multi-dose Kd profiling. One of the <lb/>compounds (GW819776) was shipped separately in a tube, whereas the other 14 compounds <lb/>were supplied as 10 mM stocks in DMSO, and the Kd profiling done was done using the same <lb/>KINOMEscan competitive binding assay protocol as for the Round 1 and Round 2 pairs. <lb/>Scoring of the model predictions <lb/>We used the following six metrics to score the predictions from the participants: <lb/>• Root-mean-square error (RMSE): square root of the average squared difference <lb/>between the predicted pK d and measured pK d , to score continuous activity predictions. <lb/>• Pearson correlation: Pearson correlation coefficient between the predicted and <lb/>measured pK d &apos;s, which quantifies the linear relationship between the activity values. <lb/>• Spearman correlation: Spearman&apos;s rank correlation coefficient between the predicted <lb/>and measured pK d &apos;s, which quantifies the ability to rank pairs in correct order. <lb/>• Concordance index (CI) 22 : probability that the predictions for two randomly drawn <lb/>compound-kinase pairs with different pK d values are in the correct order. <lb/>• F1 score: the harmonic mean of the precision and recall metrics. Interactions were <lb/>binarized by their pK d values into positive class (pK d &gt; 7) and negative class (pK d ≤ 7). <lb/>• Average AUC: average area under ten receiver operating characteristic (ROC) curves <lb/>generated using ten interaction threshold values from the pK d interval [6, 8] to binarize <lb/>pK d &apos;s into true class labels. <lb/>The submissions in Round 1 were scored across the six metrics but the teams remained <lb/>unranked. The Round 2 consisted of two sub-challenges, the top-performers of which were <lb/>determined based on RMSE and Spearman correlation, respectively. Spearman correlation <lb/>evaluated the prediction in terms of accuracy at ranking of the compound-kinase pairs based on <lb/>the measured K d values, whereas RMSE considers the absolute errors in the quantitative <lb/>binding affinity profiles. The tie-breaking metric for both Rounds was averaged area under the <lb/>curve (AUC) metric that evaluated the accuracy of the models to classify the pK d values into <lb/>active and inactive classes based on multiple K d thresholds. <lb/>Statistical evaluation of the predictions <lb/>
			Determination of the top-performers was made by calculation of a Bayes factor relative to the <lb/>top-ranked submission in each category. Briefly, we bootstrapped all submissions (10000 <lb/>iterations of sampling with replacement), and calculated RMSE and Spearman correlation to the <lb/>test dataset to generate a distribution of scores for each submission. A Bayes factor was then <lb/>calculated using the challengescoring R package <lb/>( https://github.com/sage-bionetworks/challengescoring ) for each submission relative to the top <lb/>submission in each subchallenge. Submissions with a Bayes factor &lt;= 3 relative to the top <lb/>submission were considered to be tied as top-performers. Tie breaking for both subchallenges <lb/>was performed by identifying submission with the highest absolute average AUC. <lb/>To create a distribution of random predictions, we randomly shuffled the 430/394 Kd values <lb/>across the set of 430/394 compound-kinase pairs in the Round 1/Round 2 datasets, and <lb/>repeated the permutation procedure 10000 times. Then we compared the actual Round <lb/>1/Round 2 prediction scores to Spearman and RMSE calculated from the permuted Kd data. We <lb/>defined a prediction as better than random if its score was higher than the maximum of the <lb/>10000 random predictions (empirical p=0.0, permutation test). <lb/>To determine the maximum possible performance practically achievable by any computational <lb/>models, we utilized replicate K d <lb/>measurements from distinct studies that a pplied a similar <lb/>biochemical assay protocol. We used the DrugTargetCommons to retrieve 863 and 835 <lb/>replicated K d values for kinases or compounds that overlapped with the Round 1 and 2 datasets, <lb/>respectively. These data originated from two comprehensive screening studies 3,4 . To better <lb/>represent the distribution of pK d values in the test data, we subset the DTC data to contain 35% <lb/>(Round 1) and 25% (Round 2) pK d =5 values, approximately matching the proportion of pK d = 5 <lb/>values in R1 and R2 test sets. For Round 1, we used 317 replicated Kds, including 111 <lb/>randomly selected pairs where pK d = 5. For Round 2, we used 416 replicated Kds, including 104 <lb/>randomly selected pairs where pK d = 5. We randomly sampled the replicate measurements of <lb/>these compound-kinase pairs (with replacement), calculated the Spearman correlation and <lb/>RMSE between the Davis and Fabian pKd&apos;s for each 430 and 394 sub-sampled sets for Round <lb/>1 and 2, respectively, and re peated this procedure for a total of 10000 samplings. <lb/>Baseline model <lb/>We used a recently-published and experimentally-validated kernel regression framework as a <lb/>baseline model for compound-kinase binding affinity prediction 12 . Our training dataset consisted <lb/>of 44186 pK d values (between 1968 compounds and 423 human kinases) extracted from DTC. <lb/>Median was taken if multiple pK d measurements were available for the same compound-kinase <lb/>pair. We constructed protein kinase kernel using normalized Smith-Waterman alignment scores <lb/>between full amino acid sequences, and four Tanimoto compound kernels based on the <lb/>following fingerprints implemented in rcdk R package 23 : (i) 881-bit fingerprint defined by <lb/>PubChem ( pubchem ), (ii) path-based 1024-bit fingerprint ( standard ), (iii) 1024-bit fingerprint <lb/>based on the shortest paths between atoms taking into account ring systems and charges <lb/>( shortestpath ), and (iv) extended connectivity 1024-bit fingerprint with a maximum diameter set <lb/>to 6 (ECFP6; circular ). We used CGKronRLS as a learning algorithm 24 (implementation available <lb/>at https://github.com/aatapa/RLScore ). We conducted a nested cross-validation in order to <lb/>evaluate the generalisation performance of CGKronRLS with each pair of kinase and compound <lb/>kernels as well as to tune the regularisation hyperparameter of the model. In particular, since the <lb/>majority of the compounds from the Challenge test datasets had no bioactivity data available in <lb/>the public domain, we implemented a nested leave-compound-out cross-validation to resemble <lb/>the setting of the Challenge as closely as possible. The model comprising of protein kernel <lb/>coupled with compound kernel built upon path-based fingerprint ( standard ) achieved the highest <lb/>predictive performance on the training dataset (as measured by RMSE), and therefore it was <lb/>used as a baseline model for compound-kinase binding affinity prediction in both Challenge <lb/>Rounds. <lb/>Top-performing models <lb/>Supplementary write ups provide details of all qualified models submitted to the Challenge <lb/>( http://www.doi.org/10.7303/syn21445941.1 ). The key components of the top-performing models <lb/>are listed in Table 1 and summarized below. <lb/>Team Q.E.D model <lb/>To enable a fine-grained discrimination of binding affinities between similar targets ( e.g. , kinase <lb/>family members), the team Q.E.D explicitly introduced similarity matrices of compounds and <lb/>targets as input features into their regression model. The regression model was implemented as <lb/>an ensemble version (uniformly averaged predictor) of 440 CGKronRLS regressors 24,25 , but with <lb/>different choices of regularization strengths [0.1, 0.5, 1.0, 1.5, 2.0], training epochs [400, 410, <lb/>…, 500], and similarity matrices: the protein similarity matrix was derived based on the <lb/>normalized striped Smith-Waterman alignment scores 26 between full protein sequences <lb/>( https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library ). Eight different <lb/>alternatives of compound similarity matrices were computed using both Tanimoto and Dice <lb/>similarity metrics for different variants of 1024-bit Morgan fingerprints 27 (&apos;radius&apos; [2, 3] and <lb/>&apos;useChirality&apos; [True, False], implementation available at https://github.com/rdkit/rdkit ). Unlike the <lb/>baseline method, which used only the available pK d values from DTC for training, the team <lb/>Q.E.D model extracted 16945 pK d , 53894 pK i and 3301 pEC 50 values from DTC. After merging <lb/>the same compound-kinase pairs from different studies by computing their medians, 60462 <lb/>affinity values between 13608 compounds and 527 kinases were used as the training data. <lb/>Team DMIS_DK model <lb/>Team DMIS_DK built a multi-task Graph Convolutional Network (GCN) model based on 953521 <lb/>bioactivity values between 474875 compounds and 1474 proteins extracted from DTC and <lb/>BindingDB. Three types of bioactivities were considered, that is, pK d , pK i , and pIC 50 . Median was <lb/>computed if multiple bioactivities were present for the same compound-protein pair. Multi-task <lb/>GCN model was designed to take compound SMILES strings as an input, which were then <lb/>converted to molecular graphs using RDKit python library (http://www.rdkit.org). Each node (i.e. <lb/>atom) in a molecular graph was represented by a 78-dimensional feature vector, including the <lb/>information of atom symbol, implicit valence, aromaticity, number of bonded neighbors in the <lb/>graph, and hydrogen count. No protein descriptors were utilized. The final model was an <lb/>ensemble of four multi-task GCN architectures described in the Supplementary writeups <lb/>( http://www.doi.org/10.7303/syn21445941.1 ). For the Challenge submission, the binding affinity <lb/>predictions from the last K epochs were averaged, and then the average was taken over the 12 <lb/>multi-task GCN models (four different architectures with three different weight initializations). <lb/>Hyper-parameters of the multi-task GCN models were selected based on the performance on a <lb/>hold-out set extracted from the training data. The GCN models were implemented using <lb/>PyTorch Geometric (PyG) library 28 . <lb/>Team AI Winter is Coming model <lb/>Team AI Winter is Coming built their prediction model using Gradient Boosted Decision Trees <lb/>(GBDT) implemented in XGBoost algorithm 29 . Training dataset included 600000 pK d , pK i , pIC 50 , <lb/>and pEC 50 values extracted from DTC and ChEMBL (version 25), considering only <lb/>compound-protein pairs with ChEMBL confidence score of 6 or greater for &apos;binding&apos; or <lb/>&apos;functional&apos; human kinase protein assays. For a given protein target, replicate compounds with <lb/>different bioactivities in a given assay (differences larger than one unit on a log scale) were <lb/>excluded. For similar replicate measurements, a single representative assay value was selected <lb/>for inclusion in the training dataset. Each compound was characterized by a 16000-dimensional <lb/>feature vector being a concatenation of four ECFP fingerprints with a diameter set to of 5, 7, 9, <lb/>and 11. No protein descriptors were used in the XGBoost algorithm. A separate model for each <lb/>protein target was trained using nested cross-validation (CV), where inner loops were used to <lb/>perform hyperparameter optimisation and recursive feature elimination. The final binding affinity <lb/>prediction was calculated as an average of the predictions from the cross-validated models <lb/>based on five outer CV loops. <lb/>Mean ensemble model construction <lb/>
			Ensemble models were generated by combining the best-scoring Round 2 predictions from each <lb/>team. We iteratively combined models starting from the highest scoring Round 2 prediction (e.g. <lb/>ensemble #1 -highest scoring prediction, ensemble #2 -2 highest scoring, ensemble #3 -3 <lb/>highest scoring, and so on) for all 54 Round 2 submissions. Three types of ensembles were <lb/>created using arithmetic mean, median, and rank-weighted summarization approaches. The <lb/>rank-weighted ensemble was calculated by multiplying each set of predictions by the total <lb/>number of submissions plus 1 minus the rank of the prediction file, summing these weighted <lb/>predictions, and then dividing by the sum of the multiplication factors. The 54 ensemble <lb/>predictions for each of the 3 summary metrics were bootstrapped and Bayes factors were <lb/>calculated as previously described to determine which models were substantially different than <lb/>the top ranked submission. <lb/>Estimating the expected inhibition levels <lb/>The KINOMEscan assay protocol utilized for both the single-dose and dose-response assays is <lb/>based on competitive binding assays, where the maximum compound concentration tested was <lb/>1000 nM in both of the assays. For a given compound-kinase pair, the K d values calculated from <lb/>the dose-response assay were then used to estimate the expected single-dose %inhibition level <lb/>(at 1000 nM of compound) using the conventional ligand occupancy formula: <lb/>ligand occupancy/expected %inhibition <lb/>% <lb/>= <lb/>M aximum ligand concentration <lb/>M aximum ligand concentration (M ) + Estimated Kd (M ) <lb/>where <lb/>aximum ligand concentration <lb/>1e <lb/>6 M <lb/>M <lb/>= <lb/>− 0 <lb/>The expected %inhibition values based various measured pK d levels are shown in the table: <lb/>Measured pK d <lb/>Measured K d [M] <lb/>Expected inhibition [%] <lb/>3 <lb/>1e -03 <lb/>0 <lb/>4 <lb/>1e -04 <lb/>1 <lb/>5 <lb/>1e -05 <lb/>10 <lb/>6 <lb/>1e -06 <lb/>50 <lb/>7 <lb/>1e -07 <lb/>91 <lb/>8 <lb/>1e -08 <lb/>99 <lb/>9 <lb/>1e -09 <lb/>100 <lb/>Activity classification analyses <lb/>Standard confusion matrix was constructed using the measured pK d values to define the true <lb/>positive and true negative classes for the 394 pairs in Round2, using either pK d &gt; 6 and pK d &gt; 7 <lb/>for indicating true positive activity, and the predicted positive and negative classes for these <lb/>pairs were defined based on either the single-dose activity measurement, using inhibition cut-off <lb/>of 80%, 5,11,13 or the Q.E.D model-predicted pK d values, using the same activity thresholds as with <lb/>the measured pK d values (i.e., either pK d = 6 or pK d = 7). Positive predictive value (PPV) and <lb/>false discovery rate (FDR) were calculated as classification performance scores. <lb/>To carry out a more systematic analysis of prediction accuracies, the 394 pairs in Round 2 were <lb/>ranked both using Q.E.D model-predicted pK d values and the measured single-dose %inhibition <lb/>values, and then these rankings were compared against the ground-truth activity classification <lb/>based on the dose-response measurements (using both pK d &gt; 6 and pK d &gt; 7 for indicating true <lb/>positive activity). The results were visualized using both receiver operating characteristic (ROC) <lb/>and precision-recall (PR) curves, implemented in pROC and pRROC R-packages, <lb/>respectively 30,31 . The area under the ROC and PR curves was calculated as summary <lb/>classification performance. <lb/></body>

			<div type="availability">Data and code availability <lb/>The Challenge test data will be made available at DTC (https://drugtargetcommons.fimm.fi/). <lb/>The Docker containers of the best-performing teams are available on the Synapse project for <lb/>this Challenge ( www.doi.org/10.7303/syn15667962 ). The codes for reproducing the results and <lb/>figures <lb/>are <lb/>available <lb/>at <lb/>GitHub <lb/>( https://github.com/Sage-Bionetworks/IDG-DREAM-Challenge-Analysis/ ). Key R packages used <lb/>for this work beyond those mentioned elsewhere in this section include tidyverse 32 and <lb/>synapser 33 ; all packages used and their versions can be found in the renv lockfile in the <lb/>previously mentioned GitHub repository. <lb/></div>

			<div type="acknowledgement">Acknowledgements <lb/>The authors thank the IDG Kinase Data and Resource Generation Center for providing the <lb/>unpublished target activity data for the Challenge Rounds 1 and 2, Zia Rehman and Olle <lb/>Hansson (University of Helsinki, FIMM) for technical assistance with DrugTargetCommons, <lb/>Tianduanyi Wang (FIMM) for his help with the baseline submissions, Anna Goldenberg <lb/>(University of Toronto, Canada) and Chloe-Agathe Azencott (Institut Curie, France) for <lb/>organizing the DREAM Idea Challenge, and Barbara Rieck and Ladan Naghavian for the <lb/>bioactivity profiling at DiscoverX (Eurofins Corporation). <lb/></div>

			<div type="funding">Funding: <lb/>TA: Academy of Finland (grants 310507, 313267), the Cancer Society of Finland, the Sigrid <lb/>Jusélius Foundation. CW, TW, DD: National Institutes of Health (1U24DK116204-01). The SGC <lb/>is a registered charity that receives funds from AbbVie, Bayer Pharma AG, Boehringer <lb/>Ingelheim, Canada Foundation for Innovation, Eshelman Institute for Innovation, Genome <lb/>Canada, Innovative Medicines Initiative (ULTRA-DD 115766), Wellcome Trust, Janssen, Merck <lb/>Kga, Merck Sharp &amp; Dohme, Novartis Pharma AG, Ontario Ministry of Economic Development <lb/>and Innovation, Pfizer, São Paulo Research Foundation-FAPESP, and Takeda. <lb/></div>

			<div type="annex">IDG-DREAM Drug Kinase Binding Prediction Challenge Consortium members: <lb/>User oselot: Mehmet Tan; Team N121: Chih-Han Huang, Edward S. C. Shih, Tsai-Min Chen, <lb/>Chih-Hsun Wu, Wei-Quan Fang, Jhih-Yu Chen, and Ming-Jing Hwang; Team Let_Data_Talk: <lb/>Xiaokang Wang, Marouen Ben Guebila, Behrouz Shamsaei, Sourav Singh; User thinng: Thin <lb/>Nguyen; Team KKT: Mostafa Karimi, Di Wu, Zhangyang Wang, Yang Shen, Team Boun: <lb/>Hakime Öztürk, Elif Ozkirimli, and Arzucan Özgür; Team Aydin: Zafer Aydin, Halil Ibrahim <lb/>Bilgin; Team KinaseHunter: Hansaim Lim, Lei Xie; Team AmsterdamUMC-KU-team: Georgi <lb/>K. Kanev, Albert J. Kooistra, Bart A. Westerman; Team DruginaseLearning: Panagiotis <lb/>Terzopoulos, Konstantinos Ntagiantas, Christos Fotis, Leonidas Alexopoulos; Team <lb/>KERMIT-LAB -Ghent University: Dimitri Boeckaerts, Michiel Stock, Bernard De Baets, Yves <lb/>Briers; Team QED: Fangping Wan, Shuya Li, Yunan Luo, Hailin Hu, Jian Peng, Jianyang Zeng; <lb/>Team METU_EMBLEBI_CROssBAR: Tunca Dogan, Ahmet S. Rifaioglu, Heval Atas, Rengul <lb/>Cetin Atalay, Volkan Atalay, Maria J. Martin; Team DMIS_DK: Sungjoon Park, Minji Jeon, <lb/>Sunkyu Kim, Junhyun Lee, Seongjun Yun, Bumsoo Kim, Buru Chang, Jaewoo Kang; Team AI <lb/>Winter is Coming: Mariya Popova, Stephen Capuzzi, Olexandr Isayev; Team hulab: Bence <lb/>Szalai, Gabor Turu, Ádám Misák, László Hunyady; Team ML-Med: Matthias Lienhard, Paul <lb/>Prasse, Ivo Bachmann, Julia Ganzlin, Gal Barel, and Ralf Herwig, Team Prospectors: Davor <lb/>Oršolić, Bono Lučić, Višnja Stepanić, Tomislav Šmuc <lb/></div>

			<listBibl>References <lb/>1. Arrowsmith, C. H. et al. The promise and peril of chemical probes. Nat. Chem. Biol. 11 , <lb/>536-541 (2015). <lb/>2. Santos, R. et al. A comprehensive map of molecular drug targets. Nat. Rev. Drug Discov. <lb/>16 , <lb/>19-34 (2017). <lb/>3. Fabian, M. A. et al. A small molecule-kinase interaction map for clinical kinase inhibitors. Nat. <lb/>Biotechnol. 23 , 329-336 (2005). <lb/>4. Davis, M. I. et al. Comprehensive analysis of kinase inhibitor selectivity. Nat. Biotechnol. <lb/>29 , <lb/>1046-1051 (2011). <lb/>5. Elkins, J. M. et al. Comprehensive characterization of the Published Kinase Inhibitor Set. Nat. <lb/>Biotechnol. 34 , 95-103 (2016). <lb/>6. Mendez, D. et al. ChEMBL: towards direct deposition of bioassay data. Nucleic Acids Res. <lb/>47 , D930-D940 (2019). <lb/>7. Gilson, M. K. et al. BindingDB in 2015: A public database for medicinal chemistry, <lb/>computational chemistry and systems pharmacology. Nucleic Acids Res. 44 , D1045-1053 <lb/>(2016). <lb/>8. Oprea, T. I. et al. Unexplored therapeutic opportunities in the human genome. Nat. Rev. Drug <lb/>Discov. 17 , 317-332 (2018). <lb/>9. Tang, J. et al. Drug Target Commons: A Community Effort to Build a Consensus Knowledge <lb/>Base for Drug-Target Interactions. Cell Chem. Biol. 25 , 224-229.e2 (2018). <lb/>10. <lb/>Omberg, L. et al. Enabling transparent and collaborative computational analysis of 12 <lb/>tumor types within The Cancer Genome Atlas. Nat. Genet. <lb/>45 , 1121-1126 (2013). <lb/>11. <lb/>Drewry, D. H. et al. Progress towards a public chemogenomic set for protein kinases and <lb/>a call for contributions. PLOS ONE 12 , e0181585 (2017). <lb/>12. <lb/>Cichonska, A. et al. Computational-experimental approach to drug-target interaction <lb/>mapping: A case study on kinase inhibitors. PLOS Comput. Biol. 13 , e1005678 (2017). <lb/>13. <lb/>Wells, C. I., Kapadia, N. R., Couñago, R. M. &amp; Drewry, D. H. In depth analysis of kinase <lb/>cross screening data to identify chemical starting points for inhibition of the Nek family of <lb/>kinases. MedChemComm 9 , 44-66 (2018). <lb/>14. <lb/>Ertl, P. Cheminformatics Analysis of Organic Substituents: Identification of the Most <lb/>Common Substituents, Calculation of Substituent Properties, and Automatic Identification of <lb/>Drug-like Bioisosteric Groups. J. Chem. Inf. Comput. Sci. 43 , 374-380 (2003). <lb/>15. <lb/>Reymond, J.-L. &amp; Awale, M. Exploring Chemical Space for Drug Discovery Using the <lb/>Chemical Universe Database. ACS Chem. Neurosci. <lb/>3 , 649-657 (2012). <lb/>16. <lb/>Vamathevan, J. et al. Applications of machine learning in drug discovery and <lb/>development. Nat. Rev. Drug Discov. <lb/>18 , 463-477 (2019). <lb/>17. <lb/>Marbach, D. et al. Wisdom of crowds for robust gene network inference. Nat. Methods <lb/>9 , <lb/>796-804 (2012). <lb/>18. <lb/>Eduati, F. et al. Prediction of human population responses to toxic compounds by a <lb/>collaborative competition. Nat. Biotechnol. 33 , 933-940 (2015). <lb/>19. <lb/>Saez-Rodriguez, J. et al. Crowdsourcing biomedical research: leveraging communities <lb/>as innovation engines. Nat. Rev. Genet. <lb/>17 , 470-486 (2016). <lb/>20. <lb/>Guinney, J. &amp; Saez-Rodriguez, J. Alternative models for sharing confidential biomedical <lb/>data. Nat. Biotechnol. 36 , 391-392 (2018). <lb/>21. <lb/>Neto, E. C. et al. Reducing overfitting in challenge-based competitions. ArXiv Prepr. <lb/>ArXiv160700091 (2016). <lb/>22. <lb/>Pahikkala, T. et al. Toward more realistic drug-target interaction predictions. Brief. <lb/>Bioinform. 16 , 325-337 (2015). <lb/>23. <lb/>Guha, R. Chemical Informatics Functionality in R. J. Stat. Softw. <lb/>18 , 1-16 (2007). <lb/>24. <lb/>Airola, A. &amp; Pahikkala, T. Fast Kronecker Product Kernel Methods via Generalized Vec <lb/>Trick. IEEE Trans. Neural Netw. Learn. Syst. <lb/>29 , 3374-3387 (2018). <lb/>25. <lb/>Pahikkala, T. &amp; Airola, A. RLScore: Regularized Least-Squares Learners. J. Mach. <lb/>Learn. Res. 17 , 1-5 (2016). <lb/>26. <lb/>Zhao, M., Lee, W.-P., Garrison, E. P. &amp; Marth, G. T. SSW library: an SIMD <lb/>Smith-Waterman C/C++ library for use in genomic applications. PloS One 8 , e82138 (2013). <lb/>27. <lb/>Rogers, D. &amp; Hahn, M. Extended-Connectivity Fingerprints. J. Chem. Inf. Model. <lb/>50 , <lb/>742-754 (2010). <lb/>28. <lb/>Fey, M. &amp; Lenssen, J. E. Fast Graph Representation Learning with PyTorch Geometric. <lb/>ArXiv190302428 Cs Stat (2019). <lb/>29. <lb/>Chen, T. &amp; Guestrin, C. XGBoost: A Scalable Tree Boosting System. in Proceedings of <lb/>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining <lb/>785-794 (ACM, 2016). doi:10.1145/2939672.2939785. <lb/>30. <lb/>Robin, X. et al. pROC: an open-source package for R and S+ to analyze and compare <lb/>ROC curves. BMC Bioinformatics <lb/>12 , 77 (2011). <lb/>31. <lb/>Grau, J., Grosse, I. &amp; Keilwagen, J. PRROC: computing and visualizing precision-recall <lb/>and receiver operating characteristic curves in R. Bioinforma. Oxf. Engl. <lb/>31 , 2595-2597 <lb/>(2015). <lb/>32. <lb/>Wickham, H. et al. Welcome to the Tidyverse. J. Open Source Softw. <lb/>4 , 1686 (2019). <lb/>33. <lb/>Hoff, B. &amp; Ladia, K. synapser: R language bindings for Synapse API . (2019). </listBibl>


	</text>
</tei>
