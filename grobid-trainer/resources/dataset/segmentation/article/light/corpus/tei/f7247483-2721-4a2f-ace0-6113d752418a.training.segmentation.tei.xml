<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>How True is GPT-2? <lb/>An Empirical Analysis of Intersectional Occupational Biases <lb/>Hannah Kirk * 1 2 Yennie Jun * 1 2 Haider Iqbal * 1 3 Elias Benussi * 1 3 Filippo Volpin * 1 4 Frédéric A. Dreyer * 1 5 <lb/>Aleksandar Shtedritski 1 6 Yuki M. Asano 1 6 <lb/>Abstract <lb/>The capabilities of natural language models <lb/>trained on large-scale data have increased im-<lb/>mensely over the past few years. Downstream <lb/>applications are at risk of inheriting biases con-<lb/>tained in these models, with potential negative <lb/>consequences especially for marginalized groups. <lb/>In this paper, we analyze the occupational biases <lb/>of a popular generative language model, GPT-2, <lb/>intersecting gender with five protected categories: <lb/>religion, sexuality, ethnicity, political affiliation, <lb/>and name origin. Using a novel data collection <lb/>pipeline we collect 396k sentence completions <lb/>of GPT-2 and find: (i) The machine-predicted <lb/>jobs are less diverse and more stereotypical for <lb/>women than for men, especially for intersections; <lb/>(ii) Fitting 262 logistic models shows intersec-<lb/>tional interactions to be highly relevant for occu-<lb/>pational associations; (iii) For a given job, GPT-2 <lb/>reflects the societal skew of gender and ethnicity <lb/>in the US, and in some cases, pulls the distribu-<lb/>tion towards gender parity, raising the normative <lb/>question of what language models should learn. <lb/>Code is available at https://github.com/ <lb/>oxai/intersectional_gpt2. <lb/></front>

			<body>1. Introduction <lb/>The advent of deep learning and massive growth in training <lb/>data have led to natural language models surpassing humans <lb/>on numerous benchmarks (Wang et al., 2018; 2019; He et al., <lb/>2020; Adiwardana et al., 2020). However, as Bender et al. <lb/>(2021) states, these models can exacerbate existing biases <lb/>in data and perpetuate stereotypical associations to the harm <lb/>of marginalized communities. Simultaneously, pre-trained <lb/>models have become readily accessible via APIs, allowing <lb/>non-experts to apply these tools in their own applications. <lb/></body>

			<front>* Equal contribution 1 Oxford Artificial Intelligence Society <lb/>2 Oxford Internet Institute, 3 Dept. of Computer Science, 4 Dept. <lb/>of Economics, 5 Dept. of Physics, 6 Dept. of Engineering Science, <lb/>University of Oxford, Oxford, United Kingdom. Correspondence <lb/>to: Hannah Kirk &lt;hannah.kirk@oii.ox.ac.uk&gt;. <lb/> Preprint. <lb/> </front>

			<body>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Share of Women (GPT-2 pred.) <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Share of Women (US 2019 data) <lb/>babysitter <lb/>bus driver <lb/>carpenter <lb/>chef <lb/>cook <lb/>laborer <lb/>maid <lb/>mechanic <lb/>model <lb/>nurse <lb/>photographer <lb/>plumber <lb/>receptionist <lb/>secretary <lb/>security guard <lb/>social worker <lb/>teacher <lb/>truck driver <lb/>Woman <lb/>[0.0238] <lb/>+Asian <lb/>[0.0004] <lb/>+Black <lb/>[0.0013] <lb/>+Hispanic <lb/>[0.0049] <lb/>+White <lb/>[0.0182] <lb/>Figure 1. GPT-2 Monte-Carlo prediction vs ground truth US <lb/>population share. GPT-2&apos;s predictions with regards to intersec-<lb/>tional characteristics are highly stereotypical -yet they are closely <lb/>aligned to the US population data. We show the predicted values <lb/>for gender intersected with ethnicity along with the [Mean-Squared <lb/>Errors] and annotate example jobs for the gender-only predictions. <lb/>These developments in generative language models substan-<lb/>tiate a need to understand the potential for biases towards <lb/>protected classes, such as gender and ethinicity. Within the <lb/>specific context of fairness in AI-assisted hiring, we focus <lb/>on analyzing one popular, publicly available language model <lb/>(GPT-2). By generating 396k samples, we empirically an-<lb/>alyze which occupations GPT-2 preferentially associates <lb/>with intersections of gender and protected classes. The <lb/>paper provides the following contributions: <lb/>• a detailed data collection protocol for studying inter-<lb/>sectional biases of generative language models, <lb/>• the analysis of the intersectional biases of (gender × <lb/>[ethnicity, religion, sexuality, political affiliation, name <lb/>origin]) present in GPT-2, <lb/>• a comparison of GPT-2&apos;s predictions with real-world <lb/>occupation frequencies, as shown in Fig. 1. <lb/></body>

			<front>arXiv:2102.04130v1 [cs.CL] 8 Feb 2021 <lb/></front>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<body>Asian <lb/>Black <lb/>Hispanic <lb/>White <lb/>Ethnicity <lb/>Buddhist <lb/>Christian <lb/>Hindu <lb/>Jewish <lb/>Muslim <lb/>Religion <lb/>Gay/Lesbian <lb/>Straight <lb/>Sexuality <lb/>Conservative <lb/>Liberal <lb/>Political <lb/>Africa <lb/>Americas <lb/>Asia <lb/>Europe <lb/>Oceania <lb/>Continent <lb/>X <lb/>Y <lb/>Man <lb/>Woman <lb/>Base <lb/>GPT-2 <lb/>NER <lb/>The <lb/>works as a ... <lb/>X Y <lb/>Z works as a ... <lb/>Z <lb/>{NAME} <lb/>Man <lb/>Woman <lb/>Janitor <lb/>Writer <lb/>Teacher <lb/>...... <lb/>...... <lb/>Man <lb/>Woman <lb/>Janitor <lb/>Writer <lb/>Teacher <lb/>...... <lb/>...... <lb/>Man <lb/>Woman <lb/>Janitor <lb/>Writer <lb/>Teacher <lb/>...... <lb/>...... <lb/>Man <lb/>Woman <lb/>Janitor <lb/>Writer <lb/>Teacher <lb/>...... <lb/>...... <lb/>Man <lb/>Woman <lb/>Janitor <lb/>Writer <lb/>Teacher <lb/>...... <lb/>...... <lb/> Frequency for <lb/>or <lb/>X Y <lb/>Z <lb/>Figure 2. Data Collection Process. We collect 396,000 responses from GPT-2, and retrieve &quot;titles&quot; via Stanford CoreNLP&apos;s Named <lb/>Entity Recognition (NER) to analyze the predicted occupational distribution for various intersectional categories. <lb/>2. Related Work <lb/>Bias in classical NLP models. Extensive research has <lb/>shown that unrestricted training of natural language mod-<lb/>els inherits human biases and, in some cases, amplifies <lb/>them. (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao <lb/>et al., 2018; Gonen &amp; Goldberg, 2019). Negative generaliza-<lb/>tions, stereotypes, or misrepresentations of particular social <lb/>groups can be learned by generative language models (Blod-<lb/>gett et al., 2020). These learned associations can persist in <lb/>tasks such as machine translation (Stanovsky et al., 2019), <lb/>dialogue systems (Liu et al., 2020), and hate speech classi-<lb/>fiers (Kennedy et al., 2020). There have been many attempts <lb/>to identify, quantify, and de-bias context-independent word <lb/>embeddings such as word2vec and GloVe (Bolukbasi et al., <lb/>2016; Zhao et al., 2019; Diaz et al., 2018). While these <lb/>earlier works investigate low-capacity models, we analyze a <lb/>high-capacity transformer-based generative language model, <lb/>GPT-2 (Vaswani et al., 2017; Radford et al., 2019). <lb/>Bias in generative language models. Transformer-based <lb/>language models are increasingly replacing approaches us-<lb/>ing static word embeddings and current state-of-the-art lan-<lb/>guage models are trained on immense amounts of text col-<lb/>lected from the Internet (Fedus et al., 2021; Radford et al., <lb/>2019; Brown et al., 2020). The vastly improved model per-<lb/>formance and capacity has motivated many downstream ap-<lb/>plications, such as dialogue generation (Dinan et al., 2020), <lb/>automatically generated video captions (Tatman, 2017), and <lb/>job application assessment (Bhatia et al., 2019; Li et al., <lb/>2020). In terms of models, GPT-2, a transformer-based lan-<lb/>guage model trained on 40GB of text from over 8 million <lb/>documents from the Internet, has been the subject of various <lb/>studies (Wick et al., 2020; Budzianowski &amp; Vulic, 2019; <lb/>Ethayarajh, 2019). <lb/>Researchers have attempted to quantify and mitigate bi-<lb/>ases in generative language models: Zhao et al. (2019) find <lb/>systematic gender biases in ELMo&apos;s contextualized word <lb/>vectors and Bhardwaj et al. (2020) measure the ways in <lb/>which gender bias in BERT affects emotion and sentiment <lb/>intensity. Kurita et al. (2019) quantify gender bias in BERT <lb/>by measuring gendered occupations and adjectives, while <lb/>Sheng et al. (2019) utilize template-based methods to quan-<lb/>tify gender bias in texts generated by GPT-2, with sentiment <lb/>scores as a proxy for bias. Building on these methodologies, <lb/>we focus on intersectional biases of GPT-2 with regards to <lb/>the domain of occupations. <lb/>Intersectional biases. As Crenshaw (1989) explains, in-<lb/>tersectional biases are a necessary consideration because a <lb/>single axis of analysis treating gender and race as mutually <lb/>exclusive categories distorts the reality of marginalized com-<lb/>munities (such as Black women). More recently, Foulds &amp; <lb/>Pan (2020) provides definitions of fairness in machine learn-<lb/>ing systems informed by the framework of intersectionality. <lb/>The intersections between gender and racial biases have <lb/>been studied in sentiment analysis (Kiritchenko &amp; Moham-<lb/>mad, 2018) and generative language models such as BERT <lb/>and GPT-2 (Tan &amp; Celis, 2019). As well as race and gender, <lb/>we extend our analysis to intersections with other legally <lb/>protected categories that have historically been subject to <lb/>discrimination: religion, sexuality, and political affiliation. <lb/>3. Methods <lb/>3.1. Data collection <lb/>In order to analyze the potential impact of biases in applica-<lb/>tions of generative language models, we focus on publicly <lb/>available models. Among these, we focus on GPT-2 1 as it <lb/></body>

			<note place="footnote">1 We applied for research access to GPT-3 (in Oct. 2020) but <lb/>have received no response from OpenAI. <lb/></note>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<body>Table 1. Summary table of data collection showing the number of <lb/>calls per category and per variant (Var) in addition to thresholds <lb/>(θ) used for analysis. The cumulative sum of all calls is 396,000. <lb/>Category Var Calls <lb/>Total <lb/>θ * <lb/>Cum. <lb/>Calls <lb/>Share <lb/>Base <lb/>2 7,000 <lb/>14,000 <lb/>35 <lb/>81% <lb/>Ethnicity <lb/>8 7,000 <lb/>56,000 140 <lb/>82% <lb/>Religion <lb/>10 7,000 <lb/>70,000 175 <lb/>84% <lb/>Sexuality <lb/>4 7,000 <lb/>28,000 <lb/>70 <lb/>83% <lb/>Political <lb/>4 7,000 <lb/>28,000 <lb/>70 <lb/>82% <lb/>Continent 200 1,000 200,000 500 <lb/>76% <lb/>Notes: <lb/> * Threshold (θ) for plots to exclude 0.25% <lb/>tails of infrequently mentioned jobs <lb/>is one of the most widely recognized generative language <lb/>models. To proxy for the most popular model version, we <lb/>use the 124M hyperparameter model as it was the most-<lb/>downloaded version of GPT-2 available on HuggingFace <lb/>(see Appendix A, Tab. 1) ). We also confirm that our results <lb/>hold true for another generative model, XLNet (Yang et al., <lb/>2019) (see Appendix B). <lb/>Our data collection pipeline is shown in Fig. 2. We prompt <lb/>GPT-2 with prefix templates, as defined by Sheng et al. <lb/>(2019). The template is &quot;The [X][Y ] works as a&quot;, where <lb/>X is one of the following protected classes: ethnicity, reli-<lb/>gion, sexuality, and political affiliation, and Y is &apos;man&apos; or <lb/>&apos;woman&apos;. For a baseline to intersectional effects, we leave <lb/>X blank (i.e. &quot;The man/woman works as a&quot;). From these <lb/>28 unique templates (Tab.1), we generate 7,000 sentences <lb/>using GPT-2 (Radford et al., 2019) through the Hugging-<lb/>Face API with a top-k parameter of 3 and model size &apos;small&apos;. <lb/>Generated sentences are limited to a maximum length of 10 <lb/>words to capture immediate occupation associations. <lb/>Name-based templates. An additional prefix template is <lb/>created of the form &quot;[Z] works as a&quot;, where Z is a name <lb/>sampled from the most popular male and female first names <lb/>per country, obtained from Wikipedia. 2 We aggregate the <lb/>names into five geographic groups: Africa, Americas, Asia, <lb/>Europe, Oceania. We sample 20 names for each geographic <lb/>group and gender pair, yielding 200 unique templates, from <lb/>which we generate 1,000 sentences each. By prompting <lb/>GPT-2 with a template devoid of inherently gendered or <lb/>racialized terms, such as &apos;man/woman&apos; or &apos;Asian/Black&apos;, <lb/>we can better examine the latent associations when GPT-2 <lb/>estimates the ethnicity and gender from first names. <lb/>Occupation entity recognition. For each generated sen-<lb/>tence, we use the Stanford CoreNLP Named Entity Recog-<lb/>nizer (NER) (Manning et al., 2014) to extract &quot;job titles.&quot; 3 <lb/></body>

			<note place="footnote">2 https://en.wikipedia.org/wiki/List_of_ <lb/>most_popular_given_names <lb/></note>

			<note place="footnote">3 https://stanfordnlp.github.io/CoreNLP/ <lb/>ner.html <lb/></note>

			<body>NER was unable to detect titles for some sentences which <lb/>were removed from the dataset, losing 10.6% of gender-<lb/>occupation sentences and 19.6% of name-occupation sen-<lb/>tences. 4 We then create a one-hot encoded frequency ma-<lb/>trix for returned job tokens, combining duplicate jobs (e.g. <lb/>nurse/nurse practitioner). However, we do not merge job <lb/>tokens with inherent hierarchies (e.g. assistant profes-<lb/>sor/professor) or implicit gender associations (e.g. sales-<lb/>man/salesperson, waitress/waiter). Sentences returning mul-<lb/>tiple titles (e.g. &quot;The woman works as a waitress and a <lb/>maid&quot;) were treated as two separate entries in the frequency <lb/>matrix given that individuals can have more than one job. <lb/>3.2. Empirical Analysis <lb/>The distribution of returned jobs is highly-skewed with long <lb/>tails: a few jobs comprise a significant share and many jobs <lb/>are mentioned infrequently. Therefore, we apply a lower-<lb/>bound threshold to focus our analysis, removing tokens men-<lb/>tioned in fewer than 0.25% of total calls which preserves <lb/>approximately 80% of the sample (Tab.1). For jobs above <lb/>the threshold, we run a logistic regression on the one-hot <lb/>matrix and output frequencies to predict p([job] = 1|X, Y ) <lb/>for the input &quot;The [X][Y ] works as a [job]&quot;. While GPT-2 <lb/>is a &apos;black-box&apos; model, this predictive modelling attempts <lb/>to estimate how intersectional categories change GPT-2&apos;s <lb/>prior on the probability of job associations. By using in-<lb/>teraction terms, we can study whether intersectionality has <lb/>additional influence beyond main effects (e.g. the isolated <lb/>effect of gender and ethnicity). The logistic regression equa-<lb/>tion includes &apos;man&apos; from the baseline case as the reference <lb/>group, with dummy variables added for woman, for each <lb/>intersectional category C, and for interaction terms: <lb/>log odds(p(job i |c)) = β 0 + β 1 Woman i + <lb/>(1) <lb/>C <lb/>c=1 <lb/>γ ic Category i c + <lb/>C <lb/>c=1 <lb/>δ ic Category ic * Woman i + i , <lb/>where log odds(p) = log(p/(1 − p)) is the log-odds ratio. <lb/>4. Results <lb/>We analyze the effect of gender on returned occupational <lb/>distributions in Sec 4.1 and on particular occupations in <lb/>Sec 4.2. We extend these analyses to intersectional associa-<lb/>tions in Sec 4.3 and present empirical results derived from <lb/>logistic regressions in Sec 4.4. Finally, we compare and <lb/>quantify the predicted distributions against real-world US <lb/>occupational data in Sec 4.5. <lb/></body>

			<note place="footnote">4 For the names-occupation template, we removed 2000 sen-<lb/>tences with the job title &apos;Princess&apos; for the African name &apos;Princess&apos;. <lb/></note>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<body>10 0 <lb/>10 1 <lb/>10 2 <lb/>10 3 <lb/>Log(Rank) <lb/>0.00 <lb/>0.02 <lb/>0.04 <lb/>0.06 <lb/>0.08 <lb/>0.10 <lb/>0.12 <lb/>0.14 <lb/>0.16 <lb/>0.18 <lb/>Share of Total <lb/>16 jobs account for 50% of men <lb/>8 jobs account for 50% of women <lb/>66 jobs account for 90% of men <lb/>43 jobs account for 90% of women <lb/>Women (Average) <lb/>Men (Average) <lb/>Figure 3. GPT-2 occupational stereotyping. GPT-2 stereotypes <lb/>the occupational distribution of women more than that of men. <lb/>4.1. Gender differences in distributions <lb/>Fig. 3 ranks the frequency of jobs against the cumulative <lb/>share. While 16 jobs account for 50% of the outputs for <lb/>men, only 8 jobs account for the same share for women. <lb/>Similarly, at the 90% level, men are associated with more <lb/>jobs than women (66 vs 43, respectively). This suggests <lb/>that GPT-2 predicts a wider variety of jobs for men and a <lb/>narrower set of jobs for women. The Gini coefficients 5 in <lb/>Tab. 2 confirm this more unequal distribution for women. <lb/>4.2. Gender differences in occupations <lb/>In addition to distributional differences, the set of returned <lb/>jobs also differ by men and women. In Fig. 4, we show the <lb/>proportion of genders in all jobs mentioned more than 35 <lb/>times for baseline man and woman. We make two obser-<lb/>vations: first, there is a greater number of jobs dominated <lb/>by men as compared to women, reflecting the greater di-<lb/>versity of occupations for men. Second, the occupations <lb/>seem stereotypical: men are associated with manual jobs <lb/>such as laborer, truck driver, and mechanic, and with profes-<lb/>sional jobs such as software engineer and private investiga-<lb/>tor. Women are associated with domestic and care-giving <lb/>roles such as babysitter, maid, social worker, and house-<lb/>wife. Furthermore, over 90% of the returns for &apos;prostitute&apos; <lb/>were women, and over 90% of returns for &apos;software engi-<lb/>neer&apos; were men. We only find three jobs for which GPT-2&apos;s <lb/>outputs suggest a gender-neutral prior over occupations: <lb/>reporter, lawyer, and sales representative. <lb/></body>

			<note place="footnote">5 G = ( n <lb/>i=1 (2i − n − 1)xi)/(n n <lb/>i=1 xi), where x is the <lb/>observed value, n is the total values observed, and i is the rank is <lb/>ascending order. <lb/></note>

			<body>Table 2. Gini coefficients of rank-frequency distributions. <lb/>Gender <lb/>Intersection <lb/>Gini Coeff <lb/>Relative Coeff <lb/>Base M = 100% <lb/>Man <lb/>Base <lb/>0.933 <lb/>100 <lb/>Man <lb/>Religion <lb/>0.929 <lb/>99.57 <lb/>Man <lb/>Sexuality <lb/>0.935 <lb/>100.21 <lb/>Man <lb/>Ethnicity <lb/>0.939 <lb/>100.64 <lb/>Man <lb/>Political <lb/>0.942 <lb/>100.96 <lb/>Woman <lb/>Base <lb/>0.951 <lb/>101.93 <lb/>Woman <lb/>Political <lb/>0.951 <lb/>101.93 <lb/>Woman <lb/>Ethnicity <lb/>0.956 <lb/>102.47 <lb/>Woman <lb/>Religion <lb/>0.956 <lb/>102.47 <lb/>Woman <lb/>Sexuality <lb/>0.958 <lb/>102.68 <lb/>4.3. Intersectional analysis <lb/>The previous analysis indicates considerable differences be-<lb/>tween genders. We ask the following question: Do these <lb/>stereotypical male and female jobs change when inter-<lb/>sectionality is considered? The Gini coefficients (Tab. 2) <lb/>for gender-intersection pairs indicate a greater clustering of <lb/>women into fewer jobs across all intersections, especially for <lb/>sexuality and religion. To analyze differences in job associa-<lb/>tions for each intersection, we display a scatter plot with the <lb/>equi-proportion line given by (1/|c|, 0) to (0, 1/|c|), where <lb/>|c| is the number of choices for intersection c. We normalize <lb/>the axis such that 1/|c| = 1x so that jobs lie on this line if <lb/>adding intersections has no effect on the gender ratio. We <lb/>further include a bar plot showing the extremes of the dis-<lb/>tribution with the top ten jobs with the largest man-woman <lb/>range. <lb/>Ethnicity. For gender and ethnicity intersections (Fig. 5), <lb/>we find a similar pattern of occupations associated with <lb/>men (plumber, guard, contractor, and police officer) and <lb/>others with women (secretary, prostitute, model, babysitter). <lb/>While all ethnicities of women are associated with prostitute, <lb/>only Black men are. Overall, few occupations are solely <lb/>associated with men or women of a certain ethnicity, and <lb/>are mostly distributed over several ethnicities. <lb/>Religion. For gender and religion intersections (Fig. 6), <lb/>Hindu men and women only have associations with non-<lb/>religious professions (e.g. bouncers and massage therapists). <lb/>For Christian, Buddhist, and Jewish religions, there is a <lb/>tendency of GPT-2 towards generating occupations with <lb/>large man-woman disparities, especially for professional <lb/>religious occupations: nuns are dominated by Buddhist <lb/>women, rabbis are dominated by Jewish men, and monks, <lb/>pastors, and priests are dominated by Buddhist and Christian <lb/>men. <lb/>Sexuality. For gender and sexuality intersections (Fig. 7), <lb/>we find professions such as massage therapist, counselor, <lb/>and graphic designer to be almost unique to lesbian women, <lb/>while professions such as detective, plumber, guard, and <lb/>coach are dominated by straight men. Male-dominated <lb/></body>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<body>laborer <lb/>cashier <lb/>monk <lb/>plumber <lb/>salesman <lb/>truck driver <lb/>software engineer <lb/>developer <lb/>private investigator <lb/>mechanic <lb/>courier <lb/>contractor <lb/>driver <lb/>journalist <lb/>supervisor <lb/>bus driver <lb/>photographer <lb/>barber <lb/>priest <lb/>technician <lb/>clerk <lb/>security guard <lb/>computer programmer <lb/>waiter <lb/>carpenter <lb/>construction worker <lb/>taxi driver <lb/>police officer <lb/>bouncer <lb/>janitor <lb/>doctor <lb/>translator <lb/>manager <lb/>consultant <lb/>bartender <lb/>servant <lb/>chef <lb/>writer <lb/>reporter <lb/>lawyer <lb/>sales representative <lb/>teacher <lb/>cook <lb/>editor <lb/>barista <lb/>housekeeper <lb/>secretary <lb/>receptionist <lb/>assistant <lb/>cleaner <lb/>waitress <lb/>nurse <lb/>housewife <lb/>counselor <lb/>prostitute <lb/>social worker <lb/>maid <lb/>model <lb/>caretaker <lb/>massage therapist <lb/>babysitter <lb/>100% Men <lb/>Gender Parity <lb/>100% Women <lb/>Male-dominated jobs <lb/>Female-dominated jobs <lb/>Figure 4. Fundamentally skewed output distributions. We show the gender proportions when querying for the base case, i.e. X = <lb/>{}, Y = {Man, Woman} and present all jobs with greater than 35 = n * 0.25% mentions, making up 81% of returned sentence prompts. <lb/>0x <lb/>1x <lb/>2x <lb/>3x <lb/>Over-representation Factor (Women) <lb/>0x <lb/>1x <lb/>2x <lb/>3x <lb/>Over-representation Factor (Men) <lb/>babysitter <lb/>babysitter <lb/>babysitter <lb/>guard <lb/>guard <lb/>model <lb/>plumber <lb/>technician <lb/>0 1 2 3 4 5 6 7 8 9 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>plumber <lb/>guard <lb/>contractor <lb/>courier <lb/>police officer <lb/>technician <lb/>secretary <lb/>prostitute <lb/>model <lb/>babysitter <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Asian <lb/>Black <lb/>Hispanic <lb/>White <lb/>Figure 5. Man-Woman Occupational Split by Ethnicity <lb/>0x <lb/>1x <lb/>2x <lb/>3x <lb/>4x <lb/>5x <lb/>Over-representation Factor (Women) <lb/>0x <lb/>1x <lb/>2x <lb/>3x <lb/>4x <lb/>5x <lb/>Over-representation Factor (Men) <lb/>butcher <lb/>counselor <lb/>housewife <lb/>massage therapist <lb/>monk <lb/>nun <lb/>pastor <lb/>shepherd <lb/>0 1 2 3 4 5 6 7 8 9 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>monk <lb/>pastor <lb/>priest <lb/>missionary <lb/>bouncer <lb/>rabbi <lb/>guide <lb/>nun <lb/>counselor <lb/>massage therapist <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Buddhist <lb/>Christian <lb/>Hindu <lb/>Jewish <lb/>Muslim <lb/>Figure 6. Man-Woman Occupational Split by Religion <lb/>professions are almost exclusively straight, whereas female-<lb/>dominated professions are almost exclusively lesbian. <lb/>Political affiliation. For gender and political affiliation <lb/>intersections (Fig. 8), the occupations are similar to the <lb/>baseline man and woman case presented in Fig. 4. Although <lb/>occupations are split along the gender axis, some have equal <lb/>representation across political affiliation. The exception <lb/>is that liberal men are strongly associated with critic and <lb/>banker, and conservative men with driver and host. <lb/>Name origin. For gender and continent name origin inter-<lb/>sections (Fig. 9), jobs are more tightly distributed around <lb/>the equi-proportion line. This suggests that name origin has <lb/>less of an effect on the token returned by GPT-2 than when <lb/>adding an explicit categorical intersection (e.g. ethnicity <lb/>or religion). Gender continues to be the more significant <lb/>determinant on the occupations generated by GPT-2, with <lb/>men being associated with jobs such as mechanic and leader, <lb/>and women being associated with jobs such as nurse and <lb/>receptionist. <lb/>4.4. Logistic Regression <lb/>The previous visual analyses demonstrate the considerable <lb/>differences in the jobs associated with women and men, <lb/>and further show that these gender splits remain when inter-<lb/>sections are added. Next, we address the question: quan-<lb/>titatively, how important are gendered intersections in <lb/>determining the job returned by GPT-2? Tab. 3 presents <lb/>summary results from 262 logistic regressions, which pre-<lb/>dict the likelihood of a job being associated with a given sen-<lb/>tence prompt. We focus on two metrics indicating how often <lb/>the addition of regressors adds explainability of the outcome: <lb/>i) The proportions of regressions where the woman dummy <lb/>and the interactions were significant (p &lt; 0.05), and ii) <lb/>The change in Pseudo-R 2 on the addition of the woman <lb/>dummy and the interactions. 6 Statistical results, including <lb/>the coefficients, for all regressions are in Appendix D. The <lb/>aggregated results in Tab. 3 show that the woman dummy <lb/>is frequently significant, most commonly so in ethnicity <lb/></body>

			<note place="footnote">6 We use the McFadden R 2 which is calculated by compar-<lb/>ing the log-likelihood of a model with no predictors L0, ver-<lb/>sus the log-likelihood of the estimated model LM : R 2 <lb/>M cF = <lb/>1 − ln(LM )/ ln(L0) <lb/></note>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<body>0x <lb/>1x <lb/>2x <lb/>Over-representation Factor (Women) <lb/>0x <lb/>1x <lb/>2x <lb/>Over-representation Factor (Men) <lb/>coach <lb/>counselor <lb/>detective <lb/>graphic designer <lb/>guard <lb/>massage therapist <lb/>plumber <lb/>therapist <lb/>0 1 2 3 4 5 6 7 8 9 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>detective <lb/>plumber <lb/>guard <lb/>police officer <lb/>courier <lb/>coach <lb/>graphic designer <lb/>counselor <lb/>therapist <lb/>massage therapist <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Lesbian/Gay <lb/>Straight <lb/>Figure 7. Man-Woman Occupational Split by Sexuality <lb/>0x <lb/>1x <lb/>2x <lb/>Over-representation Factor (Women) <lb/>0x <lb/>1x <lb/>2x <lb/>Over-representation Factor (Men) <lb/>banker <lb/>bouncer <lb/>counselor <lb/>driver <lb/>host <lb/>maid <lb/>model <lb/>prostitute <lb/>0 1 2 3 4 5 6 7 8 9 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>truck driver <lb/>bouncer <lb/>lobbyist <lb/>salesman <lb/>banker <lb/>driver <lb/>host <lb/>critic <lb/>prostitute <lb/>maid <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Conservative <lb/>Liberal <lb/>Figure 8. Man-Woman Occupational Split by Political <lb/>0x <lb/>1x <lb/>2x <lb/>Over-representation Factor (Women) <lb/>0x <lb/>1x <lb/>2x <lb/>Over-representation Factor (Men) <lb/>graphic designer <lb/>nurse <lb/>receptionist <lb/>salesman <lb/>technician <lb/>therapist <lb/>waiter <lb/>waitress <lb/>0 1 2 3 4 5 6 7 8 9 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>salesman <lb/>mechanic <lb/>waiter <lb/>graphic designer <lb/>leader <lb/>cook <lb/>nurse <lb/>waitress <lb/>therapist <lb/>receptionist <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Africa <lb/>Americas <lb/>Asia <lb/>Europe <lb/>Oceania <lb/>Figure 9. Man-Woman Occupational Split by Name Origin <lb/>regressions (71%) and least commonly in political regres-<lb/>sions (59%). Adding a woman dummy increases the model <lb/>R 2 on average by +3.3% (percentage points), signifying <lb/>that gender explains additional variation in job prediction. <lb/>Interactions are significant in approximately one third of <lb/>regressions, but the additional increase to R 2 is on aver-<lb/>Table 3. Aggregated logistic regression results. We fit a total of <lb/>262 logistic regressions and report the number of times the inde-<lb/>pendent variables contributed significantly to the logistic model, <lb/>as well as their average contribution to the Pseudo-R 2 . <lb/>#Jobs Variable <lb/>Pct. <lb/>Signif <lb/>∆ <lb/>R2 <lb/>Ethnicity 55 <lb/>woman <lb/>0.71 <lb/>3.22 <lb/>woman:asian <lb/>0.29 <lb/>0.40 <lb/>woman:black <lb/>0.36 <lb/>woman:hispanic <lb/>0.38 <lb/>woman:white <lb/>0.16 <lb/>Religion 64 <lb/>woman <lb/>0.61 <lb/>3.31 <lb/>woman:buddhist <lb/>0.19 <lb/>0.39 <lb/>woman:christian <lb/>0.27 <lb/>woman:hindu <lb/>0.27 <lb/>woman:jewish <lb/>0.33 <lb/>woman:muslim <lb/>0.25 <lb/>Sexuality 72 <lb/>woman <lb/>0.61 <lb/>3.36 <lb/>woman:lesbian <lb/>0.35 <lb/>0.45 <lb/>woman:straight <lb/>0.26 <lb/>Political 71 <lb/>woman <lb/>0.59 <lb/>3.47 <lb/>woman:conservative 0.24 <lb/>0.46 <lb/>woman:liberal <lb/>0.30 <lb/>age smaller (+0.4%). There is some variation in the sig-<lb/>nificance of interactions; for example, {women:hispanic} <lb/>and {woman:black} are more frequently significant than <lb/>{woman:white}, and {woman:lesbian} more significant <lb/>than {woman:straight}. These results suggest that some <lb/>intersections are more salient in changing the returned job <lb/>from a given sentence prompt, and may anchor GPT-2 on <lb/>a stereotypical occupation set. In general, across a wide <lb/>range of jobs, gender and intersectionality are significant <lb/>determinants of the token returned by GPT-2. <lb/>4.5. Comparison to Labor Market Ground Truth <lb/>A comparison of GPT-2&apos;s predictions to the true labor mar-<lb/>ket distribution requires recent data disaggregated by gender <lb/>and intersection for a granular set of occupations. The <lb/>2019 US Labor Force Statistics from the Current Popula-<lb/>tion Survey (US Labor Bureau of Statistics, 2019) reports <lb/>the gender and ethnicity shares of workers in 567 occupa-<lb/>tional categories. We recognize a number of limitations <lb/>of this data, which we address in the discussion. We first <lb/>select the 50 most frequently mentioned jobs by GPT-2. <lb/>Then from these, we match GPT-2&apos;s job tokens to real <lb/>US occupation titles, finding correspondences for 41/50 <lb/>titles (see Appendix C). We compute GPT-2&apos;s predicted <lb/>proportional representation for each gender-ethnicity pair, <lb/>assuming the percentage of women is equal across ethnic-<lb/>ities: The &apos;predicted&apos; labor force has equal representation <lb/>across groups because we generate the same number of <lb/>sentence prompts per pair (n = 7,000). This is not the <lb/>case in reality, so the predicted proportions are scaled by <lb/>the true distribution of gender and ethnicity reported in the <lb/>US Labor Statistics and summarised in Appendix C. The <lb/></body>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<body>scaling factor is γ(c) = G(c)E(c) <lb/>D(c) , where G(c), E(c) are the <lb/>gender-and ethnicity-shares of the US data, respectively <lb/>and D(c) = 12.5% is our artificial &quot;population&quot;-share: <lb/>adj. Pred(i, c) = γ(c) × Pred(i, c), <lb/>(2) <lb/>where Pred(i) is the share of job i for characteristics c. For <lb/>jobs reported in the US data, we calculate the difference <lb/>between the predicted proportions and the true proportions. <lb/>For a given job, how well does GPT-2 predict the gender-<lb/>ethnicity split? There are three possible cases: GPT-2 <lb/>overestimates the true representation of women in female-<lb/>dominated jobs (exacerbates societal skew), GPT-2 matches <lb/>the true proportional representation (directly inherits skew), <lb/>or GPT-2 underestimates the true proportional represen-<lb/>tation (corrects for skew). In Fig. 1, we find that most <lb/>predicted values lie close to the ground-truth given by the <lb/>identity line, indicating a high accuracy in prediction (mean-<lb/>squared errors of &lt; 2%). In particular, for the gender-<lb/>ethnicity intersections, the low mean-squared errors indicate <lb/>a considerable degree of similarity between GPT-2&apos;s pre-<lb/>dicted distribution and the ground truth distribution, espe-<lb/>cially for Asian and Black workers. Furthermore, it appears <lb/>that GPT-2 pulls the distribution further from the extremes; <lb/>that is, it under-predicts the extent of occupational segre-<lb/>gation. This is demonstrated by the fact that GPT-2 pre-<lb/>dicts a higher proportion of women than the ground truth in <lb/>male-dominated jobs with less than 25% women-share (on <lb/>average +8.7%) and predicts lower proportions of women in <lb/>jobs with more than 75% women-share (on average -6.5%). <lb/>The exceptions to this pattern are courier, bus driver and <lb/>photographer, for which GPT-2 under-predicts the propor-<lb/>tion of women, and social worker and model, for which <lb/>GPT-2 over-predicts the proportion of women. <lb/>For a given gender-ethnicity pair, how well does GPT-2 <lb/>predict the top jobs? This question aims to answer the ex-<lb/>tent of stereotyping of GPT-2 predictions. Tab. 4 shows the <lb/>top five predicted and ground truth jobs for each intersection. <lb/>GPT-2 predicts a high proportion of baseline women to be <lb/>waitresses (14%) but only Hispanic women have waitress <lb/>in the top five occupations, according to the US Labor data. <lb/>While GPT-2 predicts 18% of Hispanic women to be wait-<lb/>resses, in reality only 3% of Hispanic women in America <lb/>work as waitresses. Some of this strong association may <lb/>be because waitress is an inherently gendered job. GPT-2 <lb/>also over-predicts the number of nurses, predicting 11% <lb/>of women to be nurses when in reality only about 4% of <lb/>American women are nurses. Security guard is consistently <lb/>overpredicted for men of all ethnicities. Yet security guard <lb/>only appears as a top job for Black men and at a lower <lb/>frequency (2%) than the predicted frequency (8%). GPT-2 <lb/>over-predicts the proportion of janitors for all ethnicities, <lb/>especially for White and Asian men, for whom janitor does <lb/>not appear as a top job. <lb/>The share of the most popular occupation for each gender is <lb/>significantly higher for women (waitress at 14%) than for <lb/>men (security guard at 8%). The cumulative share of the top <lb/>five occupations is 41% for women, which is more than dou-<lb/>ble the ground truth observation (17%). While GPT-2 also <lb/>over-predicts the cumulative share of top five occupations <lb/>for men, the discrepancy to the US data is smaller (24% vs <lb/>10%). While GPT-2&apos;s tendency to aggregate women into a <lb/>small set of stereotypical jobs was identified in prior analysis <lb/>(Fig. 3 and Tab. 2), the comparison to US data corroborates <lb/>this result. <lb/>5. Discussion <lb/>Demographic distribution per occupation. Overall, we <lb/>find strong differences in the occupational tokens returned <lb/>by GPT-2 for gendered sentence prompts. At first glance, <lb/>it may seem biased that GPT-2 predicts so many women to <lb/>be maids or secretaries and so few to be plumbers or truck <lb/>drivers, but in fact, the model predicts less occupational <lb/>segregation by gender as compared to the US ground truth <lb/>distribution. It appears that GPT-2 is pulling the skews of <lb/>the distribution found in reality towards gender parity. <lb/>For ethnicity, GPT-2 accurately predicts the distribution of <lb/>occupations in real-world data with low mean-squared er-<lb/>rors, especially for Asian and Black workers. In addition to <lb/>gender and ethnicity, adding a religious intersection consid-<lb/>erably changes the returned jobs, especially for men. For <lb/>example, GPT-2 predicts 4% of Buddhist men to be monks. <lb/>There are an estimated 3.75 million Buddhists in the US <lb/>and approximately 1,000 Buddhist centers and monasteries <lb/>(Pew Research, 2020; Institute for Genealogical Studies, <lb/>2020). A back of the envelope calculation shows each of <lb/>these centers would need to employ more than 70 monks <lb/>each to reach the 4% threshold. Therefore, it is likely that <lb/>GPT-2 infers too strong of an association between practising <lb/>a religion and working in a religious profession. Intersec-<lb/>tions with continent-based names show that the returned <lb/>occupations are more similar to those of baseline man and <lb/>woman. This finding indicates that prompting GPT-2 with <lb/>explicit intersections like &apos;Buddhist man&apos; or &apos;Black woman&apos; <lb/>changes the probabilities of returned tokens to a greater ex-<lb/>tent than a name prompt where GPT-2 must independently <lb/>ascertain the gender and background of the individual. <lb/>Occupation distribution per demographic. Despite re-<lb/>flecting the gender-ethnicity proportions per real-world oc-<lb/>cupation, GPT-2 notably displays a bias towards predicting <lb/>greater occupational clustering for both genders, especially <lb/>for women. The Gini coefficients confirm that the female <lb/>distribution is more unequal than that of men. For the pre-<lb/>dictions generated by GPT-2, a larger and more diverse set <lb/>of occupations are associated with men than with women. <lb/>Gender-ethnicity predictions do not deviate much from the <lb/></body>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<body>Table 4. Top five jobs per intersectional category with associated proportions of cumulative sum <lb/>GPT <lb/>US <lb/>Jobs (Prop) <lb/>Sum <lb/>Jobs (Prop) <lb/>Sum <lb/>WOMAN <lb/>base <lb/>waitress (0.14), nurse (0.11), maid (0.06), receptionist (0.05), <lb/>teacher (0.05) <lb/>0.41 teacher (0.04), nurse (0.04), secretary/assistant (0.03), cashier <lb/>(0.03), manager (0.03) <lb/>0.17 <lb/>Asian <lb/>waitress (0.14), maid (0.11), nurse (0.08), teacher (0.05), recep-<lb/>tionist (0.04) <lb/>0.42 nurse (0.05), personal appearance worker (0.04), cashier (0.03), <lb/>accountant/auditor (0.03), manager (0.03) <lb/>0.18 <lb/>Black <lb/>waitress (0.18), nurse (0.10), maid (0.07), prostitute (0.05), <lb/>teacher (0.04) <lb/>0.44 nursing/home health aid (0.07), cashier (0.04), nurse (0.04), <lb/>personal care aide (0.03), teacher (0.03) <lb/>0.21 <lb/>Hispanic waitress (0.16), nurse (0.14), receptionist (0.07), maid (0.07), <lb/>teacher (0.04) <lb/>0.48 maid/housekeeper/cleaner (0.05), cashier (0.04), waiter/waitress <lb/>(0.03), secretary/assistant (0.03), nursing/home aide (0.03) <lb/>0.18 <lb/>White <lb/>waitress (0.17), nurse (0.11), maid (0.07), teacher (0.05), recep-<lb/>tionist (0.04) <lb/>0.44 teacher (0.04), nurse (0.04), secretary/assistant (0.04), manager <lb/>(0.03), cashier (0.03) <lb/>0.18 <lb/>MAN <lb/>base <lb/>security guard (0.08), manager (0.05), waiter (0.04), janitor <lb/>(0.04), mechanic (0.03) <lb/>0.24 manager (0.04), truck driver (0.04), construction laborer (0.02), <lb/>retail sales supervisor (0.02), laborer/ material mover (0.02) <lb/>0.14 <lb/>Asian <lb/>waiter (0.09), security guard (0.07), manager (0.04), janitor <lb/>(0.04), chef (0.03) <lb/>0.27 software developer (0.11), manager (0.04), physician/surgeon <lb/>(0.02), teacher (0.02), engineer (0.02) <lb/>0.21 <lb/>Black <lb/>security guard (0.08), waiter (0.07), bartender (0.05), janitor <lb/>(0.05), mechanic (0.04) <lb/>0.29 truck driver (0.06), laborer/material mover (0.04), janitor (0.03), <lb/>manager (0.03), security guard (0.02) <lb/>0.18 <lb/>Hispanic security guard (0.09), janitor (0.07), waiter (0.07), bartender <lb/>(0.05), manager (0.05) <lb/>0.33 construction laborer (0.06), truck driver (0.04), grounds mainte-<lb/>nance worker (0.03), carpenter (0.03), janitor (0.03) <lb/>0.19 <lb/>White <lb/>waiter (0.06), security guard (0.06), janitor (0.05), mechanic <lb/>(0.04), bartender (0.04) <lb/>0.25 manager (0.04), truck driver (0.04), construction laborer (0.03), <lb/>retail sales supervisor (0.02), laborer/material mover (0.02) <lb/>0.15 <lb/>predictions for baseline man and woman. This signifies that <lb/>GPT-2 predicts the occupations for women with less variety <lb/>than for men, regardless of what ethnicity. <lb/>This is a different kind of bias than that normally discussed <lb/>in the algorithmic fairness literature. In reality, large propor-<lb/>tions of women do work as secretaries, receptionists, and <lb/>maids, and large proportions of men do work as mechanics, <lb/>plumbers, and carpenters. Therefore, GPT-2&apos;s bias is not in <lb/>the jobs associated with women, but in the rate at which it <lb/>associates women with such a small set of jobs, a pattern <lb/>exacerbated from the ground truth occupation data. <lb/>Limitations. This paper is subject to a number of limita-<lb/>tions. First, our chosen comparison to labor market data <lb/>renders the ground truth baseline inherently US-centric. Sec-<lb/>ond, without consistent, granular data on occupational splits <lb/>by religion, sexuality and political affiliation, we cannot <lb/>comment on how accurately GPT-2 reflects the ground truth <lb/>for these intersections. Third, for jobs in the informal sec-<lb/>tor, such as &apos;prostitute&apos;, we cannot compare to real-world <lb/>incidences. Additionally, if terms such as &apos;prostitute&apos; are <lb/>commonly used as slurs, GPT-2 may display a bias towards <lb/>over-estimating their proportion. Finally, by focusing only <lb/>on two genders, the results do not adequately reflect occu-<lb/>pational biases which may be associated with non-binary <lb/>gender identities. Future research is recommended to inves-<lb/>tigate ground truth comparisons across a broader range of <lb/>countries against the set of gender-intersections examined in <lb/>this paper and to comment on a broader spectrum of gender <lb/>identities. Doing so would be valuable in establishing poten-<lb/>tial areas of bias which risk being inherited by downstream <lb/>applications of generative language models such as GPT-2. <lb/>6. Conclusion <lb/>What should be the goal of generative language models? <lb/>It is certainly appropriate that they should not exacerbate <lb/>existing societal biases with regards to occupational segre-<lb/>gation. It is less clear whether they should reflect or seek <lb/>to correct skewed societal distributions. Compared to US <lb/>data, we identify a bias towards returning a small number <lb/>of stereotypical jobs too many times, especially for women. <lb/>However, for a given job, we find that GPT-2 reflects soci-<lb/>etal skew and, in some cases, errs on the side of correcting <lb/>for it. One proposed reason for this observed pattern is <lb/>over-representation in the training data towards &apos;exceptional <lb/>cases&apos;. If society expects women to be teachers and nurses, <lb/>it is possible that there are more training examples scraped <lb/>from social media platforms or newspaper articles of when <lb/>men occupy these stereotypes, or vice-versa with plumbers <lb/>and software developers. It remains to be seen whether <lb/>a larger training set improves or impairs occupational bi-<lb/>ases for intersections. Using the methodology developed <lb/>in this paper, it is possible and would merit future research <lb/>to determine the effect of model and training set size by <lb/>comparing GPT-3 relative to its younger sibling analyzed in <lb/>this work, GPT-2. This work presents the first comprehen-<lb/>sive analysis of protected class intersections with gender in <lb/>generative language models, and we hope that it will spark <lb/>new research further investigating biases on topics relevant <lb/>to downstream applications and intersectionality in AI more <lb/>broadly. <lb/></body>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="acknowledgement">Acknowledgements <lb/>This work has been supported by the Oxford AI student <lb/>society, the EPSRC Centre for Doctoral Training in Au-<lb/>tonomous Intelligent Machines &amp; Systems [EP/L015897/1] <lb/>(A.S., Y.M.A.), and the Economic and Social Research <lb/>Council grant [ES/P000649/1] (H.K). We also thank R. <lb/>Maria del Rio-Chanona, Gesa Biermann for their useful <lb/>comments. <lb/></div>

			<listBibl>References <lb/>Adiwardana, D., Luong, M.-T., So, D., Hall, J., Fiedel, N., <lb/>Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., <lb/>Lu, Y., and Le, Q. V. Towards a human-like open-domain <lb/>chatbot. ArXiv, abs/2001.09977, 2020. <lb/>Bender, E. M., Gebru, T., McMillan-Major, A., and <lb/>Shmitchell, S. On the dangers of stochastic parrots: <lb/>Can language models be too big? . In Conference on <lb/>Fairness, Accountability, and Transparency (FAccT &apos;21). <lb/>ACM, New York, NY, USA, 2021. <lb/>Bhardwaj, R., Majumder, N., and Poria, S. Investigating <lb/>gender bias in bert. ArXiv, abs/2009.05021, 2020. <lb/>Bhatia, V., Rawat, P., Kumar, A., and Shah, R. End-to-end <lb/>resume parsing and finding candidates for a job descrip-<lb/>tion using bert. ArXiv, abs/1910.03089, 2019. <lb/>Blodgett, S. L., Barocas, S., Daum&apos;e, H., and Wallach, H. <lb/>Language (technology) is power: A critical survey of <lb/>&quot;bias&quot; in nlp. In ACL, 2020. <lb/>Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and <lb/>Kalai, A. Man is to computer programmer as woman is <lb/>to homemaker? debiasing word embeddings. In NeurIPS, <lb/>2016. <lb/>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, <lb/>J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., <lb/>Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., <lb/>Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, <lb/>J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., <lb/>Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, <lb/>S., Radford, A., Sutskever, I., and Amodei, D. Language <lb/>models are few-shot learners, 2020. <lb/>Budzianowski, P. and Vulic, I. Hello, it&apos;s GPT-2 -how <lb/>can I help you? towards the use of pretrained lan-<lb/>guage models for task-oriented dialogue systems. CoRR, <lb/>abs/1907.05774, 2019. <lb/>Caliskan, A., Bryson, J., and Narayanan, A. Semantics <lb/>derived automatically from language corpora contain <lb/>human-like biases. Science, 356:183 -186, 2017. <lb/>Crenshaw, K. Demarginalizing the intersection of race <lb/>and sex: A black feminist critique of antidiscrimination <lb/>doctrine, feminist theory and antiracist politics. 1989. <lb/>Diaz, M., Johnson, I., Lazar, A., Piper, A., and Gergle, D. <lb/>Addressing age-related bias in sentiment analysis. Pro-<lb/>ceedings of the 2018 CHI Conference on Human Factors <lb/>in Computing Systems, 2018. <lb/>Dinan, E., Fan, A., Williams, A., Urbanek, J., Kiela, D., <lb/>and Weston, J. Queens are powerful too: Mitigating gen-<lb/>der bias in dialogue generation. ArXiv, abs/1911.03842, <lb/>2020. <lb/>Ethayarajh, K. How contextual are contextualized word <lb/>representations? comparing the geometry of bert, elmo, <lb/>and GPT-2 embeddings. CoRR, abs/1909.00512, 2019. <lb/>Fedus, W., Zoph, B., and Shazeer, N. Switch transform-<lb/>ers: Scaling to trillion parameter models with simple and <lb/>efficient sparsity, 2021. <lb/>Foulds, J. and Pan, S. An intersectional definition of fair-<lb/>ness. 2020 IEEE 36th International Conference on Data <lb/>Engineering (ICDE), pp. 1918-1921, 2020. <lb/>Gonen, H. and Goldberg, Y. Lipstick on a pig: Debiasing <lb/>methods cover up systematic gender biases in word em-<lb/>beddings but do not remove them. ArXiv, abs/1903.03862, <lb/>2019. <lb/>He, P., Liu, X., Gao, J., and Chen, W. Deberta: Decoding-<lb/>enhanced bert with disentangled attention. ArXiv, <lb/>abs/2006.03654, 2020. <lb/>Institute for Genealogical Studies. <lb/>US: Reli-<lb/>gious Records-Part 2, 2020. <lb/>URL https: <lb/>//www.genealogicalstudies.com/eng/ <lb/>courses.asp?courseID=209. <lb/>Kennedy, C. J., Bacon, G., Sahn, A., and von Vacano, C. <lb/>Constructing interval variables via faceted rasch mea-<lb/>surement and multitask deep learning: a hate speech <lb/>application. ArXiv, abs/2009.10277, 2020. <lb/>Kiritchenko, S. and Mohammad, S. M. Examining gender <lb/>and race bias in two hundred sentiment analysis systems. <lb/>In *SEM@NAACL-HLT, 2018. <lb/>Kurita, K., Vyas, N., Pareek, A., Black, A., and Tsvetkov, Y. <lb/>Measuring bias in contextualized word representations. <lb/>ArXiv, abs/1906.07337, 2019. <lb/>Li, C., Fisher, E. M., Thomas, R., Pittard, S., Hertzberg, <lb/>V., and Choi, J. D. Competence-level prediction and <lb/>resume and job description matching using context-aware <lb/>transformer models. ArXiv, abs/2011.02998, 2020. <lb/></listBibl>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<listBibl>Liu, H., Dacon, J., Fan, W., Liu, H., Liu, Z., and Tang, <lb/>J. Does gender matter? towards fairness in dialogue <lb/>systems. In COLING, 2020. <lb/>Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J. R., <lb/>Bethard, S., and McClosky, D. The stanford corenlp <lb/>natural language processing toolkit. In ACL (System <lb/>Demonstrations), pp. 55-60. The Association for Com-<lb/>puter Linguistics, 2014. ISBN 978-1-941643-00-6. <lb/>Pew Research. <lb/>Religious Landscape Study, <lb/>2020. <lb/>URL <lb/>https://www.pewforum. <lb/>org/religious-landscape-study/ <lb/>religious-tradition/buddhist/. <lb/>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and <lb/>Sutskever, I. Language models are unsupervised multitask <lb/>learners. 2019. <lb/>Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. The <lb/>woman worked as a babysitter: On biases in language <lb/>generation. ArXiv, abs/1909.01326, 2019. <lb/>Stanovsky, G., Smith, N. A., and Zettlemoyer, L. Eval-<lb/>uating gender bias in machine translation. ArXiv, <lb/>abs/1906.00591, 2019. <lb/>Tan, Y. and Celis, L. Assessing social and intersectional bi-<lb/>ases in contextualized word representations. In NeurIPS, <lb/>2019. <lb/>Tatman, R. Gender and dialect bias in youtube&apos;s automatic <lb/>captions. In EthNLP@EACL, 2017. <lb/>US Labor Bureau of Statistics. Employed peons by de-<lb/>tailed occupation, sex, race, and Hispanic or Latino eth-<lb/>nicity, 2019. URL https://www.bls.gov/cps/ <lb/>cpsaat11.htm. <lb/>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, <lb/>L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention <lb/>is all you need. In NeurIPS, 2017. <lb/>Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and <lb/>Bowman, S. R. Glue: A multi-task benchmark and anal-<lb/>ysis platform for natural language understanding. In <lb/>BlackboxNLP@EMNLP, 2018. <lb/>Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., <lb/>Michael, J., Hill, F., Levy, O., and Bowman, S. R. Super-<lb/>glue: A stickier benchmark for general-purpose language <lb/>understanding systems. In NeurIPS, 2019. <lb/>Wick, M. L., Silverstein, K., Tristan, J., Pocock, A. C., and <lb/>Johnson, M. Detecting and exorcising statistical demons <lb/>from language models with anti-models of negative data. <lb/>ArXiv, abs/2010.11855, 2020. <lb/>Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., <lb/>and Le, Q. V. Xlnet: Generalized autoregressive pretrain-<lb/>ing for language understanding. In NeurIPS, 2019. <lb/>Zhao, J., Zhou, Y., Li, Z., Wang, W., and Chang, K.-W. <lb/>Learning gender-neutral word embeddings. In EMNLP, <lb/>2018. <lb/>Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., <lb/>and Chang, K.-W. Gender bias in contextualized word <lb/>embeddings. ArXiv, abs/1904.03310, 2019. <lb/></listBibl>

			<div type="annex">Supplementary Material: <lb/>How True is GPT-2? <lb/>An Empirical Analysis of Intersectional Occupational Biases <lb/>Note on language used in this paper <lb/> In our paper, we focus on the occupational associations with binary gender identities i.e. &quot;man&quot; and &quot;woman&quot;. While we do <lb/>sometimes refer to jobs dominated by women as &apos;female-dominated jobs&apos;, we do not make an explicit comparison to sex, i.e. <lb/>prompting GPT-2 with the &apos;female worker is a...&apos;. We feel strongly about the importance in studying non-binary gender and <lb/>in ensuring the field of machine learning and AI does not diminish the visibility of non-binary gender identities. In future <lb/>work, we hope to extend our analysis with the same data collection pipeline. For example, womxn is a umbrella term used in <lb/>the intersectional feminist community to be inclusive of transgender woman and non-binary individuals. The sentences <lb/>returned when prompting GPT-2 with &apos;womxn&apos; are primarily of two types: (i) stereotypical job associations e.g. &apos;drag <lb/>queen&apos;, &apos;feminist&apos;, &apos;crossdresser&apos; or &apos;nurse&apos;, and (ii) not recognizing &apos;womxn&apos; as a person noun e.g. &apos;The womxn works as a <lb/>kind of a noodle shop&apos;, &apos;The womxn works as a battery&apos;, &apos;The womxn works as a mauve-wool hat&apos; or &apos;The womxn works as <lb/>a kind of virtual sex toy&apos;. These preliminary findings suggest it is critical for future work to study occupational biases with <lb/>non-binary gender identities in generative language models. <lb/>A. GPT Model Downloads <lb/>We select the most downloaded version of GPT-2 available on HuggingFace as a proxy for popularity in use-cases by <lb/>experts and non-experts alike. In Tab. 5 we show the original GPT-2 models released by OpenAI (Radford et al., 2019) <lb/>and available on HuggingFace. The small version has an order of magnitude more downloads as compared to the medium <lb/>and XL versions. Further, larger models of GPT-2 have been shown to have an increased capability to memorize training <lb/>information, introducing privacy concerns (Carlini, 2020). Finally, while the environment cost of inference is cheap, Bender <lb/>et al. (2021) discuss how the environmental impact of training scales with model size, and the associated consequences <lb/>likely disproportionately affect marginalized populations. <lb/>Table 5. GPT-2 model available on Huggingface by number by total downloads (accessed 3rd February 2021) <lb/>Model <lb/># Hyperparameters # Public Downloads <lb/>GPT-2 Small <lb/>124M <lb/>623k <lb/>GPT-2 Medium <lb/>355M <lb/>77k <lb/>GPT-2 Large <lb/>774M <lb/>14k <lb/>GPT2 XL <lb/>1.5B <lb/>70k <lb/>B. Comparison with XLNet <lb/>XLNet sample generation. In addition to the suite of models released by Open-AI, XLNet is a generalized autoregressive <lb/>pre-training method which outperforms BERT across a number of benchmark tasks (Yang et al., 2019). To assess <lb/>the generalizability of our findings, we generate 7,000 sentences for the gender-occupation template (X = {}, Y = <lb/>{Man, Woman}), and analyze the returned occupational tokens from XLNet. Out of the total 14,000 returned sentences, <lb/>4,442 had no title recognized by the Stanford NLP Named Entity Recognizer. This sample loss of 31% is higher than GPT-2 <lb/>(Tab. 6). A plausible reason for this higher sample loss is in the way XLNet generates text which includes extra inverted <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">commas. <lb/>Table 6. Sample loss from sentences with no detected job title <lb/>Model <lb/>Template <lb/>Missing Titles Sample Loss <lb/>GPT-2 <lb/>Gender-occupation <lb/>20,689 <lb/>10.6% <lb/>GPT-2 <lb/>Names-occupation <lb/>39,203 <lb/>19.6% <lb/>XLNET Gender-occupation <lb/>4,442 <lb/>31.7% <lb/>Distributional Analysis. Fig. 10 shows the rank of jobs against the cumulative share. While 11 jobs account for 50% of the <lb/>outputs for men, only 5 jobs account for the same share for women. Similarly, considering 90% of the output, women are <lb/>associated with fewer jobs than men (31 vs 46, respectively). This disparity is similar to the one that we found in GPT-2, <lb/>suggesting that XLNet also predicts a wider variety of jobs for men and a narrower set of jobs for women. <lb/>Table 7. XLNet: Top five jobs for base man and base woman <lb/>XLNet Jobs (Proportions) <lb/>Sum <lb/>Woman maid (0.27), waitress (0.14), prostitute (0.05), servant (0.04), nurse (0.04) <lb/>0.54 <lb/>Man <lb/>carpenter (0.11), mechanic (0.07), maid (0.05), waiter (0.05), taxi driver (0.04) 0.32 <lb/>Top occupations. Tab. 7 shows the top five jobs for men and women as predicted by XLNet. Similar to our observations for <lb/>gender differences predicted by GPT-2, we see a higher cumulative share in the top jobs for women as compared to men. <lb/>The top job for woman (maid at 27%) represents a substantially larger proportion than the top job for man (carpenter at <lb/>11%). Interestingly, men are predicted to be maids 5% of the time, which was a pattern that we did not see with GPT-2. <lb/>Fig. 11 shows the proportion of genders in all jobs mentioned more than 35 times for baseline man and woman. This is the <lb/>same threshold as the one we used to calculate the analogous gender parity graph for GPT-2 jobs. Men and woman are <lb/>associated with stereotypical jobs, but slightly different ones than those predicted by GPT-2. In this case, we see that men <lb/>are associated with a variety of jobs, such as courier, barber, teller, magician, and builder. Women are, yet again, associated <lb/>with domestic and care-giving jobs, such as nanny, housewife, and nurse. Women are also highly associated with jobs such <lb/>as gardener, bartender, secretary, and prostitute. <lb/>10 0 <lb/>10 1 <lb/>10 2 <lb/>Log(Rank) <lb/>0.00 <lb/>0.05 <lb/>0.10 <lb/>0.15 <lb/>0.20 <lb/>0.25 <lb/>Share of Total <lb/>11 jobs account for 50% of men <lb/>5 jobs account for 50% of women <lb/>46 jobs account for 90% of men <lb/>31 jobs account for 90% of women <lb/>base_W <lb/>base_M <lb/>Figure 10. XLNet: Occupational distribution for men and women (baseline case). As with GPT-2, the job titles predicted by XLNet <lb/>are less diverse and more stereotypical for women than for men. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">courier <lb/>barber <lb/>teller <lb/>contractor <lb/>magician <lb/>builder <lb/>printer <lb/>plumber <lb/>tailor <lb/>carpenter <lb/>driver <lb/>trader <lb/>painter <lb/>weaver <lb/>hunter <lb/>cutter <lb/>taxi driver <lb/>salesman <lb/>butcher <lb/>mechanic <lb/>baker <lb/>merchant <lb/>doctor <lb/>dresser <lb/>factory worker <lb/>messenger <lb/>waiter <lb/>manager <lb/>laborer <lb/>shopkeeper <lb/>cashier <lb/>teacher <lb/>shoemaker <lb/>owner <lb/>clerk <lb/>therapist <lb/>vendor <lb/>janitor <lb/>servant <lb/>cook <lb/>housekeeper <lb/>dressmaker <lb/>maid <lb/>assistant <lb/>cleaner <lb/>prostitute <lb/>nurse <lb/>secretary <lb/>waitress <lb/>housewife <lb/>bartender <lb/>gardener <lb/>nanny <lb/>100% Men <lb/>Gender Parity <lb/>100% Women <lb/>Male-dominated jobs <lb/>Female-dominated jobs <lb/>Figure 11. XLNet: gender proportions when querying for the base case, i.e. X = {}, Y = {Man, Woman} and show all jobs with <lb/>greater than 35 = n * 0.25% mentions, making up 65% of returned valid responses. <lb/>C. Processing <lb/>C.1. Named Entity Recognition <lb/>We used Stanford CoreNLP Named Entity Recognition (NER) to extract job titles from the sentences generated by GPT-2. <lb/>Using this approach resulted in the sample loss of 10.6% for gender-occupation sentences and 19.6% for name-occupation <lb/>sentences (see Tab. 6). The sample loss was due to Stanford CoreNLP NER not recognizing some job titles e.g. &quot;Karima <lb/>works as a consultant-development worker&quot;, &quot;The man works as a volunteer&quot;, or &quot;The man works as a maintenance man at a <lb/>local...&quot;. <lb/>C.2. Adjustment Factors <lb/>When comparing to the US data, some adjustments are made to ensure fair comparison. Firstly, there are no breakdowns <lb/>by gender and ethnicity in the US Labor Bureau data so we assume the proportion of women are equal across ethnicities. <lb/>Secondly, for each gender-ethnicity pair, we generate the same number of sentence prompts per pair (n = 7,000). This <lb/>implies the &apos;predicted&apos; labor force has equal representation across groups which is not the case in reality. Accordingly, the <lb/>predicted proportions are scaled by the true distribution of gender and ethnicity reported in the US Labor Statistics. The <lb/>scaling factor is: γ(c) = G(c)E(c) <lb/>D(c) , where G(c), E(c) are the gender-and ethnicity-shares of the US data, respectively and <lb/>D(c) = 12.5% is our artificial &quot;population&quot;-share: <lb/>adj. Pred(i, c) = γ(c) × Pred(i, c), <lb/>(3) <lb/>where Pred(i) is the share of job i for characteristics c. Tab. 8 shows the true proportions and the steps made in the adjustment <lb/>process. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">Table 8. Adjustment calculations. <lb/>US Eth. US Gender G-E. Distr. GPT Distr. Correction <lb/>(E) <lb/>(G) <lb/>(D = G * E) <lb/>( D) <lb/>(γ) <lb/>Man <lb/>NA <lb/>0.530 <lb/>0.530 <lb/>0.500 <lb/>1.060 <lb/>Woman <lb/>NA <lb/>0.470 <lb/>0.470 <lb/>0.500 <lb/>0.940 <lb/>Asian Man <lb/>0.065 <lb/>0.530 <lb/>0.034 <lb/>0.125 <lb/>0.276 <lb/>Asian Woman <lb/>0.065 <lb/>0.470 <lb/>0.031 <lb/>0.125 <lb/>0.244 <lb/>Black Man <lb/>0.123 <lb/>0.530 <lb/>0.065 <lb/>0.125 <lb/>0.522 <lb/>Black Woman <lb/>0.123 <lb/>0.470 <lb/>0.058 <lb/>0.125 <lb/>0.462 <lb/>Hispanic Man <lb/>0.176 <lb/>0.530 <lb/>0.093 <lb/>0.125 <lb/>0.746 <lb/>Hispanic Woman 0.176 <lb/>0.470 <lb/>0.083 <lb/>0.125 <lb/>0.662 <lb/>White Man <lb/>0.777 <lb/>0.530 <lb/>0.412 <lb/>0.125 <lb/>3.294 <lb/>White Woman <lb/>0.777 <lb/>0.470 <lb/>0.365 <lb/>0.125 <lb/>2.922 <lb/>C.3. Matching GPT-2 and US Jobs <lb/>The US data has four nested levels of disaggregation e.g. Management, professional, and related occupations → Professional <lb/>and related occupations → Computer and mathematical occupations → Computer Programmer. For GPT-2&apos;s 50 most <lb/>frequently mentioned jobs, we match the GPT-2 job title to one in the US data at the lowest nested level, apart from <lb/>&apos;salesperson&apos; and &apos;manager&apos; which are too general to match to the lowest disaggregation. For these, we match to &apos;sales and <lb/>related occupations&apos;, and &apos;management occupations&apos;, respectively. In total, we find correspondences for 41/50 jobs. Jobs <lb/>were not matched for three reasons: (i) there were too many varied mentions of a job e.g. &apos;clerk&apos; was associated with 25 <lb/>different jobs spanning finance, law and hospitality sectors, (ii) there was no match for a job e.g. &apos;prostitute&apos; and &apos;translator&apos;, <lb/>(iii) the jobs were inherently gendered e.g. &apos;waitress&apos; and &apos;salesman&apos;. There are two further considerations in matching. First, <lb/>when a GPT-2 job is less general than the US categories. For example, while GPT-2 gave separate predictions for taxi drivers <lb/>and chauffeurs, the US data only reports &apos;taxi drivers and chauffeurs&apos;. Similarly, while GPT-2 gives separate predictions for <lb/>maids, housekeepers and cleaners, the US category amalgamates these into &apos;maids and housekeeping cleaners&apos;. For these <lb/>cases, we average across GPT-2&apos;s predictions for the relevant jobs, i.e. combining the predictions for maid, housekeeper and <lb/>cleaner. Second, when GPT-2&apos;s predictions are more general than the US categories. For example, when GPT-2 returns the <lb/>token of &apos;teacher&apos; but the US data reports &apos;postsecondary teachers, &apos;preschool and kindergarten teachers&apos;, etc. For these <lb/>cases, we sum across the US sub-categories. Tab. 9 gives details on these matches. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">Table 9. Job matches between GPT-2 predicted jobs and US data. <lb/>GPT <lb/>US DATA <lb/>babysitter <lb/>Childcare workers <lb/>secretary / assistant <lb/>Secretaries and administrative assistants <lb/>receptionist <lb/>Receptionists and information clerks <lb/>cleaner / housekeeper / <lb/>maid <lb/>Maids and housekeeping cleaners <lb/>nurse <lb/>Registered nurses <lb/>social worker <lb/>Social workers <lb/>teacher <lb/>Postsecondary teachers, Preschool and kindergarten teachers, Elementary and middle school teachers, <lb/>Special education teachers <lb/>model <lb/>Models, demonstrators, and product promoters <lb/>writer <lb/>Writers and authors <lb/>barista <lb/>Counter attendants, cafeteria, food concession, and coffee shop <lb/>bartender <lb/>Bartenders <lb/>photographer <lb/>Photographers <lb/>bus driver <lb/>Bus drivers <lb/>reporter / journalist <lb/>News analysts, reporters and correspondents <lb/>cook <lb/>Cooks <lb/>doctor <lb/>Physicians and surgeons <lb/>manager <lb/>Management occupations <lb/>janitor <lb/>Janitors and building cleaners <lb/>lawyer <lb/>Lawyers <lb/>barber <lb/>Barbers <lb/>chef <lb/>Chefs and head cooks <lb/>guard / security guard <lb/>/ bouncer <lb/>Security guards and gaming surveillance officers <lb/>courier <lb/>Couriers and messengers <lb/>computer programmer Computer programmers <lb/>police officer <lb/>Police and sheriff&apos;s patrol officers <lb/>taxi driver / chauffeur / <lb/>driver <lb/>Taxi drivers and chauffeurs <lb/>truck driver <lb/>Driver/sales workers and truck drivers <lb/>construction worker / <lb/>laborer <lb/>Construction laborers <lb/>carpenter <lb/>Carpenters <lb/>plumber <lb/>Pipelayers, plumbers, pipefitters, and steamfitters <lb/>mechanic <lb/>Automotive service technicians and mechanics <lb/>salesperson <lb/>Sales and related occupations <lb/>EXCLUDED JOBS <lb/>clerk <lb/>Too many sub-categories <lb/>technician <lb/>Too many sub-categories <lb/>consultant <lb/>No entry <lb/>contractor <lb/>No entry <lb/>prostitute <lb/>No entry <lb/>translator <lb/>No entry <lb/>salesman <lb/>Gendered title <lb/>waitress <lb/>Gendered title <lb/>waiter <lb/>Gendered title <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">D. Regression Analysis <lb/>D.1. Percentage of Significant Coefficients <lb/>Tab. 10 shows the percentage of significant coefficients for each intersection. To produce these results, we run regressions <lb/>for all jobs mentioned more times than the same threshold values used in the paper. Each regression includes all main effects <lb/>and interaction terms. We then compute the percentage of significant coefficients for each term across all regressions with <lb/>baseline man as the reference group. We repeat these steps for each intersection: ethnicity, religion, sexuality and political <lb/>affiliation. We did not run regression for continent name origin because there was no suitable baseline category given every <lb/>first name has geographic and gender associations. <lb/>Considering religion, the Buddhist term has the higher percentage significance across all regressions (78%), while the <lb/>Hindu term has the lowest (55%). This supports the findings in the paper that some religions are stronger determinants <lb/>of jobs than others. Of the interaction terms, woman:buddhist is the least significant (19%). This finding suggests that <lb/>male jobs are more highly determined by Buddhist membership, but female jobs are less strongly associated with this <lb/>affiliation. Considering ethnicity, the Hispanic term is most commonly significant (64%), while the Asian term is less <lb/>commonly significant (42%). The interactions for Hispanic and Black women are more frequently significant than those <lb/>for White and Asian women. This finding suggests some ethnicity-gender pairs more saliently affect GPT-2&apos;s priors on <lb/>job associations. Considering sexuality, both sexuality categories (gay/straight) are significant in approximately 50% of <lb/>regressions. A woman&apos;s intersectional association with being lesbian is more commonly significant than an association with <lb/>being straight. Considering political affiliation, the liberal term is more commonly significant than the conservative term, <lb/>and the same pattern apply to gender-political interaction terms. <lb/>Finally, we can compare the average significance of categories, gender and their intersections across religion, ethnicity, <lb/>sexuality and political regressions. Religion main effects are on average significant in 66% of regressions, ethnicity main <lb/>effects in 53% of regressions, sexuality main effects in 48% of regressions and political main effects in 60% of regressions. <lb/>This suggests for men, there is higher across-religion variation in predicted jobs than say for across-sexuality variation. <lb/>The woman dummy is significant in 61% of religion regressions, in 71% of ethnicity regressions, in 61% of sexuality <lb/>regressions and in 59% of political regressions. This finding demonstrates the woman and man variation is more influential <lb/>in distinguishing between job affiliations for ethnicity and least influential for political affiliation. Across all regressions, the <lb/>woman dummy is highly significant suggesting gender is an important determinant of job predictions. Finally, the interaction <lb/>terms are significant in 26% of religion regressions, in 30% of ethnicity regressions, in 31% of sexuality regressions and in <lb/>27% of political regressions. This suggests for women, sexuality and ethnicity are stronger determinants of job associations. <lb/>Interaction terms are significant in approximately one-third of regressions, while the woman dummy is significant in <lb/>approximately two-thirds of regressions. This finding suggests, while intersectionality is an relevant determinant of predicted <lb/>job, gender more strongly influences GPT-2&apos;s priors over occupational associations. <lb/>Table 10. Percentage of significant coefficients in logistic regressions by intersection <lb/>RELIGION <lb/>ETHNICITY <lb/>SEXUALITY <lb/>POLITICAL <lb/>Intercept <lb/>0.94 Intercept <lb/>0.95 <lb/>Intercept <lb/>0.90 Intercept <lb/>0.92 <lb/>buddhist <lb/>0.78 asian <lb/>0.42 <lb/>gay <lb/>0.51 conservative <lb/>0.55 <lb/>christian <lb/>0.69 black <lb/>0.55 <lb/>straight <lb/>0.44 liberal <lb/>0.66 <lb/>hindu <lb/>0.55 hispanic <lb/>0.64 <lb/>woman <lb/>0.61 woman <lb/>0.59 <lb/>jewish <lb/>0.66 white <lb/>0.49 <lb/>woman:lesbian <lb/>0.35 woman:conservative <lb/>0.24 <lb/>muslim <lb/>0.64 woman <lb/>0.71 <lb/>woman:straight <lb/>0.26 woman:liberal <lb/>0.30 <lb/>woman <lb/>0.61 woman:asian <lb/>0.29 <lb/>woman:buddhist <lb/>0.19 woman:black <lb/>0.36 <lb/>woman:christian <lb/>0.27 woman:hispanic <lb/>0.38 <lb/>woman:hindu <lb/>0.27 woman:white <lb/>0.16 <lb/>woman:jewish <lb/>0.33 <lb/>woman:muslim <lb/>0.25 <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">D.2. All Regression Results <lb/>Fig. 12 presents the significant p-values in all regressions for main effects and interaction terms. Significant p-values <lb/>(p &lt; 0.05) are shaded in black, while non-significant terms are left as white. Considering for example ethnicity, there are <lb/>two axes of variation. First, some jobs have significant p-values across all terms such as supervisor and teacher, indicating <lb/>these jobs are highly segmented by gender and by ethnicity, but also by their interaction. Jobs with no significant p-values <lb/>represents cases where the model did not converge which occurred when there was insufficient variation across different <lb/>demographics. In Fig. 13, we present the direction and magnitude of significant coefficients. Any negative coefficients, <lb/>i.e. those that make the job prediction less likely, are shaded in red. Any positive coefficients, i.e. those that make the <lb/>job association more likely, are shaded in blue. Any insignificant coefficients (p &gt; 0.05) are left as white. A darker color <lb/>indicates a larger strength of coefficient. We present all the results so an interested reader can select a certain job and find <lb/>the associated coefficients for gender and intersections, alongside their interaction terms. <lb/>Finally, Fig. 14 presents the change in Pseudo-R 2 for all job regressions across ethnicity when the woman dummy is added <lb/>and when the interaction terms are added. To produce these results, we first run a regression with all the main effects of <lb/>categorical membership e.g. (&apos;Asian&apos;, &apos;Black&apos;, &apos;Hispanic&apos;, &apos;White&apos;) but without the woman dummy. Given baseline &apos;man&apos; <lb/>is the reference group, all gender variation resides in the intercept. Next, we re-add the woman dummy, and observe how <lb/>the model fit improves. Finally, we run a regression with all main effects and all interaction terms and see what additional <lb/>variation is explained. The general pattern observed is that the woman dummy has a greater effect on the model fit than the <lb/>interactions. This finding suggests that while interaction terms for intersectional associations are significant in approximately <lb/>one-third of job regressions, they explain a lower proportion of variation than gender. Once again, there is considerable <lb/>variation by job and by intersection, so for detailed insights we invite readers to examine particular occupation-demographic <lb/>patterns. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">guard <lb/>monk <lb/>supervisor <lb/>massage_therapist <lb/>technician <lb/>prostitute <lb/>secretary <lb/>laborer <lb/>caretaker <lb/>translator <lb/>plumber <lb/>police_officer <lb/>truck_driver <lb/>maid <lb/>housewife <lb/>model <lb/>courier <lb/>salesman <lb/>contractor <lb/>sales_clerk <lb/>barber <lb/>cleaner <lb/>social_worker <lb/>mechanic <lb/>driver <lb/>reporter <lb/>photographer <lb/>nurse <lb/>waitress <lb/>construction_worker <lb/>receptionist <lb/>servant <lb/>editor <lb/>waiter <lb/>chauffeur <lb/>clerk <lb/>journalist <lb/>consultant <lb/>writer <lb/>taxi_driver <lb/>assistant <lb/>bus_driver <lb/>security_guard <lb/>bartender <lb/>carpenter <lb/>janitor <lb/>sales_representative <lb/>doctor <lb/>teacher <lb/>housekeeper <lb/>manager <lb/>barista <lb/>chef <lb/>lawyer <lb/>cook <lb/>Intercept <lb/>asian <lb/>black <lb/>hispanic <lb/>white <lb/>woman <lb/>woman:asian <lb/>woman:black <lb/>woman:hispanic <lb/>woman:white <lb/>ETHNICITY <lb/>pastor <lb/>monk <lb/>rabbi <lb/>technician <lb/>guide <lb/>priest <lb/>shepherd <lb/>missionary <lb/>massage_therapist <lb/>housewife <lb/>farmer <lb/>painter <lb/>baker <lb/>laborer <lb/>tailor <lb/>contractor <lb/>photographer <lb/>reporter <lb/>shopkeeper <lb/>gardener <lb/>nurse <lb/>courier <lb/>bouncer <lb/>construction_worker <lb/>salesman <lb/>servant <lb/>plumber <lb/>truck_driver <lb/>mechanic <lb/>maid <lb/>secretary <lb/>translator <lb/>cleaner <lb/>security_guard <lb/>writer <lb/>receptionist <lb/>sales_representative <lb/>porter <lb/>paralegal <lb/>editor <lb/>bus_driver <lb/>consultant <lb/>taxi_driver <lb/>waitress <lb/>driver <lb/>barber <lb/>doctor <lb/>librarian <lb/>assistant <lb/>waiter <lb/>bartender <lb/>teacher <lb/>clerk <lb/>police_officer <lb/>journalist <lb/>chauffeur <lb/>carpenter <lb/>barista <lb/>janitor <lb/>manager <lb/>chef <lb/>lawyer <lb/>cook <lb/>housekeeper <lb/>Intercept <lb/>buddhist <lb/>christian <lb/>hindu <lb/>jewish <lb/>muslim <lb/>woman <lb/>woman:buddhist <lb/>woman:christian <lb/>woman:hindu <lb/>woman:jewish <lb/>woman:muslim <lb/>RELIGION <lb/>detective <lb/>therapist <lb/>guard <lb/>coach <lb/>graphic_designer <lb/>developer <lb/>stand_in <lb/>software_engineer <lb/>counselor <lb/>bodyguard <lb/>babysitter <lb/>massage_therapist <lb/>monk <lb/>plumber <lb/>contractor <lb/>private_investigator <lb/>caretaker <lb/>construction_worker <lb/>courier <lb/>model <lb/>technician <lb/>priest <lb/>computer_programmer <lb/>truck_driver <lb/>cleaner <lb/>housewife <lb/>supervisor <lb/>mechanic <lb/>bus_driver <lb/>salesman <lb/>maid <lb/>driver <lb/>police_officer <lb/>bouncer <lb/>laborer <lb/>nurse <lb/>security_guard <lb/>waitress <lb/>librarian <lb/>chauffeur <lb/>taxi_driver <lb/>clerk <lb/>assistant <lb/>sales_clerk <lb/>journalist <lb/>waiter <lb/>barber <lb/>servant <lb/>secretary <lb/>receptionist <lb/>house_painter <lb/>carpenter <lb/>photographer <lb/>barista <lb/>reporter <lb/>doctor <lb/>janitor <lb/>director <lb/>manager <lb/>translator <lb/>teacher <lb/>producer <lb/>editor <lb/>bartender <lb/>writer <lb/>consultant <lb/>artist <lb/>chef <lb/>sales_representative <lb/>housekeeper <lb/>cook <lb/>lawyer <lb/>Intercept <lb/>gay <lb/>straight <lb/>woman <lb/>woman:gay <lb/>woman:straight <lb/>SEXUALITY <lb/>plumber <lb/>courier <lb/>banker <lb/>host <lb/>supervisor <lb/>driver <lb/>critic <lb/>lobbyist <lb/>maid <lb/>technician <lb/>mechanic <lb/>barber <lb/>massage_therapist <lb/>construction_worker <lb/>housewife <lb/>counselor <lb/>taxi_driver <lb/>truck_driver <lb/>personal_trainer <lb/>activist <lb/>producer <lb/>journalist <lb/>prostitute <lb/>financial_adviser <lb/>director <lb/>contractor <lb/>nurse <lb/>salesman <lb/>bouncer <lb/>translator <lb/>model <lb/>police_officer <lb/>private_detective <lb/>editor <lb/>private_investigator <lb/>security_guard <lb/>secretary <lb/>developer <lb/>executive <lb/>social_worker <lb/>waitress <lb/>artist <lb/>sales_manager <lb/>analyst <lb/>carpenter <lb/>receptionist <lb/>software_engineer <lb/>assistant <lb/>professor <lb/>lawyer <lb/>president <lb/>specialist <lb/>student <lb/>writer <lb/>servant <lb/>reporter <lb/>barista <lb/>cook <lb/>clerk <lb/>waiter <lb/>computer_programmer <lb/>photographer <lb/>consultant <lb/>manager <lb/>janitor <lb/>teacher <lb/>doctor <lb/>housekeeper <lb/>chef <lb/>sales_representative <lb/>bartender <lb/>Intercept <lb/>conservative <lb/>liberal <lb/>woman <lb/>woman:conservative <lb/>woman:liberal <lb/>POLITICAL <lb/>Figure 12. Significant p-values (p &lt; 0.05 for job regressions: significant (black), non-significant (white) <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">guard <lb/>monk <lb/>supervisor <lb/>massage_therapist <lb/>technician <lb/>prostitute <lb/>secretary <lb/>laborer <lb/>caretaker <lb/>translator <lb/>plumber <lb/>police_officer <lb/>truck_driver <lb/>maid <lb/>housewife <lb/>model <lb/>courier <lb/>salesman <lb/>contractor <lb/>sales_clerk <lb/>barber <lb/>cleaner <lb/>social_worker <lb/>mechanic <lb/>driver <lb/>reporter <lb/>photographer <lb/>nurse <lb/>waitress <lb/>construction_worker <lb/>receptionist <lb/>servant <lb/>editor <lb/>waiter <lb/>chauffeur <lb/>clerk <lb/>journalist <lb/>consultant <lb/>writer <lb/>taxi_driver <lb/>assistant <lb/>bus_driver <lb/>security_guard <lb/>bartender <lb/>carpenter <lb/>janitor <lb/>sales_representative <lb/>doctor <lb/>teacher <lb/>housekeeper <lb/>manager <lb/>barista <lb/>chef <lb/>lawyer <lb/>cook <lb/>Intercept <lb/>asian <lb/>black <lb/>hispanic <lb/>white <lb/>woman <lb/>woman:asian <lb/>woman:black <lb/>woman:hispanic <lb/>woman:white <lb/>ETHNICITY <lb/>pastor <lb/>monk <lb/>rabbi <lb/>technician <lb/>guide <lb/>priest <lb/>shepherd <lb/>missionary <lb/>massage_therapist <lb/>housewife <lb/>farmer <lb/>painter <lb/>baker <lb/>laborer <lb/>tailor <lb/>contractor <lb/>photographer <lb/>reporter <lb/>shopkeeper <lb/>gardener <lb/>nurse <lb/>courier <lb/>bouncer <lb/>construction_worker <lb/>salesman <lb/>servant <lb/>plumber <lb/>truck_driver <lb/>mechanic <lb/>maid <lb/>secretary <lb/>translator <lb/>cleaner <lb/>security_guard <lb/>writer <lb/>receptionist <lb/>sales_representative <lb/>porter <lb/>paralegal <lb/>editor <lb/>bus_driver <lb/>consultant <lb/>taxi_driver <lb/>waitress <lb/>driver <lb/>barber <lb/>doctor <lb/>librarian <lb/>assistant <lb/>waiter <lb/>bartender <lb/>teacher <lb/>clerk <lb/>police_officer <lb/>journalist <lb/>chauffeur <lb/>carpenter <lb/>barista <lb/>janitor <lb/>manager <lb/>chef <lb/>lawyer <lb/>cook <lb/>housekeeper <lb/>Intercept <lb/>buddhist <lb/>christian <lb/>hindu <lb/>jewish <lb/>muslim <lb/>woman <lb/>woman:buddhist <lb/>woman:christian <lb/>woman:hindu <lb/>woman:jewish <lb/>woman:muslim <lb/>RELIGION <lb/>detective <lb/>therapist <lb/>guard <lb/>coach <lb/>graphic_designer <lb/>developer <lb/>stand_in <lb/>software_engineer <lb/>counselor <lb/>bodyguard <lb/>babysitter <lb/>massage_therapist <lb/>monk <lb/>plumber <lb/>contractor <lb/>private_investigator <lb/>caretaker <lb/>construction_worker <lb/>courier <lb/>model <lb/>technician <lb/>priest <lb/>computer_programmer <lb/>truck_driver <lb/>cleaner <lb/>housewife <lb/>supervisor <lb/>mechanic <lb/>bus_driver <lb/>salesman <lb/>maid <lb/>driver <lb/>police_officer <lb/>bouncer <lb/>laborer <lb/>nurse <lb/>security_guard <lb/>waitress <lb/>librarian <lb/>chauffeur <lb/>taxi_driver <lb/>clerk <lb/>assistant <lb/>sales_clerk <lb/>journalist <lb/>waiter <lb/>barber <lb/>servant <lb/>secretary <lb/>receptionist <lb/>house_painter <lb/>carpenter <lb/>photographer <lb/>barista <lb/>reporter <lb/>doctor <lb/>janitor <lb/>director <lb/>manager <lb/>translator <lb/>teacher <lb/>producer <lb/>editor <lb/>bartender <lb/>writer <lb/>consultant <lb/>artist <lb/>chef <lb/>sales_representative <lb/>housekeeper <lb/>cook <lb/>lawyer <lb/>Intercept <lb/>gay <lb/>straight <lb/>woman <lb/>woman:gay <lb/>woman:straight <lb/>SEXUALITY <lb/>plumber <lb/>courier <lb/>banker <lb/>host <lb/>supervisor <lb/>driver <lb/>critic <lb/>lobbyist <lb/>maid <lb/>technician <lb/>mechanic <lb/>barber <lb/>massage_therapist <lb/>construction_worker <lb/>housewife <lb/>counselor <lb/>taxi_driver <lb/>truck_driver <lb/>personal_trainer <lb/>activist <lb/>producer <lb/>journalist <lb/>prostitute <lb/>financial_adviser <lb/>director <lb/>contractor <lb/>nurse <lb/>salesman <lb/>bouncer <lb/>translator <lb/>model <lb/>police_officer <lb/>private_detective <lb/>editor <lb/>private_investigator <lb/>security_guard <lb/>secretary <lb/>developer <lb/>executive <lb/>social_worker <lb/>waitress <lb/>artist <lb/>sales_manager <lb/>analyst <lb/>carpenter <lb/>receptionist <lb/>software_engineer <lb/>assistant <lb/>professor <lb/>lawyer <lb/>president <lb/>specialist <lb/>student <lb/>writer <lb/>servant <lb/>reporter <lb/>barista <lb/>cook <lb/>clerk <lb/>waiter <lb/>computer_programmer <lb/>photographer <lb/>consultant <lb/>manager <lb/>janitor <lb/>teacher <lb/>doctor <lb/>housekeeper <lb/>chef <lb/>sales_representative <lb/>bartender <lb/>Intercept <lb/>conservative <lb/>liberal <lb/>woman <lb/>woman:conservative <lb/>woman:liberal <lb/>POLITICAL <lb/>8 <lb/>6 <lb/>4 <lb/>2 <lb/>0 <lb/>2 <lb/>4 <lb/>8 <lb/>6 <lb/>4 <lb/>2 <lb/>0 <lb/>2 <lb/>4 <lb/>8 <lb/>6 <lb/>4 <lb/>2 <lb/>0 <lb/>2 <lb/>4 <lb/>8 <lb/>6 <lb/>4 <lb/>2 <lb/>0 <lb/>2 <lb/>4 <lb/>Figure 13. Significant coefficients for job regressions: negative (red), positive (blue), and insignificant (white) <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">guard <lb/>supervisor <lb/>technician <lb/>prostitute <lb/>secretary <lb/>laborer <lb/>caretaker <lb/>translator <lb/>plumber <lb/>police_officer <lb/>truck_driver <lb/>maid <lb/>housewife <lb/>model <lb/>courier <lb/>salesman <lb/>contractor <lb/>sales_clerk <lb/>barber <lb/>cleaner <lb/>social_worker <lb/>mechanic <lb/>driver <lb/>reporter <lb/>photographer <lb/>nurse <lb/>waitress <lb/>construction_worker <lb/>receptionist <lb/>servant <lb/>editor <lb/>waiter <lb/>chauffeur <lb/>clerk <lb/>journalist <lb/>consultant <lb/>writer <lb/>taxi_driver <lb/>assistant <lb/>bus_driver <lb/>security_guard <lb/>bartender <lb/>carpenter <lb/>janitor <lb/>sales_representative <lb/>doctor <lb/>teacher <lb/>housekeeper <lb/>manager <lb/>barista <lb/>chef <lb/>lawyer <lb/>cook <lb/>Add Woman Dummy <lb/>Add Interactions <lb/>ETHNICITY <lb/>pastor <lb/>technician <lb/>guide <lb/>priest <lb/>shepherd <lb/>missionary <lb/>housewife <lb/>farmer <lb/>painter <lb/>baker <lb/>laborer <lb/>tailor <lb/>contractor <lb/>photographer <lb/>reporter <lb/>shopkeeper <lb/>gardener <lb/>nurse <lb/>courier <lb/>bouncer <lb/>construction_worker <lb/>salesman <lb/>servant <lb/>plumber <lb/>truck_driver <lb/>mechanic <lb/>maid <lb/>secretary <lb/>translator <lb/>cleaner <lb/>security_guard <lb/>writer <lb/>receptionist <lb/>sales_representative <lb/>porter <lb/>paralegal <lb/>editor <lb/>bus_driver <lb/>consultant <lb/>taxi_driver <lb/>waitress <lb/>driver <lb/>barber <lb/>doctor <lb/>librarian <lb/>assistant <lb/>waiter <lb/>bartender <lb/>teacher <lb/>clerk <lb/>police_officer <lb/>journalist <lb/>chauffeur <lb/>carpenter <lb/>barista <lb/>janitor <lb/>manager <lb/>chef <lb/>lawyer <lb/>cook <lb/>housekeeper <lb/>Add Woman Dummy <lb/>Add Interactions <lb/>RELIGION <lb/>detective <lb/>therapist <lb/>guard <lb/>coach <lb/>graphic_designer <lb/>developer <lb/>stand_in <lb/>counselor <lb/>bodyguard <lb/>babysitter <lb/>monk <lb/>plumber <lb/>contractor <lb/>private_investigator <lb/>caretaker <lb/>construction_worker <lb/>courier <lb/>model <lb/>technician <lb/>priest <lb/>computer_programmer <lb/>truck_driver <lb/>cleaner <lb/>housewife <lb/>supervisor <lb/>mechanic <lb/>bus_driver <lb/>salesman <lb/>maid <lb/>driver <lb/>police_officer <lb/>bouncer <lb/>laborer <lb/>nurse <lb/>security_guard <lb/>waitress <lb/>librarian <lb/>chauffeur <lb/>taxi_driver <lb/>clerk <lb/>assistant <lb/>sales_clerk <lb/>journalist <lb/>waiter <lb/>barber <lb/>servant <lb/>secretary <lb/>receptionist <lb/>house_painter <lb/>carpenter <lb/>photographer <lb/>barista <lb/>reporter <lb/>doctor <lb/>janitor <lb/>director <lb/>manager <lb/>translator <lb/>teacher <lb/>producer <lb/>editor <lb/>bartender <lb/>writer <lb/>consultant <lb/>artist <lb/>chef <lb/>sales_representative <lb/>housekeeper <lb/>cook <lb/>lawyer <lb/>Add Woman Dummy <lb/>Add Interactions <lb/>SEXUALITY <lb/>plumber <lb/>courier <lb/>banker <lb/>host <lb/>supervisor <lb/>driver <lb/>critic <lb/>lobbyist <lb/>maid <lb/>technician <lb/>mechanic <lb/>barber <lb/>massage_therapist <lb/>construction_worker <lb/>housewife <lb/>counselor <lb/>taxi_driver <lb/>truck_driver <lb/>personal_trainer <lb/>activist <lb/>producer <lb/>journalist <lb/>prostitute <lb/>financial_adviser <lb/>director <lb/>contractor <lb/>nurse <lb/>salesman <lb/>bouncer <lb/>translator <lb/>model <lb/>police_officer <lb/>private_detective <lb/>editor <lb/>private_investigator <lb/>security_guard <lb/>secretary <lb/>developer <lb/>executive <lb/>social_worker <lb/>waitress <lb/>artist <lb/>sales_manager <lb/>analyst <lb/>carpenter <lb/>receptionist <lb/>software_engineer <lb/>assistant <lb/>professor <lb/>lawyer <lb/>president <lb/>specialist <lb/>student <lb/>writer <lb/>servant <lb/>reporter <lb/>barista <lb/>cook <lb/>clerk <lb/>waiter <lb/>computer_programmer <lb/>photographer <lb/>consultant <lb/>manager <lb/>janitor <lb/>teacher <lb/>doctor <lb/>housekeeper <lb/>chef <lb/>sales_representative <lb/>bartender <lb/>Add Woman Dummy <lb/>Add Interactions <lb/>POLITICAL <lb/>2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>0 <lb/>2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>0 <lb/>2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>12 <lb/>Figure 14. Change in R 2 from addition of woman dummy and interaction terms for job regressions. The plots show that the <lb/>addition of woman has a greater effect on R 2 than the addition of interaction terms. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">E. Further Analysis for Intersectional Breakdowns <lb/>Distributional Analysis. Fig. 15 shows the distributional analysis for man and woman by intersection. The distributions for <lb/>ethnicity, religion, and sexuality intersections show job titles predicted by GPT-2 are less diverse and more stereotypical for <lb/>women than for men. For political intersections and for continent-based name intersections, the disparity is not as apparent. <lb/>For these latter two cases, the distribution of jobs predicted for men and women are more similar. <lb/>10 0 <lb/>10 1 <lb/>10 2 <lb/>10 3 <lb/>Log(Rank) <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Share of Total <lb/>base_W <lb/>base_M <lb/>10 0 <lb/>10 1 <lb/>10 2 <lb/>10 3 <lb/>Log(Rank) <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Share of Total <lb/>ethnicity_W <lb/>ethnicity_M <lb/>10 0 <lb/>10 1 <lb/>10 2 <lb/>10 3 <lb/>Log(Rank) <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Share of Total <lb/>religion_W <lb/>religion_M <lb/>10 0 <lb/>10 1 <lb/>10 2 <lb/>10 3 <lb/>Log(Rank) <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Share of Total <lb/>sexuality_W <lb/>sexuality_M <lb/>10 0 <lb/>10 1 <lb/>10 2 <lb/>10 3 <lb/>Log(Rank) <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Share of Total <lb/>political_W <lb/>political_M <lb/>10 0 <lb/>10 1 <lb/>10 2 <lb/>10 3 <lb/>Log(Rank) <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Share of Total <lb/>continent_W <lb/>continent_M <lb/>Figure 15. Occupational distribution for men and women by intersection. With the exception of the continent name origin intersection <lb/>(bottom-right), all the others intersections show that the job titles predicted by GPT-2 are less diverse and more stereotypical for women <lb/>than for men. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">Lorenz Curve Analysis. Fig. 16 shows the Lorenz Curve for men and women by intersection. With the exception of <lb/>intersections with continent-based names, women are concentrated in a smaller number of job titles as compared to men. <lb/>This can be seen clearly in Fig. 17, which zooms in on the interesting part of the curve (y = [0, 0.2]). We see that the largest <lb/>distributional difference is in the religion and sexuality intersections. This distributional difference is smaller for political <lb/>intersections, agreeing with our finding in the paper that political affiliation has less of an effect by gender in GPT-2&apos;s <lb/>occupational predictions. The curves for continent-based name intersections are nearly identical, suggesting that GPT-2 <lb/>predicts a distribution with less disparity when it is prompted with first names rather than an explicit intersection e.g. &apos;Black <lb/>woman&apos;/ &apos;Buddhist man&apos;. <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Jobs <lb/>base_W <lb/>base_M <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Jobs <lb/>ethnicity_W <lb/>ethnicity_M <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Jobs <lb/>religion_W <lb/>religion_M <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Jobs <lb/>sexuality_W <lb/>sexuality_M <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Jobs <lb/>political_W <lb/>political_M <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Jobs <lb/>continent_W <lb/>continent_M <lb/>Figure 16. Lorenz curve for men and women by intersection. For all intersections -except for continent-based names -the majority <lb/>of occupations for women are concentrated in a smaller number of job titles compared to men. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Cumulative Share of Jobs <lb/>base_W <lb/>base_M <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Cumulative Share of Jobs <lb/>ethnicity_W <lb/>ethnicity_M <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Cumulative Share of Jobs <lb/>religion_W <lb/>religion_M <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Cumulative Share of Jobs <lb/>sexuality_W <lb/>sexuality_M <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Cumulative Share of Jobs <lb/>political_W <lb/>political_M <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Cumulative Share of Total Workers <lb/>0.000 <lb/>0.025 <lb/>0.050 <lb/>0.075 <lb/>0.100 <lb/>0.125 <lb/>0.150 <lb/>0.175 <lb/>0.200 <lb/>Cumulative Share of Jobs <lb/>continent_W <lb/>continent_M <lb/>Figure 17. Focused lorenz curve (y = [0, 0.2]) for men and women by intersection. The largest distributional difference is in the <lb/>religion intersection, whereas the smallest is in the continent-based name origin. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">Occupations by intersections. In each of the stacked bar charts, we show the man-woman share of occupations for each <lb/>gender-intersection pair. In Fig. 18, the majority of jobs remain split across all four ethnicities. There are no jobs dominated <lb/>by a single ethnicity. In Fig. 19, the distribution of religion for each job is relatively equally distributed, with the exception <lb/>of a few jobs. For example, monks are composed mostly of Buddhist men and nuns are composed mostly of Buddhist <lb/>women, an observation noted in the paper. As expected, religious occupations tend to be more dominated by one or two <lb/>religions, while non-religious occupations are more evenly distributed across religions. <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>plumber <lb/>laborer <lb/>guard <lb/>salesman <lb/>mechanic <lb/>contractor <lb/>truck driver <lb/>courier <lb/>barber <lb/>driver <lb/>bouncer <lb/>police officer <lb/>clerk <lb/>waiter <lb/>security guard <lb/>technician <lb/>carpenter <lb/>taxi driver <lb/>construction worker <lb/>sales clerk <lb/>chauffeur <lb/>janitor <lb/>doctor <lb/>bartender <lb/>manager <lb/>photographer <lb/>chef <lb/>lawyer <lb/>bus driver <lb/>translator <lb/>reporter <lb/>journalist <lb/>consultant <lb/>barista <lb/>sales representative <lb/>editor <lb/>writer <lb/>cook <lb/>cleaner <lb/>housekeeper <lb/>assistant <lb/>teacher <lb/>receptionist <lb/>secretary <lb/>prostitute <lb/>maid <lb/>waitress <lb/>nurse <lb/>social worker <lb/>caretaker <lb/>model <lb/>babysitter <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Asian <lb/>Black <lb/>Hispanic <lb/>White <lb/>Figure 18. Man-woman share by ethnicity for all jobs with greater than 140 = n * 0.25% mentions, making up 82% of returned valid <lb/>responses. <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>farmer <lb/>plumber <lb/>shepherd <lb/>gardener <lb/>banker <lb/>butcher <lb/>monk <lb/>painter <lb/>truck driver <lb/>salesman <lb/>laborer <lb/>pastor <lb/>contractor <lb/>mechanic <lb/>construction worker <lb/>courier <lb/>priest <lb/>missionary <lb/>tailor <lb/>driver <lb/>waiter <lb/>carpenter <lb/>barber <lb/>baker <lb/>shopkeeper <lb/>bouncer <lb/>security guard <lb/>police officer <lb/>clerk <lb/>doctor <lb/>rabbi <lb/>porter <lb/>lawyer <lb/>journalist <lb/>taxi driver <lb/>janitor <lb/>servant <lb/>chef <lb/>writer <lb/>bus driver <lb/>bartender <lb/>manager <lb/>guide <lb/>translator <lb/>chauffeur <lb/>photographer <lb/>consultant <lb/>cook <lb/>housekeeper <lb/>librarian <lb/>secretary <lb/>teacher <lb/>paralegal <lb/>cleaner <lb/>barista <lb/>assistant <lb/>prostitute <lb/>housewife <lb/>nurse <lb/>receptionist <lb/>maid <lb/>waitress <lb/>nun <lb/>counselor <lb/>social worker <lb/>model <lb/>caretaker <lb/>babysitter <lb/>massage therapist <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Buddhist <lb/>Christian <lb/>Hindu <lb/>Jewish <lb/>Muslim <lb/>Figure 19. Man-woman share by religion for all jobs with greater than 175 = n * 0.25% mentions, making up 84% of returned valid <lb/>responses. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">In Fig. 20, there are number of jobs dominated by one sexuality. For example, occupations such as detective, plumber, and <lb/>guard are dominated by straight men, whereas occupations such as massage therapist, counsellor, and graphic designer <lb/>are dominated by lesbian women. Some more female jobs are associated with gay men such as social worker, prostitute <lb/>and housewife, but the overall share of men remains low. In Fig. 21, less jobs are dominated by one political affiliation, <lb/>especially at the extremes of the distribution, mirroring our observation seen in the Lorenz curves. However, there are a few <lb/>exceptions: occupations such as banker and critic are dominated by liberal men, driver and host by conservative men, barista <lb/>and translator by liberal women. Drivers are concentrated in conservative women, but the overall share of women is low. <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>detective <lb/>truck driver <lb/>plumber <lb/>guard <lb/>salesman <lb/>contractor <lb/>police officer <lb/>courier <lb/>mechanic <lb/>coach <lb/>driver <lb/>bouncer <lb/>barber <lb/>laborer <lb/>taxi driver <lb/>waiter <lb/>clerk <lb/>stand-in <lb/>carpenter <lb/>security guard <lb/>doctor <lb/>janitor <lb/>bartender <lb/>manager <lb/>chauffeur <lb/>lawyer <lb/>journalist <lb/>director <lb/>supervisor <lb/>chef <lb/>reporter <lb/>translator <lb/>photographer <lb/>producer <lb/>technician <lb/>consultant <lb/>barista <lb/>sales representative <lb/>librarian <lb/>housekeeper <lb/>cook <lb/>writer <lb/>editor <lb/>secretary <lb/>teacher <lb/>assistant <lb/>prostitute <lb/>receptionist <lb/>cleaner <lb/>housewife <lb/>social worker <lb/>model <lb/>waitress <lb/>graphic designer <lb/>nurse <lb/>counselor <lb/>maid <lb/>therapist <lb/>bodyguard <lb/>massage therapist <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Lesbian/Gay <lb/>Straight <lb/>Figure 20. Man-woman share by sexuality for all jobs with greater than 70 = n * 0.25% mentions, making up 83% of returned valid <lb/>responses. <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>police officer <lb/>truck driver <lb/>bouncer <lb/>lobbyist <lb/>salesman <lb/>banker <lb/>contractor <lb/>driver <lb/>mechanic <lb/>producer <lb/>host <lb/>director <lb/>waiter <lb/>servant <lb/>private investigator <lb/>clerk <lb/>developer <lb/>critic <lb/>computer programmer <lb/>journalist <lb/>lawyer <lb/>security guard <lb/>editor <lb/>carpenter <lb/>financial adviser <lb/>reporter <lb/>consultant <lb/>barista <lb/>activist <lb/>analyst <lb/>doctor <lb/>software engineer <lb/>janitor <lb/>photographer <lb/>writer <lb/>bartender <lb/>executive <lb/>manager <lb/>translator <lb/>chef <lb/>professor <lb/>sales representative <lb/>housekeeper <lb/>cook <lb/>teacher <lb/>secretary <lb/>assistant <lb/>social worker <lb/>receptionist <lb/>waitress <lb/>counselor <lb/>model <lb/>nurse <lb/>prostitute <lb/>maid <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Conservative <lb/>Liberal <lb/>Figure 21. Man-woman share by political affiliation for all jobs with greater than 70 = n * 0.25% mentions, making up 82% of <lb/>returned valid responses <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">Lastly, in Fig. 22, we see that there are no jobs dominated by one continent-based name origin and it seems that there is <lb/>less disparity in jobs as predicted by GPT-2 by gender. This agrees with the observations seen in the Lorenz curve. When <lb/>GPT-2 is prompted by first name, gender is a greater prediction of job titles rather than geographic origin of the name, but <lb/>the gender-split is still less stark than explicit &apos;man&apos;, &apos;woman&apos; prompts. <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Woman <lb/>salesman <lb/>mechanic <lb/>waiter <lb/>lecturer <lb/>contractor <lb/>commentator <lb/>analyst <lb/>developer <lb/>graphic designer <lb/>scientist <lb/>columnist <lb/>technician <lb/>author <lb/>engineer <lb/>professor <lb/>coach <lb/>strategist <lb/>private investigator <lb/>programmer <lb/>director <lb/>software engineer <lb/>consultant <lb/>filmmaker <lb/>producer <lb/>journalist <lb/>musician <lb/>lawyer <lb/>designer <lb/>computer programmer <lb/>editor <lb/>manager <lb/>writer <lb/>researcher <lb/>artist <lb/>police officer <lb/>security guard <lb/>photographer <lb/>illustrator <lb/>reporter <lb/>activist <lb/>specialist <lb/>sales representative <lb/>bartender <lb/>blogger <lb/>psychologist <lb/>translator <lb/>marketing manager <lb/>doctor <lb/>chef <lb/>janitor <lb/>teacher <lb/>assistant <lb/>leader <lb/>student <lb/>social worker <lb/>counselor <lb/>model <lb/>cook <lb/>nurse <lb/>waitress <lb/>therapist <lb/>receptionist <lb/>0.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Man <lb/>Africa <lb/>Americas <lb/>Asia <lb/>Europe <lb/>Oceania <lb/>Figure 22. Man-woman share by continent name-origin for all jobs with greater than 500 = n * 0.25% mentions, making up 76% of <lb/>returned valid responses <lb/>E.1. Most Frequent Jobs Per Gender-Intersection <lb/>Tab. 11 shows the top five jobs per intersectional category with associated proportions of the category total. In general, the <lb/>top five jobs for women of all intersections (except continent-based names) does not deviate too far from the top five jobs <lb/>predicted for the baseline woman case. In fact, the top job predicted for baseline women, which is waitress, is within the top <lb/>five predicted jobs for women of all intersections, at similar levels of proportions. <lb/>The top five jobs for men of all intersections (except continent-based names) has more variety from the top five jobs predicted <lb/>for the baseline man case. While security guard (the top job predicted for baseline men) is still one of the most common job <lb/>for men with all intersections, it is not included in the top job for some intersections (i.e. Buddhist man, Christian man, <lb/>Jewish man, liberal man). Of the religion intersections, only Hindu and Muslim men are predicted to be security guards, <lb/>raising the question of whether GPT-2 associates some religions differently with religion and non-religious occupations (i.e. <lb/>treats Muslim and Hindu men as different from Christian, Buddhist, and Jewish men). For political intersections, the job <lb/>distributions for liberal and conservative men vary more from distribution for baseline men, with interesting top jobs not <lb/>seen before like writer, journalist, consultant, and lawyer. <lb/>The exception to these patterns are jobs predicted for continent-based name origins. For jobs predicted by name, the top jobs <lb/>look similar across gender: writer, consultant, journalist, and lawyer. This finding suggests that if we do not prompt GPT-2 <lb/>with an explicit gender (man/woman), GPT-2 predicts a similar set of jobs for men and women. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">Table 11. Top five jobs per intersectional category with associated proportions of category total. <lb/>Woman Jobs <lb/>Man Jobs <lb/>Base <lb/>[waitress, nurse, maid, receptionist, teacher] <lb/>[security guard, manager, waiter, janitor, mechanic] <lb/> [0.14, 0.11, 0.06, 0.05, 0.05] <lb/>[0.08, 0.05, 0.04, 0.04, 0.03] <lb/>Ethnicity <lb/>Asian <lb/>[waitress, maid, nurse, teacher, receptionist] <lb/>[waiter, security guard, manager, janitor, chef] <lb/>[0.14, 0.11, 0.08, 0.05, 0.04] <lb/>[0.09, 0.07, 0.04, 0.04, 0.03] <lb/>Black <lb/>[waitress, nurse, maid, prostitute, teacher] <lb/>[security guard, waiter, bartender, janitor, mechanic] <lb/>[0.18, 0.1, 0.07, 0.05, 0.04] <lb/>[0.08, 0.07, 0.05, 0.05, 0.04] <lb/>Hispanic <lb/>[waitress, nurse, receptionist, maid, teacher] <lb/>[security guard, janitor, waiter, bartender, manager] <lb/>[0.16, 0.14, 0.07, 0.07, 0.04] <lb/>[0.09, 0.07, 0.07, 0.05, 0.05] <lb/>White <lb/>[waitress, nurse, maid, teacher, receptionist] <lb/>[waiter, security guard, janitor, mechanic, bartender] <lb/>[0.17, 0.11, 0.07, 0.05, 0.04] <lb/>[0.06, 0.06, 0.05, 0.04, 0.04] <lb/>Religion <lb/>Buddhist <lb/>[nurse, waitress, maid, teacher, cook] <lb/>[teacher, janitor, waiter, doctor, monk] <lb/>[0.12, 0.11, 0.09, 0.08, 0.04] <lb/>[0.06, 0.05, 0.05, 0.04, 0.04] <lb/>Christian <lb/>[waitress, nurse, maid, teacher, prostitute] <lb/>[clerk, doctor, waiter, janitor, teacher] <lb/>[0.13, 0.12, 0.1, 0.07, 0.06] <lb/>[0.06, 0.04, 0.04, 0.04, 0.04] <lb/>Hindu <lb/>[maid, waitress, nurse, teacher, cleaner] <lb/>[waiter, janitor, security guard, teacher, cleaner] <lb/>[0.18, 0.12, 0.06, 0.05, 0.05] <lb/>[0.09, 0.06, 0.04, 0.04, 0.03] <lb/>Jewish <lb/>[waitress, nurse, maid, teacher, prostitute] <lb/>[waiter, doctor, clerk, janitor, teacher] <lb/>[0.15, 0.1, 0.09, 0.06, 0.05] <lb/>[0.08, 0.05, 0.04, 0.04, 0.04] <lb/>Muslim <lb/>[waitress, maid, nurse, teacher, cook] <lb/>[waiter, security guard, janitor, taxi driver, mechanic] <lb/>[0.16, 0.14, 0.08, 0.05, 0.04] <lb/>[0.11, 0.06, 0.06, 0.05, 0.04] <lb/>Sexuality <lb/>Lesbian/Gay <lb/>[waitress, nurse, teacher, maid, receptionist] <lb/>[waiter, bartender, janitor, security guard, waitress] <lb/>[0.15, 0.12, 0.06, 0.06, 0.05] <lb/>[0.07, 0.06, 0.05, 0.05, 0.04] <lb/>Straight <lb/>[waitress, nurse, maid, teacher, receptionist] <lb/>[waiter, bartender, security guard, manager, clerk] <lb/>[0.19, 0.08, 0.07, 0.04, 0.04] <lb/>[0.06, 0.05, 0.04, 0.04, 0.04] <lb/>Political <lb/>Liberal <lb/>[waitress, nurse, writer, teacher, receptionist] <lb/>[writer, journalist, lawyer, consultant, waiter] <lb/>[0.12, 0.08, 0.07, 0.05, 0.05] <lb/>[0.1, 0.08, 0.08, 0.06, 0.05] <lb/>Conservative [waitress, nurse, receptionist, writer, consultant] <lb/>[consultant, lawyer, writer, security guard, reporter] <lb/>[0.13, 0.08, 0.06, 0.05, 0.05] <lb/>[0.09, 0.06, 0.05, 0.05, 0.05] <lb/>Continent <lb/>Africa <lb/>[writer, consultant, journalist, lawyer, teacher] <lb/>[writer, consultant, journalist, lawyer, translator] <lb/>[0.1, 0.08, 0.05, 0.04, 0.04] <lb/>[0.09, 0.08, 0.07, 0.05, 0.04] <lb/>Americas <lb/>[writer, consultant, journalist, lawyer, teacher] <lb/>[writer, consultant, journalist, lawyer, manager] <lb/>[0.1, 0.08, 0.05, 0.04, 0.04] <lb/>[0.1, 0.1, 0.06, 0.05, 0.04] <lb/>Asia <lb/>[writer, consultant, translator, journalist, teacher] <lb/>[consultant, writer, journalist, lawyer, translator] <lb/>[0.09, 0.06, 0.05, 0.05, 0.04] <lb/>[0.1, 0.09, 0.06, 0.04, 0.04] <lb/>Europe <lb/>[writer, consultant, journalist, nurse, teacher] <lb/>[writer, consultant, journalist, lawyer, producer] <lb/>[0.1, 0.07, 0.05, 0.05, 0.04] <lb/>[0.11, 0.1, 0.06, 0.04, 0.04] <lb/>Oceania <lb/>[writer, consultant, teacher, nurse, journalist] <lb/>[writer, consultant, journalist, teacher, lawyer] <lb/>[0.09, 0.07, 0.05, 0.04, 0.04] <lb/>[0.11, 0.08, 0.05, 0.04, 0.04] <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">F. Further Analysis for US Comparison <lb/>F.1. Gender Predictions <lb/>Fig. 23 plots the percentage of women for each occupation as predicted by GPT-2 and as observed in the US Labor Bureau <lb/>data. The bar plot shows the difference in predicted percentage and true percentage. We see that GPT-2 pulls the skewed <lb/>real-life distribution towards gender parity. For example, GPT-2 predicts there to be more women mechanics, carpenters, <lb/>taxi drivers, and police officers than there are in real life. Additionally, GPT-2 predicts there to be fewer women secretaries, <lb/>maids, nurses, and models than observed in reality. Both of these examples suggest that GPT-2 under-predicts the number of <lb/>women in heavily women-dominated jobs, and GPT-2 over-predicts the number of women in heavily men-dominated jobs. <lb/>This supports our finding in the paper: although it may seem initially biased that GPT-2 predicts so many women to be <lb/>secretaries and maids, the share of women within these occupations is actually higher in the US data. <lb/>0 <lb/>20 <lb/>40 <lb/>60 <lb/>80 <lb/>pct of women in occupation <lb/>babysitter <lb/>secretary / assistant <lb/>receptionist <lb/>cleaner / housekeeper / maid <lb/>nurse <lb/>social worker <lb/>teacher <lb/>model <lb/>writer <lb/>editor <lb/>barista <lb/>bartender <lb/>photographer <lb/>salesperson <lb/>bus driver <lb/>reporter / journalist <lb/>cook <lb/>doctor <lb/>manager <lb/>janitor <lb/>lawyer <lb/>barber <lb/>chef <lb/>security guard / bouncer <lb/>courier <lb/>computer programmer <lb/>police officer <lb/>taxi driver / chaffeur <lb/>truck driver <lb/>construction worker / laborer <lb/>carpenter <lb/>plumber <lb/>mechanic <lb/>GPT Pred <lb/>True <lb/>20 <lb/>0 <lb/>20 <lb/>pred pct -true pct <lb/>babysitter <lb/>secretary / assistant <lb/>receptionist <lb/>cleaner / housekeeper / maid <lb/>nurse <lb/>social worker <lb/>teacher <lb/>model <lb/>writer <lb/>editor <lb/>barista <lb/>bartender <lb/>photographer <lb/>salesperson <lb/>bus driver <lb/>reporter / journalist <lb/>cook <lb/>doctor <lb/>manager <lb/>janitor <lb/>lawyer <lb/>barber <lb/>chef <lb/>security guard / bouncer <lb/>courier <lb/>computer programmer <lb/>police officer <lb/>taxi driver / chaffeur <lb/>truck driver <lb/>construction worker / laborer <lb/>carpenter <lb/>plumber <lb/>mechanic <lb/>Figure 23. GPT-2 predictions versus US data by gender share. Difference in percentage of women predicted by GPT-2 and the <lb/>percentage of women in the 2019 US Labor Force Statistics data, per occupation. <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">F.2. Gender-Ethnicity Predictions <lb/>Fig. 24 presents the difference between US data and GPT-2&apos;s predicted proportions of gender-ethnicity pairs for the top 50 <lb/>most frequently mentioned jobs which aligned with US occupational categories. The jobs on the y-axis are sorted by the <lb/>true share of women in the US data. In line with the low mean-squared errors presented in the paper, GPT-2 accurately <lb/>predicts the gender-ethnicity split for a given job, especially for Asian and Black workers. For jobs with a wide gender <lb/>split, GPT-2 seems to corrects for societal skew. For example, it under-predicts the proportion of Hispanic women who are <lb/>cleaners, housekeepers and maids by 34% (percentage points). Similarly, it under-predicts the proportion of Black men <lb/>who are taxi drivers, chauffeurs or drivers, and the proportion of Hispanic men who are mechanics, plumbers, carpenters <lb/>and construction workers. The proportion of White workers is less accurately predicted but the same pattern is observed <lb/>towards under-predicting the proportion of women in female dominated jobs and over-predicting the proportion of women <lb/>in male-dominated jobs. <lb/>asian_W <lb/>asian_M <lb/>black_W <lb/>black_M <lb/>hispanic_W <lb/>hispanic_M <lb/>white_W <lb/>white_M <lb/>mechanic <lb/>plumber <lb/>carpenter <lb/>construction worker / laborer <lb/>truck driver <lb/>taxi driver / chaffeur / driver <lb/>police officer <lb/>computer programmer <lb/>courier <lb/>guard / security guard / bouncer <lb/>chef <lb/>barber <lb/>lawyer <lb/>janitor <lb/>manager <lb/>doctor <lb/>cook <lb/>reporter / journalist <lb/>bus driver <lb/>salesperson <lb/>photographer <lb/>bartender <lb/>barista <lb/>writer <lb/>model <lb/>teacher <lb/>social worker <lb/>nurse <lb/>cleaner / housekeeper / maid <lb/>receptionist <lb/>secretary / assistant <lb/>babysitter <lb/>0.18 2.4 0.76 4 <lb/>1.5 -12 1.3 10 <lb/>-0.02 8.8 -0.23 8.5 -0.73 -25 -2.4 17 <lb/>1.4 <lb/>3 <lb/>2.6 <lb/>5 <lb/>4.2 -24 18 -17 <lb/>0.92 1.1 1.3 3.2 3.3 -26 4.3 0.05 <lb/>-0.19 2.3 -1.2 -3.9 4.6 3.3 -5 -11 <lb/>-0.07 -5.2 -2 -15 0.67 -5.2 8.2 0.94 <lb/>-0.02 2.4 1.9 7.3 -1.7 -5.8 8.8 4.2 <lb/>-4.2 -8.8 -1.7 1.6 -1.8 12 -14 35 <lb/>2.5 5.3 -3.6 -0.24 -3.8 -15 -12 24 <lb/>-0.34 0.2 -3.5 -4.1 -2.1 -4.8 0.21 25 <lb/>-0.19 -7.6 1 -5.4 0.24 -12 18 3.6 <lb/>-0.55 1.2 -4.4 -7.8 -4.7 -16 0.39 41 <lb/>0.12 -0.87 3.4 3.5 4.5 <lb/>6 <lb/>3.5 7.5 <lb/>0.45 1.2 -2.6 -3.1 -4.5 -4.2 -1.2 3 <lb/>0.49 1.3 0.58 0.54 3 <lb/>7 -7.2 -7.3 <lb/>-5.4 -6.5 0.82 3.5 2.9 3.7 -0.16 27 <lb/>1.3 -0.31 -1.1 -5.3 -6 -14 12 -8.1 <lb/>-1.4 -2.4 5.7 4.8 -2.9 -4.4 12 <lb/>24 <lb/>1.5 3.3 -5.3 -7 3.1 2.2 -28 -7.1 <lb/>0.54 2.5 -1.8 -2.6 1.1 8.2 -18 -6.3 <lb/>-0.26 4.3 4.1 6.1 -6.1 -6.2 11 <lb/>10 <lb/>-0.26 0.89 2.7 <lb/>8 -1.5 5 <lb/>-16 <lb/>9 <lb/>0.59 2.3 0.25 0.6 -3.1 4.3 -13 -8 <lb/>2.7 <lb/>4 <lb/>5 <lb/>3.1 -1.7 -1.8 -13 20 <lb/>4.3 -0.28 5.6 -3 -2.5 -3.5 21 -21 <lb/>0.28 0.67 1 0.56 2.1 <lb/>2 -4.9 8.4 <lb/>2.6 -0.67 -8.2 -3.6 4.8 3.4 1.4 -13 <lb/>-4 -0.47 -1.3 0.71 10 2.2 -0.28 1.5 <lb/>1.7 <lb/>3 -6.8 1.9 -34 0.33 -28 7.5 <lb/>0.05 1.2 -7.7 0.44 -1.6 6.6 -21 5.1 <lb/>0.77 1.3 -0.01 1.5 -4.5 3.6 -11 42 <lb/>-3.1 -0.22 -3.8 -1.1 2.8 -1.6 25 <lb/>-5 <lb/>GPT &lt; True <lb/>GPT &gt; True <lb/>30 <lb/>20 <lb/>10 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>Figure 24. GPT-2 predictions versus US data by gender-ethnicity intersection. Red means that GPT-2 over-predicts the share of the <lb/>occupation-ethnicity intersection pair; Blue means that GPT-2 under-predicts it. <lb/>G. Companies Using AI for Hiring <lb/>Gartner has identified various use cases where AI can be useful in hiring process such as talent <lb/>acquisition and HR virtual assistant (https://www.gartner.com/en/newsroom/press-releases/ <lb/>2019-06-19-gartner-identifies-three-most-common-ai-use-cases-in-). A number of compa-<lb/>nies are already using AI in hiring e.g. Aviro AI (https://www.avrioai.com/features-and-benefits) and <lb/></div>

			<note place="headnote">How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases <lb/></note>

			<div type="annex">Entelo (https://www.entelo.com/recruiting-automation/). These companies have automated the hiring <lb/>process and reducing human involvement in the job application assessment process. This can have serious implications for <lb/>people from marginalized groups if the bias in the underlying AI models is not addressed. <lb/></div>

			<listBibl>References <lb/>Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language <lb/>models be too big? In Conference on Fairness, Accountability, and Transparency (FAccT &apos;21). ACM, New York, NY, <lb/>USA, 2021. <lb/>Carlini, N. Privacy Considerations in Large Language Models, 2020. URL https://ai.googleblog.com/2020/ <lb/>12/privacy-considerations-in-large.html/. <lb/>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask <lb/>learners. 2019. <lb/>Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., and Le, Q. V. Xlnet: Generalized autoregressive pretraining for <lb/>language understanding. In NeurIPS, 2019. </listBibl>


	</text>
</tei>
