<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Structured Principal Component Analysis <lb/>Kristin M. Branson and Sameer Agarwal <lb/>Department of Computer Science and Engineering <lb/>University of California, San Diego <lb/>La Jolla, CA 92193-0114 <lb/>Abstract <lb/>Many tasks involving high-dimensional data, such as face <lb/>recognition, suffer from the curse of dimensionality: the <lb/>number of training samples required to accurately learn a <lb/>classifier increases exponentially with the dimensionality of <lb/>the data. Structured Principal Component Analysis (SPCA) <lb/>reduces the dimensionality of the data while preserving its <lb/>discriminative power. The algorithm finds clusters of simi-<lb/>lar features, where the similarity between features is mea-<lb/>sured using the class-conditional Chi-squared distance be-<lb/>tween the distributions of the features. As features in a clus-<lb/>ter are similar and thus redundant, an entire cluster can be <lb/>represented by a small number of principal components ex-<lb/>tracted from each cluster. We test the algorithm on two face <lb/>recognition databases, the Ekman and Friesen Pictures of <lb/>Facial Affect Database and the Yale Face Database, with <lb/>encouraging results. <lb/></front>

			<body>1. Introduction <lb/>Many tasks in machine learning and computer vision re-<lb/>quire learning a classifier from a small number of high-<lb/>dimensional training samples. These tasks are particularly <lb/>difficult because the potential complexity of a classifier in-<lb/>creases exponentially with the dimensionality of the data. <lb/>For example, consider the task of image classification. <lb/>The goal is to learn a simple classifier, say a perceptron, <lb/>that will accurately classify novel images. The number of <lb/>parameters to learn is more than the number of pixels in the <lb/>image, and the number of samples required to accurately <lb/>and confidently learn a perceptron is many more than the <lb/>number of parameters. A standard dataset has over 10,000 <lb/>pixels per image and 100 training images, thus an accurate <lb/>perceptron cannot be learned from this data. <lb/>Fortunately, real-world data sets contain large amounts <lb/>of redundancy, thus the data can be represented by small(er) <lb/>sets of features. The pixels in an image are redundant, as <lb/>images are highly structured. JPEG compression and image <lb/>subsampling exploit redundancy to reduce dimensionality <lb/>while retaining most of the image structure. <lb/>Two properties make dimensionality reduction for clas-<lb/>sification tasks efficient. First, classification tasks are re-<lb/>stricted to small domains. For example, in face recognition, <lb/>all data samples are images of faces. As all faces share the <lb/>same structure and deviations between different face images <lb/>are small, a few features can represent any face. <lb/>Second, dimensionality reduction with the ultimate goal <lb/>of learning a classifier need only preserve the properties <lb/>of the data relevant to the classification task. For exam-<lb/>ple, if the classification task is face expression recognition, <lb/>only the features relevant to describing the posed expression <lb/>must be preserved, not the features describing the person&apos;s <lb/>identity. Recent research has shown that features useful for <lb/>identity recognition are orthogonal to those useful for ex-<lb/>pression recognition [4]. Including features encoding iden-<lb/>tity increases noise and complexity that will obfuscate clas-<lb/>sifier learning algorithms. Most dimensionality reduction <lb/>techniques do not take advantage of this second property. <lb/>These unsupervised algorithms instead ignore the class la-<lb/>bels of the data and find the features that best represent all <lb/>the properties of the data. <lb/>In this paper, we present a new supervised algorithm for <lb/>dimensionality reduction, Structured Principal Component <lb/>Analysis (SPCA), that preserves the class-conditional struc-<lb/>ture of the data. If two features are similar within every <lb/>class, then given one feature, the second feature does not <lb/>add much information useful for classifying the data. This <lb/>means the dimensionality can be reduced by replacing clus-<lb/>ters of features that are similar within every class with a <lb/>small number of features. SPCA structures the features of <lb/>the data into groups with high within-class similarity, then, <lb/>for each cluster, performs Principal Component Analysis <lb/>(PCA) on the data projected on the features in that clus-<lb/>ter. SPCA thus finds a linear projection of the data that <lb/>preserves class discriminability. <lb/>2. Related Work <lb/>SPCA was conceived with the faults of two classical algo-<lb/>rithms in mind, Principal Component Analysis and Fisher&apos;s <lb/>Linear Discriminant Analysis. In this section, we discuss <lb/>these two algorithms and their flaws. In addition, we dis-<lb/>cuss Factor Analysis, which shares some ideas with SPCA. <lb/></body>

			<page>1 <lb/></page>

			<body>Finally, we emphasize the dissimilarity of SPCA and mix-<lb/>ture algorithms like Mixtures of Gaussians. <lb/>The classical unsupervised dimensionality reduction al-<lb/>gorithm is PCA. PCA selects the orthonormal features <lb/>among the linear combinations of the original features that <lb/>maximize the variance of the projected data. While PCA <lb/>is optimal in terms of its criterion, it is generally not op-<lb/>timal for classification tasks as PCA ignores the class la-<lb/>bels. In the case of facial expression recognition, maximiz-<lb/>ing the variance of the projected data is not ideal, for face <lb/>images vary more over identity than expression. Most fea-<lb/>tures found will not be useful in expression classification. <lb/>Fisher&apos;s Linear Discriminant Analysis (LDA), on the <lb/>other hand, uses a supervised criterion to choose a set of <lb/>orthonormal features from all linear transformations of the <lb/>original features. The features are selected to minimize the <lb/>variance of the data within each class while maximizing the <lb/>variance of the means of each class of data. The standard <lb/>tradeoff between these two goals is to maximize the quo-<lb/>tient: the variance of the means of each class divided by the <lb/>summed variance within each class. Note that LDA only <lb/>depends on the mean and variance of the data. However, <lb/>these two statistics are sufficient to describe the data only <lb/>if the data is normally distributed. If this assumption does <lb/>not hold, then it is not clear that LDA is optimizing the right <lb/>criterion. For example, in many domains the distribution of <lb/>a feature cann be bimodal within a class. Another problem <lb/>with LDA is that it can select at most c 1 features, where <lb/>c is the number of classes. In most cases, this is not enough <lb/>to generalize to novel data samples. <lb/>The theme of Factor Analysis (FA) is similar to that of <lb/>SPCA: if features are highly correlated, they can be rep-<lb/>resented by a few features in the directions of their corre-<lb/>lation. FA represents each D-dimensional data sample x i <lb/>by a d-dimensional factor z i such that z i probabilistically <lb/>characterizes as much of the correlation between each di-<lb/>mension of x i as possible. Therefore, given z i , the dimen-<lb/>sions of each x i are independent. There are two main dif-<lb/>ferences between SPCA and FA that make SPCA a more <lb/>powerful method. Most importantly, FA is not a supervised <lb/>algorithm. SPCA is supervised because it uses the class-<lb/>conditional proximity of the features&apos; distributions to mea-<lb/>sure similarity, as opposed to the unsupervised correlation <lb/>between features used by FA. This results directly from the <lb/>deep embedding of the similarity measure of FA in the al-<lb/>gorithm, whereas in SPCA the similarity measure is explicit <lb/>and flexible. Because it is unsupervised, FA finds features <lb/>that are not useful for the ultimate classification task. Sec-<lb/>ond, FA represents similar features by features in the di-<lb/>rection of maximum covariance, whereas SPCA represents <lb/>similar features by features in the direction of maximum <lb/>variance. The maximum variance is a more robust mea-<lb/>sure, since if the variance of even one feature of a group in <lb/>a certain direction is large, this direction is most probably <lb/>important for representing the data. Thus SPCA does not <lb/>rely on the grouping of features being exact. <lb/>When one thinks of clustering, it is difficult not to think <lb/>of clustering the data samples. However, Factor Analysis <lb/>and SPCA group together features, not data samples. Mix-<lb/>ture methods like Mixtures of Gaussians and Mixtures of <lb/>Principal Component Analyzers find soft clusterings of the <lb/>data samples, not the dimensions. <lb/>3. SPCA Algorithm Description <lb/>Figure 1: Illustration of the SPCA algorithm. On the left are four <lb/>training data samples, two in each class. Each box represents a <lb/>feature of the data. Color encodes feature values. SPCA groups <lb/>together features that have similar distributions within classes. The <lb/>clusters are shown on the left. PCA is performed on the data pro-<lb/>jected on each cluster of features separately. <lb/>SPCA finds features that preserve the class-conditional <lb/>structure of the data. It clusters the features of the data into <lb/>groups that have similar class-conditional distributions. No <lb/>one cluster necessarily has more discriminative power than <lb/>any other cluster. The hypothesis is that each cluster of fea-<lb/>tures can be represented by just a few features, and that the <lb/>collection of these few features from each cluster will pre-<lb/>serve the discriminability of the data. <lb/>SPCA is an algorithmic framework because the pairwise <lb/>similarity measure, the clustering algorithm used to group <lb/>similar variables, and the method used to choose representa-<lb/>tive features from each cluster can all be varied. In this sec-<lb/>tion, we describe the instantiation of the SPCA framework <lb/>we implemented. First, the pairwise distance between each <lb/>pair of features is measured by the class-conditional Chi-<lb/>squared distance between the distributions of the features. <lb/>Second, the Normalized Cut criterion is used to cluster the <lb/>features. Finally, a few features are chosen to represent each <lb/>cluster of features by PCA. <lb/>3.1 Feature Similarity Measure <lb/>A supervised measure of the similarity between two fea-<lb/>tures, u and v, is the class-conditional distance between the <lb/>distributions of the features. This is the weighted sum of the <lb/>within-class distance between the distributions, <lb/>c <lb/>∑ <lb/>c <lb/>¡ ¢ <lb/>1 <lb/>d <lb/>£ <lb/>f u ¤ c <lb/>¡ <lb/>£ ¦¥ ¨ § © <lb/>f v ¤ c <lb/>¡ <lb/>£ ¦¥ ¨ § § <lb/>P <lb/>£ <lb/>c <lb/> § © <lb/>where c is the number of classes, d is an unsupervised <lb/>function of distance between distributions, and P <lb/>£ <lb/>c <lb/> § <lb/>is the <lb/></body>

			<page>2 <lb/></page>

			<body>probability of class c . Because f u ¤ c <lb/>¡ , f v ¤ c <lb/>¡ , and P <lb/>£ <lb/>c <lb/> § <lb/>are <lb/>unknown, they must be estimated from the training data. <lb/>The distributions are estimated as the histogrammed data; <lb/>h u ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § <lb/>is the number of samples of class c such that the <lb/>value of feature u falls within the interval i. The distance <lb/>between the distributions is estimated as the distance be-<lb/>tween the histogrammed data, d <lb/>£ <lb/>h u ¤ c <lb/>¡ <lb/>£ ¦¥ ¨ § © <lb/>h v ¤ c <lb/>¡ <lb/>£ ¥ § § <lb/>. The class <lb/>probability is estimated by the Maximum Likelihood Esti-<lb/>mate, P <lb/>£ <lb/>c <lb/> § ¡ n c <lb/>¡ ¢ <lb/>n, where n is the total number of samples <lb/>and n c <lb/>¡ is the number of samples of class c . <lb/>Any distance metric may be used for the unsupervised <lb/>distance d <lb/>£ ¦¥ © ¥ <lb/>¨ § . We chose the Chi-squared distance metric: <lb/>d <lb/>£ <lb/>h u ¤ c <lb/>¡ <lb/>£ ¥ § © <lb/>h v ¤ c <lb/>¡ <lb/>£ ¥ § § ¡ <lb/>k <lb/>∑ <lb/>i <lb/>¢ <lb/>i <lb/>£ <lb/>h u ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § <lb/>h v ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § § 2 <lb/>h u ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § ¤£ <lb/>h v ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § <lb/>© <lb/>where k is the number of intervals into which the data is <lb/>divided. The Chi-squared distance was chosen because it <lb/>is the standard, historically used metric to compare his-<lb/>togrammed data. The class-conditional pairwise distance <lb/>between features u and v is therefore <lb/>c <lb/>∑ <lb/>c <lb/>¡ <lb/>¨¢ 1 <lb/>k <lb/>∑ <lb/>i <lb/>¢ <lb/>i <lb/>£ <lb/>h u ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § <lb/>h v ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § § 2 <lb/>h u ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § ¥£ <lb/>h v ¤ c <lb/>¡ <lb/>£ <lb/>i <lb/> § P <lb/>£ <lb/>c <lb/> § §¦ <lb/>3.2 Clustering Using Normalized Cut <lb/>SPCA uses the Normalized Cut algorithm to cluster the <lb/>features so that features in the same cluster are similar, <lb/>while features in different clusters are dissimilar [10]. Thus, <lb/>SPCA clusters the features so that intra-cluster affinity is <lb/>maximized while inter-cluster affinity is minimized, where <lb/>affinity is group similarity. The similarity between features <lb/>u and v is inversely proportional to the distance between <lb/>them, d <lb/>£ <lb/>u <lb/>© <lb/>v <lb/> § <lb/>: W <lb/>£ <lb/>u <lb/>© <lb/>v <lb/> § ¡ e¨d © u v 2 σ (σ is a constant that de-<lb/>scribes what distances are considered far). The inter-cluster <lb/>affinity between clusters S 1 and S 2 is: <lb/>Aff <lb/>£ <lb/>S 1 <lb/>© <lb/>S 2 <lb/> § ¡ ∑ <lb/>u S 1 <lb/>∑ <lb/>v S 2 <lb/>W <lb/>£ <lb/>u <lb/>© <lb/>v <lb/> § §¦ <lb/>Similarly, the intra-cluster affinity of cluster S is: <lb/>Aff <lb/>£ <lb/>S <lb/>© <lb/>S <lb/> § ¡ ∑ <lb/>u v S <lb/>W <lb/>£ <lb/>u <lb/>© <lb/>v <lb/> § ¦ <lb/>The criterion function minimized by Normalized Cut is: <lb/>NCut <lb/>£ <lb/>S 1 <lb/>© <lb/>S 2 <lb/> § ¡ <lb/>Aff <lb/>£ <lb/>S 1 <lb/>© <lb/>S 2 <lb/> § <lb/>Aff <lb/>£ <lb/>S 1 <lb/>© <lb/>S 1 S 2 <lb/> § <lb/>£ <lb/>Aff <lb/>£ <lb/>S 1 <lb/>© <lb/>S 2 <lb/> § <lb/>Aff <lb/>£ <lb/>S 2 <lb/>© <lb/>S 1 S 2 <lb/> § <lb/>¦ <lb/>This quantity increases with inter-cluster affinity and de-<lb/>creases with intra-cluster affinity. <lb/>The membership vector, y, that indicates which cluster <lb/>each feature should be in would ideally be discrete val-<lb/>ued, with a single value for each class. Finding the optimal <lb/>discrete-valued membership vector is an NP-hard problem. <lb/>However, y can be approximated by solving a generalized <lb/>eigenvector problem, W y ¡ λDy. The pairwise affinity ma-<lb/>trix, W , is a N x N matrix, where N is the original number <lb/>of features in the data. Each element of the affinity matrix is <lb/>the pairwise similarity W <lb/>£ <lb/>u <lb/>© <lb/>v <lb/> § <lb/>between two features, u and <lb/>v. The degree matrix, D, is a diagonal matrix in which each <lb/>diagonal element represents the total similarity of a feature <lb/>to all other features. That is, D <lb/>£ <lb/>u <lb/>© <lb/>u <lb/> § ¡ ∑ N <lb/>v <lb/>¢ <lb/>1 W <lb/>£ <lb/>u <lb/>© <lb/>v <lb/> § <lb/>[11]. <lb/>The vector y is thresholded to determine which features are <lb/>members of the same cluster. <lb/>The above formulation can be extended to a k-<lb/>partitioning of the graph by using additional eigenvec-<lb/>tors [9]. We do so by stacking the 2 nd to the k th eigenvectors <lb/>columnwise, normalizing the rows of the resulting matrix, <lb/>and performing k-means clustering on them. <lb/>Given that our data is high-dimensional, solving the <lb/>eigenvector problem is a computationally intensive task. <lb/>However, our high dimensional data is highly redundant, <lb/>i.e. there are a large number of features in our data that <lb/>are similar to each other, implying that a number of rows <lb/>of our weight matrix W are similar to each other. Having <lb/>made this observation, we approximate the eigenvector de-<lb/>composition by solving the problem for a random sample <lb/>from the data and extrapolating the resulting eigenvectors <lb/>to the full dataset. This is known as the Nyström approx-<lb/>imation. The original eigenvector problem has complexity <lb/>O <lb/>£ <lb/>D 3 § <lb/>in the dimensionality of the data. Using the Nyström <lb/>approximation we can compute the eigenvalue decomposi-<lb/>tion in O <lb/>£ <lb/>s 3 D <lb/> § <lb/>, where s is the number of samples used. Em-<lb/>pirical evidence shows that for data with a clear clustering <lb/>structure, a fairly small number of samples can be used to <lb/>approximate the eigenvectors to a small error [7]. <lb/>3.3 Representation of Each Cluster <lb/>SPCA clusters the features of the data into groups that, be-<lb/>cause of their high affinity, can be represented by a small <lb/>number of components to reduce dimensionality. As il-<lb/>lustrated in Figure 1, if features u 1 <lb/>© ¦ ¦ ¦ © <lb/>u m are grouped <lb/>into one cluster, then a few features are chosen based <lb/>on the data samples X 1 ¡ £ <lb/>x 1 u 1 <lb/>© ¦ ¦ ¦ © <lb/>x 1 u m <lb/> § T through X n ¡ <lb/>£ <lb/>x n u 1 <lb/>© ¦ ¦ ¦ © <lb/>x n u m <lb/> § T , where x 1 <lb/>© ¦ ¦ ¦ © <lb/>x n are the training data sam-<lb/>ples. This is repeated for each cluster. <lb/>The concise representation closest to the actual data <lb/>in each cluster is the top principal components of the <lb/>data. PCA chooses the components that minimize the sum-<lb/>squared distance between the projected data and the original <lb/>data. These components are the eigenvectors of the sample <lb/>covariance matrix, ∑ i <lb/>£ <lb/>X i µ <lb/> § £ <lb/>X i µ <lb/> § T <lb/>© <lb/>in order of the cor-<lb/>responding eigenvalue. Thus, there are two parameters in <lb/>SPCA: the number of clusters and the number of features <lb/>extracted from each cluster. As in PCA, the mean of the <lb/></body>

			<page>3 <lb/></page>

			<body>data samples in each cluster is not represented in SPCA. <lb/>4 Experiments <lb/>The SPCA algorithm was compared to PCA and LDA on <lb/>three sets of data. The first set is a synthetic set designed <lb/>to demonstrate the weaknesses of PCA and LDA. The sec-<lb/>ond set is the Ekman and Friesen POFA database, with the <lb/>task of expression recognition. The third set is the Yale <lb/>Face database, with the task of identity recognition. SPCA <lb/>achieves 100% accuracy on the synthetic data, compared to <lb/>SPCA and LDA which did no better than chance. SPCA <lb/>also outperforms PCA and LDA on the POFA database. <lb/>SPCA outperforms PCA on the Yale database and has sim-<lb/>ilar performance to LDA. <lb/>4.1 Synthetic Data <lb/>PCA and LDA both have weaknesses that limit their effec-<lb/>tiveness. If a feature has high variance but is uncorrelated <lb/>with the class labels, PCA will highly represent this fea-<lb/>ture because of its variance, neglecting features with smaller <lb/>variance but more correlated with the classification of the <lb/>data. On the other hand, LDA assumes the class-conditional <lb/>distribution of the data over each feature is normal. Suppose <lb/>this assumption is false, for instance if a feature&apos;s data for <lb/>one class is bimodally distributed and for another class is <lb/>normally distributed. This is the case for the pixels in the <lb/>smiles (which may or may not show teeth) of happy faces <lb/>versus pixels in the mouths of sad faces. As LDA chooses <lb/>the components that separate the class means as much as <lb/>possible, it will choose to offset the means of the bimodal <lb/>and normal distributions. This could result in one of the <lb/>modes of the bimodal distribution being projected to nearly <lb/>the same value as the mean of the normal distribution. <lb/>Class-Conditional Distributions of Features <lb/>f x ¡ c ¢ 1£ f x ¡ c ¢ 2£ g x ¡ c ¢ 1£ g x ¡ c ¢ 2£ <lb/>h x£ <lb/>Figure 2: Distributions of the features of the synthesized data. <lb/>With these limitations in mind, we synthesized 100 train-<lb/>ing and 100 test samples, all i.i.d. Each sample has 1000 <lb/>features with three possible distributions, f <lb/>£ <lb/>x¤ c <lb/> § <lb/>, g <lb/>£ <lb/>x¤ c <lb/> § <lb/>, <lb/>and h <lb/>£ <lb/>x¤ c <lb/> § <lb/>. Only the features with distribution f or g are <lb/>useful in classification. These distributions are shown in <lb/>Figure 2. f <lb/>£ <lb/>x ¤ c ¡ 1 <lb/> § <lb/>and g <lb/>£ <lb/>x¤ c ¡ 2 <lb/> § <lb/>are bimodal distri-<lb/>butions, with modes ¥ 0 <lb/>¦ <lb/>5 and a standard deviation of 1. <lb/>f <lb/>£ <lb/>x ¤ c ¡ 2 <lb/> § <lb/>and g <lb/>£ <lb/>x¤ c ¡ 1 <lb/> § <lb/>are normal distribution with mean <lb/>0 and standard deviation 1. h <lb/>£ <lb/>x <lb/> § <lb/>is uniformally distributed <lb/>between 0 and 1. 100 features have distribution f , 100 fea-<lb/>tures have distribution g, and 800 features have distribution <lb/>h. The optimal dimensionality reduction technique for this <lb/>data set would ignore all 800 features of distribution h and <lb/>use any of the features of distribution f or g. <lb/>Figure 3(a-d) shows the projection of the data on the fea-<lb/>tures chosen by SPCA, LDA, and PCA. When grouping the <lb/>data into three clusters, SPCA put all but two of the fea-<lb/>tures with distribution f in one cluster, all but one of the <lb/>features with distribution g in the second cluster, and all the <lb/>rest of the features in the third cluster. Thus the first and sec-<lb/>ond principal components generated by SPCA are useful in <lb/>discriminating the data, while the third is not. SPCA per-<lb/>forms equally well when only two clusters of the features <lb/>are found. <lb/>As hypothesized, LDA was not able to separate the test <lb/>data. It was able to find a projection to separate the training <lb/>data, but this projection relied heavily on the features of dis-<lb/>tribution h which are not correlated with the classification. <lb/>Thus, when generalizing to the test data, LDA fails. <lb/>PCA was distracted by the 800 features of distribution <lb/>h that were not correlated with the classification, and thus <lb/>was unable to separate the training and test data. <lb/>In fact, SPCA performs well while the other two al-<lb/>gorithms fail on data in which the separation between the <lb/>modes of the bimodal distribution is small. For separations <lb/>greater than 0.1, SPCA achieves 100% accuracy using a <lb/>nearest-neighbor classifier. No matter how small the sep-<lb/>aration, LDA and PCA are not able to separate the data, <lb/>despite the distributions approaching a normal distribution, <lb/>as shown in Figure 3(e). <lb/>These experiments on the synthetic data set show that <lb/>SPCA is robust to features that are uncorrelated with clas-<lb/>sification, unlike PCA. They also show that SPCA is robust <lb/>to non-normal distributions of the data, unlike LDA. <lb/>4.2 The Ekman and Friesen POFA Database <lb/>SPCA, PCA, and LDA were tested on the Ekman and <lb/>Friesen Database of Pictures of Facial Affect [6]. This data <lb/>set includes 14 trained actors posing six expressions, plus <lb/>neutral. There are 110 greyscale images in this data set, <lb/>96 of which are not neutral. Examples from are shown in <lb/>Figure 4(a). <lb/>An expression classifier must generalize over identity <lb/>and concentrate only on the expression in an image. A <lb/>supervised algorithm would be able to find a more accu-<lb/>rate and concise representation that is tailored to expression <lb/>recognition, in comparison to PCA. However, PCA signifi-<lb/>cantly outperforms LDA, by a margin of 10% accuracy. We <lb/>hypothesized that this was partially due to the limited num-<lb/>ber of components LDA can extract (6 1 ¡ 5). Even trying <lb/></body>

			<page>4 <lb/></page>

			<body>−2 <lb/>2 <lb/>−2 <lb/>2 <lb/>−1 <lb/>1 <lb/>PC 1 <lb/>Projection of Test Data (SPCA) <lb/>PC 2 <lb/>PC 3 <lb/>Class 1 <lb/>Class 2 <lb/>−2 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>−1.5 <lb/>−1 <lb/>−0.5 <lb/>0 <lb/>0.5 <lb/>1 <lb/>1.5 <lb/>Projection of Test Data (SPCA) <lb/>PC 1 <lb/>PC 2 <lb/>Class 2 <lb/>Class 1 <lb/>Class 1 <lb/>Class 2 <lb/>Class 1 <lb/>Class 2 <lb/>Training Test <lb/>Projection of Data (LDA) <lb/>Class 1 <lb/>Class 2 <lb/>Projection of Test Data (PCA) <lb/>0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>90 <lb/>100 <lb/>Distance Between Modes of Bimodal Distribution <lb/>Percent Correctly Classified <lb/>Comparison of Methods for Synthesized Data <lb/>SPCA <lb/>LDA <lb/>PCA <lb/>(a) <lb/>(b) <lb/>(c) <lb/>(d) <lb/>(e) <lb/>Figure 3: (a) Projection of the test data on the features chosen by SPCA, 3 clusters (b) 2 clusters (c) Projection of the training data and <lb/>test data on the feature chosen by LDA. (d) Projection of the test data on the top two Principal Components chosen by PCA. (e) Results of <lb/>SPCA, LDA, and PCA followed by a nearest neighbor classifier on classifying the synthetic data, with varying distance between the modes <lb/>of the bimodal distributions. <lb/>(a) <lb/>(b) <lb/>(c) <lb/>Figure 4: (a) Example cropped and aligned images from the POFA database (b) Example full-face images from the Yale database (c) <lb/>Example closely-cropped images from the Yale database. <lb/>different criterion functions which allow LDA to produce <lb/>more features does not greatly improve LDA&apos;s performance. <lb/>To compare SPCA with previous experiments in which <lb/>PCA and LDA performed well, we perform the same image <lb/>preprocessing. The images were aligned so that the eyes <lb/>and the bottom of the top row of teeth were in the same <lb/>position for all images, and cropped inside the contours of <lb/>the face. Next, the images were subsampled and convolved <lb/>with Gabor wavelet jets of 40 Gabor filters (five scales and <lb/>eight orientations), resulting in a 40,600 dimensional vec-<lb/>tor. Finally, the outputs of the Gabor filters were z-scored <lb/>(normalized so that the mean intensity value for each pixel <lb/>is zero and the standard deviation is one). After preprocess-<lb/>ing, the dimensionality of the data is reduced using PCA, <lb/>LDA, or SPCA. A perceptron is learned from images of 12 <lb/>of the actors, training is stopped at the best performance on <lb/>a held out actor, and evaluated on a novel actor [5]. <lb/>SPCA only finds clusters of features with high affinity, <lb/>not necessarily important to classification. Clusters differ <lb/>in number of features and correlation with the classification, <lb/>yet the number of principal components extracted from each <lb/>is equal. Thus, each cluster is weighted equally. For the <lb/>POFA data set, we added an extra layer of PCA to weight <lb/>the principal components extracted by SPCA by the amount <lb/>of variance of the data projected on them. <lb/>This extra layer proved necessary when using a percep-<lb/>tron for classification, as a perceptron is greatly influenced <lb/>by input variables that have small variance in the training <lb/>data. For example, suppose a feature has a constant, low <lb/>value for all the training data except for one of class c. The <lb/>perceptron will find this feature useful in determining class <lb/>c, and could weight its inputs to classify an example as class <lb/>c if ever the value of this feature is high. If the inconsistent <lb/>value for this one training sample is merely noise, the per-<lb/>ceptron will mistake all test examples with an inconsistent <lb/>value for this variable as class c. <lb/>With an extra layer of PCA added, SPCA achieves 92.7% <lb/>accuracy on this data set, compared to 90% accuracy for <lb/>PCA (using 50 principal components), and 79.3% accuracy <lb/>achieved by LDA. These are the optimal results obtained by <lb/>SPCA, PCA and LDA. These results are impressive because <lb/>92.7% is 0.3% less than the accuracy humans achieve on <lb/>this dataset. <lb/>SPCA proved to be relatively insensitive to the num-<lb/>ber of clusters and the number of principal components ex-<lb/>tracted from each cluster. Figure 5(a) and (b) show the re-<lb/>sults of varying these parameters. While SPCA performs <lb/>better with 30 clusters than 20 clusters, the classification er-<lb/>ror difference is small, 2%. In addition, using 30 clusters, <lb/>optimal results are obtained extracting two and four princi-<lb/>pal components from each cluster, and extracting three prin-<lb/>cipal components is only 1% worse in classification error. <lb/>4.3 The Yale Face Database <lb/>The Yale Database [2] consists of images of 15 actors under <lb/>11 different conditions, including different lighting, facial <lb/>expressions, and occlusion effects. Identity recognition is <lb/>difficult, particularly for PCA, because the classifier must <lb/>generalize over all these distractions [1]. This dataset was <lb/>created with LDA in mind, thus LDA performs extremely <lb/>well while PCA performs poorly in this experiment. <lb/>Two experiments were performed, one in which the im-<lb/>ages were cropped outside the face contour (full-face im-<lb/>ages) and one in which the images were cropped inside <lb/>the face contour (closely-cropped images). Examples are <lb/>shown in Figure 4. The preprocessing of this dataset is the <lb/>same as that of the POFA dataset. After preprocessing, the <lb/>dimensionality of the data is reduced using PCA, LDA, or <lb/>SPCA. A perceptron is trained by backpropagation on 164 <lb/></body>

			<page>5 <lb/></page>

			<body>0 <lb/>10 <lb/>20 <lb/>30 <lb/>Classification Error (%) <lb/>1 2 3 4 <lb/>2 3 4 PCs <lb/>1 <lb/>20 Clusters 30 Clusters (a) <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>Classification Error (%) <lb/>1 2 3 4 <lb/>1 2 3 4 <lb/>20 Clusters 30 Clusters <lb/>15 <lb/>15 <lb/>45 30 <lb/>10 <lb/>35 70 20 <lb/>(b) <lb/>0 <lb/>5 <lb/>10 <lb/>15 <lb/>20 <lb/>Classification Error (%) <lb/>20 Clusters <lb/>1 PC <lb/>2 PC <lb/>3 PC <lb/>4 PC <lb/>(c) <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>0 <lb/>10 <lb/>20 <lb/>Number of Clusters <lb/>Classification Error (%) <lb/>1 PC <lb/>2 PC <lb/>3 PC <lb/>4 PC <lb/>(d) <lb/>Figure 5: Comparison of parameter settings on POFA and Yale databases. (a) POFA, 20 and 30 clusters, without an extra layer of PCA <lb/>(b) POFA, 20 and 30 clusters, with an extra layer of PCA. The numbers above each bar are the number of principal components extracted <lb/>in the extra layer of PCA (c) Yale, Full-face images (d) Yale, Closely-Cropped images. <lb/>of the samples and tested on a novel image. <lb/>SPCA achieves 100% accuracy on the full-face samples, <lb/>compared to LDA which obtains 99.4% classification accu-<lb/>racy and PCA which obtains 90% classification accuracy. <lb/>On the closely-cropped samples, LDA outperforms SPCA. <lb/>LDA achieves 97% classification accuracy, compared to <lb/>SPCA with 94.6% accuracy and PCA with 76.4% accuracy. <lb/>A comparison of the effects of the parameters for SPCA is <lb/>shown in Figure 5(c) and (d). <lb/>5. Discussion <lb/>SPCA uses a supervised measure of similarity to cluster the <lb/>features into groups of high intra-cluster affinity and low <lb/>inter-cluster affinity. It extracts a small number of principal <lb/>components from each cluster to represent the data. Exper-<lb/>imentally, we have shown that the supervised measure of <lb/>similarity allows SPCA to distinguish features that are cor-<lb/>related with the classification from those that are not. Be-<lb/>cause of this, SPCA outperforms PCA in all experiments. <lb/>We have also shown that when the assumptions made by <lb/>LDA do not hold, LDA performs very poorly. In these <lb/>cases, we have experimentally shown that SPCA outper-<lb/>forms LDA. If the assumptions made by LDA do hold, then <lb/>LDA performs optimally. In addition, we hypothesize that <lb/>additional experimentation with non-aligned databases will <lb/>show that SPCA is more robust than PCA and LDA to small <lb/>translations and rotations in the images. <lb/>As stated earlier, SPCA is actually a versatile framework <lb/>of algorithms. In the future, we hope to experiment with <lb/>other instantiations, including different methods of repre-<lb/>senting the features of each cluster. Instead of selecting <lb/>from the linear combinations of the features in a cluster, <lb/>we could select directly from the features in the cluster. <lb/>This would be useful in applications in which linear com-<lb/>binations of features are meaningless. We would also like <lb/>to try mutual information measures of similarity, like the <lb/>Kullback-Liebler distance. <lb/>Finally, we believe that the distance measure chosen for <lb/>SPCA could be applied to LDA. In such an algorithm, the <lb/>data would be projected onto the feature space which maxi-<lb/>mizes the Chi-squared distance between the distributions of <lb/>the data of each class and minimizes the Chi-squared dis-<lb/>tance of the distributions within each class. <lb/></body>

			<div type="acknowledgement">Acknowledgments <lb/>We would like to thank Serge Belongie, Gary Cottrell and <lb/>GURU, Sanjoy Dasgupta, Virginia de Sa, Charles Elkan, <lb/>and Bianca Zadrozny for helpful discussion and advice. <lb/></div>

			<listBibl>References <lb/>[1] Belhumeur, P. N., Hespanha, J., and Kriegman, D. J., &quot;Eigen-<lb/>faces using class specific linear projection,&quot; European Con-<lb/>ference of Computer Vision, Vol. 1, pp 45-58, 1996. <lb/>[2] Belhumeur, P. N. and Kriegman, D. J., The Yale Face <lb/>Database, 1997. <lb/>[3] Bishop, C. M., Neural Networks for Pattern Recognition, Ox-<lb/>ford University Press, 1995. <lb/>[4] Cottrell, G. W., Branson, K. M., and Calder, A. J., &quot;Do ex-<lb/>pression and identity need separate representations?,&quot; Pro-<lb/>ceedings of the 24th Annual Conference of the Cognitive Sci-<lb/>ence Society, Fairfax, Virginia, pp 283-243, 2002. <lb/>[5] Dailey, M. N., Cottrell, G. W., and Adolphs, R., &quot;A Six-Unit <lb/>Network Is All You Need to Discover Happiness,&quot; Proceed-<lb/>ings of the 22nd Annual Conference of the Cognitive Science <lb/>Society, Mahwah, New Jersey, 2000. <lb/>[6] Ekman, P. and Friesen, W., Pictures of Facial Affect, Consult-<lb/>ing Psychologists, Palo Alto, 1976. <lb/>[7] Fowlkes, C., Belongie, S., and Malik, J., &quot;Efficient Spatiotem-<lb/>poral Grouping Using the Nyström Method,&quot; Computer Vi-<lb/>sion and Pattern Recognition, Vol. 1, pp. 723-730, 2001. <lb/>[8] Ghahramani, Z. and Hinton, G. E., &quot;The EM Algorithm for <lb/>Mixtures of Factor Analyzers,&quot; Technical Report CRG-TR-<lb/>96-1, University of Toronto, 1996. <lb/>[9] Ng, A. Y., Jordan, M. I., and Weiss, Y., &quot;On Spectral Cluster-<lb/>ing: Analysis and an Algorithm,&quot; NIPS Vol. 14, 2002. <lb/>[10] Shi, J. and Malik, J., &quot; Normalized Cuts and Image Segmen-<lb/>tation,&quot; IEEE Transactions on Pattern Analysis and Machine <lb/>Intelligence, Vol. 22(8), pp 888-905, 2000. <lb/>[11] Weiss, Y., &quot;Segmentation using eigenvectors: A unifying <lb/>view,&quot; International Conference on Computer Vision, Volume <lb/>2, pp 975-982, 1999. <lb/></listBibl>

			<page>6 </page>


	</text>
</tei>
