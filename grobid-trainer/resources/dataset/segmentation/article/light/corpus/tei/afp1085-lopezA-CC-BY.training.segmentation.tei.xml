<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Mining Software Entities in Scientific Literature: <lb/>Document-level NER <lb/>for an Extremely Imbalance and Large-scale Task <lb/>Patrice Lopez <lb/>science-miner <lb/>France <lb/>patrice.lopez@science-miner.com <lb/>Caifan Du <lb/>Johanna Cohoon <lb/>University of Texas at Austin <lb/>USA <lb/>Karthik Ram <lb/>Berkeley Institute for Data Science <lb/>USA <lb/>James Howison <lb/>University of Texas at Austin <lb/>USA <lb/>ABSTRACT <lb/>We present a comprehensive information extraction system dedi-<lb/>cated to software entities in scientific literature. This task combines <lb/>the complexity of automatic reading of scientific documents (PDF <lb/>processing, document structuring, styled/rich text, scaling) with <lb/>challenges specific to mining software entities: high heterogeneity <lb/>and extreme sparsity of mentions, document-level cross-references, <lb/>disambiguation of noisy software mentions and poor portability <lb/>of Machine Learning approaches between highly specialized do-<lb/>mains. While NER is a key component to recognize new and unseen <lb/>software, considering this task as a simple NER application fails to <lb/>address most of these issues. <lb/>In this paper, we propose a multi-model Machine Learning ap-<lb/>proach where raw documents are ingested by a cascade of docu-<lb/>ment structuring processes applied not to text, but to layout token <lb/>elements. The cascading process further enriches the relevant struc-<lb/>tures of the document with a Deep Learning software mention <lb/>recognizer adapted to the high sparsity of mentions. The Machine <lb/>Learning cascade culminates with entity disambiguation to alleviate <lb/>false positives and to provide software entity linking. A bibliograph-<lb/>ical reference resolution is integrated to the process for attaching <lb/>references cited alongside the software mentions. <lb/>Based on the first gold-standard annotated dataset developed <lb/>for software mentions, this work establishes a new reference end-<lb/>to-end performance for this task. Experiments with the CORD-19 <lb/>publications have further demonstrated that our system provides <lb/>practically usable performance and is scalable to the whole scientific <lb/>corpus, enabling novel applications for crediting research software <lb/>and for better understanding the impact of software in science. <lb/>CCS CONCEPTS <lb/>‚Ä¢ Computing methodologies ‚Üí Information extraction; ‚Ä¢ Ap-<lb/>plied computing ‚Üí Document analysis. <lb/>Permission to make digital or hard copies of part or all of this work for personal or <lb/>classroom use is granted without fee provided that copies are not made or distributed <lb/>for profit or commercial advantage and that copies bear this notice and the full citation <lb/>on the first page. Copyrights for third-party components of this work must be honored. <lb/>For all other uses, contact the owner/author(s). <lb/>CIKM &apos;21, November 1-5, 2021, Virtual Event, QLD, Australia <lb/>¬© 2021 Copyright held by the owner/author(s). <lb/>ACM ISBN 978-1-4503-8446-9/21/11. <lb/>https://doi.org/10.1145/3459637.3481936 <lb/>KEYWORDS <lb/>Software; Scientific Literature; Entity Recognition; Entity Disam-<lb/>biguation; Document Analysis <lb/>ACM Reference Format: <lb/>Patrice Lopez, Caifan Du, Johanna Cohoon, Karthik Ram, and James Howi-<lb/>son. 2021. Mining Software Entities in Scientific Literature: Document-level <lb/>NER for an Extremely Imbalance and Large-scale Task. In Proceedings of the <lb/>30th ACM International Conference on Information and Knowledge Manage-<lb/>ment (CIKM &apos;21), November 1-5, 2021, Virtual Event, QLD, Australia. ACM, <lb/>New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3481936 <lb/></front>

			<body>1 INTRODUCTION <lb/>Software is increasingly used to support research activities. Many <lb/>researchers include software with their results, as shown by ser-<lb/>vices like rOpenSci [5] and Papers with Code 1 . Quality software <lb/>facilitates scientific progress and research reproducibility. Software, <lb/>however, is still relatively invisible in research, citation databases, <lb/>and academic search engines. Because it is impossible to search <lb/>for research software as easily as for articles, users miss relevant <lb/>software and software is not easily found by potential users. In ad-<lb/>dition, because identifying and crediting contributions of software <lb/>developers is difficult, researchers have less incentives to develop <lb/>better and more reusable software. While some recent works have <lb/>focused on improving software cataloging [11] and standards for <lb/>software citation [15], mining software mentions in the scientific <lb/>literature as currently published offers a factual approach, directly <lb/>usable to increase research software visibility. <lb/>Named entity recognition (NER) in scholarly literature has been <lb/>successfully applied across a range of entity types, including species <lb/>names, chemical, and biomedical entities [6, 12, 13, 30]. Mining <lb/>software entities has attracted a lot of interest in the recent years. A <lb/>recent review article [16] offers a comprehensive analysis of prior <lb/>works that extract software and data mentioned in research articles. <lb/>From 48 reviewed studies, of which 15 cover software and packages, <lb/>Kr√ºger and Schindle categorize four extraction approaches: (1) <lb/>term search, (2) manual extraction, (3) rule-based extraction, and <lb/>(4) supervised learning. <lb/>Term search was employed in 12 studies, although only one <lb/>study related to software extraction. Term search refers to searching <lb/>bibliographical databases for known string identifiers, such as URL, <lb/></body>

			<note place="footnote">1 https://paperswithcode.com <lb/></note>

			<front>This work is licensed under a Creative Commons Attribution International 4.0 License. <lb/>CIKM &apos;21, November 1-5, 2021, Virtual Event, Australia. <lb/>¬© 2021 Copyright is held by the owner/author(s). <lb/>ACM ISBN 978-1-4503-8446-9/21/11. <lb/>https://doi.org/10.1145/3459637.3481936 <lb/></front>

			<body>Table 1: Distribution of software citations in the Biomedical domain from [14] <lb/>types <lb/>examples <lb/>proportion <lb/>mention with reference <lb/>... was calculated using biosys (Swofford &amp; Selander 1981)... <lb/>37% <lb/>mention <lb/>... were analuzed using MapQTL (4.0) software... <lb/>31% <lb/>device-like <lb/>... GraphPad prism software (San Diego, CA, USA) was used for data analyses <lb/>19% <lb/>URL <lb/>... freely available from http://www.cibiv.at/software/pda/... <lb/>5% <lb/>mention with website <lb/>... using BrainSuite2 (http://brainsuite.usc.edu)... <lb/>5% <lb/>user manual reference <lb/>... as analyzed by the BIAevaluation software (Biacore, 1997)... <lb/>2% <lb/>unnamed ... was carried out using software implemented in the Java programming language 1% <lb/>DOI, or particular software names known in advance, strongly <lb/>limiting the capacity of recognition. Manual extraction, applied in <lb/>16 studies with six addressing software, is relevant to occasional <lb/>review work, but is not able to scale. <lb/>Rule-based approaches rely on pre-defined heuristics to auto-<lb/>matically extract mentions of software from text. 16 of the studies <lb/>reviewed by Kr√ºger and Schindle used rule-based approaches; six of <lb/>these were related to software extraction. While rules can achieve <lb/>good precision, they are usually limited in terms of coverage. They <lb/>are time-consuming to create and to maintain. Furthermore, since <lb/>this approach is often used because of a lack of annotated data, rule <lb/>evaluation can be unreliable and their portability to new domains <lb/>is uncertain. Supervised learning has been used in four studies, two <lb/>only covering software recognition [9, 10]. These and [27] use a <lb/>CRF approach, while [16] experiments with a BiLSTM-CRF model. <lb/>However, they are all based on datasets with very limited number <lb/>of documents that over-represent software mentions. While strict <lb/>supervised learning provides the best performance for NER appli-<lb/>cations, they typically require large and realistic sets of annotated <lb/>data, which are missing for software entities. <lb/>Our system relies on a gold-standard annotated corpus, the Soft-<lb/>cite Dataset [8], presented in section 2 and available online 2 under <lb/>CC-BY license. This new dataset, by its size and quality, makes it <lb/>possible to experiment with fully supervised Machine Learning <lb/>(ML) approaches and to provide more rigorous evaluations. The <lb/>analysis of the Softcite Dataset helped us understand how software <lb/>are mentioned and identify the main requirements and challenges <lb/>of this task (section 3), which go far beyond the traditional NER <lb/>used by prior research. In this paper we provide details on our sys-<lb/>tem, the Softcite recognizer, available as open source 3 (including <lb/>models and evaluation scripts) under the Apache 2 license. <lb/>To better capture the specificities of mining scholarly literature, <lb/>our efforts are realized from an end-to-end perspective. In particular, <lb/>we address the concrete challenge of applying state-of-the-art ML <lb/>methods to dozens of millions of published PDFs across different <lb/>scientific domains, where software mentions represent only a few <lb/>relevant tokens out of several thousands in every document. Finally, <lb/>to validate our approach, we mined the CORD-19 PDF publication <lb/>set [32], with results available online 4 . <lb/></body>

			<note place="footnote">2 https://zenodo.org/record/4445202 <lb/></note>

			<note place="footnote">3 https://github.com/ourresearch/software-mentions <lb/></note>

			<note place="footnote">4 https://zenodo.org/record/5140437 <lb/></note>

			<body>Table 2: Overview of the annotations in the gold-standard <lb/>Softcite Dataset, with the proportion per 1000 of annotated <lb/>tokens given the total number of tokens of the corpus (total <lb/>of 46,052,050 tokens). <lb/>annot. types # annot. # tokens ‚Ä∞tokens <lb/>software name <lb/>5,172 <lb/>6,396 0.139 <lb/>version <lb/>1,591 <lb/>3,627 0.079 <lb/>publisher <lb/>1,358 <lb/>2,686 0.058 <lb/>URL <lb/>215 <lb/>2,571 0.056 <lb/>total <lb/>8336 <lb/>15,280 0.332 <lb/>2 THE SOFTCITE DATASET <lb/>We developed the Softcite Dataset [8] as a on-going long term effort <lb/>to understand software citations and to support experiments with <lb/>supervised learning. This dataset includes the mentions of software <lb/>names and related attributes version (version number or version <lb/>date), publisher, and URL from 4,971 full-text research articles, a <lb/>total of 8,336 annotations. Articles were randomly selected from <lb/>the PubMed Central (PMC) Open Access Subset (2,521 articles), and <lb/>from open access articles in Economic (2,450 articles) via Unpay-<lb/>wall [29]. The manual annotation was realized with a multi-round <lb/>annotation process starting from the complete article content and in-<lb/>volved 38 human annotators. Percentage inter-annotator agreement <lb/>for the annotations is 75.5%. All annotations were then reconciled <lb/>and checked for a final choice by two expert annotators, ensuring <lb/>consistency in the labeling and a gold-standard quality. <lb/>Table 2 presents an overview of the annotations in the dataset. <lb/>The final published dataset includes only the paragraphs with at <lb/>least one annotation. Before the Softcite Dataset, the largest similar <lb/>dataset used for supervised learning experiments [9, 10, 16] was a <lb/>set of 85 annotated documents (versus 4,971 in the Softcite Dataset) <lb/>presented in [9], with average name mention frequency per docu-<lb/>ment of 40.3 for the training set and 70.0 for the test set, whereas <lb/>mention frequency is only 1.04 software name per document in the <lb/>randomly selected Softcite Dataset. <lb/>3 TASK DESCRIPTION <lb/>3.1 Heterogeneity of software citations <lb/>In a previous study [14], summarized by Table 1, we observed that <lb/>software citations in Biomedicine came in many different forms, <lb/>dominated by informal mentions. These results show the neces-<lb/>sity of identifying various types of information beyond a software <lb/>name: associated bibliographical references, URL, software publish-<lb/>ers and creators, and version numbers. Recognizing the associated <lb/>bibliographical references takes several steps: identify the reference <lb/>markers locally associated with a software mention, link these mark-<lb/>ers to their reference entries usually in the bibliography section, <lb/>parse the references, and match them to registered bibliographical <lb/>records such as a CrossRef DOI entries. URLs are frequently in <lb/>footnotes, which necessitates cross-reference linking within the <lb/>document. <lb/>3.2 Sparsity of mentions <lb/>As the count of software mentions in Table 2 shows, one of the key <lb/>challenges in automatic software recognition is the low frequency <lb/>of the mentions in documents. Only 1,441 out of 4,971 (29.0%) of the <lb/>articles contain at least one software mention. This is even more <lb/>significant at the level of tokens (including traditional delimiters <lb/>but excluding space characters), which is the level of prediction in <lb/>a sequence labeling process like NER. The 4,971 full-texts contain a <lb/>total of around 46 million tokens, but only 15,280 tokens are relevant <lb/>to a software mention; so around one token would be positively <lb/>labeled for each 3,000 &quot;negative&quot; tokens, with a ratio as low as one <lb/>token per 17,500 tokens for publishers and URL fields. <lb/>This issue is usually referred as the Class Imbalance Problem <lb/>(CIP) in Machine Learning. CIP is determined by the Imbalance <lb/>Ratio (IR), i.e., the ratio between the negative samples and the <lb/>positives. An IR value above 500 is usually already considered to <lb/>be extreme [20]. With the higher observed IR, an ML approach to <lb/>finding new unseen software mentions is a major difficulty. We <lb/>observed that very few studies investigated extreme CIP in the <lb/>context of NER. These studies usually focused on chemical and <lb/>biomedical entities, which, while highly imbalanced, do not present <lb/>such extreme IR as we observe for software mentions [2, 31]. <lb/>3.3 Document-level information <lb/>As mentioned previously, information related to cited software is <lb/>often spread in different places within a document. In addition to <lb/>this constraint, the Softcite Dataset shows that the distribution of <lb/>software mentions in documents is not uniform and the same entity <lb/>is frequently mentioned several times in one document [8]. Each <lb/>context of citation can bring different information. A global view <lb/>of the software mentions at document level is therefore a practical <lb/>requirement for fully recognizing all mentions to software and <lb/>further matching them to unique software entities. <lb/>3.4 PDF processing <lb/>Mining for specific entities is only relevant to certain textual struc-<lb/>tures of a scientific document, such as paragraphs, abstracts, figure <lb/>captions, etc. In the Softcite corpus, we calculated that, on average, <lb/>28% of publication content should be filtered out. This includes <lb/>metadata (author, affiliations), bibliographical sections, table and <lb/>figure content, formulas, headnotes, page numbers, reference mark-<lb/>ers, or editorial annexes (like conflicts of interest). <lb/>Text mining work usually assumes the availability of clean and <lb/>structured text enabling recognition only on relevant document <lb/>parts, but this is not the condition of real world applications. The <lb/>most widespread and easily available scientific publication format is <lb/>raw PDF, a presentation-oriented format that destroys the semantics <lb/>and the original structure of data, introducing noise in text encoding <lb/>and text order stream. This format raises major issues for text <lb/>mining applications, both in terms of significant source of errors <lb/>and technical feasibility [33]. <lb/>Publishers&apos; structured XML including the text body, such as JATS, <lb/>is text-mining friendly, but has limited availability. Decades of back <lb/>files are only available in PDF. Even when available, XML full-texts <lb/>are in a variety of different native publisher XML formats, often <lb/>incomplete and inconsistent from one to another, difficult to use <lb/>at scale. Moreover, XML full-texts are usually subject to numerous <lb/>complex and time-consuming commercial agreements. Thus, sup-<lb/>porting PDF by using a layout aware parsing and conversion tool <lb/>appeared very early as a key requirement for our task. <lb/>3.5 Specialized domains <lb/>Technical and scientific documents aim at supporting specialist <lb/>communication and are thus written in specialist language, 30-80% <lb/>of which is composed of terminology according to [1]. Scientific <lb/>texts follow thus a specialized language with specific vocabularies <lb/>and nomenclatures highly depending on technical domains. <lb/>The usage of existing general-purpose NLP components on sci-<lb/>entific text results most of the time in a significant loss of accuracy <lb/>as compared to custom models. For example, ScispaCy improves <lb/>F1-scores by 3.3 to 23.5 points on some tasks related to Biomedical <lb/>literature as compared to the general domain spaCy [25]. SciB-<lb/>ert surpasses Bert-base model by 0.1 to 7.07 points in F1-score <lb/>depending on the task [3]. We also observed in these works that <lb/>the portability of ML models from one scientific field to another is <lb/>uneven, even when customized. This directly challenges the recog-<lb/>nition of cross-domain entities like software across fields. <lb/>4 DOCUMENT PROCESSING AS A CASCADE <lb/>OF MACHINE LEARNING MODELS <lb/>Figure 1 presents the Softcite pipeline, which tries to address these <lb/>challenges in an accurate, scalable, and generic manner. The dif-<lb/>ferent approaches and components are presented in details in the <lb/>next sections. <lb/>4.1 Document structuring <lb/>We rely on GROBID 5 for parsing, extracting, and structuring the <lb/>content of scientific articles in PDF, but also to drive further entity <lb/>mining in relevant sections. GROBID is an open source library <lb/>specialized for scholarly PDF implementing a cascade of ML models <lb/>to mark up the structure of a document. The tool, created by the <lb/>first author of this paper [22], is used for this purpose by many <lb/>large scientific information service providers such as ResearchGate, <lb/>Academia.edu, and Internet Archive, by large-scale citation services, <lb/>for example scite.ai, which has extracted more than 800 million <lb/>bibliographical citation contexts with this tool [26], or for creating <lb/>machine-friendly datasets of research papers, like the recent PDF <lb/>set of the CORD-19 dataset [32]. <lb/></body>

			<note place="footnote">5 https://github.com/kermitt2/grobid <lb/></note>

			<body>Figure 1: Overview of the Softcite software extraction <lb/>pipeline. Blue boxes represent the main processing compo-<lb/>nents, and ovals the different data results. <lb/>Each ML model in GROBID is a sequence labeling model. Se-<lb/>quence labeling is defined in an abstract manner and its concrete <lb/>implementation can be selected by choosing among standard ML <lb/>architectures, including a fast linear chain CRF and a variety of <lb/>state-of-the-art Deep Learning (DL) models. Sequence labeling mod-<lb/>els are limited to the labeling of a linear sequence of text, therefore <lb/>they associate a one-dimension structure to a stream of tokens. One <lb/>way to create additional levels of embedded structures is to cas-<lb/>cade several sequence labeling models, the output of a first model <lb/>being piped to one or several models. This is the approach taken <lb/>by GROBID, and Figure 2 shows the current model cascade. Each <lb/>model can use a different sequence labeling algorithm, different <lb/>features, and different tokenizers, depending on the labels used by <lb/>the particular model, on the amount of available training data, on <lb/>the runtime, memory, and accuracy constraints, etc. <lb/>In addition to layout recognition and recovery of the reading <lb/>order, GROBID and its PDF parsing component pdfalto handle a va-<lb/>riety of text cleaning processes: UTF-8 encoding and character com-<lb/>position, de-hyphenization, reconnecting paragraphs interrupted <lb/>by a page break, a figure or a table, special font resolution, recog-<lb/>nition of formula, etc. Complementary to the support of ALTO, a <lb/>modern format for OCR output, pdfalto implements additional fea-<lb/>tures relevant to scientific documents, in particular the recognition <lb/>of superscript/subscript style and the robust recognition of line <lb/>numbers for review manuscripts. <lb/>Bibliographical references and the corresponding reference call-<lb/>outs within the text body are also identified and parsed following <lb/>a set of dedicated ML models, with a F1-score between 80 and <lb/>86%, depending on the evaluation approach, for a dataset of 1,943 <lb/>PubMed Central articles. Finally a parsed reference with incomplete <lb/>metadata is resolved against the current 105 million DOI records of <lb/>CrossRef via biblio-glutton 6 , a fast open source reference matching <lb/>service showing an accuracy of 95.39 F1-score on a set of 17,015 <lb/>raw bibliographical reference/DOI pairs extracted from PubMed <lb/>Central PDF/JATS articles (see biblio-glutton repository). <lb/>Various evaluations for the different GROBID models and for the <lb/>end-to-end process are available at the project repository 7 . These <lb/>evaluations are continuously updated with new releases. <lb/>4.2 Layout tokens, not text <lb/>If processing PDF is very challenging from a text mining perspec-<lb/>tive, this format provides a lot of advantages to the human readers 8 . <lb/>We think it is possible to exploit some of these advantages to better <lb/>present and interact with text mining results. More precisely by cal-<lb/>culating the bounding box coordinates of the extracted information <lb/>in the PDF, we make possible augmented interactive PDF. <lb/>In the complete cascading of process, GROBID does not manipu-<lb/>late text but Layout Tokens, a structure containing the Unicode text <lb/>token but also the associated available rich text information (font <lb/>size and name, style attributes, etc.) and the location in the PDF <lb/>expressed by bounding boxes. Layout Tokens are grouped following <lb/>layout criteria (lines, blocks, columns) as a first result of the PDF <lb/>layout analysis by pdfalto, and then further semantically grouped <lb/>through the ML process, as a succession of labeled fields. <lb/>Operations on 2D bounding boxes are well known and straight-<lb/>forward to apply to Layout elements. By synchronizing the bound-<lb/>ing boxes with the sequence labeling, we can render any text mining <lb/>results on their original PDF source. Text mining is then not limited <lb/>to populating a database, it allows user-friendly visualizations of <lb/>semantically enriched documents and new interactions. <lb/>Layout information is also used to instantiate layout features, <lb/>which can be exploited or not depending on the capacity of the ML <lb/>models. Layout features are crucial for the reliable recognition of <lb/>structures such as titles, abstracts, section titles, figures, tables, ref-<lb/>erence markers which are often only characterized by their relative <lb/>position (vertical space, indentation, blocks, etc.) and font style (e.g. <lb/>superscript for reference markers). While the layout features are <lb/>not directly involved in the recognition of software names, they <lb/>are used to identify and select relevant structures to be processed, <lb/>to recover valid text order, encoding, and text tokens, and in the <lb/>identification of reference markers attached to software mentions. <lb/>4.3 Identification of Candidate Mentions <lb/>Similarly as the models in GROBID, software mention recognition <lb/>is implemented as a sequence labeling task, where the labels are <lb/></body>

			<note place="footnote">6 https://github.com/kermitt2/biblio-glutton <lb/></note>

			<note place="footnote">7 https://grobid.readthedocs.io/en/latest/Benchmarking/ <lb/></note>

			<note place="footnote">8 The main advantages for the users are a fixed layout, a preservation format, a universal <lb/>support by document visualization and editing tools, the ease of storage, annotations <lb/>and additions of web links and, finally, a professional printing typeset quality. <lb/></note>

			<body>segmentation <lb/>fulltext <lb/>table <lb/>recognition of software mention candidates <lb/>figure <lb/>reference-segmenter <lb/>reference <lb/>name-citation <lb/>header <lb/>date <lb/>name-header <lb/>Figure 2: The GROBID cascade of sequence labeling models. The software mention model (in red) is added as a downstream <lb/>model applied on certain outputs of the existing GROBID models. These GROBID models correspond to the relevant textual <lb/>structures where a software mention can take place (title, abstract, and keywords from the article header; paragraphs and sec-<lb/>tion titles from the fulltext body; captions from the figure and table parts; and finally footnotes from the main segmentation). <lb/>applied to sequences of Layout Tokens, identifying relevant soft-<lb/>ware names and attributes. This sequence labeling model is added <lb/>to the GROBID cascade, applied to a selection of relevant structures <lb/>identified by upstream GROBID models. Figure 2 presents the cas-<lb/>cading process and which structures are considered for software <lb/>recognition. We call this approach structure-aware document anno-<lb/>tation, in contrast to the large majority of text mining approaches <lb/>for scientific literature which ignore the document structure and <lb/>its related semantics. Tokens are propagated to downstream mod-<lb/>els with information about the structures where they appear and <lb/>layout attributes, allowing the pipeline to exploit more contextual <lb/>information than usual approaches. <lb/>4.3.1 Undersampling, holdout set, and training set. In section 3.2, <lb/>we stressed the very challenging nature of the Imbalance Ratio <lb/>for this NER application. In ML, the CIP is usually addressed by <lb/>sampling strategies used either for removing data from the major-<lb/>ity class (undersampling) or adding data to the minority class via <lb/>artificially generated examples or Active Learning (oversampling). <lb/>With these sampling techniques, the training data is viewed as a <lb/>working resource which is crafted to maximize the accuracy of the <lb/>trained model. An n-fold cross evaluation or any random splitting <lb/>on this working set is here misleading because the distribution <lb/>of the classes is artificially modified to boost the accuracy of the <lb/>less frequent classes. For evaluating a model, in particular against <lb/>the CIP effects, we need to create a stable holdout set as close as <lb/>possible to the observed class distribution and data diversity. <lb/>Holdout set. The Holdout set is defined at document-level in <lb/>order to allow document-level evaluation. We selected 20% of the <lb/>Softcite Dataset full-texts (994 articles), reproducing the overall <lb/>distribution of documents with annotation (29.0%), the distribution <lb/>between Biomedicine and Economics fields, and we used a stratified <lb/>sampling to reproduce the overall distribution of mentions per <lb/>document. <lb/>Training set. The remaining 80% of documents (3,977 articles) <lb/>was then divided at paragraph-level into positive (1,886 paragraphs <lb/>with at least one manual annotation) and negative (612,597 para-<lb/>graphs without manual annotations). Negative examples here are <lb/>viewed as a pool of possible negative samples to be added to the <lb/>positives examples, depending on the undersampling strategies. <lb/>Undersampling methods. In this work, we focused on adapting <lb/>undersampling approaches to NER where undersampling being the <lb/>most commonly used approach for addressing CIP in classification <lb/>problems [21]. We experimented with two different approaches to <lb/>reducing the weight of the negative majority class: <lb/>‚Ä¢ Random negative sampling. Different ratio of random <lb/>negative paragraph examples are used in combination with <lb/>all the positive paragraph examples to create training sets. <lb/>We considered ratio from 1 to 50 and selected the best ratio <lb/>using a validation set. We identified the best ratio at 15, with <lb/>overall accuracy decreasing beyond 20, the positive samples <lb/>becoming too diluted in comparison with the negative ones. <lb/>‚Ä¢ Active negative sampling. A model trained only with pos-<lb/>itive examples was first created. This model was then applied <lb/>to all the paragraphs without manual annotations, and we <lb/>selected those where a mention is wrongly predicted, comple-<lb/>mented with random sampling to reach the experimentally <lb/>defined ratio. With this approach, we suppose that negative <lb/>examples where such errors occur can better contribute to <lb/>the improvement of the models for controlling false posi-<lb/>tives. <lb/>As baseline (noted none), we also ran NER models trained only <lb/>on positive paragraphs, which corresponds to the Softcite Dataset, <lb/>without any sampling. <lb/>Sequence labeling algorithms. The labeling is done paragraph <lb/>by paragraph, as extracted by the GROBID upstream models. The <lb/>following sequence labeling algorithms have been benchmarked: <lb/>Table 3: Summary of scores (P: Precision, R: Recall, F: F1-score) at span level (exact match) against the holdout set (994 complete <lb/>articles). no sampling refers to a training with only paragraphs containing at least one annotation from the 80% remaining <lb/>articles. Paragraphs without annotations (negative sampling) are then added to the training data via random sampling or active <lb/>sample. Bold indicates the best scores for a given field. Reported scores for DL models are averaged over 5 training/runs. <lb/>model <lb/>under-<lb/>software name <lb/>publisher <lb/>version <lb/>URL <lb/>F1 micro <lb/>sampling <lb/>P <lb/>R <lb/>F <lb/>P <lb/>R <lb/>F <lb/>P <lb/>R <lb/>F <lb/>P <lb/>R <lb/>F <lb/>average <lb/>CRF (custom none <lb/>29.2 58.5 38.9 41.5 76.6 53.8 51.9 84.9 64.4 18.2 68.6 28.7 <lb/>45.8 <lb/>features) <lb/>random <lb/>66.9 53.7 59.6 70.4 75.1 72.7 79.8 83.6 81.6 34.8 45.7 39.5 <lb/>66.3 <lb/>active <lb/>69.0 52.8 59.8 70.3 73.7 72.0 80.9 82.7 81.8 32.6 42.9 37.0 <lb/>66.2 <lb/>BiLSTM-CRF none <lb/>21.9 68.5 33.2 45.3 82.8 58.5 53.6 90.5 67.3 16.7 57.1 25.8 <lb/>41.9 <lb/>random <lb/>57.1 71.9 63.7 67.4 85.2 75.3 73.0 88.7 80.1 51.0 74.3 60.5 <lb/>69.0 <lb/>active <lb/>62.7 68.5 65.5 69.0 85.2 76.2 63.5 92.6 75.4 63.2 68.6 65.8 <lb/>69.8 <lb/>BiLSTM-CRF none <lb/>20.9 74.5 32.7 45.7 85.7 59.6 58.4 91.8 71.4 14.5 48.6 22.4 <lb/>41.4 <lb/>+features <lb/>random <lb/>54.1 73.6 62.4 68.5 84.2 75.5 72.2 92.2 81.0 50.0 65.7 56.8 <lb/>68.3 <lb/>active <lb/>54.5 73.3 62.5 68.2 85.2 75.7 79.5 92.2 85.4 47.5 80.0 59.6 <lb/>69.3 <lb/>BiLSTM-CRF none <lb/>35.6 74.9 48.2 71.6 79.4 75.3 72.9 88.3 79.8 11.6 80.0 20.3 <lb/>54.5 <lb/>+Elmo <lb/>random <lb/>67.4 63.0 65.1 63.9 83.7 72.5 83.1 84.9 83.9 54.8 48.6 51.5 <lb/>70.2 <lb/>active <lb/>61.9 70.4 65.9 74.1 84.7 79.0 77.7 90.5 83.6 48.0 68.6 56.5 <lb/>71.6 <lb/>Bert-base <lb/>none <lb/>15.1 74.2 25.1 40.2 79.4 53.4 42.1 87.9 56.9 04.5 71.4 08.5 <lb/>30.4 <lb/>-CRF <lb/>random <lb/>52.8 67.8 59.3 61.6 79.0 69.2 65.9 85.3 74.3 15.0 54.3 23.5 <lb/>61.9 <lb/>active <lb/>56.9 67.9 61.9 66.1 78.5 71.8 73.5 85.3 79.0 19.0 54.3 28.2 <lb/>65.3 <lb/>SciBert-CRF none <lb/>25.7 80.4 39.0 44.1 84.7 58.0 71.7 92.2 80.7 27.8 71.4 40.0 <lb/>47.6 <lb/>random <lb/>60.5 77.0 67.8 68.1 82.8 74.7 75.4 91.3 82.6 40.3 71.4 51.6 <lb/>71.4 <lb/>active <lb/>69.3 72.8 71.0 75.6 82.8 79.0 80.2 87.9 83.9 45.3 68.6 54.6 <lb/>74.6 <lb/>‚Ä¢ linear chain CRF: Conditional Random Fields with custom <lb/>feature engineering [17], <lb/>‚Ä¢ BiLSTM-CRF: Bidirectional LSTM-CRF with Gloves static <lb/>embeddings [18], <lb/>‚Ä¢ BiLSTM-CRF + features: Bidirectional LSTM-CRF with <lb/>Glove static embeddings including a feature channel, <lb/>‚Ä¢ BiLSTM-CRF+Elmo: Bidirectional LSTM-CRF with Gloves <lb/>static embeddings and Elmo dynamic embeddings [28] <lb/>‚Ä¢ Bert-base-CRF: fine-tuned Bert base model [7] with CRF <lb/>activation layer, pre-trained on general English text <lb/>‚Ä¢ SciBert-CRF: fine-tuned Bert base model pre-trained on <lb/>scientific text [3] with CRF activation layer, <lb/>The CRF implementation is based on a custom optimized fork of <lb/>Wapiti 9 [19]. The other algorithms rely on the Deep Learning library <lb/>DeLFT 10 built on top of TensorFlow and Keras. All are natively <lb/>integrated in GROBID JVM to optimize runtime. Transformers use <lb/>CRF as final activation layer, which provides a stable improvement <lb/>of 0.3 to 0.5 points in F1-score as compared to a softmax layer. <lb/>All these models take exactly the same input sequence, associ-<lb/>ated with the same features, and use strictly the same evaluation <lb/>method, avoiding some common experimental biases related to pre-<lb/>processing and evaluation. Hyperparameters of the Deep Learning <lb/>models have been set experimentally on a validation set part of the <lb/>training data and an early stop is used. <lb/>The different implemented sequence labeling algorithms give <lb/>a comprehensive picture of the best named entity recognition ap-<lb/>proaches available today. In order to select the most adapted to <lb/></body>

			<note place="footnote">9 https://github.com/kermitt2/wapiti <lb/></note>

			<note place="footnote">10 https://github.com/kermitt2/delft <lb/></note>

			<body>our application, we evaluated them against three criteria: accuracy, <lb/>runtime, and domain portability. <lb/>4.3.2 Accuracy. We observe with Table 3 that CIP is indeed a ma-<lb/>jor concern for software mention recognition. Training models <lb/>without consideration of CIP can lead to models two times less <lb/>accurate. SciBert-CRF, which is pretrained on scientific texts, per-<lb/>forms better than the general-purpose Bert-base or BiLSTM-CRF <lb/>with Gloves embeddings. BiLSTM-CRF+Elmo, although pre-trained <lb/>on general language, provides solid F1-score but benefits less from <lb/>the negative sampling than SciBert-CRF. In general, we observe <lb/>that DL models benefit significantly more from the undersampling <lb/>than CRF only. We further observe that additional custom features <lb/>has no significant impact when used by the DL models. <lb/>4.3.3 Runtime. Given the scaling constraint of scientific text min-<lb/>ing, the accuracy of the models must be considered in balance with <lb/>their runtime. Table 5 shows that the runtimes can differ extremely <lb/>from one model to another one. CRF scales particularly well on <lb/>commodity hardware, running more than three times faster than <lb/>the fastest DL approach with GPU, BiLSTM-CRF. Although accu-<lb/>rate, BiLSTM-CRF+Elmo is more than 250 times slower than CRF, <lb/>which makes impossible a usage for mining scientific literature <lb/>at scale. SciBert-CRF combines a strong accuracy with a more <lb/>acceptable runtime, 14 times faster than BiLSTM-CRF+Elmo. <lb/>4.3.4 Domain portability. One aspect often neglected in the evalu-<lb/>ation of information extraction models is the capacity of a model <lb/>to perform well on a domain different from the one it has been <lb/>trained on. As mentioned in section 3.5, this is particularly relevant <lb/>in science, a mosaic of different highly specialized languages. <lb/>Table 4: Evaluation of domain portability. The models have been trained on the PMC sub-collection and are evaluated on the <lb/>Economics domain. The numbers are F1-scores, averaged over 5 training for the indicated DL models. <lb/>Trained on Biomedicine Evaluated on Economics <lb/>micro-<lb/>models software publisher version URL average <lb/>CRF (custom features) <lb/>37.9 <lb/>13.7 <lb/>48.6 <lb/>16.0 <lb/>35.9 <lb/>BiLSTM-CRF <lb/>51.0 <lb/>22.0 <lb/>57.8 <lb/>58.3 <lb/>49.1 <lb/>BiLSTM-CRF+Elmo <lb/>53.4 <lb/>19.1 <lb/>57.1 <lb/>53.3 <lb/>51.2 <lb/>Bert-base-CRF <lb/>45.6 <lb/>17.0 <lb/>66.7 <lb/>17.0 <lb/>42.6 <lb/>SciBert-CRF <lb/>58.6 <lb/>34.8 <lb/>80.7 <lb/>46.2 <lb/>57.9 <lb/>Table 5: Average runtimes of different sequence labeling <lb/>models. The runtimes were obtained on a Ubuntu 18.04 <lb/>server Intel i7-4790 (4 CPU), 4.00 GHz with 16 GB memory. <lb/>The runtimes for the Deep Learning architectures are based <lb/>on the same machine with an Nvidia GPU GeForce 1080Ti <lb/>(11 GB). Runtime can be reproduced with a python script in <lb/>the project GitHub repository. <lb/>model best modality <lb/>layout tokens/s <lb/>CRF CPU threads: 8 <lb/>100,879 <lb/>BiLSTM-CRF GPU batch size: 200 <lb/>30,520 <lb/>BiLSTM-CRF+Elmo GPU batch size: 7 <lb/>365 <lb/>SciBert-CRF GPU batch size: 6 <lb/>5,060 <lb/>For evaluating domain portability, we have trained the models <lb/>on the PubMed Central collection (1,109 documents with at least <lb/>one software mention for a total of 6,096 annotations), covering <lb/>the biomedical domain, and performed an evaluation on articles in <lb/>Economics, a notably different domain (119 articles with at least <lb/>one software mention for a total of 538 annotations). Results are <lb/>summarized by Table 4. <lb/>While we see that covering a new domain is challenging, Deep <lb/>Learning models all show a much stronger average recognition ac-<lb/>curacy than CRF on all the fields, in particular for SciBert-CRF, the <lb/>difference of accuracy being significantly amplified in the unseen <lb/>domain. The capacity to recognize new, unseen software informa-<lb/>tion in a variety of domains is critical for the creation of knowledge <lb/>bases. SciBert-CRF shows the best portability, while combining <lb/>best accuracy and a manageable scalability. This model appears <lb/>today as the best choice for large-scale text mining and entity dis-<lb/>covery in scientific literature. <lb/>4.4 Attachment of software attributes and <lb/>reference markers <lb/>We use simple proximity rules to attach the software attributes <lb/>(version, publisher, URL) to their corresponding software name men-<lb/>tion and to attach a bibliographical reference marker to a software <lb/>mention. These rules were experimentally developed and evaluated <lb/>against the Softcite Dataset, where the relations between attributes <lb/>and software names are manually encoded. <lb/>4.4.1 Attachment of software attributes. Software attributes can be <lb/>attached to a software name (viewed as head of the full software <lb/>Table 6: Evaluation of attribute attachment to the correct <lb/>software name, for a total of 2,537 expected attachments. <lb/>attribute fields precision recall F1-score <lb/>version 99.6 <lb/>98.5 <lb/>99.1 <lb/>publisher 99.5 <lb/>98.3 <lb/>98.9 <lb/>URL 98.7 <lb/>95.4 <lb/>97.0 <lb/>biblio. reference 97.5 <lb/>100 <lb/>98.7 <lb/>all (micro-avg) 99.4 <lb/>98.2 <lb/>98.8 <lb/>component) only if they occur in the same paragraph. We did not <lb/>observe attachment beyond a paragraph in the Softcite Dataset. <lb/>Attachment of a software attribute with character offsets a_start <lb/>and a_end is as follow: <lb/>(1) attribute is discarded if no software name is present in the <lb/>paragraph, <lb/>(2) attribute ùëé is attached to the software name ùëõ at position <lb/>offsets (n_start, n_end) minimizing the following distance <lb/>function ùëë: <lb/>ùëë (ùëé, ùëõ) = <lb/>(n_start -a_end) √ó 2, if a_end ‚â§ n_start <lb/>a_start -n_end, <lb/>otherwise <lb/>4.4.2 Attachment of bibliographical reference markers. Given a soft-<lb/>ware component (composed of a software name plus zero or several <lb/>software attributes), we note ùëõ_ùëíùëõùëë the end character offset of <lb/>the software name and ùëî_ùëíùëõùëë the end character offset of the com-<lb/>plete component (maximum offsets of all the software attributes <lb/>and name positions of the software component, so if no attribute <lb/>present n_end = g_end). A bibliographical reference marker is at-<lb/>tached to a software mention group if it is overlapping the chunk <lb/>[n_end, g_end + ùëò], where ùëò is set experimentally. Based on the <lb/>Softcite Dataset, we use ùëò = 5. <lb/>4.4.3 Evaluation of the attachment rules. Table 6 presents the accu-<lb/>racy of attaching software attributes to the correct software names <lb/>in the Softcite Dataset. Given that these simple rules perform ac-<lb/>curately in the case of software mentions, we did not investigate <lb/>more sophisticated approaches. <lb/>4.5 Software usage detection <lb/>Whether or not a software mentioned in an article was in fact used <lb/>in the research is important to understand why a software is cited <lb/>and to better credit research developers for the actual impact of <lb/>Table 7: Evaluation of the usage prediction for mentioned <lb/>software. Annotated examples are from the Softcite Dataset <lb/>and the scores (P: Precision, R: Recall, F: F1-score) are micro-<lb/>average average over 10-folds. Implementations are realized <lb/>with DeLFT, a library based on Keras/TensorFlow. <lb/>software annot. <lb/>BidGRU √ó 10 <lb/>SciBERT <lb/>usage count <lb/>P <lb/>R <lb/>F <lb/>P <lb/>R <lb/>F <lb/>used 3736 <lb/>96.5 99.2 97.9 95.6 99.5 97.5 <lb/>not used 357 <lb/>86.4 57.6 69.1 88.2 45.4 60.0 <lb/>their contributions. To explore the feasibility of an ML approach <lb/>to this task, complementary binary Deep Learning classifiers have <lb/>been trained to predict if the mentioned software is used or not <lb/>in the research work described in a publication. Usage informa-<lb/>tion are encoded in the Softcite Dataset with a silver-level quality <lb/>(annotation by possibly several annotators with majority vote, but <lb/>no final reconciliation by a an expert). We hypothesize here that <lb/>the wording used to introduce and describe a software mention <lb/>can characterize its possible usage. The sentences containing the <lb/>mentions are used as classifier input, without additional features. <lb/>Results are shown in Table 7. We found that a BidGRU archi-<lb/>tecture with Glove embeddings in a 10-classifiers ensemble (com-<lb/>bination of 10 classifiers trained on 10 different partitions of the <lb/>training data), the best RNN model we have experimented, sur-<lb/>passes SciBert significantly. This result is relatively unexpected <lb/>because SciBert provides the best performance in many classifica-<lb/>tion tasks related to scientific text [3]. However, the accuracy of the <lb/>minority class not used is low for a practical usage. We will address <lb/>the performance of the models by developing more training data <lb/>and by increasing the data labeling quality to gold-standard. <lb/>4.6 Software Candidates and Entity <lb/>disambiguation <lb/>Entity disambiguation, or entity linking, is the task of matching a <lb/>raw mention to the concept it references. We consider here Wikidata <lb/>as the reference set of entities. Wikidata currently contains around <lb/>13K entities corresponding to software (excluding video games), <lb/>but also several million other scientific entities that can be used to <lb/>detect false positives. <lb/>We use entity-fishing 11 [23] as the entity disambiguation library <lb/>for the following reasons: the library is able to match in-context <lb/>mentions (exploiting wording around the mention) against the com-<lb/>plete Wikidata, is very fast in comparison to all other alternatives <lb/>(full linking up to 4800 tokens/s on a 4-core server), it does not <lb/>require a GPU, is highly memory-efficient as compared to other <lb/>existing systems, and is designed to disambiguate any terms (not <lb/>just Named Entities). It provides an accuracy close to the state of <lb/>the art. For instance, in the overall unnormalized accuracy scenario <lb/>for AIDA-CONLL-testb, it performs at 76.5 F1-score, compared to <lb/>80.27 for the recent BLINK system [34], a fine-tuned Bert consid-<lb/>erably more resource-hungry and slower, limited to the English <lb/>Wikipedia-and surpasses this system for the AQUAINT (89.1 vs. <lb/>85.88) and MSNBC (86.7 vs. 85.09) evaluation sets. <lb/></body>

			<note place="footnote">11 https://github.com/kermitt2/entity-fishing <lb/></note>

			<body>Table 8: Comparison between paragraph-level (default, <lb/>noted ) and document-level (noted doc.) processing with <lb/>and without entity disambiguation filtering (F1-scores on <lb/>holdout set, average of 5 train/runs). <lb/>fields <lb/>SciBERT-CRF <lb/>SciBERT-CRF + <lb/>entity disambiguation <lb/>doc. <lb/>doc. <lb/>software <lb/>71.0 74.1 (+3.1) 74.3 (+3.3) 76.7 (+5.7) <lb/>publisher 79.0 76.2 (-2.8) 81.9 (+2,9) 78.3 (-0.7) <lb/>version <lb/>83.9 84.7 (+0.8) 87.1 (+3.2) 88.3 (+4.4) <lb/>URL <lb/>54.6 64.9 (+10.3) 55.4 (+0.8) 66.7 (+12,1) <lb/>micro-avg 74.6 76.4 (+1.8) 77.7 (+3.1) 79.1 (+4.5) <lb/>The disambiguation is realized with an ensemble approach (Gra-<lb/>dient Tree Boosting) using a variety of features, including semantic <lb/>vector similarity [4] and graph-based relatedness measure [24]. The <lb/>context to be disambiguated is defined as the paragraph where a <lb/>candidate mention occurs. The amount of text to be disambiguated <lb/>is thus limited on average to a few paragraphs per document and <lb/>remains scalable. <lb/>The first usage of the entity disambiguation is to filter out false <lb/>positives. If a candidate software mention is likely a known scien-<lb/>tific entity other than software in its context of usage, we discard <lb/>the candidate. Table 8 shows the corresponding improvement of <lb/>software mention recognition with a disambiguation score thresh-<lb/>old of 0.4 (any non-software entities predicted above this threshold <lb/>is considered as false positive software mention). We think that <lb/>these results are encouraging and could be further improved by <lb/>additional custom training of the disambiguation, the selection of <lb/>improved contexts and extending Wikidata to cover more research <lb/>software entities from other curated sources. <lb/>The second usage is more traditional entity linking. The men-<lb/>tions successfully disambiguated are enriched with Wikidata and <lb/>English Wikipedia page identifiers. These identifiers and related <lb/>information can help to further deduplicate mentions correspond-<lb/>ing to the same software across different publications. Although <lb/>entity-fishing is comprehensively benchmarked for general-domain <lb/>texts, we do not currently have data manually annotated at entity <lb/>level to evaluate the particular case of software entity linking. <lb/>4.7 Document-level processing <lb/>When processing a complete document, it is frequent that only some <lb/>instances of mentions of the same software are identified, while <lb/>others remained unlabeled. The wording around some mentions and <lb/>the presence of other software attributes impact the performance of <lb/>recognition. To address this, we apply a propagation of the matched <lb/>software names to other unlabeled occurrences of the same term <lb/>in the same document. <lb/>To avoid propagating a mention to spurious common and short <lb/>strings, we apply a TF-IDF threshold to the software names to be <lb/>propagated, keeping only those which are significant given the <lb/>background full Softcite corpus. Table 8 presents an evaluation of <lb/>this step with a TF-IDF threshold experimentally set at 0.001 using <lb/>a validation set of the training data. <lb/>Table 9: Results of a re-harvested version of CORD-19 using <lb/>the metadata file dated 2021-03-22. <lb/>total count <lb/>total Open Access full texts <lb/>211,213 <lb/>-with at least one annot. <lb/>76,448 <lb/>software names <lb/>295,609 <lb/>-with linked Wikidata ID <lb/>117,193 <lb/>publishers <lb/>61,804 <lb/>versions <lb/>104,199 <lb/>URL <lb/>27,916 <lb/>-with biblio. references <lb/>49,184 <lb/>references with DOI <lb/>15,931 <lb/>references with PMID <lb/>10,611 <lb/>5 APPLICATION TO CORD-19 <lb/>We applied the Softcite software recognizer with a SciBert-CRF <lb/>model trained on the complete Softcite Dataset, to the CORD-19 <lb/>publications [32], see Table 9. The CORD-19 corpus contains partial <lb/>full-texts as JSON files. However, we started with the CORD-19 <lb/>metadata file only and carried out a new and complete harvesting <lb/>of the Open Access documents, in order to be able to use the PDF <lb/>versions and more complete structured full-texts. Our full-text har-<lb/>vester is available online 12 and relies on the Unpaywall dataset [29] <lb/>to identify URL of full-text papers based on metadata and DOI. <lb/>Our harvesting retrieved around 41K full-texts more than the offi-<lb/>cial CORD-19 document set, allowing us to exploit more PDFs for ex-<lb/>tracting annotations coordinates and more accurate text structures. <lb/>As indicated in Figure 1, PDF are processed via pdfalto and GROBID <lb/>components while XML JATS files are processed by Pub2TEI 13 , a <lb/>collection of style sheets developed over 12 years able to transform <lb/>a variety of publisher XML format to the same TEI XML format as <lb/>produced by GROBID. This common format, which supersedes a <lb/>dozen of publisher formats and many of their variants, centralizes <lb/>further processing across PDF and heterogeneous XML sources. <lb/>Figure 3 shows how annotations can be visualized on the har-<lb/>vested PDF. When successful, entity linking allows users to access <lb/>Wikidata and Wikipedia descriptions of the software. Similarly, <lb/>when a bibliographical reference has been associated with a soft-<lb/>ware in the document, users can visualize the metadata information <lb/>of the publication, and, when available, directly access its Open <lb/>Access full-text via Unpaywall. <lb/>A single 4-core server with one GPU (GeForce 1080Ti) and 16 <lb/>GB RAM can process an average of 0.53 PDFs per second. The <lb/>processing includes the full PDF processing, document structuring, <lb/>the software recognition with the best SciBert-CRF model, entity <lb/>disambiguation of the candidates software, additional document-<lb/>level propagation and finally the resolution against 105M CrossRef <lb/>records of possible bibliographical references attached to the soft-<lb/>ware mentions. This runtime makes it possible to process 20 million <lb/>articles (an estimate of the total available Open Access research <lb/>articles) in one month with around 15 similar servers, a reasonable <lb/>setting with today&apos;s scientific computing capacities. <lb/></body>

			<note place="footnote">12 https://github.com/kermitt2/article-dataset-builder <lb/>13 https://github.com/kermitt2/Pub2TEI <lb/></note>

			<body>Figure 3: Augmented PDF using the Softcite text mining tool: <lb/>mentions of software and their attributes are display on top <lb/>of the PDF as HTML dynamic layout, via the standard PDF.js <lb/>library (left part). The user can interact directly in situ with <lb/>the annotations, opening info boxes with Wikidata disam-<lb/>biguated information and local consolidated bibliographical <lb/>reference relevant to the software. <lb/>6 CONCLUSION <lb/>In this work, we addressed several practical challenges often ne-<lb/>glected when applying state-of-the-art NLP techniques to scientific <lb/>literature: processing of raw PDF documents, rich text, specialized <lb/>vocabularies, extremely imbalance NER, and scalability require-<lb/>ments. In addition, we developed a &quot;layout-aware&quot; text mining <lb/>approach for scientific documents, where layout information can <lb/>be propagated throughout the entire pipeline, making possible <lb/>for users to directly interact with annotations on PDF and access <lb/>entity-level information. We think that this approach can enable <lb/>novel applications to value research software as first-class scientific <lb/>contributions and more generally better credit research software. <lb/>We are currently applying the Softcite software mention rec-<lb/>ognizer on a set of 15 million Open Access articles to create a <lb/>Knowledge Base of research software in combination with curated <lb/>resources 14 . We hope that this Knowledge Base and our data-driven <lb/>approach will help to better understand the impact of software in <lb/>scientific research, support reuse and collaboration around open <lb/>source development, and ultimately benefit the dissemination of <lb/>science and technologies across society. <lb/></body>

			<div type="acknowledgement">ACKNOWLEDGMENTS <lb/>We would like to acknowledge the support of the Alfred P. Sloan <lb/>Foundation, Grant/Award Number: 2016-7209, and of the Gordon <lb/>and Betty Moore Foundation, Grant Number: 8622. <lb/></div>

			<page>14 https://github.com/softcite/softcite_kb <lb/></page>

			<listBibl>REFERENCES <lb/>[1] K. Ahmad and S. Collingham. 1996. POINTER Project Final Report. Technical Re-<lb/>port. University of Surrey. http://www.computing.surrey.ac.uk/ai/pointer/report. <lb/>[2] Abbas Akkasi, Ekrem Varoƒülu, and Nazife Dimililer. 2018. Balanced undersam-<lb/>pling: a novel sentence-based undersampling method to improve recognition of <lb/>named entities in chemical and biomedical text. Applied Intelligence 48, 8 (01 Aug <lb/>2018), 1965-1978. https://doi.org/10.1007/s10489-017-0920-5 <lb/>[3] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language <lb/>Model for Scientific Text. arXiv:1903.10676 [cs.CL] <lb/>[4] Roi Blanco, Giuseppe Ottaviano, and Edgar Meij. 2015. Fast and Space-Efficient <lb/>Entity Linking in Queries. In Proceedings of the Eight ACM International Conference <lb/>on Web Search and Data Mining (Shanghai, China) (WSDM 15). ACM, 10 pages. <lb/>[5] C Boettiger, S Chamberlain, E Hart, and K Ram. 2015. Building Software, Build-<lb/>ing Community: Lessons from the rOpenSci Project. Journal of Open Research <lb/>Software 3, 1 (2015), e8. https://doi.org/10.5334/jors.bu <lb/>[6] Franck Dernoncourt, Ji Young Lee, and Peter Szolovits. [n.d.]. NeuroNER: an <lb/>easy-to-use program for named-entity recognition based on neural networks. <lb/>([n. d.]). arXiv:1705.05487 http://arxiv.org/abs/1705.05487 <lb/>[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: <lb/>Pre-training of Deep Bidirectional Transformers for Language Understanding. <lb/>arXiv:1810.04805 [cs.CL] <lb/>[8] Caifan Du, Johanna Cohoon, Patrice Lopez, and James Howison. 2021. Softcite <lb/>dataset: A dataset of software mentions in biomedical and economic research <lb/>publications. Journal of the Association for Information Science and Technology <lb/>(2021). https://doi.org/10.1002/asi.24454 <lb/>[9] Geraint Duck, Aleksandar Kovacevic, David L Robertson, Robert Stevens, and <lb/>Goran Nenadic. 2015. Ambiguity and variability of database and software names <lb/>in bioinformatics. Journal of biomedical semantics 6, 1 (2015), 29. <lb/>[10] Geraint Duck, Goran Nenadic, Michele Filannino, Andy Brass, David L Robertson, <lb/>and Robert Stevens. 2016. A survey of bioinformatics database and software <lb/>usage through mining the literature. PloS one 11, 6 (2016). <lb/>[11] Daniel Garijo, Maximiliano Osorio, Deborah Khider, Varun Ratnakar, and Yolanda <lb/>Gil. 2019. OKG-Soft: An Open Knowledge Graph with Machine Readable Scientific <lb/>Software Metadata. In 2019 15th International Conference on eScience (eScience). <lb/>IEEE, San Diego, CA, USA, 349-358. https://doi.org/10.1109/eScience.2019.00046 <lb/>[12] Martin Gerner, Goran Nenadic, and Casey M. Bergman. [n.d.]. LINNAEUS: A <lb/>species name identification system for biomedical literature. 11, 1 ([n. d.]), 85. <lb/>https://doi.org/10.1186/1471-2105-11-85 <lb/>[13] Maryam Habibi, Leon Weber, Mariana Neves, David Luis Wiegandt, and Ulf Leser. <lb/>[n.d.]. Deep learning with word embeddings improves biomedical named entity <lb/>recognition. 33, 14 ([n. d.]), i37-i48. https://doi.org/10.1093/bioinformatics/ <lb/>btx228 <lb/>[14] James Howison and Julia Bullard. 2016. Software in the Scientific Literature: <lb/>Problems with Seeing, Finding, and Using Software Mentioned in the Biology <lb/>Literature. Journal of the Association for Information Science and Technology 67, 9 <lb/>(2016), 2137-2155. https://doi.org/10.1002/asi.23538 <lb/>[15] Daniel S. Katz, Daina Bouquin, Neil P. Chue Hong, Jessica Hausman, Catherine <lb/>Jones, Daniel Chivvis, Tim Clark, Merc√® Crosas, Stephan Druskat, Martin Fenner, <lb/>Tom Gillespie, Alejandra Gonzalez-Beltran, Morane Gruenpeter, Ted Habermann, <lb/>Robert Haines, Melissa Harrison, Edwin Henneken, Lorraine Hwang, Matthew B. <lb/>Jones, Alastair A. Kelly, David N. Kennedy, Katrin Leinweber, Fernando Rios, <lb/>Carly B. Robinson, Ilian Todorov, Mingfang Wu, and Qian Zhang. 2019. Software <lb/>Citation Implementation Challenges. arXiv:1905.08674 [cs] (May 2019). http: <lb/>//arxiv.org/abs/1905.08674 arXiv: 1905.08674. <lb/>[16] Frank Kr√ºger and David Schindler. 2020. A Literature Review on Methods for <lb/>the Extraction of Usage Statements of Software and Data. Computing in Science <lb/>Engineering 22, 1 (Jan. 2020), 26-38. https://doi.org/10.1109/MCSE.2019.2943847 <lb/>[17] John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional <lb/>random fields: Probabilistic models for segmenting and labeling sequence data. <lb/>(2001). <lb/>[18] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, <lb/>and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. <lb/>arXiv:1603.01360 [cs.CL] <lb/>[19] Thomas Lavergne, Olivier Capp√©, and Fran√ßois Yvon. 2010. Practical Very Large <lb/>Scale CRFs. In Proceedings the 48th Annual Meeting of the Association for Com-<lb/>putational Linguistics (ACL) (Uppsala, Sweden). Association for Computational <lb/>Linguistics, 504-513. http://www.aclweb.org/anthology/P10-1052 <lb/>[20] Jaebeen Lee and Lea Deleris. 2020. Sequencing, Combining and Sampling Classi-<lb/>fiers to Help Find Needles in Haystacks. In 24th European Conference on Artificial <lb/>Intelligence, ECAI (Santiago de Compostela, Spain). <lb/>[21] Joffrey L. Leevy, Taghi M. Khoshgoftaar, Richard A. Bauder, and Naeem Seliya. <lb/>2018. A survey on addressing high-class imbalance in big data. Journal of Big <lb/>Data 5, 1 (01 Nov 2018), 42. https://doi.org/10.1186/s40537-018-0151-6 <lb/>[22] Patrice Lopez. 2009. GROBID: Combining automatic bibliographic data recogni-<lb/>tion and term extraction for scholarship publications. In International conference <lb/>on theory and practice of digital libraries. Springer, 473-474. <lb/>[23] Patrice Lopez. 2017. entity-fishing. In WikiDataCon. Berlin, Germany. https: <lb/>//upload.wikimedia.org/wikipedia/commons/5/50/Entity-fishing.pdf <lb/>[24] David Milne and Ian H. Witten. 2013. An open-source toolkit for mining <lb/>Wikipedia. Artificial Intelligence 194 (2013), 222-239. https://doi.org/10.1016/ <lb/>j.artint.2012.06.007 Artificial Intelligence, Wikipedia and Semi-Structured Re-<lb/>sources. <lb/>[25] Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast <lb/>and Robust Models for Biomedical Natural Language Processing. In Proceedings <lb/>of the 18th BioNLP Workshop and Shared Task. Association for Computational <lb/>Linguistics, Florence, Italy, 319-327. https://doi.org/10.18653/v1/W19-5034 <lb/>[26] J.M. Nicholson, M. Mordaunt, P. Lopez, A. Uppala, D. Rosati, N.P. Rodrigues, <lb/>P. Grabitz, and S.C. Rife. 2021. scite: a smart citation index that displays the <lb/>context of citations and classifies their intent using deep learning. bioRxiv (2021). <lb/>https://doi.org/10.1101/2021.03.15.435418 <lb/>[27] IB Ozyurt, JS Grethe, ME Martone, and AE Bandrowski. 2016. Resource Dis-<lb/>ambiguator for the Web: Extracting Biomedical Resources and their Citations <lb/>from the Scientific Literature. PLoS ONE 11, 1 (2016), e0146300. <lb/>https: <lb/>//doi.org/10.1371/journal.pone.0146300 <lb/>[28] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher <lb/>Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word <lb/>representations. In Proceedings of NAACL-HLT. 2227-2237. <lb/>[29] Heather Piwowar, Jason Priem, and Richard Orr. 2019. <lb/>The Fu-<lb/>ture of OA: A large-scale analysis projecting Open Access publica-<lb/>tion and readership. <lb/>bioRxiv (2019). <lb/>https://doi.org/10.1101/795310 <lb/>arXiv:https://www.biorxiv.org/content/early/2019/10/09/795310.full.pdf <lb/>[30] Tim Rockt√§schel, Michael Weidlich, and Ulf Leser. [n.d.]. ChemSpot: a hybrid <lb/>system for chemical named entity recognition. 28, 12 ([n. d.]), 1633-1640. https: <lb/>//doi.org/10.1093/bioinformatics/bts183 <lb/>[31] Katrin Tomanek and Udo Hahn. 2009. Reducing class imbalance during active <lb/>learning for named entity annotation. In Proceedings of the fifth international <lb/>conference on Knowledge capture. 105-112. <lb/>[32] Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, <lb/>Darrin Eide, Kathryn Funk, Rodney Michael Kinney, Ziyang Liu, William. Merrill, <lb/>Paul Mooney, Dewey A. Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, <lb/>Brandon Stilson, Alex D. Wade, Kuansan Wang, Christopher Wilhelm, Boya Xie, <lb/>Douglas M. Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. <lb/>2020. CORD-19: The Covid-19 Open Research Dataset. ArXiv (2020). <lb/>[33] David Westergaard, Hans-Henrik Staerfeldt, Christian T√∏nsberg, Lars Juhl Jensen, <lb/>and S√∏ren Brunak. 2017. Text mining of 15 million full-text scientific articles. <lb/>(jul 2017). https://doi.org/10.1101/162099 <lb/>[34] Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. <lb/>2020. Zero-shot Entity Linking with Dense Entity Retrieval. In EMNLP. </listBibl>


	</text>
</tei>
