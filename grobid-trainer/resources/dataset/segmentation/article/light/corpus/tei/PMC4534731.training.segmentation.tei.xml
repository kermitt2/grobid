<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>©Copyrights 2015. The Korean Academy of Conservative Dentistry. <lb/> 249 <lb/> This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ <lb/>by-nc/3.0) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited. <lb/>Statistical notes for clinical researchers: Type I and <lb/>type II errors in statistical decision <lb/>Open lecture on statistics <lb/>ISSN 2234-7658 (print) / ISSN 2234-7666 (online) <lb/>http://dx.doi.org/10.5395/rde.2015.40.3.249 <lb/></front>

			<body>Statistical inference is a procedure that we try to make a decision about a population <lb/>by using information from a sample which is a part of it. In modern statistics it is <lb/>assumed that we never know about a population, and there is always a possibility to <lb/>make errors. Theoretically a sample statistic may have values in a wide range because <lb/>we may select a variety of different samples, which is called a sampling variation. To <lb/>get practically meaningful inference we preset a certain level of error. In statistical <lb/>inference we presume two types of error, type I and type II errors. <lb/>Null hypothesis and alternative hypothesis <lb/>The first step of statistical testing is the setting of hypotheses. When comparing <lb/>multiple group means we usually set a null hypothesis. For example, &quot;There is no <lb/>true mean difference,&quot; is a general statement or a default position. The other side is <lb/>an alternative hypothesis such as &quot;There is a true mean difference.&quot; Often the null <lb/>hypothesis is denoted as H 0 and the alternative hypothesis as H 1 or H a . To test a <lb/>hypothesis, we collect data and measure how much the data support or contradict the <lb/>null hypothesis. If the measured results are similar to or only slightly different from <lb/>the condition stated by the null hypothesis, we do not reject and accept H 0 . However, <lb/>if the dataset shows a big and significant difference from the condition stated by the <lb/>null hypothesis, we regard that there is enough evidence that the null hypothesis is <lb/>not true and reject H 0 . When a null hypothesis is rejected, the alternative hypothesis is <lb/>adopted. <lb/>Type I and type II errors <lb/>As we assume that we never directly know the information of the population, we <lb/>never know whether the statistical decision is right or wrong. Actually, the H 0 may be <lb/>right or wrong and we could make a decision of the acceptance or the rejection of H 0 . <lb/>In a situation of statistical decision, there may be four different occasions as presented <lb/>in Table 1. Two situations lead correct conclusions that true H 0 is accepted and false H 0 <lb/>is rejected. However, the others are two incorrect erroneous situations that false H 0 is <lb/>accepted and true H 0 is rejected. A Type I error or alpha (α) error refers to an erroneous <lb/>rejection of true H 0 . Conversely, a Type II error or beta (β) error refers to an erroneous <lb/>acceptance of false H 0 . <lb/>Making some level of error is unavoidable because fundamental uncertainty lies in <lb/>a statistical inference procedure. As allowing errors is basically harmful, we need to <lb/>control or limit the maximum level of errors. Which type of error is more risky between <lb/>type I and type II errors? Traditionally, committing type I error has been considered <lb/>more risky, and thus more strict control of type I error has been performed in statistical <lb/>inference. <lb/>When we have interest in the null hypothesis only, we may think about type I error <lb/>only. Let&apos;s consider a situation that someone develops a new method and insists that <lb/></body>

			<front>Hae-Young Kim* <lb/>Department of Health Policy and <lb/>Management, College of Health <lb/>Science, and Department of Public <lb/>Health Sciences, Graduate School, <lb/>Korea University, Seoul, Korea <lb/>*Correspondence to <lb/>Hae-Young Kim, DDS, PhD. <lb/>Associate Professor, Department <lb/>of Health Policy and Management, <lb/>College of Health Science, and <lb/>Department of Public Health <lb/>Sciences, Graduate School, Korea <lb/>University, 145 Anam-ro, Seongbuk-<lb/>gu, Seoul, Korea 136-701 <lb/>TEL, +82-2-3290-5667; FAX, +82-2-<lb/>940-2879; E-mail, kimhaey@korea. <lb/>ac.kr <lb/></front>

			<page>250 <lb/></page>

			<note place="footnote">www.rde.ac <lb/></note>

			<note place="headnote">Kim HY <lb/></note>

			<body>H 0 <lb/>H 1 <lb/>Reject H 0 <lb/>Fail to reject H 0 <lb/>Reject H 0 <lb/>-2 <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>1 -α = 0.95 <lb/>β <lb/>1 -β <lb/>α/2 = 0.025 <lb/>X <lb/>Figure 1. Illustration of type I (α) and type II (β) errors. <lb/>it is more efficient than conventional methods but the new method is actually not more efficient. The truth is H 0 that says <lb/>&quot;The effects of conventional and newly developed methods are equal.&quot; Let&apos;s suppose the statistical test results support the <lb/>efficiency of the new method, which is an erroneous conclusion that the true H 0 is rejected (type I error). According to the <lb/>conclusion, we consider adopting the newly developed method and making effort to construct a new production system. <lb/>The erroneous statistical inference with type I error would result in an unnecessary effort and vain investment for nothing <lb/>better. Otherwise, if the statistical conclusion was made correctly that the conventional and newly developed methods <lb/>were equal, then we could comfortably stay with the familiar conventional method. Therefore, type I error has been strictly <lb/>controlled to avoid such useless effort for an inefficient change to adopt new things. <lb/>In other example, let&apos;s think that we are interested in a safety issue. Someone developed a new method which is actually <lb/>safer compared to the conventional method. In this situation, null hypothesis states that &quot;Degrees of safety of both <lb/>methods are equal&quot;, when the alternative hypothesis that &quot;The new method is safer than conventional method&quot; is true. Let&apos;s <lb/>suppose that we erroneously accept the null hypothesis (type II error) as the result of statistical inference. We erroneously <lb/>conclude equal safety and we stay on the less safe conventional environment and have to be exposed to risks continuously. <lb/>If the risk is a serious one, we would stay in a danger because of the erroneous conclusion with type II error. Therefore, not <lb/>only type I error but also type II error need to be controlled. <lb/>Schematic example of type I and type II errors <lb/>Figure 1 shows a schematic example of relative sampling distributions under a null hypothesis (H 0 ) and an alternative <lb/>hypothesis (H 1 ). Let&apos;s suppose they are two sampling distributions of sample means ( ). H 0 states that sample means are <lb/>normally distributed with population mean zero. H 1 states the different population mean of 3 under the same shape of <lb/>sampling distribution. For simplicity, let&apos;s assume the standard error of two distributions is one. Therefore, the sampling <lb/>distribution under H 0 is assumed as the standard normal distribution in this example. In statistical testing on H 0 with an <lb/>alpha level 0.05, the critical values are set at ± 2 (or exactly 1.96). If the observed sample mean from the dataset lies <lb/>within ± 2, then we accept H 0 , because we don&apos;t have enough evidence to deny H 0 . Or, if the observed sample mean lies <lb/>beyond the range, we reject H 0 and adopt H 1 . In this example we can say that the probability of alpha error (two-sided) <lb/>is set at 0.05, because the area beyond ± 2 is 0.05, which is the probability of rejecting the true H 0 . As seen in Figure <lb/>X <lb/>Table 1. Possible results of hypothesis testing <lb/>Conclusion based on data <lb/>Truth <lb/>H 0 True <lb/>H 0 False <lb/>Reject H 0 <lb/>Type I error (α) <lb/>Correct conclusion <lb/>(Power = 1 -β) <lb/>Fail to reject H 0 <lb/>Correct conclusion (1 -α) <lb/>Type II error (β) <lb/></body>

			<note place="footnote">http://dx.doi.org/10.5395/rde.2015.40.3.249 <lb/></note>

			<page>251 <lb/></page>

			<note place="footnote">www.rde.ac <lb/></note>

			<body>1, extreme values larger than absolute 2 can appear under H 0 with the standard normal distribution ranging to infinity. <lb/>However, we practically decide to reject H 0 , because the extreme values are too different from the assumed mean, zero. <lb/>Though the decision includes a probability of error of 0.05, we allow the risk of error because the difference is considered <lb/>sufficiently big to reach a reasonable conclusion that the null hypothesis is false. As we never know the truth whether the <lb/>sample dataset we have is from the population H 0 or H 1 , we can make decision only based on the value we observe from the <lb/>sample data. <lb/>Type II error is shown as the area lower than 2 under the distribution of H 1 . The amount of type II error can be calculated <lb/>only when the alternative hypothesis suggest a definite value. In Figure 1, a definite mean value of 3 is used in the alternative <lb/>hypothesis. The critical value 2 is one standard error (= 1) smaller than mean 3 and is standardized to z = -1 (= <lb/>) in a <lb/>standard normal distribution. The area less than z = -1 is 0.16 (yellow area) in standard normal distribution. Therefore, the <lb/>amount of type II error is obtained as 0.16 in this example. <lb/>Relationship and affecting factors on type I and type II errors <lb/>1. Related change of both errors <lb/>Type I and type II errors are closely related. If all other conditions are the same, the reduction of Type I error level <lb/>accompanies the increase of type II error level. When we decrease alpha error level from 0.05 to 0.01, the critical value <lb/>moves outward to around ± 2.58. As the result, beta level will increase to around 0.34 in Figure 1, if all other conditions <lb/>are the same. Conversely, moving the determinant line to the left side will cause both decrease of type II error level and <lb/>increase of type I error level. Therefore, the determination of error level should be a procedure considering both error types <lb/>simultaneously. <lb/>2. Effect of distance between H 0 and H 1 <lb/>If H 1 suggest a bigger center, e.g., 4 instead of 3, then the distribution moves to the right. If we fix the alpha level as <lb/>0.05, then the beta level gets smaller than ever. If the center value is 4 then z value is -2 and the area less than -2 in the <lb/>standard normal distribution is obtained as 0.025. If all other condition is the same, the increase of distance between H 0 <lb/>and H 1 decrease the beta error level. <lb/>3. Effect of sample size <lb/>Then how do we maintain both error levels lower? Increasing the sample size is one answer, because a large sample size <lb/>reduce standard error (standard deviation/√sample size) when all other conditions retained as the same. Smaller standard <lb/>error can produce more concentrated sampling distributions with slender curve under both null and alternative hypothesis <lb/>and the consequent overlapping area gets smaller. As sample size increases, we can get satisfactory low levels of both alpha <lb/>and beta errors. <lb/>Statistical significance level <lb/>Type I error level of is often called a significance level. In a statistical testing, we reject the null hypothesis when the <lb/>observed value from the dataset is located in area of extreme 0.05 and conclude there is evidence of difference from the null <lb/>hypothesis when we set the alpha level at 0.05. As we consider the difference over the level is statistically significant, the <lb/>level is called a significance level. Sometimes the significance level is expressed using p value, e.g., &quot;Statistical significance <lb/>was determined as p &lt; 0.05.&quot; P value is defined as the probability of obtaining the observed value or more extreme values when <lb/>the null hypothesis is true. Figure 2 shows that type I error level at 0.05 and a two-sided p value of 0.02. The observed z <lb/>value 2.3 was located in the rejection region with p value of 0.02, which is smaller than the significance level 0.05. Small <lb/>p value indicates that the probability of observing such a dataset or more extreme cases is very low under the assumed null <lb/>hypothesis. <lb/>Statistical power <lb/>Power is the probability of rejecting a false null hypothesis, which is the other side of type II error. Power is calculated <lb/>as 1-Type II error (β). In Figure 1, type II error level is 0.16 and power is obtained as 0.84. Usually a power level of 0.8 -<lb/>0.9 is required in experimental studies. Because of the relationship between type I and type II errors, we need to keep a <lb/>minimum required level of both errors. Sufficient sample size is needed to keep the type I error low as 0.05 or 0.01 and the <lb/>power high as 0.8 or 0.9. <lb/>2 -3 <lb/>1 <lb/></body>

			<note place="footnote">http://dx.doi.org/10.5395/rde.2015.40.3.249 <lb/></note>

			<page>252 <lb/></page>

			<note place="footnote">www.rde.ac <lb/></note>

			<back>

				<listBibl>Reference <lb/>1. Rosner B. Fundamentals of Biostatistics. 6th ed. Belmont: Duxbury Press; 2006. p226-252. <lb/>H 0 <lb/>Reject H 0 <lb/>Fail to reject H 0 <lb/>Reject H 0 <lb/>-2.3 <lb/>2.3 <lb/>α/2 = 0.025 <lb/>Figure 2. Significance level and p value. <lb/>Z <lb/>α/2 = 0.025 <lb/>p/2 = 0.01 <lb/>p/2 = 0.01 <lb/></listBibl>
			
			</back>
			
			<note place="headnote">Kim HY <lb/></note>

			<note place="footnote">http://dx.doi.org/10.5395/rde.2015.40.3.249 </note>


	</text>
</tei>
