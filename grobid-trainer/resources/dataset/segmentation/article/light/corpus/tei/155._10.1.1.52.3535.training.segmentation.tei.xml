<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<titlePage>Random Striping for <lb/>News on Demand Servers <lb/>Juan Alemany and Jayram S. Thathachar <lb/>Technical Report UW-CSE-97-02-02 <lb/>February, 1997 <lb/>Department of Computer Science and Engineering <lb/>University of Washington <lb/>Box 352350 <lb/>Seattle, WA 98195 <lb/></titlePage>

			<front>Random Striping for News on Demand Servers <lb/>Juan Alemany and Jayram S. Thathachar <lb/>fjuanito,jayramg@cs.washington.edu <lb/>Department of Computer Science and Engineering <lb/>University of Washington <lb/>Box 352350 <lb/>Seattle, Washington 98195{2350 <lb/>Abstract <lb/>We address the problem of assigning blocks of video streams to disks in News on Demand video <lb/>servers. Speci cally, given the common assumption that video streams are striped across the disks, <lb/>we consider how to map successive blocks to disks to maximize performance. <lb/>Our work begins by noting that the commonly proposed round-robin assignment, in which suc-<lb/>cessive blocks of every stream are mapped to successive disks, provides the guarantee that once a <lb/>video stream starts it will never miss a deadline, but that the latency to start a stream may be high <lb/>due to the \convoy e ect.&quot; <lb/>We propose and analyze an alternative scheme, the random mapping of successive blocks to <lb/>disks. Using both analytic and simulation techniques, we show that random mapping and replication <lb/>signi cantly reduce the latency to start a new stream, and the number of missed deadlines is negligible. <lb/>Further, we prove that with enough copies, no request sequence will miss any deadlines, for loads <lb/>that are a signi cant fraction of the maximum supportable by the hardware. <lb/></front>

			<body>1 Introduction <lb/>News on Demand servers are expected to provide thousands of users with immediate access to hundreds <lb/>of video clips with the latest breaking news. These servers will store a huge number of short clips with <lb/>a constantly changing distribution concentrated only on the latest or more sensational news. <lb/>Using the same implementation strategies of Movies on Demand servers to implement News on <lb/>Demand servers results in high latency. Most video server prototypes use round robin striping to <lb/>provide high throughput and real time guarantees under arbitrary request distributions. However, a <lb/>combination of a skewed access pattern and short clips can cause a signi cant fraction of requests to <lb/>experience relatively high latency. Storing multiple copies of each clip in the server alleviates this <lb/>problem but does not eliminate it altogether. <lb/>We present random striping, an alternative organization for news servers that guarantees minimal <lb/>response time at the expense of strong real time guarantees. This scheme replicates each video a small <lb/>number of times and randomly assigns pieces of each copy to all disks. Given the random nature of <lb/>the assignment, this scheme introduces the possibility that the server will be unable to deliver all video <lb/></body>

			<page>1 <lb/></page>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>2 <lb/></page>

			<body>frames to some set of requests in a timely fashion. If this happens, some viewers will experience a <lb/>discontinuity in video playback, also known as a hiccup. <lb/>In this paper we provide experimental evidence that random striping is a better strategy than round <lb/>robin striping for News on Demand servers. Our experiments show that random striping provides <lb/>minimal latency and does not su er from hiccups even under loads close to the full server bandwidth. <lb/>In contrast, our experiments show that round robin striping at similar loads produces high latency for <lb/>a signi cant fraction of requests. <lb/>We present theoretical evidence that con rms that random striping does not su er from hiccups <lb/>even under high loads. We prove that with enough copies, for any request distribution, and for any <lb/>xed fraction of the maximum server bandwidth, random striping produces no hiccups. We provide <lb/>conservative lower bounds on the number of copies necessary to make sure that there are no hiccups. <lb/>For comparison, we show the estimates of the maximum hiccup{free load predicted by our result, and <lb/>in a simple experiment, we show a much more practical estimate of the hiccup{free load supported by <lb/>our scheme. <lb/>The paper is organized as follows: Section 2 introduces the basic facts about video servers, striping <lb/>and replication. Section 3 introduces random striping and explains why it guarantees low latency but <lb/>may su er hiccups. Section 4 describes round robin striping and explains why requests might su er high <lb/>latency. Section 5 contains an experimental evaluation of random striping and round robin striping for <lb/>a News on Demand application. Section 6 presents our theoretical result guaranteeing no hiccups for <lb/>any distribution of requests; the proof of this result is presented in the appendix. The last two sections, <lb/>7 and 7.1, contain respectively, a summary of related work and a conclusion. <lb/>2 Video Servers, Striping and Replication <lb/>This section brie y describes the basic operation of disk based video servers, and introduces the basics <lb/>of striping and replication. <lb/>A video server consists of a collection of disks connected to a group of viewers by a network. Video <lb/>clips are digitally compressed and divided into blocks of equal size before they are stored on disks. The <lb/>server operates in a series of rounds of equal length: in each round, the server retrieves the next block <lb/>of data for each playing clip and the network delivers it to the appropriate viewer. We assume that a <lb/>block is stored contiguously on one disk. In order to provide a smooth ow of images, a block must <lb/>contain enough video data to last each viewer for the duration of a round and the server must be able <lb/>to retrieve the next blocks for all viewers before the end of the round. If the server fails to retrieve a <lb/>block before the end of the round, the viewer will perceive a discontinuity which we call a hiccup. <lb/>Striping is a technique for spreading out data on multiple disks to balance load and improve latency <lb/>and throughput. A striped server partitions each video into small consecutive blocks which are dis-<lb/>tributed among multiple disks. Spreading blocks of a video clip over several disks allows multiple disks <lb/>to serve each request, balancing load. The key parameters a ecting performance in striping are the size <lb/>of each block and the number of disks that a clip is striped over. Smaller blocks result in lower latency <lb/>and allow better load balancing when clips are very short. Larger blocks result in higher latency but <lb/>utilize disks more e ciently, improving throughput. Striping for video servers is discussed in BMG + 94], <lb/>Che94] and BBD + 96]. <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>3 <lb/></page>

			<body>Replication is a technique for improving latency and throughput. A replicated server stores multiple <lb/>copies of each video clip in di erent sets of disks. This improves latency because a new request for a clip <lb/>is more likely to nd a disk with free bandwidth and a copy of the rst block of the requested clip. In a <lb/>News on Demand server, replication is essential for throughput because clips are so short that striping <lb/>may not span all disks in the server. Replication in combination with striping is discussed in Che94]. <lb/>3 Random Striping <lb/>Random striping is a scheme that combines replication and striping to randomly lay out video data on <lb/>disks. In this section we describe this layout and present an algorithm that assigns requests to disks in <lb/>a way that obeys the bandwidth constraints of each disk. The following table de nes the parameters <lb/>that characterize the disks, clips, degree of replication and the maximum load on the system. <lb/>d number of replicas of each clip. <lb/>F number of blocks that must be stored in the system. <lb/>S number of block requests in a round. <lb/>N c maximum number of blocks that can be stored on each disk. <lb/>S c maximum number of blocks served by a disk in a round. <lb/>3.1 The Data Layout <lb/>We make d copies of each block and randomly assign all the copies to dF <lb/>Nc disks as follows: we number <lb/>the copies (there are totally dF of them) and the \block slots&quot; available in all disks (each disk stores <lb/>N c blocks so there are dF <lb/>Nc N c = dF block slots available) and then choose a one-to-one assignment <lb/>of copies to slots at random. Note that consecutive blocks of any single clip are placed completely <lb/>independent of each other. <lb/>We represent the above layout as a bipartite multi-graph G = (A; B; E), where A is the set of F <lb/>blocks, B is the set of (dF )=N c disks, and E denotes the set of edges between A and B. Each block in <lb/>A is connected to exactly those disks in B where its d copies are stored. Note that in this graph the <lb/>vertices in A have degree d whereas all vertices in B have degree N c . G is a multi-graph because two <lb/>copies of the same block may be assigned to the same disk. <lb/>3.2 Assigning Requests to Disks <lb/>We now describe how the server assigns requests for blocks to the appropriate disks in each round. <lb/>Finding an assignment of requests to disks is not trivial, because each block request should be assigned <lb/>to a disk containing one of its copies and no disk should be assigned more requests than it can serve in <lb/>a round(S c ). <lb/>Our algorithm for assigning requests to disks treats all requests identically without distinction between <lb/>new requests and requests which have already started. At each round, each of the S users requests a <lb/>block, which results in a set X A of at most S blocks 1 that the server must supply. We de ne a <lb/>bipartite subgraph H = (X; Y; E 0 ) associated with X. The set Y consists of dF <lb/>Nc S c vertices obtained <lb/></body>

			<note place="footnote">1 It is fewer than S blocks if two users arrived in the same round and requested the same clip. <lb/></note>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>4 <lb/></page>

			<body>by replicating each vertex in B exactly S c times. As before, a vertex x of X is connected to a vertex y <lb/>of Y if and only if one of the copies of the block x is stored in the disk associated with y. <lb/>At each round, we compute the graph H and run the bipartite matching algorithm on this graph to <lb/>maximally match the vertices in X. This assigns each matched vertex in X to a unique vertex in Y . <lb/>Since each disk is represented by S c vertices in Y this matching assigns no more than S c requests to <lb/>each disk. If we match every vertex in X, then we have an assignment of all the requests to disks such <lb/>that at most S c blocks are assigned to each disk, so it will be possible to serve all blocks requests in this <lb/>round without hiccups. Hiccups only occur when H does not have a matching that includes all vertices <lb/>in X. In this case, we satisfy those requests that are matched and all the viewers with unmatched <lb/>requests skip their blocks. <lb/>3.3 Properties of Random Striping <lb/>Random striping ensures that all requests are satis ed with minimal latency but exposes requests to <lb/>the possibility of hiccups. Requests are satis ed immediately because when a request arrives, the server <lb/>will include a block request for it that will get matched in the next round. On the other hand, there <lb/>are no a priori guarantees that all blocks will be matched in each round, so a viewer may su er hiccups <lb/>whenever the block it requested does not get matched. <lb/>4 Round Robin Striping <lb/>Most movie server prototypes use a variant of round robin striping to lay out video data on disks and <lb/>serve requests. This section describes a version of robin striping and shows how data is laid out, and <lb/>how the server assigns requests to disks. <lb/>4.1 Description of Round Robin Striping <lb/>First, we describe how round robin lays out data on disks. We describe wide striping, where each clip <lb/>is striped across all disks in the server. Suppose that we make d copies of each clip. Let us order the <lb/>disks arbitrarily and number them consecutively. The rst block of each copy of any clip is placed at <lb/>an arbitrarily chosen disk (e.g. at random) and the remaining blocks of that copy, in order, are placed <lb/>in consecutive disks (with wrap-around if necessary). <lb/>We assign requests for the blocks to disks as follows: when a new request arrives, the server noti es <lb/>the least loaded disk which contains the rst block of the new request that it should serve that block <lb/>at the start of the next round. For each clip already playing, once a disk has served some block of that <lb/>clip in a round, the next disk (with wrap-around if necessary) will serve the next block (if any) in the <lb/>next round. Whenever a disk receives more requests for blocks than it can serve, it gives priority to <lb/>clips which are already playing and delays serving the rst block to a new request until a subsequent <lb/>round. A new request may queue up for multiple rounds at the disk which has been assigned to it until <lb/>the disk has free bandwidth to serve it. Therefore, random robin exposes requests to latency. <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>5 <lb/></page>

			<body>4.2 Properties of Round Robin Striping <lb/>Prioritizing old requests guarantees that viewers never su er hiccups. This is because all the requests <lb/>move from disk to disk in the same direction so, at the beginning of each round, each disk will be able <lb/>to serve all the old requests which were served by the previous disk in the previous round. <lb/>In round robin striping, requests are susceptible to latency when the system is heavily loaded. Under <lb/>heavy load, each disk is likely to be serving many requests during a round. When a new request <lb/>arrives, it is likely to be delayed by the old requests that are passing through that disk in that round. <lb/>Furthermore, when the new request starts, it will join the tail of the convoy of requests that initially <lb/>held it up, worsening the situation for the next new request. This e ect is exacerbated when a few clips <lb/>are very popular, and when storing short clips because a lot of requests enter the system at each round, <lb/>and all these requests descend on a small set of disks. <lb/>5 Experimental Evaluation of Striping Strategies for News on Demand <lb/>In this section we compare both striping strategies for a news on demand server. We describe our <lb/>simulations, show our results, and discuss the relative merits of round robin and random striping. <lb/>5.1 News on Demand Workload <lb/>We simulated a 50 disk server with 1, 2, 3, and 4 copies of 100 clips. We assumed that each disk can <lb/>serve 5 seconds of video in a one second round. These values correspond to the transfer rate of a current <lb/>disk drive and a display rate of 6.0 Mbs. Each clip lasts 30 seconds, and the request pattern obeys a <lb/>Zipf Knu73] distribution. In both striping schemes, we divided each clip into blocks of one second of <lb/>video. Thus S c = 5, F = 3000, d varies, and N c is su ciently large. <lb/>We chose these parameters (number of clips, disks and requests) to reduce simulation time. Simulating <lb/>one round with random striping requires that we compute a matching on a bipartite graph which takes <lb/>about half a second in a SGI workstation so simulating a long run of the server requires in the order of <lb/>an hour of computing time. <lb/>5.2 Experimental Methodology <lb/>We subjected the server to varying loads; for each load we xed the number of initial requests in the <lb/>system and replaced each request that nished with a new request. To prevent requests from moving <lb/>in lockstep through the system our simulation waits a random interval between 1 and 5 rounds before <lb/>a new request starts. Therefore for each load level, the maximum and average load di er slightly. The <lb/>rst two rows of Figure 1 show the maximum and average loads we placed on the server for both striping <lb/>schemes. We allowed the simulations to warm up for 600 rounds, and then we took our measurements <lb/>for the next 600 requests. <lb/>For a server using random striping, our goal was to evaluate the number of hiccups as we increased <lb/>the load for varying number of copies of each clip. We counted hiccups by agging all the unmatched <lb/>vertices. For simplicity, we assumed that when a request su ers a hiccup it skips the current block and <lb/>requests the next block in the next round. <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>6 <lb/></page>

			<body>For a server using round robin striping we measured the response time of all requests. For each load, <lb/>we measured the maximum response time experienced by 90 and 95 percent of the requests. <lb/>5.3 Performance of Random Striping <lb/>In this section, we present our simulation results for random striping. Figure 1 shows the result of our <lb/>experiments: with as few as two copies the number of hiccups is zero for most loads and negligible at <lb/>maximum load. The rst two rows of the gure show the increasing load on the server. The rst row <lb/>shows each value of the load while the second row shows the average number of requests for that load. <lb/>For each d (the number of copies in the server) the gure shows the number of hiccups, the number of <lb/>blocks that were requested, the number of requests served and the number of requests that su ered a <lb/>hiccup. <lb/>Maximum Number of Requests <lb/>125 <lb/>150 <lb/>175 <lb/>200 <lb/>225 <lb/>250 <lb/>Average Number of Requests <lb/>118 <lb/>141 <lb/>165 <lb/>188 <lb/>212 <lb/>236 <lb/>One Copy <lb/>Blocks Requested <lb/>175709 210953 246026 281292 316337 351423 <lb/>Hiccups (Blocks Not Served) <lb/>5051 10190 18398 28562 42557 58290 <lb/>Total Requests <lb/>5867 <lb/>7031 <lb/>8211 <lb/>9388 10548 11724 <lb/>Requests w. Hiccups <lb/>2130 <lb/>3324 <lb/>4403 <lb/>5562 <lb/>6717 <lb/>7844 <lb/>Two Copies <lb/>Blocks Requested <lb/>175820 210900 246052 281251 316263 351343 <lb/>Hiccups (Blocks Not Served) <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>9 <lb/>Total Requests <lb/>5862 <lb/>7038 <lb/>8213 <lb/>9385 10555 11719 <lb/>Requests w. Hiccups <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>9 <lb/>Three Copies <lb/>Blocks Requested <lb/>175835 210800 246137 281143 316185 351356 <lb/>Hiccups (Blocks Not Served) <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>Total Requests <lb/>5868 <lb/>7034 <lb/>8222 <lb/>9381 10553 11726 <lb/>Requests w. Hiccups <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>Figure 1: This gure shows the number of blocks served, the number of hiccups, the number of requests <lb/>processed by the server, and the number of requests a ected by hiccups. All requests experience a <lb/>response time of at most one round. The gure shows that random striping with a single copy su ers <lb/>a large number of hiccups with with at least two copies the number of hiccups becomes negligible. <lb/>From Figure 1 we see that with a single copy the server su ers an enormous fraction of hiccups at any <lb/>load. However, once we place at least 2 copies the matching algorithm succeeds in assigning virtually <lb/>all block requests to disks and the simulation registers almost no hiccups. <lb/>5.4 Performance of Round Robin Striping <lb/>In this section we present the result of our experimental evaluation of round robin striping. <lb/>Figure 2 shows that for the 90 and 95 percentile of requests, striping 1 or 2 copies yields relatively <lb/>large response times while striping 3 or 4 copies results in relatively small response times. Figure 2 <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>7 <lb/></page>

			<body>0 <lb/>5 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>35 <lb/>40 <lb/>120 <lb/>140 <lb/>160 <lb/>180 <lb/>200 <lb/>220 <lb/>240 <lb/>260 <lb/>Response Time <lb/>Requests (Max = 250) <lb/>Response Time of 90% of Requests <lb/>1 copy 90% <lb/>2 copy 90% <lb/>3 copy 90% <lb/>4 copy 90% <lb/>0 <lb/>5 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>35 <lb/>40 <lb/>120 <lb/>140 <lb/>160 <lb/>180 <lb/>200 <lb/>220 <lb/>240 <lb/>260 <lb/>Response Time <lb/>Requests (Max = 250) <lb/>Response Time of 95% of Requests <lb/>1 copy 95% <lb/>2 copy 95% <lb/>3 copy 95% <lb/>4 copy 95% <lb/>(a) <lb/>(b) <lb/>Figure 2: This gure shows the maximum response time number of 90 and 95 percent of requests for a <lb/>server with 1,2,3 and 4 copies of each clip. Figure (a) shows response time for 90 percent and (b) shows <lb/>the response time for 95 percent of requests. These graphs show that latency is signi cant at high load <lb/>and that replication helps reduce latency. The labels on the horizontal axis correspond to the maximum <lb/>load values. The corresponding average load is in the second row of Figure 1 <lb/>show that striping a single copy of each clip results in large response times for the 90 and 95 percentile <lb/>of the requests. Adding one extra copy improves the response times somewhat but response times do <lb/>not drop below 5 seconds even when the server uses 3 or 4 copies of each clip. <lb/>6 Theoretical Conditions for Hiccup-Free Random Striping <lb/>In this section we show that random striping with a su ciently large number of copies can satisfy any <lb/>load arbitrarily close to the maximum server bandwidth ((dF=N c ) S c ) without any hiccups. Our main <lb/>result on random striping is the following: let F, N c , S c and S be inputs, and let d be the smallest <lb/>number of copies that satis es the following inequality: <lb/>N c S <lb/>dS c F <lb/>d? 1 <lb/>Sc e <lb/>1 <lb/>Sc +1 F <lb/>S &lt; 1 <lb/>4 <lb/>(1) <lb/>Then, if we randomly stripe with d copies, with probability 1 ? O(1=F), the resulting layout produces <lb/>no hiccups for any set of S requests. In other words, this random layout has the property that all sets of <lb/>S block requests can assigned to disks such that no disk serves no more than S c requests. This ensures <lb/>that the matching algorithm always succeeds. The proof of this result appears in the appendix, where <lb/>we consider a more general problem that has potential uses in other applications. <lb/>Observe that in the expression in the left hand side of Equation 1, if we choose d to be large enough, <lb/>then the only signi cant term that determines the value of the expression is NcS <lb/>dScF . Rewriting this <lb/>expression as <lb/>S <lb/>(dF=Nc) Sc , we can see that for any S arbitrarily close to the maximum server bandwidth <lb/>((dF=N c ) S c ), we can d large enough to satisfy Equation 1. <lb/>It is important to understand the role of randomness in this result. If we assign d copies of each <lb/>block to disks at random, we have a very high probability of getting a layout that will never produce <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>8 <lb/></page>

			<body>any hiccups. We make no assumption about the distribution of requests; the probability in this result <lb/>is about nding good layouts. Our result shows that good layouts are plentiful, and once we nd one, <lb/>it will be able to satisfy all sets of requests. <lb/>Once the layout is chosen, it never changes. Although we can not e ciently verify that our layout is <lb/>good (this is co-NP hard BKV + 81]), we will know it is not good if a set of requests cannot be assigned <lb/>to disks without creating a bottleneck. If this happens we can choose another random layout. While <lb/>these re{assignment operations are expensive, they only happen with very low probability (O(1=F )). <lb/>6.1 Evaluation of Theoretical Estimates <lb/>In this section we evaluate our theoretical result by calculating the maximum load that is guaranteed to <lb/>be free of hiccups using the same parameters as our previous simulation on a News on Demand server. <lb/>Then we argue that the values we obtained are too conservative because of our theoretical analysis. <lb/>We computed the maximum load that we support using the parameters (d, F, S c and N c ) of our <lb/>round robin simulation and calculating the maximum S that satis es Equation 1. Figure 3 shows the <lb/>maximum load as a function of the number of copies for a 50 disk server with 100 clips of 30 seconds <lb/>each. The number in the second column is the maximum load that the server can support with no <lb/>hiccups, minimal latency (i.e. at most one round), under any distribution of requests. <lb/>Copies Maximum Theoretical Load <lb/>2 <lb/>8 <lb/>3 <lb/>56 <lb/>4 <lb/>96 <lb/>5 <lb/>123 <lb/>6 <lb/>143 <lb/>7 <lb/>157 <lb/>8 <lb/>168 <lb/>9 <lb/>177 <lb/>Figure 3: This gure shows the maximum load guaranteed to be hiccup free by the theoretical result. <lb/>These requests will have 1 round latency and no hiccups under any distribution of requests. The server <lb/>has bandwidth for 250 requests. We believe these values are disappointingly low due to the conservative <lb/>analysis. <lb/>Given that the maximum load the server can support is 250, the numbers in Figure 3 are surprisingly <lb/>low. We believe this is the result of our conservative analysis. We expect that a di erent proof technique <lb/>or a tighter analysis can show that the server can support a higher load without hiccups. <lb/>To validate our intuition that these values were a result of excessively conservative analysis we con-<lb/>ducted the following simple experiment. We simulated a server with 5000 blocks replicated d times on <lb/>50 d disks. Our goal was to evaluate how many blocks we could consistently retrieve in each round <lb/>without su ering any hiccups. Each disk can serve 5 blocks. We derived the theoretical maximum load <lb/>using Equation 1 for values of d ranging from 2 to 9. We also computed the maximum number of blocks <lb/>retrieved with the following experiment: for each value of d we randomly assigned 5000 d copies to <lb/>50 d disks. For each value of d we estimated the maximum hiccup free load as follows: we chose a <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>9 <lb/></page>

			<body>starting value of S and retrieved S blocks chosen uniformly at random. Whenever the match failed to <lb/>retrieve all S blocks we decreased S to the number of blocks it matched. When S did not change after <lb/>500 rounds matching S items, we declared it a maximum load. We show these values of S in the graph <lb/>in Figure 4. <lb/>0 <lb/>500 <lb/>1000 <lb/>1500 <lb/>2000 <lb/>2500 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>number of requests <lb/>number of copies <lb/>Theoretical Prediction Versus Simulation <lb/>predicted <lb/>actual <lb/>optimal <lb/>Figure 4: This graph shows the optimal load (labeled \optimal&quot;), the maximum load estimated by <lb/>our experiment (labeled \actual&quot;), and the maximum hiccup free load predicted by our result (labeled <lb/>\predicted&quot;). The vertical axis shows the maximum number of blocks that can be drawn without <lb/>hiccups. The horizontal axis shows d, the number of replicas of each block (the number of disks <lb/>increases linearly with d in this example). <lb/>The graph shows that for two or more copies, the estimate of maximum load is close to the maximum <lb/>server bandwidth and much larger than the load predicted by the result. <lb/>7 Related Work <lb/>Video servers have received a lot of attention in the last few years and several studies have touched on <lb/>the performance of round robin striping for video servers ( BMG + 94], ORS96], TDM + 95], Che94], <lb/>CPK95], BBD + 96]). <lb/>Several authors have identi ed and proposed schemes to address high latency in round robin striping. <lb/>Chervenak et al. ( Che94], CPK95]) studied round robin striping on disks with a Zipf workload and <lb/>found that for large disks arrays a signi cant fraction of requests experienced high latency. They <lb/>showed that striping 2 copies of each movie reduced latency but that replication in proportion to movie <lb/>popularity was essential to almost eliminate latency. Their solution depends on a priori knowledge of <lb/>an unchanging request distribution. Ghandeharizadeh et al. GK95b] developed an analytical model <lb/>to estimate the expected startup latency of a request as a function of load and server con guration. <lb/>They propose replication and request migration to reduce latency. Bolosky et al. BBD + 96] describe <lb/>an implementation of a server using round robin striping and mention that latency in striped servers <lb/>is analogous to insertion in a open hash table with sequential chaining. They propose using bounds <lb/>on hashing to determine the maximum load a server can handle before exceeding a threshold average <lb/>latency. <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>10 <lb/></page>

			<body>We are not aware of any available studies of random striping. In fact, we are not aware of previous <lb/>work using bipartite matching to assign requests to servers. Tewari et al. mention in TMD + 95] and <lb/>TDM + 95] an IBM Research report that compares the performance of random striping to round robin <lb/>striping. Tewari informed us that the report is not publicly available. From the brief discussion of that <lb/>report in TMD + 95] it does not appear that it considered random striping with replication. Matching <lb/>is only necessary if there are multiple copies of each block. <lb/>Several studies ( DKS95]) consider other techniques that can be used in addition to striping to <lb/>reduce latency. Most of these techniques are dynamic, such as replicating popular items to adapt to <lb/>high demand, moving requests to lightly loaded devices during playback, and caching popular items in <lb/>memory. <lb/>Several studies ( Mou96], BBD + 96]) have stated that striping makes video servers more susceptible <lb/>to disk failures and have considered replication for fault tolerance. We have not studied the fault tolerant <lb/>properties of random striping. <lb/>7.1 Summary and Conclusion <lb/>Most disk{based movie servers use round robin striping because it o ers high throughput and strong <lb/>real time guarantees. Unfortunately, round robin striping can produce high latency for an application <lb/>like News on Demand where clips are short and most requests access only a few very popular items. <lb/>Although striping with replication alleviates this problem, it does not eliminate it altogether. <lb/>Random striping is a less known striping technique that guarantees minimal latency. However, <lb/>because of the random nature of the assignment of data to disks, it is possible that the server will su er <lb/>hiccups. <lb/>Our main practical contribution is an experimental evaluation of random striping and round robin <lb/>striping for a News on Demand application. We show that, under heavy load, round robin striping <lb/>produces high latency for a signi cant fraction of all requests. In contrast, we show that random <lb/>striping not only provides minimal latency but su ers no hiccups under a wide variety of loads. At <lb/>the highest load we simulated, random striping with two copies su ers a negligible number of hiccups. <lb/>With three copies, random striping su ers no hiccups even at our maximum load. <lb/>Our main theoretical contribution is a result that gives conditions for con guring a random striped <lb/>server which guarantee no hiccups under any request distribution. Our conditions require at least two <lb/>copies of each clip, and the number of requests that can be served with strong guarantees increases with <lb/>the number of copies. <lb/>Unfortunately, our theoretical analysis is too conservative, and its estimates for when a server never <lb/>has hiccups are too low (although a xed fraction of the full server bandwidth). We believe this to be <lb/>the result of our analysis rather than an intrinsic property of the scheme. To test this hypothesis we <lb/>ran a little experiment to test the maximum load it could serve for 500 rounds without a hiccup. We <lb/>found that the load was very close to the maximum load for 2 copies of each block and extremely close <lb/>for 3 or more copies. <lb/>We believe random striping with a few copies of each clip is a simple layout strategy that o ers high <lb/>throughput and minimal latency for all request distributions. Our theoretical result shows that it su ers <lb/>no hiccups when lightly loaded and our experiments suggests it su ers no hiccups at all but the highest <lb/></body>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>11 <lb/></page>

			<body>load. <lb/></body>

			<listBibl>References <lb/>BMG + 94] Berson S., Muntz R., Ghandeharizadeh S., Xiangyu Ju, \Staggered Striping in multimedia <lb/>information systems&quot; SIGMOD Record, Vol. 23, Number 2, pp. 79{90 June 1994. <lb/>Mou96] Antoine Mourad, \Doubly{Striped Disk Mirroring: Reliable Storage for Video Servers&quot; <lb/>Multimedia Tools and Applications 2, 273-297 (1996), Kluwer Academic Publishers. <lb/>TDM + 95] Renu Tewari, Dan Dias, Rajat Mukherjee, and Harrick Vin, \High Availability in Clustered <lb/>Video Servers&quot; IBM Research Report RC-20108 , June 1995. <lb/>TMD + 95] Renu Tewari, Rajat Mukherjee, Dan Dias, and Harrick Vin \Design and Performance Trade-<lb/>o s in Clustered Multimedia Servers&quot; Proceedings of IEEE-ICMCS,Tokyo, June 1996. <lb/>ORS96] Ozden B., Rastogi R., and Silberschatz A., \Disk striping in video server environments&quot;, <lb/>Proceedings of the International Conference on Multimedia Computing and Systems, IEEE <lb/>Comput. Soc. Press. Los Alamitos, CA, USA. 1996. <lb/>BKV + 81] M. Blum, R.M. Karp, O. Vornberger, C.H. Papdimitriou, and M. Yannakakis \The Com-<lb/>plexity of Testing Whether a Graph is a Superconcentrator&quot; Information Processing Letters, <lb/>13:164-167, 1981. <lb/>BBD + 96] William J. Bolosky, Joseph S. Barrera, III, Richard P. Draves, Robert P. Fitzgerald, Garth <lb/>A. Gibson, Michael B. Jones, Steven P. Levi, Nathan P. Myhrvold, Richard F. Rashid. \The <lb/>Tiger Video Fileserver&quot;, Proceedings of the Sixth International Workshop on Network and <lb/>Operating System Support for Digital Audio and Video. IEEE Computer Society, Zushi, <lb/>Japan, April, 1996. <lb/>Che94] Ann L. Chervenak, \Tertiary Storage: An Evaluation of New Applications&quot; PhD Thesis, <lb/>University of California at Berkeley Technical Report UDB/CSD 94/847, December, 1994. <lb/>CPK95] Ann L. Chervenak, David A. Patterson, and Randy H. Katz, \Storage Systems for Movies{ <lb/>on{Demand Video Servers&quot; Proceedings of the Fourteenth IEEE Symposium on Mass Storage <lb/>Systems, Monterey, CA, USA. pp. 246-56. IEEE Comput. Soc. Tech. Committee on Mass <lb/>Storage Syst. 11-14 Sept. 1995. <lb/>DKS95] Asit Dan, Martin Kienzle, Dinkar Sitaram, \A dynamic policy of segment replication for <lb/>load{balancing in video{on{demand servers&quot; Multimedia Systems, Vol.3, Number 3. pp. <lb/>93-103. July 1995 <lb/>GK95a] Shahram Ghandeharizadeh, Seon Ho Kim, \Striping in multi-disk video servers.&quot; Proceed-<lb/>ings of the SPIE -The International Society for Optical Engineering. vol.2604. pp. 88-102. <lb/>1996. Conference on High-Density Data Recording and Retrieval Technologies, Philadelphia, <lb/>PA, USA. SPIE. 23-24 Oct. 1995. <lb/></listBibl>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>12 <lb/></page>

			<listBibl>GK95b] Shahram Ghandeharizadeh and Seon Ho Kim \An Analysis of Striping in Scalable Multi{ <lb/>Disk Video Servers&quot;, University of Southern California Technical Report 95-623, 1995. <lb/>http://www.usc.edu/dept/cs/technical reports.html <lb/>Knu73] Donald E. Knuth, \The Art of Computer Programming Vol. III&quot; Addison Wesley Publishing <lb/>Company, 1973 <lb/></listBibl>

			<div type="annex">8 Appendix <lb/>Consider the following static model: we are given a collection of disks each of which can store N c blocks <lb/>and can serve any of S c &lt; N c blocks that it stores. 2 Suppose that there are a total of F blocks and <lb/>S = F users, for some , who can each request anyone of the F blocks. We assume that there are no <lb/>more than k requests to any single block. (For the News on Demand problem, k = 1 since it is su cient <lb/>to just serve a single copy of each block even though there may be multiple requests to that block.) <lb/>By way of motivation, consider the problem of obtaining a general layout scheme that uses replication <lb/>and has the required property that all sets of S requests can be satis ed immediately. We can obtain <lb/>some crude estimates on the minimum number of disks C that is required by any such scheme. Since <lb/>F=N c is the minimum number of disks required to store at least one copy of each block and S=S c is the <lb/>minimum number of disks required to serve S distinct blocks,overcome the bandwidth constraint, it is <lb/>easy to see that C max fS=S c ; F=N c g. If we restrict the accesses so all blocks requested are distinct <lb/>(k = 1 in our workload model) we can solve this problem with F=S c disks. In this case the solution is <lb/>to store only S c items in each disk. Another solution is to make S=S c copies of each block, for a total <lb/>of FS=N c S c disks, and assign them to disks such that no two copies of a block are in the same disk. <lb/>Thus, we have C min fF=S c ; FS=N c S c g. The drawback in the rst scheme is that wastes N c ? S c <lb/>spaces in each disk. The problem with the second is that the number of copies grows linearly with the <lb/>user population. <lb/>To minimize the cost of the solution, we want to make as few replicas as possible. Unfortunately, <lb/>for arbitrary F, S, k, S c and N c it is not easy to determine which is the minimum number of copies <lb/>necessary. Even worse, even if we guess the necessary number of copies, it is not clear which assignment <lb/>of copies to disks has the property that any set of requests can be immediately satis ed. <lb/>The main problem addressed in this section is the following: given F; S; k; S c ; N c , what is the smallest <lb/>value of d such that if we replicate each block d times and distribute all the resulting blocks among dF=N c <lb/>disks at random, 3 then any set of at most S blocks can be accessed immediately. By \immediately&quot; <lb/>we mean that for any set of requests, each request is assigned to a disk in such a way that no disk is <lb/>assigned more than S c requests (so that no request has to wait for another request to complete). <lb/>The algorithm for assigning requests to disks is a slight modi cation to the algorithm used in Section 3. <lb/>The graph G is the same as before, but in the graph H, we add as many vertices for each block as the <lb/>number of times it appears in the request set. Formally, for a (multi)set of at most S requests where <lb/>each block does not appear more than k times, construct the bipartite graph H = (X; Y; E 0 ) as follows: <lb/></div>

			<note place="footnote">2 A block may be served many times as long as the total number of blocks that is served including duplicates is at most <lb/>Sc. <lb/></note>

			<note place="footnote">3 This random placement can be achieved by numbering the copies of each block (for a total of dF copies) and numbering <lb/>the \block slots&quot; available in all disks (each disk stores Nc blocks so there are dF <lb/>Nc Nc = dF block slots available) and <lb/>then choosing a one-to-one assignment of copies to slots at random. <lb/></note>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>13 <lb/></page>

			<div type="annex">For each block represented by some node v 2 L that appears j times in the request set, for some <lb/>j k, add j new vertices v 1 ; : : : ; v j to X. <lb/>For each disk represented by some node u 2 R, add S c new vertices u 1 ; : : : ; u Sc to Y . <lb/>As before a vertex x of X is connected to a vertex y of Y if and only if the block associated with <lb/>x is stored in the disk associated with y. <lb/>Now construct a matching on this graph that matches every vertex in X. This matching gives an <lb/>assignment of blocks to disks such that each disk serves no more than S c blocks. <lb/>We will prove below that the matching always exists for the parameters stated above with an appro-<lb/>priate choice of S. This value of S is given by the largest value that satis es the inequality of Equation 2, <lb/>which we state below: <lb/>N c S <lb/>dF <lb/>d? k e k+1 kF <lb/>S &lt; 1 <lb/>4 <lb/>(2) <lb/>Note that for k = 1, this corresponds to the inequality in Equation 1 Consider the \bad&quot; event that for <lb/>some set of at most S requests there is no matching of the set X 0 , jX 0 j S, associated with these set <lb/>of requests in the request graph H. By Hall&apos;s theorem, this implies that for some set X 0 X, the size <lb/>of the neighborhood set N(X 0 ) Y of X in H must be less than than jXj. <lb/>Let U L be the set of nodes (blocks) in G associated with the nodes in X 0 and letV R be the set <lb/>of nodes (disks) associated with the nodes in N(X 0 ). Observe that N(U) =V in G, jX 0 j=k jUj S <lb/>and jV j = jN(X 0 )j, where = 1=S c . Since jN(X 0 )j &lt; jX 0 j, we can infer that <lb/>jV j &lt; 1 <lb/>S c <lb/>jX 0 j <lb/>minfkjUj; Sg: <lb/>Given such aV , we can always obtain a V by adding dummy nodes toV , so that jV j = minfkjUj; Sg <lb/>and N(U) V . <lb/>Summarizing the observations above, the probability that the layout is bad is bounded by the prob-<lb/>ability that there is some U L and some V R, where jUj S and jV j = minfkjUj; Sg, such <lb/>that N(U) is contained in V . We will bound this latter quantity by estimating for an arbitrary U and <lb/>V , the probability that N(U) V and then summing up over all choices of U and V satisfying these <lb/>constraints. <lb/>Set jUj = u and jV j = v. For constructing G, we chose a random 1{1 assignment that connected d <lb/>edges from each vertex in L to the c \block slots&quot; of each vertex in R. The probability of N(U) V <lb/>is the number of assignments that map all du edges originating from U to the N c v \edge slots&quot; of all <lb/>vertices in V , divided by the total number of assignments. This is N c v <lb/>du (du)!(dF ? du)! divided by <lb/>(dF )!, which can be simpli ed to N c v <lb/>du = dF <lb/>du . <lb/>Summing up over all choices of the set U and the set V , we have that the probability of a bad layout <lb/>is bounded by <lb/>X <lb/>1 u S;v= minfku;Sg <lb/>F <lb/>u <lb/>dF=N c <lb/>v <lb/>N c v <lb/>du <lb/>dF <lb/>du <lb/>: <lb/></div>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>14 <lb/></page>

			<div type="annex">In order to show this probability is small, we will simplify and bound the above expression. We use <lb/>the standard inequalities for binomial coe cients: n <lb/>r <lb/>? ne <lb/>r <lb/>r and for m &lt; n, m <lb/>r = n <lb/>r <lb/>? m <lb/>n <lb/>r . <lb/>Applying this in the equation above, we have, <lb/>X <lb/>1 u S;v= minfku;Sg <lb/>Fe <lb/>u <lb/>u dFe <lb/>N c v <lb/>v N c v <lb/>dF <lb/>du : <lb/>(3) <lb/>We will split this sum into two main parts: for the rst part, 1 u S=k which implies that v = ku; <lb/>for the second part S=k &lt; u S whence v = S. We will now show that each part can be bounded to <lb/>a small quantity. <lb/>Case 1 (1 u S=k): <lb/>Substituting v = ku in Equation 3, we obtain <lb/>Fe <lb/>u <lb/>u dFe <lb/>N c ku <lb/>ku N c ku <lb/>dF <lb/>du <lb/>= <lb/>&quot; u <lb/>F <lb/>d? k?1 e k+1 N c k <lb/>d <lb/>d? k # u <lb/>: <lb/>(4) <lb/>We will now use the constraint on d as given by Equation 2 to simplify the above expression. First, we <lb/>re-arrange Equation 2 to derive the following inequality: <lb/>N c k <lb/>d <lb/>d? k &lt; 1 <lb/>4 e ? k?1 kF <lb/>S <lb/>d? k?1 <lb/>(5) <lb/>If we substitute this inequality in Equation 4 we get <lb/>&quot; u <lb/>F <lb/>d? k?1 e k+1 1 <lb/>4 e ? k?1 kF <lb/>S <lb/>d? k?1 # u <lb/>= <lb/>&quot; ku <lb/>S <lb/>d? k?1 1 <lb/>4 <lb/># u <lb/>(6) <lb/>Now, we can split the sum into three parts: <lb/>k <lb/>S <lb/>d? k?1 1 <lb/>4 + <lb/>log F <lb/>X <lb/>u=2 <lb/>&quot; ku <lb/>S <lb/>d? k?1 1 <lb/>4 <lb/># u <lb/>+ <lb/>S=k <lb/>X <lb/>u=log F <lb/>&quot; ku <lb/>S <lb/>d? k?1 1 <lb/>4 <lb/># u <lb/>(7) <lb/>Recall that d 3 and S c 1 means that d ? ? 1 1. The rst term of the sum is obviously O(1=F). <lb/>The second term is O((log F)( log F <lb/>F ) 2 ) which is O(1=F). The third term is O(F ( 1 <lb/>4 log F ) = O(F 1 <lb/>F 2 ) <lb/>which is O(1=F). <lb/>Case 2 (S=k &lt; u S) <lb/>Here we substitute v = S in Equation 3 to obtain <lb/>Fe <lb/>u <lb/>u dFe <lb/>N c S <lb/>S N c S <lb/>dF <lb/>du <lb/>= F <lb/>u <lb/>u e S+u N c S <lb/>dF <lb/>du? S <lb/></div>

			<note place="headnote">Random Striping for News on Demand Servers <lb/></note>

			<page>15 <lb/></page>

			<div type="annex">N c S <lb/>dF <lb/>d? k e k+1 kF <lb/>S <lb/>! u N c S <lb/>dF <lb/>k(u? S <lb/>k ) <lb/>Because F <lb/>u <lb/>u <lb/>kF <lb/>S <lb/>u and e S e ku ] <lb/>1 <lb/>4 <lb/>u <lb/>Therefore the sum is at most <lb/>P S <lb/>u= S <lb/>k <lb/>1 <lb/>4 <lb/>u which is O(1=F). <lb/>In conclusion, the probability the graph violates the conditions of the theorem is O(1=F) and the <lb/>probability that a randomly chosen graph satis es the theorem is 1 ? O(1=F). </div>


	</text>
</tei>
