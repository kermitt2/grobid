<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>data <lb/>Data Descriptor <lb/>The Automatic Detection of Dataset Names in Scientific Articles <lb/>Jenny Heddes, Pim Meerdink, Miguel Pieters and Maarten Marx * <lb/>Citation: Heddes, J.; Meerdink, P.; <lb/>Pieters, M.; Marx, M. The Automatic <lb/>Detection of Dataset Names <lb/>in Scientific Articles. Data 2021, 6, 84. <lb/>https://doi.org/10.3390/data6080084 <lb/>Academic Editor: Craig A. Knoblock <lb/>Received: 16 June 2021 <lb/>Accepted: 28 July 2021 <lb/>Published: 4 August 2021 <lb/>Publisher&apos;s Note: MDPI stays neutral <lb/>with regard to jurisdictional claims in <lb/>published maps and institutional affil-<lb/>iations. <lb/>Copyright: Â© 2021 by the authors. <lb/>Licensee MDPI, Basel, Switzerland. <lb/>This article is an open access article <lb/>distributed under the terms and <lb/>conditions of the Creative Commons <lb/>Attribution (CC BY) license (https:// <lb/>creativecommons.org/licenses/by/ <lb/>4.0/). <lb/>Informatics Institute, Faculty of Science, University of Amsterdam, Science Park 908, <lb/>1098 XH Amsterdam, The Netherlands; jenny.heddes@student.uva.nl (J.H.); <lb/>pim.meerdink@student.uva.nl (P.M.); miguel.pieters@student.uva.nl (M.P.) <lb/>* Correspondence: maartenmarx@uva.nl <lb/>Abstract: We study the task of recognizing named datasets in scientific articles as a Named Entity <lb/>Recognition (NER) problem. Noticing that available annotated datasets were not adequate for our <lb/>goals, we annotated 6000 sentences extracted from four major AI conferences, with roughly half <lb/>of them containing one or more named datasets. A distinguishing feature of this set is the many <lb/>sentences using enumerations, conjunctions and ellipses, resulting in long BI+ tag sequences. On all <lb/>measures, the SciBERT NER tagger performed best and most robustly. Our baseline rule based tagger <lb/>performed remarkably well and better than several state-of-the-art methods. The gold standard <lb/>dataset, with links and offsets from each sentence to the (open access available) articles together with <lb/>the annotation guidelines and all code used in the experiments, is available on GitHub. <lb/>Dataset: https://github.com/xjaeh/ner_dataset_recognition <lb/>Dataset License: This work is licensed under a Creative Commons Attribution 4.0 International <lb/>License CC BY 4.0. <lb/>Keywords: dataset extraction; scientific information extraction; named entity recognition; BERT; <lb/>SciBERT <lb/></front>

			<body>1. Introduction <lb/>This paper contributes to the creation of a dataset citation network, a knowledge graph <lb/>linking datasets to scientific articles when used in an article. Unlike the citation network of <lb/>papers, the dataset citation infrastructure is still primitive, due to the limited referencing <lb/>of dataset usage in scientific articles [1-4]. The use and value of such a dataset citation <lb/>network is similar to that of the ordinary scientific citation network: realizing recognition <lb/>for dataset providers by computing the impact scores of datasets based on citations [2,4], <lb/>ranking datasets in dataset search engines by impact [1], creating a representation of a <lb/>dataset by its use instead of its metadata and content [4,5], studying cooccurrences of <lb/>datasets, etc. According to Kratz and Strasser [2], researchers believe that the citation count <lb/>is the most valuable way to measure the impact of a dataset. <lb/>Creating the dataset citation network from a collection of articles involves three main <lb/>steps: scientific PDF parsing, recognizing and extracting mentioned datasets, and cross <lb/>documenting the coreference resolution (&quot;dataset name de-duplication&quot;). This paper is <lb/>only concerned with the dataset extraction process, which we view as a Named Entity <lb/>Recognition (NER) task. When focusing on the articles that use a NER method for this <lb/>task, it becomes clear that almost every article uses another approach and another dataset. <lb/>Not only do these approaches differ, but they also deviate from the core dataset NER <lb/>task, as every approach has something extra added onto it [3,6-14]. This makes it hard <lb/>to compare which method or component fits a task best. According to Beltagy et al. [15], <lb/>SciBERT has shown state-of-the-art results on one of the datasets (SciERC), while other <lb/>methods have outperformed this score on a similar task and dataset [9,10]. To fully be able <lb/>to compare the performance and annotation costs of each (basic) model, we compare their <lb/></body>

			<front>Data 2021, 6, 84. https://doi.org/10.3390/data6080084 <lb/>https://www.mdpi.com/journal/data <lb/></front>

			<note place="footnote">Data 2021, 6, 84 <lb/></note>

			<page>2 of 19 <lb/></page>

			<body>performance with them all being trained and tested on the same dataset. This results in the <lb/>following research question: <lb/>RQ Which Named Entity Recognition model is best suited for the dataset name recognition <lb/>task in scientific articles, considering both performance and annotation costs? <lb/>The comparison of the performance of each method, when only run once, is not <lb/>sufficient enough to fully compare them, as [16] showed that annotation choices have an <lb/>irrefutable impact on the system&apos;s performance. This effect was neglected in the aforemen-<lb/>tioned papers that focused on the dataset extraction task. However, not only can these <lb/>choices impact the models&apos; performances, but they can also impact the annotation costs. <lb/>We consider a number of factors that could influence the performance and annotation <lb/>costs of the models. First, domain transfer is considered, as this is shown to impact NER <lb/>performance [17-19]. This has become a trending topic in NER in an effort to reduce <lb/>the amount of training data needed [20]. Another factor that is taken into account is the <lb/>training set size, as multiple sources have shown that a small amount of training data <lb/>can lead to performance problems [7,21]. Next to the size of the training set, the effect <lb/>of the distribution of positive and negative samples is considered, as this, too, has been <lb/>shown to influence performance [22-25]. These choices all influence the amount of training <lb/>data that is needed to achieve the best performance, thus influencing the annotation costs <lb/>since adding &apos;real examples&apos; is costly. In order to further reduce the annotation costs, <lb/>the effect of adding weakly supervised examples in the training data is investigated for the <lb/>best performing model. Summing up, we answer the following questions: <lb/>RQ1 What is the performance of rule-based, CRF, BiLSTM, BiLSTM-CRF [26-28], BERT [29] <lb/>and SciBERT [15] models on the dataset name recognition task? <lb/>RQ2 How well do the models perform when tested on a scientific (sub)domain that is not <lb/>included in the training data? <lb/>RQ3 How does the amount of training data impact the models&apos; performance? <lb/>RQ4 How are the models&apos; performance affected by the ratio of negative versus positive <lb/>examples in the training and test data? <lb/>RQ5 Does adding weakly supervised examples further improve the scores of the best <lb/>performing model? Additionally, how well does the best performing model perform <lb/>without any manually annotated labels? <lb/>RQ6 Is there a difference in the performance of NER models when predicting easy (short) <lb/>or hard (long) dataset mentions? <lb/>To answer these questions on realistic input data, we created a hand-annotated dataset <lb/>of 6000 sentences based on four sets of conferences in the fields of neural machine learning, <lb/>data mining, information retrieval and computer vision (see Section 3.1). NER can be <lb/>evaluated in many ways. We mostly use the most strict and realistic, that is, the exact <lb/>match on a zero shot test set. We note, however, that, due to enumerations and ellipses, <lb/>many NER hits contain several datasets, which makes the partial and B-match also useful <lb/>(as the found NER hits have to be post-processed anyway). <lb/>Our main findings are that SciBERT performs best, particularly on realistic test data <lb/>(with &gt;90% sentences, not mentioning a dataset). Surprisingly our own developed rule <lb/>based system (using POS tags and keywords) performed almost as well, and all others, <lb/>except BERT, perform (much) worse than this rule-based system. SciBERT was also robust <lb/>when looking at the other tests performed, regarding domain adaptability, the negative <lb/>sample ratio and the train set size. However, nothing comes for free; we did not succeed <lb/>in training SciBERT to outperform the rule-based system when we gave it only weakly <lb/>supervised training examples (obtained without manual annotation). <lb/>All code and datasets used for this paper can be found at https://github.com/xjaeh/ <lb/>ner_dataset_recognition. <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>3 of 19 <lb/></page>

			<body>2. Related Work <lb/>The overwhelming volume of scientific papers have made extracting knowledge <lb/>from them an unmanageable task [30], making automatic IE especially relevant for this <lb/>domain [31]. Scientific IE has been of interest since the early 1990s [32]. Despite the growing <lb/>interest in the automatic extraction of scientific information, research on this topic is still <lb/>narrow even now [7]. The reason for the limited research in scientific IE in comparison to <lb/>the general domain is the specific set of challenges associated with the scientific domain. <lb/>The main challenge is the expertise that is needed for annotated data, making these data <lb/>costly and hard to obtain, resulting in very limited data available [7]. However, there is <lb/>a significant focus on this kind of research in the scientific sub-domains: medicine and <lb/>biology [30]. <lb/>Where at the beginning of Scientific IE, the focus mainly laid upon citations and topic <lb/>analyses [13], now, the focus has become broader and has shifted toward scientific fact <lb/>extraction (for example, population statistics, variants of genomics, material properties, <lb/>etc.) [30]. Research on the dataset name extraction task uses a great variety of methods <lb/>throughout the NER spectrum, including, but not limited to, the following: rule-based, <lb/>BiLSTM-CRF and BERT [3,6-14]. <lb/>For dataset extraction, it was found that verbs surrounding the dataset provide infor-<lb/>mation about the role or function; as such, the words, use, apply or adopt, indicate a &apos;use&apos; <lb/>function [10]. Nevertheless, not only these verbs surrounding it play an important role, <lb/>as for dataset detection, a wide range of context is needed [6], indicating that a model&apos;s <lb/>ability to grasp context could play a significant role in the performance of that model. <lb/>We briefly go through the NER models that we tested for dataset extraction. <lb/>The rule-based approach was the most prominent one in the early stages of NER [33]. <lb/>Despite the fact that most state-of-the-art results are now achieved by machine learning <lb/>methods, the rule-based model is still attractive to use, due to its transparency [34]. The au-<lb/>thors conclude that rule-based methods can achieve state-of-the-art extraction performance, <lb/>but note that the rule development is very time consuming and a manual task. Not only is <lb/>this method used as a stand-alone classification method, but it is also suitable as a form of <lb/>weak supervision [35] as an alternative to the manual labeling of data, providing training <lb/>examples for the other methods [36,37]. <lb/>Conditional Random Fields (CRF) is a probabilistic model for labeling sequential data, <lb/>which has proven its effectiveness in NER, producing state-of-the-art results around the <lb/>year 2007 [38]. A dataset extraction model solely based on CRF is missing, but it was used <lb/>for other tasks in Scientific IE. A well-known example is the GROBID parser, which extracts <lb/>bibliographic data from scientific texts (such as the title, headers, references, etc.) [39]. <lb/>The BiLSTM-CRF is a hybrid model, combining LSTM layers with a CRF layer on <lb/>top [40]. Using this combination, the advantages of both models can be joined. The ad-<lb/>vantage of BiLSTM is that it is better at predicting long sequences, predicting every word <lb/>individually [41], while CRF predicts based on the joint probability of the whole sentence, <lb/>making sure that the optimal sequence of tags is achieved [41-43]. To date, the BiLSTM-<lb/>CRF based model produces the best performance on the dataset extraction task, with an F1 <lb/>score of 0.85 [8]. <lb/>BERT produces state-of-the-art results in a range of NLP tasks [29]. It is based on <lb/>a transformer network, which is praised for its context-aware word representations, im-<lb/>proving the prediction ability [44]. BERT has revolutionized classical NLP. However, its <lb/>performance as a base for the dataset extraction tasks differs greatly, as one research study <lb/>found an F1 score of 0.68 [13], while another has found an F1 score of 0.79 [10]. Beltagy <lb/>et al. [15] developed the SciBERT model based on the BERT model. The big and only <lb/>difference between those models is that, unlike BERT, which is trained on general texts, <lb/>SciBERT is trained on 1.14 M scientific papers from Semantic Scholar, consisting of 18% <lb/>computer science papers, and the remaining 82% consisting of papers from the biomedical <lb/>domain. This model, which was specially created for knowledge extraction in the scientific <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>4 of 19 <lb/></page>

			<body>domain, indeed achieves better performance, in comparison to BERT, in the computer <lb/>science domain. <lb/>3. Materials and Methods <lb/>3.1. Description of the Data <lb/>We describe the created manually annotated dataset. The annotation guidelines in <lb/>Appendix B contain many illuminating examples. Here, we simply give two examples <lb/>(annotated datasets are in marked in gray): <lb/>â¢ <lb/>The second collection (called ClueWeb) that we used is ClueWeb09 Category B , a large-scale <lb/>web collection . . . <lb/>â¢ <lb/>Tables 3 and 4 show the average precision of 20 categories and MAP on the PASCAL VOC <lb/>2007 and 2012 testing set , respectively. <lb/>3.1.1. Origins <lb/>The sentences in the dataset originate from published articles from four different <lb/>corpora within the computer science domain: the Conference on Neural Information Pro-<lb/>cessing Systems (NIPS, 2000-2019), SIAM International Conference on Data Mining (SDM, <lb/>2000-2019), ACM SIGIR conference (2007-2018), and papers from the main conferences <lb/>(ICCC, CVPR) participating in the Computer Vision foundation (VISION, 2017-2019). <lb/>These conferences were chosen because they are top tier A * venues that have existed for <lb/>an extensive period; all of them, except SIGIR, are freely available; they cover a wide range <lb/>of topics within the information sciences; and experimental evaluation is a key aspect <lb/>in these venues. Thus, we expected most articles to contain references to the datasets. <lb/>Papers from these conferences were collected in PDF format and parsed using GROBID <lb/>to extract the text [39]. The extraction of sentences using GROBID made it possible to <lb/>exclude references, titles and tables. This way, only &apos;real&apos; sentences from the main text <lb/>were selected. From these sentences occurring in the main text, we selected sentences that <lb/>likely contained a reference to a dataset for manual annotation. These selected sentences <lb/>had to contain one of the following phrases (including their plural form): dataset, data set, <lb/>database, data base, corpus, treebank, benchmark, test/validation/testing/training data <lb/>or train/test/validation/testing/training set. The regular expression in Appendix A was <lb/>used to implement this selection. <lb/>3.1.2. Annotation <lb/>The annotation scheme used for the annotation task is based on the ACL RD-TEC <lb/>Annotation Guidelines [45]. An example of a guideline is that generic nouns (e.g., dataset) <lb/>accompanying a term should be annotated. Another example is the &apos;ellipses rule&apos;, which <lb/>states that when two noun phrases in a conjunction are linked through ellipses, the term <lb/>needs to be annotated as one. For the task of the dataset name annotation, this would mean <lb/>that the phrase PASCAL VOC 2006 and 2007 datasets are marked as one entity. Annotation <lb/>was done by four persons, each annotating 1500 sentences plus a part of the kappa calcula-<lb/>tion. The resulting Fleiss kappa of 0.72 representing a substantial agreement was calculated <lb/>based on fifty sentences [46]. The full annotation scheme is available in Appendix B. <lb/>3.1.3. Train and Test Sets <lb/>Each annotated sentence was given an ID, tokenized using the spaCy tokenizer [47], <lb/>and given POS-tags using the NLTK package [48]. The gold standard annotations them-<lb/>selves were transformed into the corresponding IOB-tags for each token. <lb/>The entire dataset contains a total of 6000 sentences, having an even distribution <lb/>between corpora, with 1500 sentences from each corpus. Slightly under half of them <lb/>contain a dataset mention. These 6000 sentences were split into a train set, test set and zero <lb/>shot test set. Sentences in the zero shot test set do not contain dataset names that occur in <lb/>the train set. All sets were created using stratified sampling, more or less keeping the equal <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>5 of 19 <lb/></page>

			<body>distribution among the four conferences. The distribution of positive and negative samples <lb/>in these sub-sets is shown in Table 1. <lb/>Table 1. Distribution of positive and negative samples across the train, test and zero-shot set. <lb/>Containing a Dataset <lb/>No Dataset <lb/>Total <lb/>Training set <lb/>2168 <lb/>2472 <lb/>4640 <lb/>Test set <lb/>543 <lb/>617 <lb/>1160 <lb/>Zero-shot set <lb/>200 <lb/>0 <lb/>200 <lb/>Total <lb/>2911 <lb/>3089 <lb/>6000 <lb/>The number of sentences containing a dataset name is not equal to the number of <lb/>datasets being named, which is 4164. This leaves an average mention of 1.43 datasets in a <lb/>sentence containing at least one dataset mention and an average mention of 0.69 overall in <lb/>this dataset. <lb/>We ran the SciBERT tagger over the complete corpus of over 15,000 articles, and ob-<lb/>served that within the VISION papers, 5% of the papers did not mention a dataset. For the <lb/>other three conferences, this was remarkably similar between 20.2 and 22.4%. A manual <lb/>total scan of 30 random NIPS papers produced a slightly higher part of nine papers without <lb/>any dataset mention. <lb/>3.2. Experimental Setup <lb/>Full details of all experiments, including more detailed measurements, are available <lb/>on the GitHub repository. All methods were evaluated using seqeval [49] for the B-and <lb/>I-tags, and nervaluate [50] for the partial-and exact-match scores. <lb/>As a natural baseline, we created a rule-based system containing rules such as &quot;If <lb/>the word is a proper-noun and one of the keywords follows: then mark the proper-noun including <lb/>its keyword as a dataset&quot;, which were developed through careful consideration, following <lb/>the annotation guidelines. As developing a rule-based system takes time [34], the rule <lb/>development was an iterative process by trial and error, each time adding or adjusting <lb/>rules as deemed necessary. The rules were made machine readable, using spaCy&apos;s rule-<lb/>based matching method [51]. This translated to 10 spaCy patterns; see the notebook <lb/>Rule-based.ipynb in the dataset belonging to this paper. <lb/>For the CRF, the sklearn_crfsuite from the scikit-learn library was used [52]. Both <lb/>BiLSTM methods are keras based. No parameter optimization was performed. The used <lb/>parameters were taken from the &quot;Depends on the definition NER series&quot; [53]. <lb/>Both BERT models are based upon a scikit-learn wrapper [54]. This wrapper provides <lb/>multiple models that can be selected. The example of [29] is followed by choosing the <lb/>cased model for NER. While the uncased variant of the model generally performs better, <lb/>the choice was made to utilize a case-sensitive model, as capitalization can be indicative of <lb/>whether a phrase refers to a dataset: words referring to datasets are often capitalized. To <lb/>compare both models equally, the BERTbase model was chosen, just like [15], as SciBERT only <lb/>has a base model. For SciBERT, the scivocab was chosen, as this represents the frequently <lb/>used words in scientific papers. The model configuration and architecture are the same as <lb/>those in the SciBERT paper [15]. The following hyperparameters were used for the training <lb/>of the model: A learning rate of 5 Ã 10 â6 for the Adam optimizer, with a batch size of 16. <lb/>Training lasted 15 epochs, and checkpoints were saved every 300 training steps. Gradient <lb/>clipping was used, with a max gradient of 1. <lb/>4. Results <lb/>We report the results grouped by the five subquestions. Appendix D contains ad-<lb/>ditional results (e.g., precision and recall scores, and scores for the B(eginning) and <lb/>I(nternal) tags. <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>6 of 19 <lb/></page>

			<body>4.1. RQ1, Overall Performances <lb/>Table 2 contains the F1 performance scores for the six different NER models we tested. <lb/>This is the only experiment we conducted with (5-fold) cross validation on the complete set <lb/>of 6000 sentences. BERT and SciBERT perform almost the same on both scores and (much) <lb/>better than all the others, except that the rule-based system performs equally well on the <lb/>partial match score. <lb/>Notice that the partial and exact match scores are closest for SciBERT. Due to conjunc-<lb/>tions, ellipses and the used annotation guidelines, NER phrases can be quite long, so a <lb/>large difference between the two ways of scoring could be expected. An error analysis <lb/>shows that SciBERT is especially good in learning the beginning of a dataset mention. <lb/>The two most interesting systems seem to be SciBERT and the rule-based one, and thus <lb/>we will mostly report results on the other subquestions for these two. <lb/>Table 2. RQ1, Partial and exact match mean F1 scores for various NER models based on 5-fold <lb/>cross-validation, using the complete dataset of 6000 sentences (all standard deviations are between <lb/>0.01 and 0.03). <lb/>Partial Match <lb/>Exact Match <lb/>Rule Based <lb/>0.81 <lb/>0.72 <lb/>CRF <lb/>0.75 <lb/>0.72 <lb/>BiLSTM <lb/>0.73 <lb/>0.67 <lb/>BiLSTM-CRF <lb/>0.77 <lb/>0.72 <lb/>BERT <lb/>0.81 <lb/>0.77 <lb/>SciBERT <lb/>0.82 <lb/>0.78 <lb/>4.2. RQ2, Domain Adaptability <lb/>The models&apos; ability to adapt to differences within the scientific domain is shown in <lb/>Table 3. These scores are achieved using one corpus as a test set, while training on the other <lb/>three corpora. The corpus on which it is tested can be found in the header. We expected <lb/>the scores to be lower than the cross validation scores, but we only found a small negative <lb/>effect when testing on the VISION conferences. We note that the VISION set is different <lb/>in that the sentences come from the last three years, while the others are from the last <lb/>two decades. <lb/>Table 3. RQ2, domain adaptability between corpora (F1 scores). <lb/>Models <lb/>Evaluation <lb/>NIPS <lb/>SDM <lb/>SIGIR <lb/>VISION <lb/>Rule-based <lb/>Partial-match <lb/>0.75 <lb/>0.75 <lb/>0.79 <lb/>0.75 <lb/>Exact-match <lb/>0.72 <lb/>0.70 <lb/>0.75 <lb/>0.71 <lb/>SciBERT <lb/>Partial-match <lb/>0.81 <lb/>0.83 <lb/>0.80 <lb/>0.78 <lb/>Exact-match <lb/>0.76 <lb/>0.80 <lb/>0.76 <lb/>0.71 <lb/>4.3. RQ3, Amount of Training Data <lb/>Here, we look at the major cost factor: the size of the training set. Figure 1 shows <lb/>the exact match F1 score on the zero-shot set for varying amounts of training sentences, <lb/>ranging from 500 to 4500. We see a clear difference between CRF and the two BERT models <lb/>on the one hand and the two BiLSTM models on the other. We now zoom in on the most <lb/>stable behaving models, CRF and SciBERT. Figure 2 zooms in on both precision and recall, <lb/>also for the (supposedly easier) test set. Both models show remarkably robust behavior: <lb/>only a slight influence of the amount of training examples and hardly any difference in <lb/>performance for the test and zero-shot test set. It is noticeable that CRF can be seen as a <lb/>precision-oriented system, while for SciBERT, precision and recall are very similar. We see <lb/>this as evidence that these two systems learn the structure of a dataset mention well and <lb/>do not overfit on the dataset names themselves. <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>7 of 19 <lb/></page>

			<body>500 1000 1500 2000 2500 3000 3500 4000 4500 <lb/>Number of training examples <lb/>0.0 <lb/>0.1 <lb/>0.2 <lb/>0.3 <lb/>0.4 <lb/>0.5 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>Score <lb/>Exact F1 scores on the zero-shot set <lb/>CRF <lb/>BiLSTM <lb/>BiLSTM-CRF <lb/>BERT <lb/>SciBERT <lb/>Figure 1. RQ3, the influence of the amount of training data for all models tested on the zero-shot <lb/>test set. <lb/>500 1000 1500 2000 2500 3000 3500 4000 4500 <lb/>Number of training examples <lb/>0.0 <lb/>0.1 <lb/>0.2 <lb/>0.3 <lb/>0.4 <lb/>0.5 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>Score <lb/>CRF <lb/>Precision -zero-shot <lb/>Recall -zero-shot <lb/>F1 -zero-shot <lb/>Precision -test set <lb/>Recall -test set <lb/>F1 -test set <lb/>500 1000 1500 2000 2500 3000 3500 4000 4500 <lb/>Number of training examples <lb/>0.0 <lb/>0.1 <lb/>0.2 <lb/>0.3 <lb/>0.4 <lb/>0.5 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>Score <lb/>SciBERT <lb/>Precision -zero-shot <lb/>Recall -zero-shot <lb/>F1 -zero-shot <lb/>Precision -test set <lb/>Recall -test set <lb/>F1 -test set <lb/>Figure 2. RQ3, the influence of the amount of training data for the CRF model (left) and for SciBERT (right). <lb/>4.4. RQ4, Negative/Positive Ratio <lb/>Recall that about half of the sentences in our dataset do not mention a dataset, while <lb/>containing one of the trigger words, such as dataset, corpus, collection, etc. We can decide <lb/>to use those in training or not. As noted in previous research, the ratio of positive and <lb/>negative sentences was found to be important for NER models trained on a dataset mention <lb/>extraction task [6]. We see a slight improvement in F1 scores for all models when adding <lb/>also negative training examples, but this is quite small. <lb/>What is more interesting is when we test on a set in which sentences mentioning a <lb/>dataset are very rare, just like in a real scientific article. Using the developed rule-based <lb/>system, we added sentences that most probably do not mention a dataset (i.e., they did not <lb/>contain any of the trigger words (more precisely, did not match the regex in Appendix A)) <lb/>to the test set until we obtained a 1 in 100 ratio. Table 4 shows the results. We see that all <lb/>F1 scores drop, compared to those in Table 2. This is expected as the task becomes harder. <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>8 of 19 <lb/></page>

			<body>However, note that the recall remains very high for the two BERT models, indicating that <lb/>the drop in F1 is caused mainly by extra false positives (of course, it might be that the <lb/>SciBERT model discovered genuine dataset mentions not containing one of the trigger <lb/>terms. We did not check for this). <lb/>Table 4. RQ4, testing on a real ratio (positive vs. negative) test set. <lb/>Model <lb/>Evaluation <lb/>Precision <lb/>Recall <lb/>F1 <lb/>CRF <lb/>Partial-match <lb/>0.72 <lb/>0.66 <lb/>0.69 <lb/>Exact-match <lb/>0.69 <lb/>0.63 <lb/>0.66 <lb/>BiLSTM <lb/>Partial-match <lb/>0.37 <lb/>0.19 <lb/>0.26 <lb/>Exact-match <lb/>0.29 <lb/>0.15 <lb/>0.20 <lb/>BiLSTM-CRF <lb/>Partial-match <lb/>0.49 <lb/>0.29 <lb/>0.37 <lb/>Exact-match <lb/>0.38 <lb/>0.23 <lb/>0.29 <lb/>BERT <lb/>Partial-match <lb/>0.57 <lb/>0.91 <lb/>0.70 <lb/>Exact-match <lb/>0.55 <lb/>0.88 <lb/>0.68 <lb/>SciBERT <lb/>Partial-match <lb/>0.65 <lb/>0.91 <lb/>0.76 <lb/>Exact-match <lb/>0.63 <lb/>0.88 <lb/>0.73 <lb/>4.5. RQ5, Weakly Annotated Data <lb/>We now see how much SciBERT can learn from positive training examples discovered <lb/>by the rule-based system. As these examples are not hand annotated, we call them weakly <lb/>supervised. We created a weakly supervised training set, SSC (for Silver Standard Corpus), <lb/>with the same number of positive and negative sentences as in the manually annotated train <lb/>set. Table 5 shows that the performance of SciBERT is substantially lower when trained on <lb/>those &apos;cost-free&apos; training examples alone than when trained on the hand-annotated data <lb/>(train set). A reason for this is that the SSC can contain false negatives or false positives, and <lb/>learning from these false data will impact the model&apos;s prediction ability, thus impacting <lb/>the scores. <lb/>Table 5. RQ5, differences between training on supervised or weakly supervised train data (F1 scores). <lb/>Training Set Evaluation <lb/>Test Set <lb/>Zero-Shot <lb/>Train set <lb/>Partial <lb/>0.83 <lb/>0.81 <lb/>Exact <lb/>0.80 <lb/>0.76 <lb/>SSC <lb/>Partial <lb/>0.73 <lb/>0.73 <lb/>Exact <lb/>0.67 <lb/>0.67 <lb/>According to [55], weakly supervised negative examples harm the performance. <lb/>To test this effect, SciBERT was also trained on a combination of the manually labeled <lb/>data and only the positive data from the SSC. The differences were very small: a 0.01 <lb/>improvement for both partial and exact match on the zero-shot test, no difference for the <lb/>partial match, and a 0.02 decrease on the test set. <lb/>4.6. RQ6, Easy vs. Hard Sentences <lb/>Sentences enumerating a number of named datasets are common in scientific arti-<lb/>cles. According to the guidelines, these are tagged as one entity, leading to long BI+ tag <lb/>sequences. We wanted to test whether SciBERT is able to learn these more complex long <lb/>entities just as well as the easier ones. So we split both the train and the zero-shot test sets <lb/>into a hard and easy set, with sentences being hard if they contained a BI+ tag sequence <lb/>of a length of four or more. We then performed all four possible train on hard/easy, test <lb/>on hard/easy experiments. Only (we also saw a 6% drop in F1 when trained on hard and <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>9 of 19 <lb/></page>

			<body>test on easy, but this may be due to much less training sentences) with train on easy, test <lb/>on hard did we see an expected but still remarkable difference in scores (a drop in F1 of <lb/>42%). This means that the network is also able to understand and interpret ellipses and <lb/>enumerations. These more complex rules and structures are not harder for the network to <lb/>identify than simple one-or two-word dataset mentions. These structures and patterns are <lb/>difficult, even for human annotators to consistently parse and classify correctly, making <lb/>the network&apos;s ability to understand the nuances of the labeling task significant. <lb/>5. Discussion <lb/>We have created a large and varied annotated set of sentences likely to contain a <lb/>dataset name, with about half actually containing one or more datasets. We have shown <lb/>that extracting these datasets using traditional NER techniques is feasible but clearly not <lb/>straightforward or solved. We believe our results show that the created gold standard is a <lb/>valuable asset for the scientific document parsing community. The set stands out because <lb/>the sentences come from all sections in scientific articles, and come with exact links to the <lb/>articles. Except for those coming from SIGIR, all articles are openly available in PDF format. <lb/>Analysis of the errors of the NER systems and the disagreements among the annotators <lb/>revealed that dataset entity recognition from scientific articles is complicated through the <lb/>use of enumerations, conjunctions and ellipsis in sentences. This means that, for example, <lb/>in the sentence &apos;We used the VOC 2007, 2008 and 2009 collections.&apos;, the phrase &apos;VOC 2007, <lb/>2008 and 2009 collections&apos; is tagged as one dataset entity mention, as individual elements <lb/>of the enumeration are nonsensical without the context provided by the other elements [7]. <lb/>We think it is this aspect that makes the task exciting and different from standard NER. <lb/>Postprocessing the found mention, extracting all dataset entities, and completing the <lb/>information hidden by the use of ellipses is an NLP task needed on top of dataset NER <lb/>before we can create a dataset citation network. Of course, a cross-document coreference <lb/>resolution of the found dataset names is then needed for the obtained network to be <lb/>useful [56]. Expanding the provided set of sentences with this extra information, linking <lb/>every sentence to a set of unique dataset identifiers is not that much work and would make <lb/>the dataset also applicable for training the dataset reconciliation task. <lb/>We wanted to know which NER system performs well and at what cost. Not surpris-<lb/>ingly, the best performing systems were BERT and SciBERT. Unsupervised pretraining <lb/>also helps for this task. Both systems (and CRF) worked already almost optimally with <lb/>relatively few training examples. They were robust on our domain adaptation experiments, <lb/>and kept a high recall at the cost of some loss in precision when we diluted the test set to a <lb/>realistic 1 in 100 ratio of sentences with a dataset. <lb/>We found the performance of our quite simple rule-based system to be remarkable. <lb/>In fact, this system can be seen as a formalization of the annotation guidelines, and having <lb/>those carefully spelled out made it almost effortless to create; this is, in our opinion, the <lb/>reason for its strong performance. The experiment in which we trained SciBERT with <lb/>extra examples found by the rule-based system was inconclusive in that we saw hardly <lb/>any change in performance. However, there may be more clever ways to combine these <lb/>two models. <lb/>Future Directions <lb/>We think a gold standard dataset reminiscent of the end-to-end task of dataset mention <lb/>extraction from scientific PDFs could lead to a big step forward in this field. In particu-<lb/>lar, we could then train and test end-to-end systems, which would link dataset DOIs to <lb/>article DOIs. <lb/>Additionally, the articles from the four chosen ML/DM/IR/CV conferences are rela-<lb/>tively easy for the dataset extraction task, as they do not contain that many named entities. <lb/>The task is likely harder with papers from biological, chemical or medical domains. <lb/></body>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>10 of 19 <lb/></page>

			<body>A different approach to this task is to start with a knowledge base of existing research <lb/>datasets containing their names and some metadata and then to use that in an informed <lb/>dataset mention extraction system. <lb/></body>

			<div type="annex">Author Contributions: Conceptualization, J.H. and M.M.; methodology, J.H., P.M., M.M.; software, <lb/>J.H., P.M., M.P., M.M.; validation, J.H., P.M., M.P.; formal analysis, J.H., P.M., M.M.; investigation, <lb/>J.H., P.M., M.M.; resources, J.H., P.M., M.P.; data curation, J.H., P.M., M.P.; writing-original draft <lb/>preparation, J.H.; writing-review and editing, M.M.; visualization, J.H.; supervision, M.M.; project <lb/>administration, M.M.; funding acquisition, M.M. All authors have read and agreed to the published <lb/>version of the manuscript. <lb/></div>

			<div type="funding">Funding: This research was supported in part by the Netherlands Organization for Scientific Research <lb/>(NWO, ACCESS project, grant No. CISC.CC.016). <lb/></div>

			<div type="availability">Data Availability Statement: The gold standard dataset, with links and offsets from each sentence <lb/>to the (open access available) articles together with the annotation guidelines and all code used in the <lb/>experiments, is available on https://github.com/xjaeh/ner_dataset_recognition. <lb/></div>

			<div type="acknowledgment">Acknowledgments: Special thanks to Wouter Haak and Elsevier for the valuable comments and <lb/>providing additional datasets. <lb/></div>

			<div type="annex">Conflicts of Interest: The authors declare no conflict of interest. The funders had no role in the design <lb/>of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, <lb/>or in the decision to publish the results. <lb/></div>

			<div type="annex">Appendix A. REGEX <lb/>The used regular expression was ( <lb/>Â¯D) Â¯, with D a isjunction of the following expres-<lb/>sions: (?:train|test|validation|testing|trainings?)\s*(?:set|data), bench-<lb/>marks?, data\s*(?:set|base)s?, corpus, corpora, tree\s*bank, collections?. <lb/>Appendix B. Annotation Guidelines <lb/>Appendix B.1. Introduction <lb/>The main idea of this task is to mark the dataset name(s) in sentences. The Oxford <lb/>dictionary defines name as follows: &quot;a word or set of words by which a person or thing is <lb/>known, addressed, or referred to&quot; [57]. A dataset is defined as a collection of data that is <lb/>treated as a single unit by a computer [58]; so mark the word or a set of words that refers <lb/>to a dataset. This leads to the following definition. <lb/>Dataset name: Term(s) that refers to a real (life) dataset. <lb/>The idea is to find the dataset name(s) in sentences and to mark them so that they <lb/>contain the needed information to find the dataset online (so that versions are included). <lb/>The sample sentences below are sentences from the NIPS papers [59]. <lb/>The following are a few examples of sentences without a dataset name: <lb/>â¢ <lb/>First, let us remark that our training setup differs from those reported in previous works. <lb/>â¢ <lb/>Section 3 demonstrates the validity of our approach by evaluating and comparing the model on <lb/>a number of image datasets. <lb/>â¢ <lb/>This dataset consists of about 5000 trainval images and 5000 test images over 20 object <lb/>categories. <lb/>The following are a few annotated examples of sentences containing dataset names. <lb/>â¢ <lb/>Over two benchmark datasets, Standford Background and Sift Flow , the model outperforms <lb/>many state-of-the-art models in accuracy and efficiency. <lb/>â¢ <lb/>Imagenet : A large-scale hierarchical image database. <lb/>â¢ <lb/>For one, we trained on the standard WSJ training dataset . <lb/>â¢ <lb/>In all experiments, we used stack1 for testing, stack2 and stack3 for training, and stack4 <lb/>as additional training data for recursive training. <lb/></div>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>11 of 19 <lb/></page>

			<div type="annex">â¢ <lb/>We tested our formulation on three datasets of object detection: PASCAL VOC 2007 , <lb/>PASCAL 2012 and Microsoft COCO . <lb/>Appendix B.2. Guidelines for Annotation <lb/>â¢ <lb/>Mark entire word (an-does not split a word). <lb/>â¢ <lb/>Mark a dataset name consisting of multiple words as one; when marked as individual <lb/>words, they will be seen as multiple names. <lb/>â¢ <lb/>Do not mark the reading signs/spaces before or after the name. <lb/>Appendix B.3. When to Annotate <lb/>â¢ <lb/>Datasets can be referred to in a number of different ways, including, but not limited <lb/>to the following: database, benchmark, collection, corpus, treebank, etc. <lb/>â¢ <lb/>Only mark a name when it is specific enough. <lb/>-<lb/>This does not include the following: <lb/>* <lb/>When talking about &apos;a&apos; dataset. <lb/>â¢ <lb/>Our model performs 1.8 times better than the only published results on an <lb/>existing image QA dataset. <lb/>* <lb/>When a dataset name is mentioned, but the data set is not talked about. <lb/>â¢ <lb/>It should be noted that our DAQUAR results are for the portion of the dataset <lb/>(98.3%) with single-word answers. <lb/>* <lb/>When they only mention data. <lb/>â¢ <lb/>Analogous to HCP data, the second task dataset thus incorporated 1404 labeled, <lb/>gray-matter masked, and z-scored activity maps from 18 diverse tasks acquired <lb/>in 78 participants. <lb/>* <lb/>When it is clear that they are talking about multiple datasets, if it is ambigu-<lb/>ous, do mark the dataset. <lb/>â¢ <lb/>However, the gain in accuracy is more substantial for the SQF datasets as <lb/>compared to the COMPAS and Adult datasets. <lb/>â¢ <lb/>In our experiments, we used two standard TREC collections: the first collection <lb/>(called Robust04 ) consists of over 500,000 news articles from different news <lb/>agencies, which are available in TREC Disks 4 and 5 (excluding Congres-<lb/>sional Records). <lb/>* <lb/>When an abbreviation is introduced for the article only. <lb/>â¢ <lb/>The second collection (called ClueWeb) that we used is ClueWeb09 Category B , <lb/>a large-scale web collection with over 50 million English documents, which is <lb/>considered a heterogeneous collection. <lb/>* <lb/>When a dataset is made specifically for the article. <lb/>â¢ <lb/>We first consider modeling a segment of the pseudo periodic synthetic dataset. <lb/>* <lb/>When no keywords (such as dataset, database, etc.) follow after a (set of) <lb/>word(s), but the sentence makes clear that the word(s) indeed does refer to <lb/>a dataset. <lb/>â¢ <lb/>We use the AOL query log [12] in our experiments and preprocess the dataset <lb/>following [8]. <lb/>â¢ <lb/>We evaluate the FS-RNN on two character level language modeling data sets, <lb/>Penn Treebank and Hutter Prize Wikipedia , where we improve state-of-the-<lb/>art results to 1.19 and 1.25 bits-per-character (BPC), respectively. <lb/></div>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>12 of 19 <lb/></page>

			<div type="annex">Appendix B.4. What to Annotate â : <lb/>â¢ <lb/>Mark as much information as possible, so include keywords, such as dataset, training <lb/>set, etc. <lb/>-<lb/>This includes version numbers. <lb/>-<lb/>However, do not include words such as &apos;the&apos;. <lb/>â¢ <lb/>When an abbreviation follows a dataset name or divides the name, mark this as one. <lb/>See examples below: <lb/>-<lb/>Several recent image models were evaluated on small image patches sampled from the <lb/>Berkeley segmentation dataset (BSDS300) [25]. <lb/>-<lb/>Labeled Faces in the Wild (LFW) database [12] is widely used for face recognition and <lb/>verification benchmark. <lb/>â¢ <lb/>To determine whether an adjective is part of the dataset name, try to imagine if the <lb/>meaning of the term would change if the adjective was removed. If this is the case, <lb/>include the adjective; otherwise, do not include the adjective. <lb/>-<lb/>Table 1: NLL scores in the test data for the binarized MNIST dataset . <lb/>-<lb/>(b) Weights W (filters) learned by LeGrad when training an SBN with H = 200 units in <lb/>the full MNIST training set . <lb/>â¢ <lb/>If one term contains information that is needed for the other term (ellipses), mark <lb/>them as one. <lb/>-<lb/>Tables 3 and 4 show the average precision of 20 categories and MAP on the <lb/>PASCAL VOC 2007 and 2012 testing set , respectively. <lb/>-<lb/>State-of-the-art performance on the ASSISTments benchmark and Khan dataset . <lb/>-<lb/>This is equivalent to finding a transport map Ï from random noises with distribution <lb/>pX (e.g., Gaussian distribution or uniform distribution) to the underlying population <lb/>distribution pY of the genuine sample, e.g., the MNIST or the ImageNet dataset . <lb/>-<lb/>For the Blizzard and Accent datasets , we process the data so that each sample duration <lb/>is 0.5 s (the sampling frequency used is 16 kHz.) <lb/>â¢ <lb/>If there is a preposition in the term, judge if it is a part of the term or if it splits terms. <lb/>-<lb/>For conditional density estimation, we use the MNIST dataset of handwritten dig-<lb/>its [17] and the CIFAR-10 dataset of natural images [14]. <lb/>-<lb/>The Twitter Part of Speech dataset [4] contains 1827 tweets annotated with 25 POS tags. <lb/>â¢ <lb/>When the name is divided by references, mark the sentences as if the references are <lb/>not there. <lb/>-<lb/>The Allstate Insurance Claim [27] and the Flight Delay [28] datasets both contain a <lb/>lot of one-hot coding features. <lb/>â¢ <lb/>If the marked part contains a spelling error, this has to be fixed (as this cannot be done <lb/>in the annotating environment, this has to be done after, so make sure to notate the <lb/>sentences that need to be adjusted). Mark the sentence as if there is no spelling error. <lb/> â  Based on the ACL RD-TEC Annotation Guideline, version 2.6 [45]. <lb/></div>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>13 of 19 <lb/></page>

			<div type="annex">Appendix C. Rule-Based Score <lb/>Table A1. Rule-based score on train and zero-shot sets. <lb/>Set <lb/>Evaluation <lb/>Precision <lb/>Recall <lb/>F1 <lb/>Test set <lb/>B-tags <lb/>0.81 <lb/>0.75 <lb/>0.77 <lb/>I-tags <lb/>0.79 <lb/>0.72 <lb/>0.76 <lb/>Partial-match <lb/>0.81 <lb/>0.71 <lb/>0.76 <lb/>Exact-match <lb/>0.76 <lb/>0.67 <lb/>0.71 <lb/>Zero-shot set <lb/>B-tags <lb/>0.90 <lb/>0.76 <lb/>0.82 <lb/>I-tags <lb/>0.88 <lb/>0.76 <lb/>0.82 <lb/>Partial-match <lb/>0.92 <lb/>0.78 <lb/>0.85 <lb/>Exact-match <lb/>0.88 <lb/>0.75 <lb/>0.81 <lb/>Appendix D. More in Depth Results <lb/>Table A2. RQ1, overall performances: cross-validation scores. <lb/>Models <lb/>Evaluation <lb/>Precision <lb/>Recall <lb/>F1-Score <lb/>CV-Score <lb/>SD <lb/>CV-Score <lb/>SD <lb/>CV-Score <lb/>SD <lb/>Rule-based <lb/>B-tags <lb/>0.82 <lb/>0.01 <lb/>0.72 <lb/>0.01 <lb/>0.77 <lb/>0.01 <lb/>I-tags <lb/>0.79 <lb/>0.01 <lb/>0.76 <lb/>0.02 <lb/>0.77 <lb/>0.02 <lb/>Partial-match <lb/>0.81 <lb/>0.01 <lb/>0.72 <lb/>0.02 <lb/>0.81 <lb/>0.01 <lb/>Exact-match <lb/>0.77 <lb/>0.01 <lb/>0.68 <lb/>0.02 <lb/>0.72 <lb/>0.01 <lb/>CRF <lb/>B-tags <lb/>0.89 <lb/>0.00 <lb/>0.65 <lb/>0.03 <lb/>0.75 <lb/>0.02 <lb/>I-tags <lb/>0.86 <lb/>0.00 <lb/>0.72 <lb/>0.02 <lb/>0.78 <lb/>0.02 <lb/>Partial-match <lb/>0.89 <lb/>0.00 <lb/>0.65 <lb/>0.03 <lb/>0.75 <lb/>0.02 <lb/>Exact-match <lb/>0.85 <lb/>0.01 <lb/>0.62 <lb/>0.03 <lb/>0.72 <lb/>0.02 <lb/>BiLSTM <lb/>B-tags <lb/>0.86 <lb/>0.01 <lb/>0.75 <lb/>0.01 <lb/>0.80 <lb/>0.00 <lb/>I-tags <lb/>0.72 <lb/>0.01 <lb/>0.74 <lb/>0.01 <lb/>0.73 <lb/>0.01 <lb/>Partial-match <lb/>0.73 <lb/>0.01 <lb/>0.72 <lb/>0.01 <lb/>0.73 <lb/>0.00 <lb/>Exact-match <lb/>0.67 <lb/>0.01 <lb/>0.66 <lb/>0.01 <lb/>0.67 <lb/>0.01 <lb/>BiLSTM-CRF <lb/>B-tags <lb/>0.82 <lb/>0.02 <lb/>0.79 <lb/>0.03 <lb/>0.80 <lb/>0.02 <lb/>I-tags <lb/>0.78 <lb/>0.02 <lb/>0.76 <lb/>0.03 <lb/>0.77 <lb/>0.03 <lb/>Partial-match <lb/>0.77 <lb/>0.03 <lb/>0.76 <lb/>0.03 <lb/>0.77 <lb/>0.02 <lb/>Exact-match <lb/>0.72 <lb/>0.03 <lb/>0.71 <lb/>0.04 <lb/>0.72 <lb/>0.03 <lb/>BERT <lb/>B-tags <lb/>0.82 <lb/>0.01 <lb/>0.85 <lb/>0.02 <lb/>0.84 <lb/>0.01 <lb/>I-tags <lb/>0.78 <lb/>0.01 <lb/>0.81 <lb/>0.01 <lb/>0.80 <lb/>0.01 <lb/>Partial-match <lb/>0.79 <lb/>0.01 <lb/>0.83 <lb/>0.01 <lb/>0.81 <lb/>0.01 <lb/>Exact-match <lb/>0.75 <lb/>0.01 <lb/>0.79 <lb/>0.02 <lb/>0.77 <lb/>0.01 <lb/>SciBERT <lb/>B-tags <lb/>0.83 <lb/>0.01 <lb/>0.85 <lb/>0.02 <lb/>0.84 <lb/>0.01 <lb/>I-tags <lb/>0.79 <lb/>0.01 <lb/>0.82 <lb/>0.01 <lb/>0.81 <lb/>0.01 <lb/>Partial-match <lb/>0.80 <lb/>0.01 <lb/>0.84 <lb/>0.01 <lb/>0.82 <lb/>0.01 <lb/>Exact-match <lb/>0.76 <lb/>0.02 <lb/>0.80 <lb/>0.02 <lb/>0.78 <lb/>0.02 <lb/></div>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>14 of 19 <lb/></page>

			<div type="annex">Table A3. RQ2, domain adaptability between corpora. <lb/>Models <lb/>Evaluation <lb/>NIPS <lb/>SDM <lb/>SIGIR <lb/>VISION <lb/>P <lb/>R <lb/>F1 <lb/>P <lb/>R <lb/>F1 <lb/>P <lb/>R <lb/>F1 <lb/>P <lb/>R <lb/>F1 <lb/>Rule-based <lb/>B-tags <lb/>0.82 0.72 0.77 0.75 0.73 0.74 0.79 0.77 0.78 0.89 0.68 0.77 <lb/>I-tags <lb/>0.80 0.75 0.77 0.75 0.75 0.75 0.78 0.77 0.78 0.83 0.75 0.79 <lb/>Partial-match 0.81 0.71 0.75 0.77 0.74 0.75 0.80 0.78 0.79 0.86 0.67 0.75 <lb/>Exact-match <lb/>0.77 0.67 0.72 0.71 0.69 0.70 0.76 0.74 0.75 0.82 0.63 0.71 <lb/>CRF <lb/>B-tags <lb/>0.89 0.61 0.72 0.87 0.67 0.76 0.87 0.64 0.73 0.92 0.57 0.70 <lb/>I-tags <lb/>0.86 0.66 0.74 0.85 0.74 0.79 0.90 0.65 0.76 0.94 0.69 0.79 <lb/>Partial-match 0.89 0.61 0.72 0.88 0.67 0.76 0.87 0.64 0.74 0.94 0.58 0.71 <lb/>Exact-match <lb/>0.84 0.57 0.68 0.84 0.64 0.73 0.83 0.61 0.71 0.90 0.55 0.68 <lb/>BiLSTM <lb/>B-tags <lb/>0.86 0.67 0.75 0.86 0.67 0.75 0.85 0.68 0.75 0.85 0.67 0.75 <lb/>I-tags <lb/>0.69 0.70 0.69 0.70 0.70 0.70 0.68 0.69 0.69 0.69 0.71 0.70 <lb/>Partial-match 0.72 0.64 0.68 0.72 0.64 0.68 0.71 0.64 0.67 0.72 0.64 0.68 <lb/>Exact-match <lb/>0.66 0.59 0.62 0.66 0.59 0.62 0.64 0.59 0.61 0.65 0.59 0.62 <lb/>BiLSTM-CRF <lb/>B-tags <lb/>0.87 0.57 0.69 0.86 0.56 0.68 0.87 0.60 0.71 0.86 0.60 0.71 <lb/>I-tags <lb/>0.67 0.57 0.62 0.66 0.56 0.61 0.67 0.62 0.64 0.66 0.63 0.64 <lb/>Partial-match 0.69 0.53 0.60 0.68 0.53 0.59 0.71 0.57 0.64 0.70 0.58 0.63 <lb/>Exact-match <lb/>0.61 0.47 0.53 0.60 0.46 0.52 0.63 0.51 0.57 0.63 0.52 0.57 <lb/>BERT <lb/>B-tags <lb/>0.84 0.81 0.83 0.85 0.86 0.85 0.78 0.86 0.82 0.80 0.79 0.80 <lb/>I-tags <lb/>0.78 0.78 0.78 0.81 0.84 0.82 0.78 0.83 0.80 0.74 0.74 0.74 <lb/>Partial-match 0.80 0.80 0.80 0.81 0.84 0.83 0.74 0.84 0.79 0.77 0.76 0.77 <lb/>Exact-match <lb/>0.76 0.75 0.75 0.78 0.80 0.79 0.71 0.81 0.76 0.70 0.71 0.71 <lb/>SciBERT <lb/>B-tags <lb/>0.84 0.83 0.83 0.85 0.86 0.86 0.77 0.88 0.82 0.81 0.80 0.81 <lb/>I-tags <lb/>0.78 0.79 0.78 0.81 0.85 0.83 0.77 0.84 0.80 0.73 0.75 0.74 <lb/>Partial-match 0.80 0.82 0.81 0.82 0.85 0.83 0.74 0.86 0.80 0.77 0.78 0.78 <lb/>Exact-match <lb/>0.76 0.77 0.76 0.78 0.82 0.80 0.71 0.83 0.76 0.71 0.72 0.71 <lb/></div>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>15 of 19 <lb/></page>

			<div type="annex">Table A4. RQ4, testing on a real ratio (positive vs. negative) test set. <lb/>Model <lb/>Evaluation <lb/>Precision <lb/>Recall <lb/>F1 <lb/>CRF <lb/>B-tags <lb/>0.72 <lb/>0.66 <lb/>0.69 <lb/>I-tags <lb/>0.67 <lb/>0.73 <lb/>0.70 <lb/>Partial-match <lb/>0.72 <lb/>0.66 <lb/>0.69 <lb/>Exact-match <lb/>0.69 <lb/>0.63 <lb/>0.66 <lb/>BiLSTM <lb/>B-tags <lb/>0.65 <lb/>0.12 <lb/>0.20 <lb/>I-tags <lb/>0.39 <lb/>0.23 <lb/>0.29 <lb/>Partial-match <lb/>0.37 <lb/>0.19 <lb/>0.26 <lb/>Exact-match <lb/>0.29 <lb/>0.15 <lb/>0.20 <lb/>BiLSTM-CRF <lb/>B-tags <lb/>0.57 <lb/>0.12 <lb/>0.20 <lb/>I-tags <lb/>0.46 <lb/>0.30 <lb/>0.36 <lb/>Partial-match <lb/>0.49 <lb/>0.29 <lb/>0.37 <lb/>Exact-match <lb/>0.38 <lb/>0.23 <lb/>0.29 <lb/>BERT <lb/>B-tags <lb/>0.59 <lb/>0.91 <lb/>0.72 <lb/>I-tags <lb/>0.69 <lb/>0.89 <lb/>0.78 <lb/>Partial-match <lb/>0.57 <lb/>0.91 <lb/>0.70 <lb/>Exact-match <lb/>0.55 <lb/>0.88 <lb/>0.68 <lb/>SciBERT <lb/>B-tags <lb/>0.66 <lb/>0.91 <lb/>0.77 <lb/>I-tags <lb/>0.73 <lb/>0.89 <lb/>0.80 <lb/>Partial-match <lb/>0.65 <lb/>0.91 <lb/>0.76 <lb/>Exact-match <lb/>0.63 <lb/>0.88 <lb/>0.73 <lb/>Table A5. RQ5, differences between training on the golden standard or the weakly annotated data. <lb/>Training Set Evaluation <lb/>Test Set <lb/>Zero-Shot <lb/>P <lb/>R <lb/>F1 <lb/>P <lb/>R <lb/>F1 <lb/>Train set <lb/>B-tags <lb/>0.87 <lb/>0.87 <lb/>0.87 <lb/>0.87 <lb/>0.79 <lb/>0.82 <lb/>I-tags <lb/>0.82 <lb/>0.84 <lb/>0.83 <lb/>0.84 <lb/>0.78 <lb/>0.81 <lb/>Partial-match <lb/>0.82 <lb/>0.84 <lb/>0.83 <lb/>0.84 <lb/>0.78 <lb/>0.81 <lb/>Exact-match <lb/>0.79 <lb/>0.82 <lb/>0.80 <lb/>0.79 <lb/>0.74 <lb/>0.76 <lb/>SSC <lb/>B-tags <lb/>0.77 <lb/>0.71 <lb/>0.74 <lb/>0.83 <lb/>0.70 <lb/>0.76 <lb/>I-tags <lb/>0.74 <lb/>0.73 <lb/>0.73 <lb/>0.80 <lb/>0.69 <lb/>0.74 <lb/>Partial-match <lb/>0.76 <lb/>0.71 <lb/>0.73 <lb/>0.79 <lb/>0.69 <lb/>0.73 <lb/>Exact-match <lb/>0.70 <lb/>0.65 <lb/>0.67 <lb/>0.72 <lb/>0.63 <lb/>0.67 <lb/></div>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>16 of 19 <lb/></page>

			<div type="annex">Table A6. RQ5, training on three different distributions of real and &apos;weak&apos; examples. <lb/>Training Set Evaluation <lb/>Test Set <lb/>Zero-Shot <lb/>P <lb/>R <lb/>F1 <lb/>P <lb/>R <lb/>F1 <lb/>Train + SSC <lb/>B-tags <lb/>0.84 <lb/>0.83 <lb/>0.84 <lb/>0.84 <lb/>0.77 <lb/>0.80 <lb/>I-tags <lb/>0.79 <lb/>0.80 <lb/>0.80 <lb/>0.86 <lb/>0.79 <lb/>0.82 <lb/>Partial-match <lb/>0.81 <lb/>0.82 <lb/>0.81 <lb/>0.84 <lb/>0.78 <lb/>0.81 <lb/>Exact-match <lb/>0.76 <lb/>0.77 <lb/>0.76 <lb/>0.80 <lb/>0.74 <lb/>0.77 <lb/>SSC + Train <lb/>B-tags <lb/>0.84 <lb/>0.81 <lb/>0.83 <lb/>0.84 <lb/>0.79 <lb/>0.81 <lb/>I-tags <lb/>0.78 <lb/>0.79 <lb/>0.78 <lb/>0.85 <lb/>0.79 <lb/>0.82 <lb/>Partial-match <lb/>0.81 <lb/>0.81 <lb/>0.81 <lb/>0.82 <lb/>0.78 <lb/>0.80 <lb/>Exact-match <lb/>0.76 <lb/>0.75 <lb/>0.76 <lb/>0.77 <lb/>0.74 <lb/>0.75 <lb/>SSC &amp; Train <lb/>shuffled <lb/>B-tags <lb/>0.83 <lb/>0.83 <lb/>0.83 <lb/>0.85 <lb/>0.81 <lb/>0.83 <lb/>I-tags <lb/>0.77 <lb/>0.81 <lb/>0.79 <lb/>0.84 <lb/>0.81 <lb/>0.83 <lb/>Partial-match <lb/>0.79 <lb/>0.82 <lb/>0.80 <lb/>0.84 <lb/>0.81 <lb/>0.83 <lb/>Exact-match <lb/>0.74 <lb/>0.77 <lb/>0.75 <lb/>0.79 <lb/>0.76 <lb/>0.78 <lb/>Table A7. RQ5, adding only positive &apos;weak&apos; examples. <lb/>Training Set <lb/>Evaluation <lb/>Test Set <lb/>Zero-Shot <lb/>P <lb/>R <lb/>F1 <lb/>P <lb/>R <lb/>F1 <lb/>Train set &amp; <lb/>positive examples <lb/>B-tags <lb/>0.83 <lb/>0.86 <lb/>0.85 <lb/>0.84 <lb/>0.79 <lb/>0.82 <lb/>I-tags <lb/>0.78 <lb/>0.83 <lb/>0.80 <lb/>0.83 <lb/>0.80 <lb/>0.82 <lb/>Partial-match <lb/>0.80 <lb/>0.85 <lb/>0.83 <lb/>0.84 <lb/>0.81 <lb/>0.82 <lb/>Exact-match <lb/>0.76 <lb/>0.81 <lb/>0.78 <lb/>0.78 <lb/>0.76 <lb/>0.77 <lb/>Appendix E. From PDF to Machine Readable Format <lb/>We initially extracted the text from the PDFs using pdftotext, an open-source command-<lb/>line utility for converting PDF files to plain text files [60]. We later discovered a much <lb/>more powerful tool for preprocessing scientific papers for scientific information extraction, <lb/>named GROBID. GROBID is a machine learning library for extracting, parsing and re-<lb/>structuring raw documents, such as PDFs, into structured XML/TEI encoded documents <lb/>with a particular focus on technical and scientific publications [39]. The advantage of using <lb/>GROBID over pdf2text is that GROBID is able to extract more information than pdf2text. <lb/>The core features as described by GROBID include the following: <lb/>1. <lb/>Header extraction and parsing (e.g., title, abstract, authors, affiliations, keywords, etc.); <lb/>2. <lb/>References extraction and parsing (including DOI identifiers); <lb/>3. <lb/>Citation contexts recognition and linking (including URL extraction); <lb/>4. <lb/>Full text extraction and structuring (e.g., paragraph, section titles, reference callout, <lb/>figure, table, and foot notes). <lb/>For each conference, we parsed the publications available to us and extracted the <lb/>following information from the GROBID conversion output: article ID, conference, title of the <lb/>publication, raw text, sections (number, title and index of starting character), list of author names, <lb/>references and status of publication. <lb/></div>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>17 of 19 <lb/></page>

			<listBibl>References <lb/>1. <lb/>Brickley, D.; Burgess, M.; Noy, N. Google Dataset Search: Building a Search Engine for Datasets in an Open Web Ecosystem. In <lb/>Proceedings of the World Wide Web Conference, San Francisco, CA, USA, 13-17 May 2019; Association for Computing Machinery: <lb/>New York, NY, USA, 2019; pp. 1365-1375. [CrossRef] <lb/>2. <lb/>Kratz, J.E.; Strasser, C. Researcher perspectives on publication and peer review of data. PLoS ONE 2015, 10, e0117619. [CrossRef] <lb/>3. <lb/>Ghavimi, B.; Mayr, P.; Vahdati, S.; Lange, C. Identifying and Improving Dataset References in Social Sciences Full Texts. arXiv <lb/>2016, arXiv:1603.01774. <lb/>4. <lb/>Zeng, T.; Wu, L.; Bratt, S.; Acuna, D.E. Assigning credit to scientific datasets using article citation networks. arXiv 2020, <lb/>arXiv:2001.05917. <lb/>5. <lb/>Mathiak, B.; Boland, K. Challenges in matching dataset citation strings to datasets in social science. D-Lib Mag. 2015, 21, 23-28. <lb/>[CrossRef] <lb/>6. <lb/>Prasad, A.; Si, C.; Kan, M.Y. Dataset Mention Extraction and Classification. In Proceedings of the Workshop on Extracting <lb/>Structured Knowledge from Scientific Publications, Minneapolis, MN, USA, 19-26 June 2019; Association for Computational <lb/>Linguistics: Minneapolis, MN, USA, 2019; pp. 31-36. [CrossRef] <lb/>7. <lb/>Luan, Y.; He, L.; Ostendorf, M.; Hajishirzi, H. Multi-task identification of entities, relations, and coreference for scientific <lb/>knowledge graph construction. arXiv 2018, arXiv:1808.09602. <lb/>8. <lb/>Ghavimi, B.; Mayr, P.; Lange, C.; Vahdati, S.; Auer, S. A semi-automatic approach for detecting dataset references in social science <lb/>texts. Inf. Serv. Use 2016, 36, 171-187. [CrossRef] <lb/>9. <lb/>Yao, R.; Hou, L.; Ye, Y.; Wu, O.; Zhang, J.; Wu, J. Method and Dataset Mining in Scientific Papers. arXiv 2019, arXiv:1911.13096. <lb/>10. Zhao, H.; Luo, Z.; Feng, C.; Zheng, A.; Liu, X. A Context-based Framework for Modeling the Role and Function of On-line <lb/>Resource Citations in Scientific Literature. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language <lb/>Processing and the 9th International Joint Conference on Natural Language Processing, Hong Kong, China, 3-7 November 2019; <lb/>Association for Computational Linguistics: Hong Kong, China, 2019; pp. 5206-5215. [CrossRef] <lb/>11. Kim, H.; Park, K.; Park, S.H. Rich Context Competition: Extracting Research Context and Dataset Usage Information from <lb/>Scientific Publications. Available online: https://rokrokss.com/assets/cv/rcc09.pdf (accessed on 1 June 2020). <lb/>12. Erera, S.; Shmueli-Scheuer, M.; Feigenblat, G.; Peled Nakash, O.; Boni, O.; Roitman, H.; Cohen, D.; Weiner, B.; Mass, Y.; Rivlin, O.; <lb/>et al. A Summarization System for Scientific Documents. arXiv 2019, arXiv:1908.11152. <lb/>13. Hou, Y.; Jochim, C.; Gleize, M.; Bonin, F.; Ganguly, D. Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores <lb/>for Scientific Leaderboards Construction. arXiv 2019, arXiv:1906.09317. <lb/>14. Duck, G.; Nenadic, G.; Brass, A.; Robertson, D.L.; Stevens, R. bioNerDS: Exploring bioinformatics&apos; database and software use <lb/>through literature mining. BMC Bioinform. 2013, 14, 194. [CrossRef] <lb/>15. Beltagy, I.; Lo, K.; Cohan, A. SciBERT: A Pretrained Language Model for Scientific Text. arXiv 2019, arXiv:1903.10676. <lb/>16. GÃ¡bor, K.; Buscaldi, D.; Schumann, A.K.; QasemiZadeh, B.; Zargayouna, H.; Charnois, T. SemEval-2018 Task 7: Semantic Relation <lb/>Extraction and Classification in Scientific Papers. In Proceedings of the 12th International Workshop on Semantic Evaluation, <lb/>New Orleans, Louisiana, 5-6 June 2018; pp. 679-688. [CrossRef] <lb/>17. Casillas, A.; Ezeiza, N.; Goenaga, I.; PÃ©rez, A.; Soto, X. Measuring the effect of different types of unsupervised word representations <lb/>on Medical Named Entity Recognition. Int. J. Med. Inform. 2019, 129, 100-106. [CrossRef] <lb/>18. Guo, H.; Zhu, H.; Guo, Z.; Zhang, X.; Wu, X.; Su, Z. Domain Adaptation with Latent Semantic Association for Named Entity <lb/>Recognition. In Proceedings of the Human Language Technologies: The 2009 Annual Conference of the North American Chapter <lb/>of the Association for Computational Linguistics, Boulder, CO, USA, 31 May 2009-5 June 2009; Association for Computational <lb/>Linguistics: Boulder, CO, USA, 2009; pp. 281-289. <lb/>19. Lee, J.; Kim, H.; Lee, J.; Yoon, S. Transfer learning for deep learning on graph-structured data. In Proceedings of the Thirty-First <lb/>AAAI Conference on Artificial Intelligence, San Francisco, CA, USA, 4-9 February 2017. <lb/>20. Zhang, L. Transfer Adaptation Learning: A Decade Survey. arXiv 2019, arXiv:1903.04687. <lb/>21. Song, Y.; Yi, E.; Kim, E.; Lee, G.G.; Park, S.J. POSBIOTM-NER: A machine learning approach for bio-named entity recognition. <lb/>In Proceedings of the Workshop on Critical Assessment of Text Mining Methods in Molecular Biology, Granada, Spain, 28-31 <lb/>March 2004. <lb/>22. Augenstein, I.; Derczynski, L.; Bontcheva, K. Generalisation in Named Entity Recognition: A quantitative analysis. Comput. <lb/>Speech Lang. 2017, 44, 61-83. [CrossRef] <lb/>23. Kim, J.; Kim, J. The impact of imbalanced training data on machine learning for author name disambiguation. Scientometrics <lb/>2018, 117, 511-526. [CrossRef] <lb/>24. Kurczab, R.; Smusz, S.; Bojarski, A.J. The influence of negative training set size on machine learning-based virtual screening. J. <lb/>Cheminform. 2014, 6, 32. [CrossRef] [PubMed] <lb/>25. Li, X.L.; Liu, B.; Ng, S.K. Negative Training Data Can Be Harmful to Text Classification. In Proceedings of the 2010 Conference on <lb/>Empirical Methods in Natural Language Processing, Cambridge, MA, USA, 9-11 October 2010; Association for Computational <lb/>Linguistics: Cambridge, MA, USA, 2010; pp. 218-228. <lb/></listBibl>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>18 of 19 <lb/></page>

			<listBibl>26. Huang, X.; Dong, L.; Boschee, E.; Peng, N. Learning A Unified Named Entity Tagger From Multiple Partially Annotated Corpora <lb/>For Efficient Adaptation. arXiv 2019, arXiv:1909.11535. <lb/>27. Khongtum, O.; Promrit, N.; Waijanya, S. The Entity Recognition of Thai Poem Compose by Sunthorn Phu by Using the <lb/>Bidirectional Long Short Term Memory Technique. In Proceedings of the International Conference on Multi-disciplinary Trends in <lb/>Artificial Intelligence, Kuala Lumpur, Malaysia, 17-19 November 2019; Springer: Berlin/Heidelberg, Germany, 2019; pp. 97-108. <lb/>28. Li, Z.; Zhang, Q.; Liu, Y.; Feng, D.; Huang, Z. Recurrent neural networks with specialized word embedding for chinese clinical <lb/>named entity recognition. In Proceedings of the Evaluation Task at the China Conference on Knowledge Graph and Semantic <lb/>Computing, Chengdu, China, 26-29 August 2017; pp. 55-60. <lb/>29. Devlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understand-<lb/>ing. arXiv 2018, arXiv:1810.04805. <lb/>30. Tchoua, R.; Ajith, A.; Hong, Z.; Ward, L.; Chard, K.; Audus, D.; Patel, S.; de Pablo, J.; Foster, I. Towards hybrid human-machine <lb/>scientific information extraction. In Proceedings of the 2018 New York Scientific Data Summit, New York, NY, USA, 6-8 August <lb/>2018; pp. 1-3. [CrossRef] <lb/>31. Humphreys, K.; Demetriou, G.; Gaizauskas, R. Bioinformatics applications of information extraction from scientific journal <lb/>articles. J. Inf. Sci. 2000, 26, 75-85. [CrossRef] <lb/>32. Liddy, E.D. The discourse-level structure of empirical abstracts: An exploratory study. Inf. Process. Manag. 1991, 27, 55-81. <lb/>[CrossRef] <lb/>33. Mohit, B. Named entity recognition. In Natural Language Processing of Semitic Languages; Springer: Berlin/Heidelberg, Germany, <lb/>2014; pp. 221-245. <lb/>34. Chiticariu, L.; Krishnamurthy, R.; Li, Y.; Reiss, F.; Vaithyanathan, S. Domain Adaptation of Rule-Based Annotators for Named-<lb/>Entity Recognition Tasks. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, <lb/>Cambridge, MA, USA, 9-11 October 2010; Association for Computational Linguistics: Cambridge, MA, USA, 2010; pp. 1002-1012. <lb/>35. Sterckx, L. Methods for Efficient Supervision in Natural Language Processing. Ph.D. Thesis, Ghent University, Ghent, <lb/>Belgium, 2018. <lb/>36. Fries, J.A.; Varma, P.; Chen, V.S.; Xiao, K.; Tejeda, H.; Saha, P.; Dunnmon, J.; Chubb, H.; Maskatia, S.; Fiterau, M.; et al. Weakly <lb/>supervised classification of aortic valve malformations using unlabeled cardiac MRI sequences. Nat. Commun. 2019, 10, 1-10. <lb/>[CrossRef] [PubMed] <lb/>37. Soni, A.; Viswanathan, D.; Pachaiyappan, N.; Natarajan, S. A Comparison of Weak Supervision methods for Knowledge Base <lb/>Construction. In Proceedings of the 5th Workshop on Automated Knowledge Base Construction, San Diego, CA, USA, 17 June <lb/>2016; Association for Computational Linguistics: San Diego, CA, USA, 2016; pp. 97-102. [CrossRef] <lb/>38. Klinger, R.; Tomanek, K. Classical Probabilistic Models and Conditional Random Fields. Available online: http://citeseerx.ist. <lb/>psu.edu/viewdoc/download?doi=10.1.1.645.5543&amp;rep=rep1&amp;type=pdf (accessed on 1 June 2020). <lb/>39. Lopez, P. GROBID. 2008-2020. Available online: https://github.com/kermitt2/grobid (accessed on 1 June 2020). <lb/>40. ColÃ³n-Ruiz, C.; Segura-Bedmar, I. Protected Health Information Recognition byBiLSTM-CRF. Available online: http://ceur-ws. <lb/>org/Vol-2421/MEDDOCAN_paper_6.pdf (accessed on 1 June 2020). <lb/>41. Wunnava, S.; Qin, X.; Kakar, T.; Rundensteiner, E.A.; Kong, X. Bidirectional LSTM-CRF for adverse drug event tagging in <lb/>electronic health records. In Proceedings of the 1st International Workshop on Medication and Adverse Drug Event Detection, <lb/>4 May 2018; pp. 48-56. <lb/>42. Simoes, G.; Galhardas, H.; Coheur, L. Information Extraction tasks: A survey. In Proceedings of the INForum 2009-SimpÃ³sio de <lb/>InformÃ¡tica, Lisboa, Portugal, 10-11 September 2009. <lb/>43. Åniegula, A.; Poniszewska-Mara Åda, A.; Chomatek, Å. Towards the Named Entity Recognition Methods in Biomedical Field. In <lb/>Proceedings of the 46th International Conference on Current Trends in Theory and Practice of Informatics, Limassol, Cyprus, <lb/>20-24 January 2020; pp. 375-387. <lb/>44. Correia, G.M.; Niculae, V.; Martins, A.F.T. Adaptively Sparse Transformers. arXiv 2019, arXiv:cs.CL/1909.00015, e-prints. <lb/>45. Schumann, A.K.; Qasemi Zadeh, B. The ACL RD-TEC Annotation Guideline: A Reference Dataset for the Evaluation of Automatic <lb/>Term Recognition and Classification. Tech. Rep. 2015. [CrossRef] <lb/>46. Nichols, T.R.; Wisner, P.M.; Cripe, G.; Gulabchand, L. Putting the kappa statistic to use. Qual. Assur. J. 2010, 13, 57-61. [CrossRef] <lb/>47. spaCy. Tokenizer. Available online: https://spacy.io/api/tokenizer (accessed on 1 June 2020). <lb/>48. NLTK. Natural Language Toolkit. Available online: https://www.nltk.org/index.html (accessed on 1 June 2020). <lb/>49. Github. A Python Framework for Sequence Labeling Evaluation (Named-Entity Recognition, Pos Tagging, etc.). 2019. Available <lb/>online: https://github.com/chakki-works/seqeval (accessed on 1 June 2020). <lb/>50. Github. Full Named-Entity (i.e., Not Tag/Token) Evaluation Metrics Based on SemEval&apos;13. 2019. Available online: https: <lb/>//github.com/ivyleavedtoadflax/nervaluate (accessed on 1 June 2020). <lb/>51. spaCy. Rule-Based Matching. Available online: https://spacy.io/usage/rule-based-matching/ (accessed on 1 June 2020). <lb/>52. Sklearn. SklearnâCrfsuite. Available online: https://sklearn-crfsuite.readthedocs.io/en/latest/ (accessed on 1 June 2020). <lb/>53. Depends on the Definition Guide to Sequence Tagging with Neural Networks. 2017. Available online: https://www.depends-<lb/>on-the-definition.com/guide-sequence-tagging-neural-networks-python/ (accessed on 1 June 2020). <lb/>54. Github. Scikit-Learn Wrapper to Finetune BERT 2019. Available online: https://github.com/charles9n/bert-sklearn (accessed <lb/>on 1 June 2020). <lb/></listBibl>

			<note place="headnote">Data 2021, 6, 84 <lb/></note>

			<page>19 of 19 <lb/></page>

			<listBibl>55. Chowdhury, M.F.M.; Lavelli, A. Assessing the practical usability of an automatically annotated corpus. In Proceedings of the 5th <lb/>Linguistic Annotation Workshop, Portland, OR, USA, 23-24 June 2011. <lb/>56. Dutta, S.; Weikum, G. Cross-document co-reference resolution using sample-based clustering with knowledge enrichment. Trans. <lb/>Assoc. Comput. Linguist. 2015, 3, 15-28. [CrossRef] <lb/>57. Stevenson, A. Oxford Dictionary of English; Oxford University Press: Oxford, UK, 2010. <lb/>58. Dictionary, O.L. Data-Set. 2019. Available online: https://www.oxfordlearnersdictionaries.com/definition/english/data-set <lb/>(accessed on 1 June 2020). <lb/>59. NIPS. NIPS Proceedings. Available online: https://papers.nips.cc/ (accessed on 1 June 2020). <lb/>60. Palmer, J. pdftotext. 2020. Available online: https://github.com/jalan/pdftotext (accessed on 1 June 2020). </listBibl>


	</text>
</tei>
