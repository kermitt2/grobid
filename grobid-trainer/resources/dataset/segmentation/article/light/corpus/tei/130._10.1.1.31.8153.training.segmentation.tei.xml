<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Pages 61 to 70 of W. Daelemans, A. van den Bosch, and A. Weijters (Editors), <lb/>Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural <lb/>Language Processing Tasks, April 26, 1997, Prague, Czech Republic <lb/>Automatic Phonetic Transcription of Words <lb/>Based On Sparse Data <lb/>Maria Wolters (i) and Antal van den Bosch (ii) <lb/>(i) Institut f ur Kommunikationsforschung und Phonetik, Universit at Bonn <lb/>Poppelsdorfer Allee 47, 53113 Bonn, Germany <lb/>mwo@asl1.ikp.uni-bonn.de <lb/>(ii) Department of Computer Science, Universiteit Maastricht <lb/>PO Box 616, 6200 MD Maastricht, The Netherlands <lb/>antal@cs.unimaas.nl <lb/>Abstract <lb/>The relation between the orthography and the phonology of a language has <lb/>traditionally been modelled by hand{crafted rule sets. Machine-learning (ML) <lb/>approaches o er a means to gather this knowledge automatically. Problems <lb/>arise when the training material is sparse. Generalising from sparse data <lb/>is a well-known problem for many ML algorithms. We present experiments <lb/>in which connectionist, instance{based, and decision{tree learning algorithms <lb/>are applied to a small corpus of Scottish Gaelic. instance-based learning in the <lb/>ib1-ig algorithm yields the best generalisation performance, and that most <lb/>algorithms tested perform tolerably well. Given the availability of a lexicon, <lb/>even if it is sparse, ML is a valuable and e cient tool for automatic phonetic <lb/>transcription of written text. <lb/></front>

			<body>1 The Problem <lb/>Experienced readers can read text aloud uently and without pronunciation errors. <lb/>But can we simulate this performance on a computer? This question is especially <lb/>relevant for text{to{speech (TTS) synthesis. In a TTS system, orthographic text <lb/>rst has to be converted into a sequence of orthophones, which describe the pro-<lb/>nunciation norm. This phonetic transcription is the main input of the synthesis <lb/>module 1 . <lb/></body>

			<note place="footnote">1 Further processing steps are not considered here; for an overview, see (Allen et al., 1987). <lb/></note>

			<body>The classic approach to automatic phonetic transcription (APT) is a large lexicon <lb/>supplemented with a hand-crafted rule set. Many researchers have tried to replace <lb/>rule sets using machine learning (ML) algorithms trained on the lexicon, but with <lb/>mixed success. The performance of most algorithms still falls far below the mark <lb/>of 80{90% correct words which is needed in high{quality text{to{speech synthesis <lb/>(Yvon, 1996). However, Bakiri and Dietterich (1993) have shown that their approach <lb/>based on ID-3 (Quinlan, 1986) decision trees outperforms the sophisticated DECTalk <lb/>rule set for English (Allen et al., 1987); (Van den Bosch and Daelemans, 1993; <lb/>Daelemans and Van den Bosch, 1997) report similar results for Dutch. In both <lb/>cases, the training corpora contained around 18000, and the test corpora around <lb/>2000 words. <lb/>With the exception of (Dietterich and Bakiri, 1995), most researchers have relied <lb/>on large machine readable pronunciation dictionaries for training and test data. <lb/>However, for most languages, the necessary corpora have to be gathered and typed <lb/>in rst, because modern standard pronunciation dictionaries are available neither on <lb/>paper nor in machine-readable form. While producing a large, well{debugged corpus <lb/>takes longer than hand{crafting a rule set, a small corpus of about 1000{2000 words <lb/>can be gathered in 1{2 weeks. Therefore, using ML algorithms is only worthwhile <lb/>if they produce good results with little data. <lb/>In this paper, we examine the performance of ML algorithms on a Scottish Gaelic <lb/>corpus of 1000 words. Section 2 provides a brief overview of the algorithms tested <lb/>and explains why they were chosen. In section 3, we compare the performance of <lb/>these algorithms on the Gaelic corpus. Section 4 presents some preliminary conclu-<lb/>sions. <lb/>2 Choice of Algorithms <lb/>Two types of ML approaches to APT can be found in the literature: <lb/>chunk{based : A sequence of letters is mapped onto a sequence of phonemes. <lb/>phoneme{based : A sequence of letters is mapped onto a phoneme. <lb/>Although chunk{based approaches are psycholinguistically plausible (cf. Glushko, <lb/>1979), they are not suitable for minority-language APT. Algorithms in the tradition <lb/>of PRONOUNCE (Dedina and Nusbaum, 1991) rely on extensive statistics about <lb/>letter/phone correspondences which cannot be estimated adequately from tiny cor-<lb/>pora. JUPA (Yvon, 1996), which recombines dictionary entries, does not produce <lb/>any output for 30{40% of the test words when trained on 2000 words only, and sim-<lb/>ilar problems should occur with the more sophisticated algorithms Yvon describes. <lb/>Therefore, we have to rely on phoneme{based approaches. Usually, a window of <lb/>2n + 1 characters is shifted across the input word. The nth character of this win-<lb/>dow is transcribed, the other 2n serve as context. Because of the limited window <lb/>length, it is di cult to capture morphophonological alternations like English Tri-<lb/>syllabic Shortening as in divine { divinity, and stress shifts as in photograph { <lb/>photography. <lb/>Three types of phoneme-based approaches have yielded good results for large cor-<lb/>pora: neural networks (Sejnowski and Rosenberg, 1987), decision trees (Dietterich <lb/>et al., 1995), and instance{based learning (Van den Bosch and Daelemans, 1993). <lb/>2.1 Neural networks <lb/>Arti cial neural networks (ann) consist of simple processing units with weighted <lb/>connections. The units are usually grouped into an input layer, an output layer, and <lb/>one or more hidden layers. The best results on APT so far have been achieved using <lb/>a simple feed-forward topology 2 and Backpropagation with Momentum (Rumelhart <lb/>et al., 1986). <lb/>The ann approach tested here was proposed in (Wolters, 1996). First, a feed{ <lb/>forward ann is trained using Backpropagation with Momentum until the error on <lb/>a validation set starts to rise (early stopping). This way, we avoid over tting of the <lb/>training data, which results in bad generalisation performance for neural networks. <lb/>Usually, we nd that the shorter the number of training epochs, the less precise <lb/>the adjustment of the weights and the more noisy the internal distributed repre-<lb/>sentations. To reduce this noise as much as possible, the net output is classi ed <lb/>again. For this second stage, we use Learning Vector Quantization (lvq, (Kohonen <lb/>et al., 1996)). lvq computes a set of no cod codebook vectors which describe no class <lb/>classes (here: orthophones). An instance is classi ed by determining the classes of <lb/>the k most similar codebook vectors and associating it with the most frequent class <lb/>(k{nearest{neighbour classi cation). <lb/>2.2 Instance{Based Learning <lb/>Like lvq, instance{based learning (ibl) descends from the k-nearest neighbour al-<lb/>gorithm (Devijver and Kittler, 1982; Aha et al., 1991). In ibl, the basis for classi -<lb/>cation is not a set of codebook vectors, but a set of exemplars, instances encountered <lb/>earlier in classi cation. ibl is a form of lazy learning, where learning only involves <lb/>storing instances in memory, while computational e ort is put into classi cation. <lb/>On the contrary, in eager learning, computational e ort is put mainly into learning. <lb/>ann and decision trees are eager algorithms. simple and robust approaches within <lb/>the group of Case{Based Reasoning algorithms (CBR) (Kolodner, 1993), because it <lb/>is based on feature-value vectors rather than on more complex expressions such as <lb/>those in rst-order logic (Kolodner, 1993; Lavra c and D zeroski, 1994). <lb/></body>

			<note place="footnote">2 feedforward: the output of the units in layer i is only fed to units in layer j &gt; i. <lb/></note>

			<body>We examine two ibl algorithms, viz. ib1 and ib1-ig. ib1 (Aha et al., 1991; Daele-<lb/>mans et al., 1997) constructs a database of instances during learning. An instance <lb/>consists of a xed-length vector of n feature-value pairs, and an information eld <lb/>containing its class(es). When the classi cation of a feature-value vector is ambigu-<lb/>ous, the frequencies of the relevant classes in the training material are calculated <lb/>and the frequency information is stored together with the instance in the instance <lb/>base. New instances X are classi ed by matching them to all instances Y in the <lb/>instance base, calculating the distance (X; Y ) between X and each of the Y s using <lb/>the distance function given in Eq. 1: <lb/>(X; Y ) = <lb/>n <lb/>X <lb/>i=1 <lb/>W (f i ) (x i ; y i ) <lb/>(1) <lb/>where W (fi) is the weight of the ith feature, and (x i ; y i ) is the distance between the <lb/>values of the ith feature in instances X and Y . When feature values are symbolic, <lb/>as with our data, (x i ; y i ) = 0 when x i = y i , and (x i ; y i ) = 1 when x i 6 = y i . <lb/>ib1-ig (Daelemans and Van den Bosch, 1992) di ers from ib1 in the weighting <lb/>function W (f i ) (cf. Eq. 1). The weighting function of ib1-ig, W 0 (f i ), represents <lb/>the information gain (Quinlan, 1993) of feature f i . The information gain of a feature <lb/>expresses the relevance of a feature for classi cation relative to the other features. <lb/>in the distance function (Eq. 1), instances that match on features with a relatively <lb/>high information gain are regarded as less distant (more alike) than instances that <lb/>match on features with a lower information gain. <lb/>2.3 Decision Trees <lb/>Top-down induction of decision trees (tdidt) is a well-developed eld within arti -<lb/>cial intelligence 3 . (tdidt) is based on the assumption that the similarity information <lb/>stored in an exemplar base can be compressed in a tree without signi cantly a ect-<lb/>ing generalisation. Learning in tdidt is eager since decision trees are constructed <lb/>during learning; classi cation e ort is low since it involves non-backtracking de-<lb/>terministic traversal through the induced tree. Two decision tree algorithms are <lb/>evaluated here: igtree (information-gain tree, (Daelemans et al., 1997)) and sct <lb/>(semantic classi cation trees, (Kuhn and De Mori, 1995)). <lb/>igtree (Daelemans et al., 1997) was designed as an optimised approximation of <lb/>ib1-ig. In igtree, information gain is used as a guiding function to compress the <lb/>instance base into a decision tree. Nodes are connected via arcs denoting feature <lb/>values. Information gain is used in igtree to determine the order in which feature <lb/>values are added as arcs to the tree. An instance is stored in the tree as a path <lb/>of arcs whose terminal node (=leaf) speci es its class. When storing feature-value <lb/>information, arcs representing the values of the feature with the highest information <lb/>gain are created rst, then arcs for the values of the feature with the second-highest <lb/></body>

			<note place="footnote">3 see e.g. (Quinlan, 1993) for an overview <lb/></note>

			<body>information gain, etc., until the classi cation information represented by a path is <lb/>unambiguous. Short paths in the tree represent instances with relatively regular <lb/>classi cations, whereas long paths represent instances with irregular, exceptional, <lb/>or noisy classi cations. <lb/>Apart from storing uniquely identi ed class labels at each leaf, igtree stores in-<lb/>formation on the default classi cation at each non-terminal node. This default is <lb/>the most frequent classi cation of those instances which are covered by the subtree <lb/>below that node. A new instance is classi ed by matching its feature values with the <lb/>arcs in the order of the overall feature information gain. When a leaf is reached, the <lb/>instance is assigned the class stored at the leaf; otherwise, it is assigned the default <lb/>classi cation associated with the last matching non-terminal node. <lb/>Semantic Classi cation Trees (SCT) were introduced by (Kuhn and De Mori, 1995) <lb/>for Natural Language Understanding and have been applied successfully to the clas-<lb/>si cation of dialogue acts by keyword spotting (Mast et al., 1995). In SCTs, the <lb/>class of an instance is determined by matching it against a set of regular expressions. <lb/>At each node, only one regular expression is tested. There are two branches, one for <lb/>&quot;match&quot; and one for &quot;no match&quot;. While tests are stored at nodes, classes are stored <lb/>at leaves. To avoid overgeneralisation, the trees are trained using the algorithm of <lb/>(Gelfand et al., 1991). <lb/>In contrast to neural nets, scts cannot extract equivalence classes of attributes from <lb/>the data such as the class of vowel graphemes. However, the algorithm does not need <lb/>any windowing; it can access the complete word quite e ciently by adequate regular <lb/>expressions. <lb/>3 Comparison of Algorithm Performance <lb/>3.1 The Data Set <lb/>The algorithms were tested on a dictionary of 1003 phonetically transcribed Scottish <lb/>Gaelic words (Wolters, 1997). The transcriptions re ect the Gaelic of Point, Isle of <lb/>Lewis, Outer Hebrides. Scottish Gaelic is a minority language with about 80,000 <lb/>speakers. Its orthography is rather complex. It was codi ed in the 18th century, <lb/>and the dialect on which it is based has nearly died out today. The corpus is hand{ <lb/>aligned and contains both zero graphemes and zero phonemes 4 . The transcriptions <lb/>are largely allophonic; 104 allophone classes are used. A window length of 7 yields <lb/>on average 64 patterns per class. which often cover several di erent grapheme-phone <lb/></body>

			<note place="footnote">4 Introducing zero elements eliminates the problem of parsing a sequence of letters into <lb/>graphemes, functional units that correspond to a phoneme. It is basically a preprocessing task <lb/>on the data level. In Gaelic, zero graphemes are necessary e.g. for prea ricated plosives, where <lb/>we have correspondences like k ! /xk/. Because the rules for inserting zero graphemes are very <lb/>regular, their presence should not distort classi cation results signi cantly. <lb/></note>

			<body>correspondences. On average, 3,78% of all training instances (1.57% of all types) are <lb/>ambiguous, but less than 1% of all test instances. Vowel graphemes are especially <lb/>susceptible to errors, since they are also used to encode consonant quality 5 . <lb/>3.2 Method <lb/>All algorithms were trained using 10-fold cross validation (Weiss and Kulikowski, <lb/>1991) to allow for signi cance tests on performance di erences. <lb/>The ann consists of 1 input layer of 7 5 units, 2 hidden layers of 100 units <lb/>each, and 1 output layer of 22 units. The size of the hidden layers was motivated <lb/>by two main considerations. First, a large number of connections means a large <lb/>variance with the potential to accomodate very complex hidden representations, <lb/>and secondly, a size of 100{200 hidden units is quite common for this problem in <lb/>the psycholinguistic/speech processing literature. <lb/>Letters were encoded using a binary code, phones using phonological features (Halle, <lb/>1992). For sparse data, it is advisable to keep the dimensions of input and output <lb/>space small, because it is harder to estimate a function of many variables (i.e., <lb/>high dimensional output space) on the basis of sparse data than it is to estimate a <lb/>function of few variables (i.e., low dimensional output space) 6 . Since there was not <lb/>enough data for a separate validation set, the test set was used for early stopping. <lb/>The lvq{codebook consisted of 2000, roughly 1=3 of the total number of patterns. <lb/>The sct input was not coded using the window technique, because it accepts input <lb/>of variable length and does not explicitly demand that features be in a certain order. <lb/>Instead, each instance consisted of the source word and the position of the phoneme <lb/>to be transcribed. This way, sct disposes of all relevant information except for part-<lb/>of-speech and semantic information needed for resolving word{level ambiguities. <lb/>3.3 Results <lb/>On the training set, we obtain near perfect recall for ib1, ib1-ig, and igtree (c.f. <lb/>Fig. 1). The small remaining error is mostly due to ambiguity in the data. Recall <lb/>is slightly worse for ann, and signi cantly worse for sct. <lb/>On the test set, however, the picture changes slightly, as can be seen in Fig. 2. Here, <lb/>ann{lvq, ib1-ig and igtree provide the best generalisation performance, with <lb/>ib1-ig signi cantly better than the other two algorithms (p &lt; 0.05). Furthermore, <lb/>weighting the contribution of the letters improves the generalisation performance of <lb/>ibl signi cantly (p &lt; 0.001). Why this superiority of the nearest neighbour classi er <lb/></body>

			<note place="footnote">5 For example, in cait (&quot;the cats&quot;), i only serves as a cue to the palatality of /t/. <lb/></note>

			<note place="footnote">6 see also the experiments reported in (Wolters, 1997) on the Gaelic corpus with di erent input <lb/>and output representations. <lb/></note>

			<body>0 <lb/>20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>ANN_LVQ IB1 <lb/>IB1-IG IGTree <lb/>SCT <lb/>% correct phonemes <lb/>Figure 1: Average reproduction accuracy on training set in percentage of correctly <lb/>classi ed phones <lb/>0 <lb/>20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>ANN_LVQ IB1 <lb/>IB1-IG IGTree <lb/>SCT <lb/>% correct phonemes <lb/>Figure 2: Average generalization accuracy on test set in percentage of correctly <lb/>classi ed phones <lb/>ib1-ig? Three aspects of learning in ib1-ig are advantageous in lgeneralising from <lb/>sparse data: <lb/>storing all training examples. Many patterns that occur in the test words are <lb/>bound to be contained in the training set, if we use disjoint sets of words for <lb/>training and testing. Classi cations of overlapping instances are bound to be <lb/>correct, hence, it is advantageous to remember all instances. <lb/>modelling the relationship between frequency and regularity. Regular corre-<lb/>spondences tend to be more frequent in the training set than irregular ones. <lb/>This counteracts the noise introduced by the irregular exemplars, because test <lb/>instances are more often matched to regular exemplars than to irregular ones. <lb/>adequate similarity function. Contrary to anns and decision trees, similarity <lb/>functions can be manipulated and adapted very easily; the information{gain <lb/>weighting is adequate for the task at hand. <lb/>sct clearly su ers from the lack of data. Instead of checking feature values in a <lb/>xed order, it attempts to induce adequate tests from the data. For this, much data <lb/>is needed if the relevant patterns are complex, as is the case with APT. <lb/>4 Conclusion <lb/>The results for Scottish Gaelic show that for minority languages, ML algorithms for <lb/>APT may well be valid alternatives to devising rule sets by hand. The generalisation <lb/>results of the best algorithms are tolerable for Scottish Gaelic, although it still <lb/>remains to be seen if the frequency of errors seriously impedes intelligibility. Scottish <lb/>Gaelic is a hard test case since its orthography is complex. For most small languages <lb/>like Native American or African languages, orthographies were only devised in the <lb/>last century and involve rather simple letter{to{phone correspondences. Therefore, <lb/>for most other minority languages, the results should be even better. <lb/>Why build a ML{based module instead of a hand{crafted rule set? The main ad-<lb/>vantage of ML is that the di culties in the phonetician&apos;s task are shifted from <lb/>the acquisition and encoding of knowledge about a language to the encoding of data. <lb/>Standard procedures exist for the latter which have been used by many eldworkers, <lb/>whereas the former may prove di cult, especially for languages with a complicated <lb/>morpho{phonology. The basic lexicon for the TTS system can be used for training <lb/>the APT module. Moreover, in building the lexicon, the user also creates a valuable <lb/>resource for the further study of the language she works on. <lb/>ibl{based algorithms provide a particularly good interface to a TTS lexicon, since <lb/>they provide a means of both accessing and generalising over the data stored there. <lb/>This eliminates the need for a separate module for the transcription of unknown <lb/>words. <lb/></body>

			<div type="acknowledgement">Acknowledgements <lb/>The sct software was kindly provided by the Institute for Computer Science V, <lb/>University of Erlangen. The Stuttgart Neural Network Simulator (University of <lb/>Stuttgart) was used for ann simulations and LVQ PAK (Helsinki University of Tech-<lb/>nology) for lvq. M.W. would like to thank the Studienstiftung des Deutschen Volkes <lb/>for funding. <lb/></div>

			<listBibl>References <lb/>Aha, D., Kibler, D., and Albert, M. (1991). Instance{based learning algorithms. <lb/>Machine Learning, 6:37{66. <lb/>Allen, J., Hunnicutt, S., and Klatt, D. (1987). From Text to Speech: the MITalk <lb/>system. MITPress, Cambridge, Mass. <lb/>Daelemans, W. and Van den Bosch, A. (1992). Generalisation performance of back-<lb/>propagation learning on a syllabi cation task. In Drossaers, M. F. J. and Ni-<lb/>jholt, A., editors, TWLT3: Connectionism and Natural Language Processing, <lb/>pages 27{37, Enschede. Twente University. <lb/>Daelemans, W. and Van den Bosch, A. (1997). Language-independent data-oriented <lb/>grapheme-to-phoneme conversion. In Van Santen, J. P. H., Sproat, R. W., Olive, <lb/>J. P., and Hirschberg, J., editors, Progress in Speech Processing, pages 77{89. <lb/>Berlin: Springer-Verlag. <lb/>Daelemans, W., Van den Bosch, A., and Weijters, A. (1997). igtree: using trees for <lb/>classi cation in lazy learning algorithms. AI Review. to be published. <lb/>Devijver, P. A. and Kittler, J. (1982). Pattern Recognition. A Statistical Approach. <lb/>Prentice-Hall, London, UK. <lb/>Dedina, M. and Nusbaum, H. (1991). Pronounce: a program for pronunciation by <lb/>analogy. Computer Speech and Language, 5:55{64. <lb/>Dietterich, T. and Bakiri, G. (1995). Solving multi{class problems using error{ <lb/>correcting codes. JAIR, 2:263{286. <lb/>Dietterich, T., Hild, H., and Bakiri, G. (1995). A comparision of ID3 and backprop-<lb/>agation for English text{to{speech mapping. Machine Learning, 18:51{80. <lb/>Gelfand, S., Ravishankar, C., and Delp, E. (1991). An iterative growing and pruning <lb/>algorithm for classi er design. IEEE Trans. PAMI, pages 163{174. <lb/>Glushko, J. (1979). The organization and activation of orthographic knowledge. J. <lb/>Experimental Psychology: Human perception and performance, pages 674{691. <lb/>Halle, M. (1992). Phonetic features. In Bright, W., editor, International Encyclo-<lb/>pedia of Linguistics, pages 207{212. Oxford University Press, Oxford. <lb/>Kohonen, T., Kangas, J., Laaksonen, J., and Torkkola, K. (1996). LVQ-PAK -the <lb/>Learning Vector Quantization package v. 3.0. Technical Report A30, Helsinki <lb/>University of Technology. <lb/>Kolodner, J. (1993). Case{Based Reasoning. San Mateo, CA: Morgan Kaufmann. <lb/>Kuhn, R. and De Mori, R. (1995). The application of semantic classi cation trees to <lb/>natural language understanding. IEEE Trans. Pattern Analysis and Machine <lb/>Intelligence, 17:449{460. <lb/>Lavra c, N. and D zeroski, S. (1994). Inductive Logic Programming. Chichester, UK: <lb/>Ellis Horwood. <lb/>Mast, M., Niemann, H., N oth, E., and Schukat-Talamazzini, E. (1995). Automatic <lb/>classi cation fo dialog acts with semantic classi cation trees and polygrams. <lb/>In IJCAI, Workshop on \New Approaches to Learning for Natural Language <lb/>Processing&quot;, pages 71{78, Montreal. <lb/>Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1:81{106. <lb/>Quinlan, J. (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan <lb/>Kaufmann. <lb/>Rumelhart, D., Hinton, G., and Williams, R. (1986). Learning internal represen-<lb/>tations by error propagation. In Rumelhart, D. and McClelland, J., editors, <lb/>Parallel Distributed Processing: Explorations in the Microstructure of Cogni-<lb/>tion, pages 318{362. MIT Press, Cambridge, MA. <lb/>Sejnowski, T. and Rosenberg, C. (1987). A parallel network that learns to pronounce <lb/>English text. Complex Systems, 1:145{168. <lb/>Van den Bosch, A. and Daelemans, W. (1993). Data-oriented methods for grapheme-<lb/>to-phoneme conversion. In Proceedings of the 6th Conference of the EACL, <lb/>pages 45{53. <lb/>Weiss, S. and Kulikowski, C. (1991). Computer Systems That Learn. San Mateo, <lb/>CA: Morgan Kaufmann. <lb/>Wolters, M. (1996). A dual{route neural{network based approach to grapheme{to{ <lb/>phoneme conversion. In v. Seelen, W., v.d. Malsburg, C., Sendho , B., and J., <lb/>editors, Proc. Intl. Conf. on Arti cial Neural Networks 1996, Lecture Notes in <lb/>Computer Science. Springer, Berlin, Heidelberg, New York. <lb/>Wolters, M. (1997). A Diphone{Based Text{to{Speech System for Scottish Gaelic. <lb/>Master&apos;s thesis, Department of Computer Science, University of Bonn. <lb/>Yvon, F. (1996). Prononciation par analogie. PhD thesis, Ecole Nationale Sup erieure <lb/>des T el ecommunications, Paris. </listBibl>


	</text>
</tei>
