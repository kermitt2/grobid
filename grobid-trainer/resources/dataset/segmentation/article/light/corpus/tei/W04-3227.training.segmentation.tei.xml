<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="__W04-3227"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Phrase Pair Rescoring with Term Weightings for <lb/>Statistical Machine Translation <lb/> Bing Zhao Stephan Vogel Alex Waibel <lb/> Carnegie Mellon University <lb/>5000 Forbes Avenue <lb/>Pittsburgh, PA, 15213, USA <lb/>{bzhao, vogel+, ahw}@cs.cmu.edu <lb/> Abstract <lb/> We propose to score phrase translation <lb/>pairs for statistical machine translation <lb/>using term weight based models. These <lb/>models employ tf.idf to encode the <lb/>weights of content and non-content words <lb/>in phrase translation pairs. The transla-<lb/>tion probability is then modeled by simi-<lb/>larity functions defined in a vector space. <lb/>Two similarity functions are compared. <lb/>Using these models in a statistical ma-<lb/>chine translation task shows significant <lb/>improvements. <lb/></front>

			<body> 1 Introduction <lb/> Words can be classified as content and func-<lb/>tional words. Content words like verbs and <lb/>proper nouns are more informative than function <lb/>words like &quot;to&apos;&apos; and &quot;the&apos;&apos;. In machine translation, <lb/>intuitively, the informative content words should <lb/>be emphasized more for better adequacy of the <lb/>translation quality. However, the standard statis-<lb/>tical translation approach does not take account <lb/>how informative and thereby, how important a <lb/>word is, in its translation model. One reason is <lb/>the difficulty to measure how informative a word <lb/>is. Another problem is to integrate it naturally <lb/>into the existing statistical machine translation <lb/>framework, which typically is built on word <lb/>alignment models, like the well-known IBM <lb/>alignment models (Brown et al 1993). <lb/>In recent years there has been a strong ten-<lb/>dency to incorporate phrasal translation into sta-<lb/>tistical machine translation. It directly translates <lb/>an n-gram from the source language into an m-<lb/>gram in the target language. The advantages are <lb/>obvious: It has built-in local context modeling, <lb/>and provides reliable local word reordering. It <lb/>has multi-word translations, and models a word&apos;s <lb/>conditional fertility given a local context. It cap-<lb/>tures idiomatic phrase translations and can be <lb/>easily enriched with bilingual dictionaries. In <lb/>addition, it can compensate for the segmentation <lb/>errors made during preprocessing, i.e. word seg-<lb/>mentation errors of Chinese. The advantage of <lb/>using phrase-based translation in a statistical <lb/>framework has been shown in many studies such <lb/>as (Koehn et al. 2003; Vogel et al. 2003; Zens et <lb/>al. 2002; Marcu and Wong, 2002). However, the <lb/>phrase translation pairs are typically extracted <lb/>from a parallel corpus based on the Viterbi align-<lb/>ment of some word alignment models. The leads <lb/>to the question what probability should be as-<lb/>signed to those phrase translations. Different <lb/>approaches have been suggested as using relative <lb/>frequencies (Zens et al. 2002), calculate prob-<lb/>abilities based on a statistical word-to-word dic-<lb/>tionary (Vogel et al. 2003) or use a linear <lb/>interpolation of these scores (Koehn et al. 2003). <lb/>In this paper we investigate a different ap-<lb/>proach with takes the information content of <lb/>words better into account. Term weighting based <lb/>vector models are proposed to encode the transla-<lb/>tion quality. The advantage is that term weights, <lb/>such as tf.idf, are useful to model the informa-<lb/>tiveness of words. Highly informative content <lb/>words usually have high tf.idf scores. In informa-<lb/>tion retrieval this has been successfully applied to <lb/>capture the relevance of a document to a query, <lb/>by representing both query and documents as <lb/>term weight vectors and use for example the <lb/>cosine distance to calculate the similarity be-<lb/>tween query vector and document vector. The <lb/>idea now is to consider the source phrase as a <lb/> &quot; query &quot; , and the different target phrases ex-<lb/>tracted from the bilingual corpus as translation <lb/>candidates as a relevant &quot; documents &quot; . The co-<lb/>sine distance is then a natural choice to model the <lb/>translation probability. <lb/> Our approach is to apply term weighting <lb/>schemes to transform source and target phrases <lb/>
			
			into term vectors. Usually content words in both <lb/>source and target languages will be emphasized <lb/>by large term weights. Thus, good phrase trans-<lb/>lation pairs will share similar contours, or, to ex-<lb/>press it in a different way, will be close to each <lb/>other in the term weight vector space. A similar-<lb/>ity function is then defined to approximate trans-<lb/>lation probability in the vector space. <lb/>The paper is structured as follows: in Section <lb/>2, our phrase-based statistical machine translation <lb/>system is introduced; in Section 3, a phrase trans-<lb/>lation score function based on word translation <lb/>probabilities is explained, as this will be used as a <lb/>baseline system; in Section 4, a vector model <lb/>based on tf.idf is proposed together with two <lb/>similarity functions; in Section 5, length regu-<lb/>larization and smoothing schemes are explained <lb/>briefly; in Section 6, the translation experiments <lb/>are presented; and Section 7 concludes with a <lb/>discussion. <lb/> 2 Phrase-based Machine Translation <lb/> In this section, the phrase-based machine transla-<lb/>tion system used in the experiments is briefly <lb/>described: the phrase based translation models <lb/>and the decoding algorithm, which allows for <lb/>local word reordering. <lb/> 2.1 Translation Model <lb/> The phrase-based statistical translation systems <lb/>use not only word-to-word translation, extracted <lb/>from bilingual data, but also phrase-to phrase <lb/>translations. . Different types of extraction ap-<lb/>proaches have been described in the literature: <lb/> syntax-based, word-alignment-based, and genu-<lb/>ine phrase alignment models. The syntax-based <lb/>approach has the advantage to model the gram-<lb/>mar structures using models of more or less <lb/>structural richness, such as the syntax-based <lb/>alignment model in (Yamada and Knight, 2001) <lb/>or the Bilingual Bracketing in (Wu, 1997). Popu-<lb/>lar word-alignment-based approaches usually <lb/>rely on initial word alignments from the IBM and <lb/>HMM alignment models (Och and Ney, 2000), <lb/>from which the phrase pairs are then extracted. <lb/>(Marcu and Wong 2002) and (Zhang et al. 2003) <lb/>do not rely on word alignment but model directly <lb/>the phrase alignment. <lb/>Because all statistical machine translation sys-<lb/>tems search for a globally optimal translation <lb/>using the language and translation model, a trans-<lb/>lation probability has to be assigned to each <lb/>phrase translation pair. This score should be <lb/>meaningful in that better translations have a <lb/>higher probability assigned to them, and balanced <lb/>with respect to word translations. Bad phrase <lb/>translations should not win over better word for <lb/>word translations, only because they are phrases. <lb/>Our focus here is not phrase extraction, but <lb/>how to estimate a reasonable probability (or <lb/>score) to better represent the translation quality <lb/>of the extracted phrase pairs. One major problem <lb/>is that most phrase pairs are seen only several <lb/>times, even in a very large corpus. A reliable and <lb/>effective estimation approach is explained in sec-<lb/>tion 3, and the proposed models are introduced in <lb/>section 4. <lb/>In our system, a collection of phrase transla-<lb/>tions is called a transducer. Different phrase ex-<lb/>traction methods result in different transducers. <lb/>A manual dictionary can be added to the system <lb/>as just another transducer. Typically, one source <lb/>phrase is aligned with several candidate target <lb/>phrases, with a score attached to each candidate <lb/>representing the translation quality. <lb/> 2.2 Decoding Algorithm <lb/> Given a set of transducers as the translation <lb/>model (i.e. phrase translation pairs together with <lb/>the scores of their translation quality), decoding <lb/>is divided into several steps. <lb/>The first step is to build a lattice by applying <lb/>the transducers to the input source sentence. We <lb/>start from a lattice, which has as its only path the <lb/>source sentence. Then for each word or sequence <lb/>
			
			of words in the source sentence for which we <lb/>have an entry in the transducer new edges are <lb/>generated and inserted into the lattice, spanning <lb/>over the source phrase. One new edge is created <lb/>for each translation candidate, and the translation <lb/>score is assigned to this edge. The resulting lat-<lb/>tice has then all the information available from <lb/>the translation model. <lb/>The second step is search for a best path <lb/>through this lattice, but not only based on the <lb/>translation model scores but applying also the <lb/>language model. We start with an initial special <lb/> sentence begin hypothesis at the first node in the <lb/> lattice. Hypotheses are then expanded over the <lb/>edges, applying the language model to the partial <lb/>translations attached to the edges. The following <lb/>algorithm summarizes the decoding process <lb/>when not considering word reordering: <lb/> Current node n, previous node n&apos;; edge e <lb/>Language model state L, L&apos; <lb/>Hypothesis h, h&apos; <lb/>Foreach node n in the lattice <lb/>Foreach incoming edge e in n <lb/>phrase = word sequence at e <lb/>n&apos; <lb/>= FromNode(e) <lb/>foreach L in n&apos; <lb/>foreach h with LMstate L <lb/>LMcost = 0.0 <lb/>foreach word w in phrase <lb/>LMcost += -log p(w|L) <lb/>L&apos; = NewState(L,w) <lb/>L = L&apos; <lb/>end <lb/>Cost= LMcost+TMcost(e) <lb/>TotalCost=TotalCost(h)+Cost <lb/>h&apos; = (L,e,h,TotalCost) <lb/>store h&apos;in Hypotheses(n,L) <lb/> The updated hypothesis h&apos; at the current node <lb/>stores the pointer to the previous hypothesis and <lb/>the edge (labeled with the target phrase) over <lb/>which it was expanded. Thus, at the final step, <lb/>one can trace back to get the path associated with <lb/>the minimum cost, i.e. the best hypothesis. <lb/>Other operators such as local word reordering <lb/>are incorporated into this dynamic programming <lb/>search (Vogel, 2003). <lb/> 3 Phrase Pair Translation Probability <lb/> As stated in the previous section, one of the <lb/>major problems is how to assign a reasonable <lb/>probability for the extracted phrase pair to repre-<lb/>sent the translation quality. <lb/>Most of the phrase pairs are seen only once or <lb/>twice in the training data. This is especially true <lb/>for longer phrases. Therefore, phrase pair co-<lb/>occurrence counts collected from the training <lb/>corpus are not reliable and have little discrimina-<lb/>tive power. In (Vogel et al. 2003) a different es-<lb/>timation approach was proposed. Similar as in <lb/>the IBM models, it is assumed that each source <lb/>word s  i  in the source phrase <lb/> ) <lb/>, <lb/>, <lb/>( <lb/> 2 <lb/>1 <lb/> I <lb/> s <lb/>s <lb/>s <lb/>s <lb/> L <lb/> v = <lb/> is <lb/>aligned to every target word t j  in the target phrase <lb/> ) <lb/>, <lb/>, <lb/>( 2 <lb/>1 <lb/> J <lb/> t <lb/>t <lb/>t <lb/>t <lb/> L <lb/>v = <lb/> with probability <lb/> ) <lb/>| <lb/>Pr( <lb/> i <lb/>j s <lb/>t <lb/> . The <lb/>total phrase translation probability is then calcu-<lb/>lated according to the following generative <lb/>model: <lb/> ∏ ∑ <lb/> = <lb/>= <lb/> = <lb/> I <lb/>i <lb/>J <lb/>j <lb/>j <lb/>i t <lb/>s <lb/>t <lb/>s <lb/> 1 <lb/>1 <lb/> )) <lb/>| <lb/>Pr( <lb/>( <lb/>) <lb/>| <lb/>Pr( <lb/> v <lb/>v <lb/> (1) <lb/>This is essentially the lexical probability as <lb/>calculated in the IBM1 alignment model, without <lb/>considering position alignment probabilities. <lb/>Any statistical translation can be used in (1) to <lb/>calculate the phrase translation probability. <lb/>However, in our experiment we typically see now <lb/>significant difference in translation results when <lb/>using lexicons trained from different alignment <lb/>models. <lb/>Also Equation (1) was confirmed to be robust <lb/>and effective in parallel sentence mining from a <lb/>very large and noisy comparable corpus (Zhao <lb/>and Vogel, 2002). <lb/>
			
			Equation (1) does not explicitly discriminate <lb/>content words from non-content words. As non-<lb/>content words such as high frequency functional <lb/>words tend to occur in nearly every parallel sen-<lb/>tence pair, they co-occur with most of the source <lb/>words in the vocabulary with non-trivial transla-<lb/>tion probabilities. This noise propagates via (1) <lb/>into the phrase translations probabilities, increas-<lb/>ing the chance that non-optimal phrase transla-<lb/>tion candidates get high probabilities and better <lb/>translations are often not in the top ranks. <lb/>We propose a vector model to better distin-<lb/>guish between content words and non-content <lb/>words with the goal to emphasize content words <lb/>in the translation. This model will be used to <lb/> rescore the phrase translation pairs, and to get a <lb/>normalized score representing the translation <lb/>probability. <lb/> 4 Vector Model for Phrase Translation <lb/>Probability <lb/> Term weighting models such as tf.idf are ap-<lb/>plied successfully in information retrieval. The <lb/>duality of term frequency (tf) and inverse docu-<lb/>ment frequency (idf), document space and collec-<lb/>tion space respectively, can smoothly predict the <lb/>probability of terms being informative (Roelleke, <lb/>2003). Naturally, tf.idf is suitable to model con-<lb/>tent words as these words in general have large <lb/> tf.idf weights. <lb/> 4.1 Phrase Pair as Bag-of-Words <lb/> Our translation model: (transducer, as defined <lb/>in 2.1), is a collection of phrase translation pairs <lb/>together with scores representing the translation <lb/>quality. Each phrase translation pair, which can <lb/>be represented as a triple <lb/> } <lb/>, <lb/>{ <lb/> p <lb/> t <lb/>s <lb/> v <lb/> v → <lb/> , is now con-<lb/>verted into a &quot; Bag-of-Words &quot;  D consisting of a <lb/>collection of both source and target words ap-<lb/>pearing in the phrase pair, as shown in (2): <lb/> } <lb/>, <lb/>, <lb/>, <lb/>, <lb/>, <lb/>{ <lb/>} <lb/>, <lb/>{ <lb/> 2 <lb/>1 <lb/>2 <lb/>1 <lb/> J <lb/>I <lb/> t <lb/>t <lb/>t <lb/>s <lb/>s <lb/>s <lb/>D <lb/>p <lb/>t <lb/>s <lb/> L <lb/>L <lb/>v <lb/>v <lb/> = <lb/>⇒ <lb/>→ <lb/> (2) <lb/> Given each phrase pair as one document, the <lb/>whole transducer is a collection of such docu-<lb/>ments. We can calculate tf.idf for each  i <lb/> s  and  j <lb/> t  , <lb/>and represent source and target phrases by vec-<lb/>tors of  s <lb/> v <lb/> v  and  t <lb/> v <lb/> v  as in Equation (3): <lb/> } <lb/>, <lb/>, <lb/>, <lb/>{ <lb/> 2 <lb/>1 <lb/> I <lb/> s <lb/>s <lb/>s <lb/>s <lb/> w <lb/>w <lb/>w <lb/>v <lb/> L <lb/>v = <lb/> } <lb/>, <lb/>, <lb/>, <lb/>{ <lb/> 2 <lb/>1 <lb/> J <lb/> t <lb/>t <lb/>t <lb/>t <lb/> w <lb/>w <lb/>w <lb/>v <lb/> L <lb/>v = <lb/> (3) <lb/> where <lb/> i <lb/> s <lb/> w  and <lb/> j <lb/> t <lb/> w  are tf.idf for  i <lb/> s  or  j <lb/> t  respectively. <lb/>This vector representation can be justified by <lb/>word co-occurrence considerations. <lb/>As the <lb/>phrase translation pairs are extracted from paral-<lb/>lel sentences, the source words  i <lb/> s  and target <lb/>words  j <lb/> t  in the source and target phrases must <lb/>co-occur in the training data. The co-occurring <lb/>words should share similar term frequency and <lb/>document frequency statistics. Therefore, the <lb/>vectors  s <lb/> v <lb/> v  and  t <lb/> v <lb/> v  have similar term weight con-<lb/>tours corresponding to the co-occurring word <lb/>pairs. So the vector representations of a phrase <lb/>translation pair can reflect the translation quality. <lb/>
			
			In addition, the content words and non-content <lb/>words are modeled explicitly by using term <lb/>weights. An over-simplified example would be <lb/>that a rare word in the source language usually <lb/>translates into a rare word in the target language. <lb/> 4.2 Term Weighting Schemes <lb/> Given the transducer, it is straightforward to <lb/>calculate term weights for source and target <lb/>words. There are several versions of tf.idf. The <lb/>smooth ones are preferred, because phrase trans-<lb/>lation pairs are rare events collected from train-<lb/>ing data. <lb/>The idf model selected is as in Equation (4): <lb/> ) <lb/>5 <lb/>. <lb/>0 <lb/>5 <lb/>. <lb/>0 <lb/>log( <lb/> + <lb/>+ <lb/>− <lb/>= <lb/> df <lb/>df <lb/>N <lb/>idf <lb/> (4) <lb/>where N is the total number of documents in the <lb/>transducer, i.e. the total number of translation <lb/>pairs, and df is the document frequency, i.e. in <lb/>how many phrase pairs a given word occurs. The <lb/>constant of 0.5 acts as smoothing. <lb/>Because most of the phrases are short, such as <lb/>2 to 8 words, the term frequency in the bag of <lb/>words representation is usually 1, and some times <lb/>2. This, in general, does not bring much dis-<lb/>crimination in representing translation quality. <lb/>The following version of tf is chosen, so that <lb/>longer target phrases with more words than aver-<lb/>age will be slightly down-weighted: <lb/> ) <lb/>( <lb/>/ <lb/>) <lb/>( <lb/>5 <lb/>. <lb/>1 <lb/>5 <lb/>. <lb/>0 <lb/>&apos; <lb/> v <lb/>avglen <lb/>v <lb/>len <lb/>tf <lb/>tf <lb/>tf <lb/> v <lb/>v <lb/> ⋅ <lb/>+ <lb/>+ <lb/>= <lb/> (5) <lb/>where tf is the term frequency, <lb/> ) <lb/>(v <lb/> len <lb/> v  is the <lb/>length in words of the phrase  v <lb/> v  , and <lb/> ) <lb/>(v <lb/> avglen <lb/> v  is <lb/>the average length of source or target phrase cal-<lb/>culated from the transducer. Again, the values of <lb/>0.5 and 1.5 are constants used in IR tasks acting <lb/>as smoothing. <lb/>Thus after a transducer is extracted from a par-<lb/>allel corpus, tf and df are counted from the collec-<lb/>tion of the &quot; bag-of-words&apos;&apos; phrase alignment <lb/>representations. For each word in the phrase pair <lb/>translation its tf.idf weight is assigned and the <lb/>source and target phrase are transformed into <lb/>vectors as shown in Equation (3). These vectors <lb/>reserve the translation quality information and <lb/>also model the content and non-content words by <lb/>the term weighting model of tf.idf. <lb/> 4.3 Vector Space Alignment <lb/> Given the vector representations in Equation <lb/>(3), a similarity between the two vectors can not <lb/>directly be calculated. The dimensions I and J <lb/> are not guaranteed to be the same. The goal is to <lb/>transform the source vector into a vector having <lb/>the same dimensions as the target vector, i.e. to <lb/>map the source vector into the space of the target <lb/>vector, so that a similarity distance can be calcu-<lb/>lated. Using the same reasoning as used to moti-<lb/>vate Equation (1), it is assumed that every source <lb/>word  i <lb/> s  contributes some probability mass to <lb/>each target word  j <lb/> t  . That is to say, given a term <lb/>weight for  j <lb/> t  , all source term weights are aligned <lb/>
			
			to it with some probability. So we can calculate <lb/> a transformed vector from the source vectors by <lb/>calculating weights  j <lb/> t <lb/>a <lb/> w  using a translation lexi-<lb/>con <lb/> ) <lb/>| <lb/>Pr(  s <lb/>t <lb/> as in Equation (6): <lb/> ∑ <lb/> = <lb/> ⋅ <lb/>= <lb/> I <lb/>i <lb/>s <lb/>i <lb/>j <lb/>t <lb/>a <lb/> i <lb/>j <lb/> w <lb/>s <lb/>t <lb/>w <lb/> 1 <lb/> ) <lb/>| <lb/>Pr( <lb/> (6) <lb/>Now the target vector and the mapped vector <lb/> a <lb/> v <lb/> v  have the same dimensions as shown in (7): <lb/> } <lb/>, <lb/>, <lb/>, <lb/>{ <lb/> 2 <lb/>1 <lb/> J <lb/> t <lb/>a <lb/>t <lb/>a <lb/>t <lb/>a <lb/>a <lb/> w <lb/>w <lb/>w <lb/>v <lb/> L <lb/>v = <lb/> } <lb/>, <lb/>, <lb/>, <lb/>{ <lb/> 2 <lb/>1 <lb/> J <lb/> t <lb/>t <lb/>t <lb/>t <lb/> w <lb/>w <lb/>w <lb/>v <lb/> L <lb/>v = <lb/> (7) <lb/> 4.4 Similarity Functions <lb/> As explained in section 4.1, intuitively, if  s <lb/> v <lb/> and  t <lb/> v  is a good translation pair, then the corre-<lb/>sponding vectors of  a <lb/> v <lb/> v  and  t <lb/> v <lb/> v  should be similar <lb/>to each other in the vector space. <lb/> Cosine distance <lb/> The standard cosine distance is defined as the <lb/>inner product of the two vectors  a <lb/> v <lb/> v  and  t <lb/> v <lb/> v  nor-<lb/>malized by their norms. Based on Equation (6), <lb/>it is easy to derive the similarity as follows: <lb/> ) <lb/>( <lb/>) <lb/>( <lb/>) <lb/>| <lb/>( <lb/>) <lb/>| <lb/>( <lb/>1 <lb/>1 <lb/>) <lb/>, <lb/>( <lb/>) <lb/>, <lb/>( <lb/> 1 <lb/>2 <lb/>1 <lb/>2 <lb/>1 <lb/>1 <lb/>1 <lb/>1 <lb/>1 <lb/>cos <lb/> ∑ <lb/>∑ <lb/>∑ ∑ <lb/>∑ ∑ <lb/>∑ <lb/> = <lb/>= <lb/>= <lb/>= <lb/>= <lb/>= <lb/>= <lb/> = <lb/>= <lb/>= <lb/>= <lb/> J <lb/>j <lb/>t <lb/>a <lb/>J <lb/>j <lb/>t <lb/>J <lb/>j <lb/>I <lb/>i <lb/>s <lb/>i <lb/>j <lb/>t <lb/>J <lb/>j <lb/>I <lb/>i <lb/>s <lb/>i <lb/>j <lb/>t <lb/>t <lb/>t <lb/>a <lb/>J <lb/>j <lb/>t <lb/>t <lb/>a <lb/>t <lb/>t <lb/>a <lb/>t <lb/>t <lb/>a <lb/>t <lb/>t <lb/>a <lb/>t <lb/>t <lb/>a <lb/> j <lb/>j <lb/>i <lb/>j <lb/>i <lb/>j <lb/>j <lb/>j <lb/> w <lb/>sqrt <lb/>w <lb/>sqrt <lb/>w <lb/>s <lb/>t <lb/>P <lb/>w <lb/>w <lb/>s <lb/>t <lb/>P <lb/>w <lb/>v <lb/>v <lb/>w <lb/>w <lb/>v <lb/>v <lb/>v <lb/>v <lb/>v <lb/>v <lb/>v <lb/>v <lb/>d <lb/> (8) <lb/>where I and J are the length of the source and <lb/>target phrases; <lb/> i <lb/> s <lb/> w  and <lb/> j <lb/> t <lb/> w  are term weights for <lb/>source word and target words; <lb/> j <lb/> t <lb/>a <lb/> w  is the trans-<lb/>formed weight mapped from all source words to <lb/>the target dimension at word  j <lb/> t  . <lb/> BM25 distance <lb/> TREC tests show that bm25 (Robertson and <lb/>Walker, 1997) is one of the best-known distance <lb/>schemes. This distance metric is given in Equa-<lb/>tion (9). The constants of <lb/> 
			
			3 <lb/>1 <lb/> , <lb/>, k <lb/>b <lb/>k <lb/> are set to be 1, 1 <lb/>and 1000 respectively. <lb/> ) <lb/>( <lb/>) <lb/>1 <lb/>( <lb/>) <lb/>( <lb/>) <lb/>1 <lb/>( <lb/> 3 <lb/>3 <lb/>1 <lb/>1 <lb/>25 <lb/> j <lb/>j <lb/>j <lb/>j <lb/> t <lb/>a <lb/>t <lb/>a <lb/>J <lb/>j <lb/>t <lb/>t <lb/>bm <lb/> w <lb/>k <lb/>w <lb/>k <lb/>w <lb/>K <lb/>w <lb/>k <lb/>w <lb/>d <lb/> + <lb/>+ <lb/>+ <lb/>+ <lb/>= ∑ <lb/> = <lb/> ) <lb/>5 <lb/>. <lb/>0 <lb/>/( <lb/>) <lb/>5 <lb/>. <lb/>0 <lb/>( <lb/> + <lb/>+ <lb/>− <lb/>= <lb/>= <lb/> j <lb/>j <lb/>j <lb/> t <lb/>t <lb/>t <lb/> df <lb/>df <lb/>N <lb/>idf <lb/>w <lb/> )) <lb/>( <lb/>/ <lb/>) <lb/>1 <lb/>(( <lb/> 1 <lb/> l <lb/>avg <lb/>J <lb/>b <lb/>k <lb/>K <lb/> + <lb/>− <lb/>= <lb/> (9) <lb/>where avg(l) is the average target phrase length <lb/>in words given the same source phrase. <lb/>Our experiments confirmed the bm25 distance <lb/>is slightly better than the cosine distance, though <lb/>the difference is not really significant. One ad-<lb/>vantage of bm25 distance is that the set of free <lb/>parameters <lb/> 3 <lb/>1 <lb/> , <lb/>, k <lb/>b <lb/>k <lb/> can be tuned to get better per-<lb/>formance e.g. via n-fold cross validation. <lb/> 4.5 Integrated Translation Score <lb/> Our goal is to rescore the phrase translation <lb/>pairs by using additional evidence of the transla-<lb/>tion quality in the vector space. <lb/>The vector based scores (8) &amp; (9) provide a <lb/>distinct view of the translation quality in the vec-<lb/>tor space. Equation (1) provides a evidence of <lb/>the translation quality based on the word align-<lb/>ment probability, and can be assumed to be dif-<lb/>ferent from the evidences in vector space. Thus, <lb/>a natural way of integrating them together is a <lb/> geometric interpolation shown in (10) or equiva-<lb/>lently a linear interpolation in the log domain. <lb/> ) <lb/>| <lb/>( <lb/>Pr <lb/>) <lb/>, <lb/>( <lb/> 1 <lb/>int <lb/> t <lb/>s <lb/>s <lb/>t <lb/>d <lb/>d <lb/> vec <lb/> v <lb/>v <lb/>v <lb/>v <lb/> β <lb/>β <lb/> − <lb/> ⋅ <lb/>= <lb/> (10) <lb/>where <lb/> ) <lb/>, <lb/>( s <lb/>t <lb/>d vec <lb/> v <lb/>v  is the score from the cosine or <lb/> bm25 vector distance, normalized within [0, 1], <lb/>like a probability. <lb/> 0 <lb/>. <lb/>1 <lb/>) <lb/>, <lb/>( <lb/> = <lb/> ∑ <lb/> t <lb/>vec <lb/> s <lb/>t <lb/>d <lb/> v <lb/> v <lb/>v <lb/> The parameter  β  can be tuned using held-out <lb/>data. In our cross validation experiments <lb/> 5 <lb/>. <lb/>0 <lb/> = <lb/> β <lb/> gave the best performance in most cases. There-<lb/>fore, Equation (10) can be simplified into: <lb/> ) <lb/>| <lb/>Pr( <lb/>) <lb/>, <lb/>( <lb/> int <lb/> t <lb/>s <lb/>s <lb/>t <lb/>d <lb/>d <lb/> vec <lb/> v <lb/>v <lb/>v <lb/>v  ⋅ <lb/>= <lb/> (11) <lb/>The phrase translation score functions in (1) <lb/>and (11) are non-symmetric. This is because the <lb/>statistical lexicon Pr(s|t) is non-symmetric. One <lb/>can easily re-write all the distances by using <lb/> Pr(t|s). But in our experiments this reverse di-<lb/>rection of using Pr(t|s) gives trivially difference. <lb/>So in all the experimental results reported in this <lb/>paper, the distances defined in (1) and (11) are <lb/>used. <lb/> 
			
			5 Length Regularization <lb/> Phrase pair extraction does not work perfectly <lb/>and sometimes a short source phrase is aligned to <lb/>a long target phrase or vice versa. Length regu-<lb/>larization can be applied to penalize too long or <lb/>too short candidate translations. Similar to the <lb/>sentence alignment work in (Gale and Church, <lb/>1991), the phrase length ratio is assumed to be a <lb/>Gaussian distribution as given in Equation (12): <lb/> ) <lb/>) <lb/>) <lb/>( <lb/>/ <lb/>) <lb/>( <lb/>( <lb/>5 <lb/>. <lb/>0 <lb/>exp( <lb/>) <lb/>, <lb/>( <lb/> 2 <lb/>2 <lb/> σ <lb/> µ <lb/> − <lb/>⋅ <lb/>− <lb/>∝ <lb/> s <lb/> l <lb/>t <lb/>l <lb/>s <lb/>t <lb/>l <lb/> v <lb/> v <lb/>v <lb/>v <lb/> (12) <lb/>where l(t) is the target sentence length. Mean  µ <lb/> and variance  σ  can be estimated using a parallel <lb/>corpus using a Maximum Likelihood criteria. <lb/>The regularized score is the product of (11) and <lb/>(12). <lb/> 6 Experiments <lb/> Experiments were carried out on the so-called <lb/>large data track Chinese-English TIDES transla-<lb/>tion task, using the June 2002 test data. The <lb/>training data used to train the statistical lexicon <lb/>and to extract the phrase translation pairs was <lb/>selected from a 120 million word parallel corpus <lb/>in such a way as to cover the phrases in test sen-<lb/>tences. The restricted training corpus contained <lb/>then approximately 10 million words.. A trigram <lb/>model was built on 20 million words of general <lb/>newswire text, using the SRILM toolkit (Stolcke, <lb/>2002). Decoding was carried out as described in <lb/>section 2.2. The test data consists of 878 Chinese <lb/>sentences or 24,337 words after word segmenta-<lb/>tion. There are four human translations per Chi-<lb/>nese sentence as references. Both NIST score <lb/>and Bleu score (in percentage) are reported for <lb/> adequacy and fluency aspects of the translation <lb/>quality. <lb/> 6.1 Transducers <lb/> Four transducers were used in our experi-<lb/>ments: LDC, BiBr, HMM, and ISA. <lb/>LDC was built from the LDC Chinese-English <lb/>dictionary in two steps: first, morphological <lb/>variations are created. For nouns and noun <lb/>phrases plural forms and entries with definite and <lb/>indefinite determiners were generated. For verbs <lb/>additional word forms with -s -ed and -ing were <lb/>generated, and the infinitive form with &apos;to&apos;. Sec-<lb/>ond, a large monolingual English corpus was <lb/>used to filter out the new word forms. If they did <lb/>not appear in the corpus, the new entries were not <lb/>added to the transducer (Vogel, 2004). <lb/>BiBr extracts sub-tree mappings from Bilin-<lb/>gual Bracketing alignments (Wu, 1997); HMM <lb/>extracts partial path mappings from the Viterbi <lb/>path in the Hidden Markov Model alignments <lb/>(Vogel et. al., 1996). ISA is an integrated seg-<lb/>mentation and alignment for phrases (Zhang et.al, <lb/>2003), which is an extension of (Marcu and <lb/>Wong, 2002). <lb/> LDC <lb/>BiBr HMM <lb/>ISA <lb/> ) <lb/>(K <lb/> N <lb/> 425K 137K 349K 263K <lb/> ) <lb/>/ <lb/>( <lb/> src <lb/>tgt l <lb/>l <lb/>avg <lb/> 1.80 <lb/>1.11 <lb/>1.09 <lb/>1.20 <lb/>Table-1 statistics of transducers <lb/> Table-1 shows some statistics of the four <lb/>transducers extracted for the translation task.  N <lb/> is the total number of phrase pairs in the trans-<lb/>ducer. LDC is the largest one having 425K en-<lb/>tries, as the other transducers are restricted to <lb/>&apos;useful&apos; entries, i.e. those translation pairs where <lb/>the source phrase matches a sequence of words in <lb/>one of the test sentence. Notice that the LDC <lb/>dictionary has a large number of long transla-<lb/>tions, leading to a high source to target length <lb/>ratio. <lb/> 6.2 Cosine vs BM25 <lb/> The normalized cosine and bm25 distances de-<lb/>fined in (8) and (9) respectively, are plugged into <lb/>(11) to calculate the translation probabilities. <lb/>Initial experiments are reported on the LDC <lb/>transducer, which gives already a good transla-<lb/>tion, and therefore allows for fast and yet mean-<lb/>ingful experimentation. <lb/>Four baselines (Uniform, Base-m1, Base-m4, <lb/>and Base-m4S) are presented in Table-2. <lb/>NIST <lb/>Bleu <lb/> Uniform <lb/>6.69 <lb/>13.82 <lb/>Base-m1 <lb/>7.08 <lb/>14.84 <lb/>Base-m4 <lb/>7.04 <lb/>14.91 <lb/>Base-m4S <lb/>6.91 <lb/>14.44 <lb/> cosine <lb/>7.17 <lb/>15.30 <lb/>bm25 <lb/>7.19 <lb/>15.51 <lb/>bm25-len <lb/> 7.21 <lb/>15.64 <lb/>  Table-2 Comparisons of different score functions <lb/>
			
			In the first uniform probabilities are assigned <lb/>to each phrase pair in the transducer. The second <lb/>one (Base-m1) is using Equation (1) with a statis-<lb/>tical lexicon trained using IBM Model-1, and <lb/>Base-m4 is using the lexicon from IBM Model-4. <lb/>Base-m4S is using IBM Model-4, but we skipped <lb/>194 high frequency English stop words in the <lb/>calculation of Equation (1). <lb/>Table-2 shows that the translation score de-<lb/>fined by Equation (1) is much better than a uni-<lb/>form model, as expected. Base-m4 is slightly <lb/>worse than Base-m1.on NIST score, but slightly <lb/>better using the Bleu metric. Both differences <lb/>are not statistically significant. The result for <lb/> Base-m4S shows that skipping English stop <lb/>words in Equation (1) gives a disadvantage. One <lb/>reason is that skipping ignores too much non-<lb/>trivial statistics from parallel corpus especially <lb/>for short phrases. These high frequency words <lb/>actually account already for more than 40% of <lb/>the tokens in the corpus. <lb/>Using the vector model, both with the cosine <lb/> cos <lb/> d  and the bm25 <lb/> 25 <lb/> bm <lb/> d <lb/> distance, is significantly <lb/>better than Base-m1 and Base-m4 models, which <lb/>confirms our intuition of the vector model as an <lb/>additional useful evidence for translation quality. <lb/>The length regularization (12) helps only slightly <lb/>for LDC. Since bm25&apos;s parameters could be <lb/>tuned for potentially better performance, we se-<lb/>lected bm25 with length regularization as the <lb/>model tested in further experiments. <lb/>A full-loaded system is tested using the <lb/>LM020 with and without word-reordering in de-<lb/>coding. The results are presented in Table-3. <lb/>Table-3 shows consistent improvements on all <lb/>configurations: the individual transducers, com-<lb/>binations of transducers, and different decoder <lb/>settings of word-reordering. Because each phrase <lb/>pair is treated as a &quot; bag-of-words &quot; , the grammar <lb/>structure is not well represented in the vector <lb/>model. Thus our model is more tuned towards <lb/>the adequacy aspect, corresponding to NIST <lb/>score improvement. <lb/>Because the transducers of BiBr, HMM, and <lb/>ISA are extracted from the same training data, <lb/>they have significant overlaps with each other. <lb/>This is why we observe only small improvements <lb/>when adding more transducers. <lb/>The final NIST score of the full system is 8.24, <lb/>and the Bleu score is 22.37. This corresponds to <lb/>3.1% and 11.8% relative improvements over the <lb/>baseline. These improvements are statistically <lb/>significant according to a previous study (Zhang <lb/>et.al., 2004), which shows that a 2% improve-<lb/>ment in NIST score and a 5% improvement in <lb/>
			
			Bleu score is significant for our translation sys-<lb/>tem on the June 2002 test data. <lb/> 6.3 Mean Reciprocal Rank <lb/> To further investigate the effects of the rescor-<lb/>ing function in (11), Mean Reciprocal Rank <lb/>(MRR) experiments were carried out. MRR for a <lb/>labeled set is the mean of the reciprocal rank of <lb/>the individual phrase pair, at which the best can-<lb/>didate translation is found (Kantor and Voorhees, <lb/>1996). <lb/>Totally 9,641 phrase pairs were selected con-<lb/>taining 216 distinct source phrases. Each source <lb/>phrase was labeled with its best translation can-<lb/>didate without ambiguity. The rank of the la-<lb/>beled candidate is calculated according to <lb/>translation scores. The results are shown in Ta-<lb/>ble-4. <lb/>baseline <lb/>cosine <lb/>bm25 <lb/>MRR <lb/>0.40 <lb/>0.58 <lb/>0.75 <lb/> Table-4 Mean Reciprocal Rank <lb/> The rescore functions improve the MRR from <lb/>0.40 to 0.58 using cosine distance, and to 0.75 <lb/>using bm25. This confirms our intuitions that <lb/>good translation candidates move up in the rank <lb/>after the rescoring. <lb/>Decoder settings <lb/>without word reordering <lb/>with word reordering <lb/> baseline <lb/>bm25 <lb/>baseline <lb/>bm25 <lb/> Scores (%) <lb/>NIST Bleu NIST Bleu NIST Bleu NIST Bleu <lb/> LDC <lb/>7.08 14.84 7.21 15.64 7.13 15.10 7.26 15.98 <lb/> LDC+ISA <lb/>7.73 19.60 7.99 19.58 7.86 20.80 8.13 20.93 <lb/> LDC+ISA+HMM <lb/>7.86 19.08 8.14 20.70 7.95 19.84 8.19 21.60 <lb/> LDC+ISA+HMM+BiBr 7.87 19.23 8.14 21.48 7.99 20.01 8.24 22.37 <lb/> Table-3 Translation using bm25 rescore function with different decoder settings <lb/> 7 Conclusion and Discussion <lb/> In this work, we proposed a way of using term <lb/>weight based models in a vector space as addi-<lb/>tional evidences for translation quality, and inte-<lb/>grated the model into an existing phrase-based <lb/>statistical machine translation system. The mod-<lb/>el shows significant improvements when using it <lb/>to score a manual dictionary as well as when us-<lb/>ing different phrase transducers or a combination <lb/>of all available translation information. Addi-<lb/>tional experiments also confirmed the effective-<lb/>ness of the proposed model in terms of of <lb/>improved Mean Reciprocal Rank of good transla-<lb/>tions. <lb/>Our future work is to explore alternatives such <lb/>as the reranking work in (Collins, 2002) and in-<lb/>clude more knowledge such as syntax informa-<lb/>tion in rescoring the phrase translation pairs. <lb/>
		
		</body>

		<back>

			<listBibl> References <lb/> A. Stolcke. 2002. SRILM --An Extensible Language <lb/>Modeling Toolkit.  In 2002 Proc. Intl. Conf. on <lb/>Spoken Language Processing,  Denver. <lb/>Peter F. Brown, Stephen A. Della Pietra, Vincent <lb/>J.Della Pietra, and Robert L. Mercer. 1993.  The <lb/>Mathematics of Statistical Machine Translation: <lb/>Parameter Estimation,  Computational Linguistics, <lb/>vol. 19, no. 2, pp. 263–311. <lb/>Michael Collins. 2000. Discriminative Reranking for <lb/>Natural Language Parsing.  Proc. 17th International <lb/>Conf. on Machine Learning.  pp. 175-182. <lb/>William A. Gale and Kenneth W. Church. 1991. A <lb/>Program for Aligning Sentences in Bilingual Cor-<lb/>pora. In  Computational Linguistics,  vol.19 pp. 75-<lb/>102. <lb/>Paul B. Kantor, Ellen Voorhees. 1996. Report on the <lb/>TREC-5 Confusion Track.  The Fifth Text Retrieval <lb/>Conference. <lb/> Philipp Koehn, Franz Josef Och, and Daniel Marcu. <lb/>2003. Statistical Phrase-Based Translation.  Pro-<lb/>ceedings of HLT-NAACL.  Edmonton, Canada. <lb/>Daniel Marcu and William Wong. 2002. A Phrase-<lb/>Based, Joint Probability Model for Statistical Ma-<lb/>chine Translation.  Proceedings of EMNLP-2002, <lb/> Philadelphia, PA. <lb/>Franz Josef Och and Hermann Ney. 2000. Improved <lb/>Statistical Alignment Models.  Proceedings of ACL-<lb/> 00, pp. 440-447, Hongkong, China. <lb/>Thomas Roelleke. 2003. A Frequency-based and a <lb/>Poisson-based Definition of the Probability of Be-<lb/>ing Informative.  Proceedings of the 26th annual in-<lb/>ternational ACM SIGIR.  pp. 227-234. <lb/>S.E. Robertson, and S. Walker. 1997. On relevance <lb/>weights with little relevance information.  In 1997 <lb/>Proceedings of ACM SIGIR.  pp. 16-24. <lb/>Dekai Wu. 1997. Stochastic inversion transduction <lb/>grammars and bilingual parsing of parallel corpora, <lb/> Computational Linguistics  23(3): pp.377-404. <lb/>K. Yamada and K. Knight. 2001. A Syntax-Based <lb/>Statistical Translation Model.  Proceedings of the <lb/>39th Annual Meeting of the Association for Compu-<lb/>tational Linguistics.  pp. 523-529. Toulouse, France. <lb/>Richard Zens, Franz Josef Och and Hermann Ney. <lb/>2002. Phrase-Based Statistical Machine Transla-<lb/>tion.  Proceedings of the 25th Annual German Con-<lb/>ference on AI: Advances in Artificial Intelligence. <lb/> pp. 18-22. <lb/>Stephan Vogel, Hermann Ney, Christian Tillmann. <lb/>1996. HMM-based word alignment in statistical <lb/>translation. In: COLING &apos;96: The 16th Int. Conf. on <lb/>Computational Linguistics, Copenhagen, Denmark <lb/>(1996) pp. 836-841. <lb/>Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-<lb/>ble, Ashish Venogupal, Bing Zhao, Alex Waibel. <lb/>2003. The CMU Statistical Translation System, <lb/> Proceedings of MT-Summit IX.  New Orleans, LA. <lb/>Stephan Vogel. 2003. SMT decoder dissected: word <lb/>reordering,  In 2003 Proceedings of Natural Lan-<lb/>guage Processing and Knowledge Engineering, <lb/> (NLP-KE&apos;03) Beijing, China. <lb/>Stephan Vogel. 2004. Augmenting Manual Dictionar-<lb/>ies for Statistical Machine Translation Systems,  In <lb/>2003 Proceedings of LREC,  Lisbon, Portugal. pp. <lb/> 1593-1596. <lb/> Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-<lb/>terpreting BLEU/NIST Scores : How much Im-<lb/>provement Do We Need to Have a Better System? <lb/> In 2004 Proceedings of LREC,  Lisbon, Portugal. <lb/>pp. 2051-2054. <lb/>Ying Zhang, Stephan Vogel, Alex Waibel. 2003. &quot;In-<lb/>tegrated Phrase Segmentation and Alignment Algo-<lb/>rithm for Statistical Machine Translation,&quot; in the <lb/>Proceedings of NLP-KE&apos;03, Beijing, China. <lb/>Bing Zhao, Stephan Vogel. 2002. Adaptative Parallel <lb/>Sentences Mining from web bilingual news collec-<lb/>tion. In  2002 IEEE International Conference on <lb/>Data Mining,  Maebashi City, Japan. </listBibl>

		</back>
	</text>
</tei>
