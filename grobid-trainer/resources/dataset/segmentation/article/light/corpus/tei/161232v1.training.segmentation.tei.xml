<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>1 <lb/>Del Grosso et al <lb/>Virtual Reality system for freely-moving rodents. <lb/>Nicholas A. Del Grosso 1 , Justin J. Graboski 1 , Weiwei Chen 1 , Eduardo Blanco-<lb/>Hernández 1 and Anton Sirota 1 <lb/>1 Bernstein Center for Computational Neuroscience Munich, Munich Cluster of <lb/>Systems Neurology (SyNergy) and Faculty of Medicine, Ludwig-Maximilians <lb/>Universität München, Planegg-Martinsried, Germany. <lb/>* Correspondence should be addressed to A.S. (sirota@biologie.uni-muenchen.de) <lb/>ABSTRACT <lb/>Spatial navigation, active sensing, and most cognitive functions rely on a tight <lb/>link between motor output and sensory input. Virtual reality (VR) systems <lb/>simulate the sensorimotor loop, allowing flexible manipulation of enriched <lb/>sensory input. Conventional rodent VR systems provide 3D visual cues linked to <lb/>restrained locomotion on a treadmill, leading to a mismatch between visual and <lb/>most other sensory inputs, sensory-motor conflicts, as well as restricted <lb/>naturalistic behavior. To rectify these limitations, we developed a VR system <lb/>(ratCAVE) that provides realistic and low-latency visual feedback directly to <lb/>head movements of completely unrestrained rodents. Immersed in this VR <lb/>system, rats displayed naturalistic behavior by spontaneously interacting with <lb/>and hugging virtual walls, exploring virtual objects, and avoiding virtual cliffs. <lb/>We further illustrate the effect of ratCAVE-VR manipulation on hippocampal <lb/>place fields. The newly-developed methodology enables a wide range of <lb/>experiments involving flexible manipulation of visual feedback in freely-moving <lb/>behaving animals. <lb/></front>

			<page>2 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>INTRODUCTION <lb/>Movement is a fundamental element in the action-perception loop that is critical for <lb/>most cognitive functions, such as decision-making, memory and spatial navigation. <lb/>Internally-driven locomotor, head and sensor movements, an exploratory repertoire of <lb/>a naturally-behaving animal, allow it to actively sample sensory information from the <lb/>outside world for its optimal detection and encoding, as well as guidance of the <lb/>behavior 1-5 . Recognition that the closed-loop link between internal dynamics, motor <lb/>output and sensory processing gives rise to predictive coding, attention and flexible <lb/>motor control 6-8 is encouraging the use of a new experimental paradigm in sensory <lb/>and cognitive neuroscience: closed-loop sensory stimulation. Traditional open-loop <lb/>experimental paradigms involving head-fixation of the animal, useful for performing <lb/>sensitive measurements of functional brain activity, are being replaced by <lb/>experimental setups that partially close the loop between action and sensation while <lb/>still retaining precise control of sensory inputs 9-13 . <lb/>Virtual reality (VR) systems close the loop between locomotion and vision. Many <lb/>rodent laboratories use head-or body-restrained VR (rVR) setups to simulate <lb/>locomotion through a 3D virtual environment (VE) via running on a treadmill 10,11 . <lb/>Spatial coding research has especially benefited from such systems; VR researchers <lb/>have taken advantage of the flexibility of a VE by implementing arbitrarily-large <lb/>environmental exploration paradigms utilizing dynamic environments 14-16 and <lb/>manipulating visuomotor gain 17 . Additionally, many researchers take advantage of <lb/>the rodent&apos;s fixed head by performing optical and intracellular recordings during <lb/>locomotion through virtual space, a normally-challenging task in freely-moving <lb/>animals 10,18,19 . <lb/>However, locomotion on a treadmill alone may not be enough for performing <lb/>closed-loop research; behavioral and physiological differences between rVR and <lb/>real-world navigation illustrate the detrimental effect of sensorimotor loop disruption <lb/>and the importance of increasing motor affordances. While head-fixed rodents in <lb/>rVR experiments are limited to navigating linear tracks 10,17-20 , likely due to an <lb/>impoverished sensory-motor loop (Schmidt-Hieber, personal communication), <lb/>rodents can navigate a two-dimensional VE if only their bodies are restrained and <lb/>their heads left free to move 14,16,21-23 . If rats are further allowed to rotate while <lb/>running on a spherical treadmill in rVR experiments, 2D hippocampal place cell <lb/>representation of the VE is comparable to that in real-world navigation 23 ; however, <lb/>this effect is lost if the rodent&apos;s body rotation range is limited 21,24 . <lb/></body>

			<page>3 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>Despite the great utility of rVR for studies of spatial navigation, animal restraint still <lb/>poses unresolved challenges. First, restrained animals exhibit constrained or limited <lb/>behavioral patterns within 2D space, which affects the way they actively sample the <lb/>3D environment. Second, locomotion-driven visual input is in conflict with <lb/>locomotion-independent, head-bound idiothetic, olfactory, tactile and auditory <lb/>inputs. Third, proprioceptive and vestibular inputs in rVR setups are diminished and <lb/>unnatural, making them potential causes of the observed reduction in frequency-and <lb/>speed-correlates of theta oscillatory dynamics, compared to rodents allowed to freely <lb/>navigate the real world 23,24 . Lastly, animals require long and complex training and <lb/>habituation to rVR setups 23,25 . <lb/>These challenges are resolved if visual feedback in VR is based on head motion in <lb/>3D space in freely-moving subjects, giving rise to a coherent visual, idiothetic and <lb/>external multisensory input, an unperturbed action-perception loop, and a full <lb/>repertoire of rodent behavior, while still preserving the precise control of visual <lb/>stimuli in VR setups 26 . One such freely-moving VR (fmVR) system was introduced <lb/>for human subjects as the Cave Automatic Virtual Environment (CAVE) 27 . A CAVE <lb/>allows observers to freely move in space and view a 3D VE on the projection <lb/>surfaces surrounding them. To date, CAVE-like VR systems for flies 28,29 and fish 30 <lb/>couple animal 3D motion to 2D contrast patterns on the projected onto cylindrical <lb/>surfaces, though a system for arthopods with more realistic visual feedback was <lb/>reported 31 . Implementation of the CAVE system in rodents, a model mammalian <lb/>system where complex interrogation and manipulation of the nervous system can be <lb/>combined with cognitive behavior, would open new dimensions in experimental <lb/>neuroscience. Designing an immersive fmVR in quickly-moving animals is <lb/>challenging, however, as it would require very-low-latency visual feedback to avoid <lb/>introducing new conflicts in the sensorimotor loop 32 and computationally-intensive <lb/>graphical operations to produce a visually-rich VE. An urgent need for and benefits <lb/>of the development of a next-generation immersive fmVR were called for in a recent <lb/>review 11 . <lb/>To provide an immersive virtual environment for untrained freely-moving rodents and <lb/>allow them to explore and interact with the virtual environment in a natural manner, <lb/>we developed a new CAVE fmVR system (ratCAVE) that produces minimal inter-<lb/>sensory conflict during self-motion using fast head-tracking and high display frame <lb/>rates, as well as enriched visual 3D cues of the virtual scene. We demonstrate the <lb/>naturalistic interaction of rats with VEs in our fmVR system in several behavioral <lb/>tasks. We further show a use case of fmVR not possible with rVR systems: to study <lb/></body>

			<page>4 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>the multisensory nature of hippocampal spatial representation. This highly-immersive <lb/>fmVR system can be a powerful tool for a broad range of neuroscience disciplines. <lb/>RESULTS <lb/>ratCAVE : VR system for freely moving rodents <lb/>We implemented a CAVE system where a VE projection on the surface of the arena <lb/>was closed-loop coupled with the real-time tracking of the head of the animal. In this <lb/>setup, animals could move freely in a rectangular arena similar to that used for <lb/>conventional open-field experiments, but the white-painted arena served as a <lb/>projection surface. We used an array of 12 high-speed cameras (240-360 fps, <lb/>NaturalPoint Inc.) to track the 3D position of the rodent&apos;s head via a rigid array of <lb/>retro-reflective spheres attached to a head-mounted 3D-printed skeleton (Fig. 1c,d). <lb/>This tracking system enabled us to update the rodent&apos;s head position with very high <lb/>spatial (&lt;0.1 mm) and temporal (&lt;2.7 msec) resolution. The VE, created using open-<lb/>source 3D modeling software (Blender 3D), was rendered each frame in a full 360-<lb/>degree arc about the rodent&apos;s head and mapped onto a 3D computer model of the <lb/>arena using custom Python and OpenGL packages (Supplementary Fig.3, Online <lb/>Methods), warped in real-time to generate a fully-interactive, geometrically-accurate <lb/>3D scene (Fig. 1b). The core cube-mapping algorithm used to perform the mapping of <lb/>the VE onto the projection surface was identical to those described in rodent rVR <lb/>setups (Supplementary Fig. 2a-c) 23 , but VE projection onto the surface of the arena is <lb/>continuously updated according to the changing 3D position of the rodent&apos;s head (Fig <lb/>1b), resulting in perception of a 3D VE that is stable in the real-world frame of <lb/>reference that the animal is freely moving about (Fig 1c,d). The resulting image was <lb/>front-projected onto the floor and slanted walls of the arena from a ceiling-mounted <lb/>high-speed (240 fps) video projector (Supplementary Fig. 4). Because the presented <lb/>virtual motion parallax cue automatically takes into account the rodent&apos;s distance <lb/>from the arena&apos;s walls, virtual objects can be made to appear both inside and outside <lb/>the arena&apos;s boundaries (Supplementary Movie 1). <lb/>Flexible design, calibration and mobility of the VR arena <lb/>Automatic arena-projector calibration ensured that the image was correctly projected <lb/>onto the arena&apos;s surface. Calibration was realized via a point cloud-modeling <lb/>procedure by projecting a random dot pattern onto the arena&apos;s surface, measuring the <lb/>3D position of each dot via a 3D tracking system, and fitting a 3D digital model of the <lb/>arena to this point cloud data (Fig. 1a). This scanning process provides the flexibility <lb/></body>

			<page>5 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>to layer a VE over an arbitrary arena surface, including smooth objects inside the <lb/>arena. The position of the arena with respect to the projector was continuously tracked <lb/>using a set of retro-reflective spheres mounted on the arena itself, allowing the arena <lb/>to be arbitrary translated and rotated during an experimental session while preserving <lb/>the correct projection. <lb/>Low latency motor-visual feedback of the ratCAVE system <lb/>Motion-to-photon (end-to-end) latency in our system cumulatively included input lag <lb/>of the tracking system, the processing lag of the tracking and ratCAVE software, as <lb/>well as &quot;display lag&quot;, the time it takes for the rendered image to be projected. <lb/>Selecting fast tracking and display hardware and optimized software allowed us to <lb/>achieve a motion-to-photon latency approaching 15 msec (Supplementary Fig.1 a-c). <lb/>This latency is significantly lower that that of any fmVR/CAVE systems reported to <lb/>date that we are aware of and additionally supplies a smoother motion stimulus than <lb/>in those with lower-framerate displays (typically 60 Hz) 31,33 . Since rats rarely reached <lb/>speeds of 50 cm/s during spontaneous exploration of the arena (Supplementary Fig. <lb/>1d), we expect that they were experiencing minimal, if any, latency-related cross-<lb/>sensory conflicts in our system. <lb/>Visual cues enhancing VR immersion <lb/>A large number of conflicting visual cues can exist in CAVE systems that can distract <lb/>from VR immersion, which we&apos;ve taken additional steps to decrease. First, we <lb/>implemented online radiosity compensation, which equalizes the image brightness <lb/>across the entire arena to decrease the visual perception of the arena itself. Second, we <lb/>implemented antialiasing to decrease the perception of the individual pixels. Third, <lb/>the location of the virtual light source was programmed to match the position of the <lb/>projector, giving the projector the impression of simply illuminating the virtual <lb/>objects, rather than creating them. Finally, to provide a richer visual scene and <lb/>additional visual depth cues to the observer 34 , we implemented both diffuse and <lb/>&quot;glossy&quot; specular reflections off the virtual objects&apos; surfaces using the Phong <lb/>reflection model, as well as casting shadows on themselves and other objects. <lb/>Additions of these visual features gave rise to a smooth and perceptually realistic VE <lb/>(Supplementary Fig. 2d). <lb/>Testing spontaneous behavior of rats in the ratCAVE <lb/>We designed a set of behavioral experiments that were aimed to explore and evaluate <lb/>the degree of rats&apos; immersion and interaction with the VE provided by ratCAVE. In <lb/>each experiment, behavior of freely-moving rats (n=3) was tested in distinct VEs that <lb/></body>

			<page>6 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>were designed to evaluate specific aspects of behavioral interaction with purely <lb/>virtual elements: virtual cliff avoidance, virtual object exploration, and interaction <lb/>with the virtual wall. These tasks were specifically chosen to require no pre-training <lb/>or reinforcement and rely on spontaneous behavior of rodents. Benefiting from high <lb/>spatial resolution tracking of position and orientation of the rats&apos; head, each rat&apos;s <lb/>natural behavior during each task was classified into walking, immobility and rearing <lb/>based on speed and head-height features (Supplementary Fig. 6a). The three <lb/>experiments were performed repeatedly across animals over several days. <lb/>Virtual cliff avoidance experiment <lb/>Visual cliff avoidance paradigm is a classical test of visual depth perception and relies <lb/>on innate behavior of the animals 35 . We designed a virtual version of this task that <lb/>tests if rats avoid jumping from the virtual cliff emulated in the VE. In each 30-<lb/>second session, rats were placed onto a board suspended above the arena&apos;s floor, <lb/>bisecting the arena into randomly-assigned safe and cliff sides, in which the virtual <lb/>floor was at and 1.5 meters below the floor level, respectively (Fig. 2a; <lb/>Supplementary Movie 3). We observed several well-defined behaviors in this task: <lb/>wall-supported rearing, visual exploration of the ledges (head dipping), and the jump <lb/>off the ledge towards one of the virtual floors (Fig. 2b, Supplementary Fig. 5a-c). <lb/>Interestingly, rats had preference to jump to the safe side if they made their decision <lb/>after short (~&lt;20 sec exploration), but decreased this preference to chance level if <lb/>longer exploration times were included (Fig. 2c, Supplementary Fig. 5d). When <lb/>excluding outlier sessions (see Online Methods), we found that rats showed a <lb/>preference toward the safe side regardless of the position of the virtual cliff (Fig. 2d). <lb/>Thus, when exposed to the VR for a limited time rats tend to avoid it similar to real <lb/>cliff avoidance paradigms. <lb/>Interaction with virtual walls <lb/>Virtual boundaries are the main elements of the VE that inform animals about <lb/>topology of the virtual space 36 . In rVR systems, rats are traditionally operantly <lb/>conditioned to respect the boundaries by freezing the VE upon collision of the <lb/>animal&apos;s virtual trajectory with the wall 16,21,37 . In order to investigate how naive rats <lb/>spontaneously interact with virtual boundaries, we introduced a virtual wall in the <lb/>middle of the arena (Fig. 3a,b). During 10-minute sessions rats were let to explore the <lb/>environment. Rats displayed noticeable change of their behavior in the vicinity of the <lb/>walls, as demonstrated by increased occupancy and rearing events around the wall <lb/>(Supplementary Fig. 6b; Supplementary Movie 4). Interestingly, orientations of the <lb/>locomotion trajectories in the vicinity of the virtual wall concentrated around <lb/></body>

			<page>7 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>perpendicular and parallel orientations to the wall (Supplementary Fig. 7b-c), <lb/>indicating that the rat moved either along or towards/away to the virtual wall. <lb/>Clustering of parallel orientation of trajectories near the virtual wall was similar to <lb/>that in near the real wall, but was not present in the matching location in the control <lb/>sessions with an empty arena (Supplementary Fig. 7). This behavior is consistent with <lb/>thigmotaxis along both virtual and real walls. We further tested whether rats treated <lb/>the virtual wall as an obstacle when approaching it. Locomotion trajectories <lb/>approaching the virtual wall were more likely to turn away from (a &quot;deflection&quot; <lb/>trajectory) than to cross through the virtual wall to the other side of the arena (a <lb/>&quot;crossing&quot; trajectory, Fig. 3c), compared to the same arena locations in control <lb/>sessions with empty arena, but not in the direction parallel to the virtual wall under <lb/>either condition (Χ 2 =48.48, n=797 trajectories, p &lt; .001, Fig. 3c-d). Thus, rats&apos; <lb/>interactive behavior towards the virtual wall is consistent with them responding to it <lb/>as a wall. <lb/>Exploration of virtual objects <lb/>Spontaneous exploration of the objects is the cornerstone for multitude of behavioral <lb/>paradigms aimed to study perception and memory 38 . Real objects have multimodal <lb/>features and affordances, but require careful and laborious handling for repeated <lb/>presentation and feature manipulation. 3D virtual objects could be arbitrarily <lb/>designed, manipulated and presented to an animal automatically. While rodents can <lb/>perceive 3D shapes 39 and navigate towards reward locations marked by virtual objects <lb/>in rVR 22,23 , naturalistic exploration of virtual objects cannot be properly tested with <lb/>any existing methods. In series of test sessions we investigated how rats <lb/>spontaneously interact with the virtual 3D objects (Supplementary Fig. 8a) pseudo-<lb/>randomly positioned inside the arena (Fig 4a; Supplementary Movie 5). Rats spent <lb/>more time in the vicinity (&lt;15 cm) of the virtual objects, especially in the center of the <lb/>arena, with their trajectories precisely approaching the object, as compared to sham <lb/>locations (Fig. 4a-b). We further quantified how rats interacted with the virtual <lb/>objects on their direct approach trajectories (&lt;10cm from the virtual object). Similar <lb/>to the interactions with the virtual walls, rats&apos; trajectories often &quot;deflected&quot; from the <lb/>virtual objects, reflecting that rats changed their direction of running (&lt;90deg arc) <lb/>after reaching the virtual object&apos;s boundaries (Fig. 4c-d, Supplementary Fig. 8c). <lb/>Deflective nature of interaction with virtual objects was qualitatively reminiscent to <lb/>that with real objects (Fig. 4c), and while less frequent, deflections were occurring <lb/>significantly more often around objects than in sham locations (Fig. 4d). Rats <lb/>occasionally displayed rearing and head-scanning behavior in the vicinity of the <lb/>virtual objects (data not shown). Interestingly, in a fraction of sessions in which the <lb/></body>

			<page>8 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>exploration of an empty arena followed the object trial, rats showed a tendency to <lb/>spend more time in the location of previous encounter with the virtual objects <lb/>(Supplementary Fig. 8e). <lb/>Effect of virtual environment on hippocampal spatial map. <lb/>While, as we&apos;ve shown above, animals immersed into the VE interact with it less <lb/>reliably than with real environment and thus behavioral readout is only partially <lb/>reflecting animal&apos;s perception of the VE, internal hippocampal representation of the <lb/>virtual space could provide an insight into animal&apos;s perception of the VE 23 . <lb/>Hippocampal spatial representation is believed to be anchored to multiple frames of <lb/>reference, which are concurrently controlled by visual geometrical features of the <lb/>boundaries and landmarks, other external sensory and idiothetic inputs, but due to <lb/>physical limitation of the real environment, dissociation of the contribution of these <lb/>different reference frames is difficult, and was so far mainly limited to rotations <lb/>around a symmetry axis 40 . Here we illustrate an application of the ratCAVE to study <lb/>complete dissociation of visual and all other multisensory systems on hippocampal <lb/>spatial representation by linearly translating visual boundaries with respect to the <lb/>physical environment. In the pilot experiment we recorded population of pyramidal <lb/>cells in CA1/2 regions of the hippocampus (166 and 154 from two days analyzed) in a <lb/>rat spontaneously exploring the arena through series of sessions in which VE was <lb/>either aligned or laterally shifted by 20 cm with respect to the physical boundaries of <lb/>the arena (Normal vs Shifted, Fig. 5a). Similar to the virtual wall interaction <lb/>experiment, the rat interacted with the virtual boundary that appeared inside the arena <lb/>in the Shifted condition at least during the first Shift session. Interestingly, population <lb/>of place cells (n=20, see Online Methods for selection criteria) remapped their place <lb/>fields within the arena between Normal and Shifted sessions in the direction of the <lb/>VE shift (Fig. 5b-c). The effect decreased over consecutive alternating sessions and <lb/>following multiple exposures to the shifted VE (3 days later) place cells showed no <lb/>remapping between Shifted and Normal conditions (Fig. 5d-e). We tested if any <lb/>visual information associated with VE boundaries is contributing to the stabilized <lb/>spatial map by immersing the rat into the VE that was unrelated to and expanded <lb/>beyond the physical boundaries of the arena. This VE as well had no effect on the <lb/>place field position (Fig. 5d-e, bottom). Thus ratCAVE is sufficiently immersive to <lb/>enable visual input control of hippocampal spatial representation, but progressive <lb/>exposure to the conflict between visual and other multisensory inputs enabled by <lb/>ratCAVE can result in complete independence of the hippocampal spatial <lb/>representation from the visual input 41,42 . <lb/></body>

			<page>9 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>DISCUSSION <lb/>We presented a ratCAVE system for freely-moving rodents that builds on and extends <lb/>previous developments of fmVR systems in arthropods and fish 28-31 to provide a high-<lb/>performance general cognitive science VR research platform by implementing a <lb/>combination of methods that provide realistic visual environments, low-latency and <lb/>high-precision closed loop feedback to animals&apos; head, and flexibility of the shape and <lb/>mobility of the arena. Using more complex lighting models, including diffuse and <lb/>specular reflections and self-shadowing, provides new visuo-spatial cues for virtual <lb/>environments and increases immersion 34,43 . In humans, sensory conflicts resulting <lb/>from out-of-phase feedback to rapid head motion arise when motion-to-photon <lb/>latency of the VR system is larger than ca. 50 msec, resulting in decreased <lb/>performance in spatial navigation, spatial perception, and sense of self-motion in the <lb/>VE 32 ; to counter this effect, we&apos;ve implemented a low-latency visual update loop (240 <lb/>fps, 15msec &quot;motion-to-photon&quot; lag) to decrease mismatch between vestibular, <lb/>proprioceptive, and visual self-motion cues, essential for proper self-motion detection <lb/>and functioning of the head-direction system 44,45 . <lb/>There are pressing improvements needed to further increase immersion in VR systems <lb/>used in neuroscience research. While rVR immersion requires animals to ignore <lb/>lacking or mismatching sensory inputs, immersion in fmVR is associated with the <lb/>minimal conflict between visual and other senses. However, both rVR and fmVR <lb/>systems suffer from the cross-sensory conflict upon collision of animal&apos;s trajectory <lb/>with the virtual boundary and can break immersion. In rVR setups, the solution has <lb/>been to simply stop visual update while still allowing rodent locomotion, creating a <lb/>locomotion-visual mismatch upon impact 16,23 . In fmVR, a similar mismatch occurs <lb/>when the virtual and real surfaces are not matched and are directly sampled by the <lb/>animal. Such situations require a careful selection of virtual environment, arena <lb/>design and method to match the research questions at hand. A few improvements can <lb/>be considered in the ratCAVE. First, VE objects and boundaries can be made <lb/>inaccessible to the animal by projecting them outside arena walls or across the gap. <lb/>Second, ratCAVE calibration procedure allows for projecting virtual objects on <lb/>smooth shapes inside the arena, thus aligning them with real countrerparts, enhancing <lb/>VR immersion via all three avenues: naturalistic interaction (via touch and smell), <lb/>increased cue salience, and reducing cross-sensory mismatch upon virtual object <lb/>contact. Third, electrical or optogenetic stimulation of olfactory or somatosensory <lb/>system 46,47 can be used to provide congruent multisensory feedback. Similarly, use of <lb/></body>

			<page>10 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>visuo-acoustic VR can be provide more cohesive VE 22,48 . In addition to motion-<lb/>dependent monocular depth cues, static binocular depth cues based on stereoscopy are <lb/>also important for forming an accurate 3D space percept 43 , a point currently ignored <lb/>in rodent VR studies. Thanks to precise head-based projection, the ratCAVE system <lb/>can be extended to generate stereo VE via implementation of head-mounted shutter <lb/>glasses to provide alternating images to the left and right eyes of the exploring rodent. <lb/>Many of these improvements can be added onto existing rVR and fmVR systems to <lb/>increase VR immersion in those setups. Further integration and cross-insemination of <lb/>open-source fmVR and rVR developments in diverse animal models will enable a <lb/>broad spectrum of neuroscientists to use these systems. <lb/>Freely-moving virtual reality represents an improvement in VE immersion over rVR, <lb/>considered as an enhancement of naturalistic interaction mechanisms with the virtual <lb/>environment, an increased salience of sensory cues associated with the virtual <lb/>environment, and a minimization of cross-sensory conflict. Naturalistic interaction <lb/>with the virtual environment is enhanced in fmVR by simply allowing the full range <lb/>of movement in an unmodified space, without training or postural alteration, while in <lb/>rVR, locomotion and virtual object interaction must be simulated via running on a <lb/>spherical treadmill. Self-motion cues through the virtual environment are enhanced in <lb/>fmVR by providing higher-frequency and shorter-latency feedback to head motions in <lb/>the virtual environment alongside the lower-frequency locomotion behaviors, while <lb/>rVR only provides locomotion feedback. In contrast to rVR that assumes a stationary <lb/>head in the virtual projection, fmVR system minimizes cross-sensory conflict by <lb/>providing feedback to head motions, as well as by matching changes in olfactory, <lb/>tactile, and auditory real-world inputs to self-motion in the virtual world. Finally, <lb/>fmVR systems do not require operant training and habituation procedures used in rVR <lb/>systems. <lb/>We demonstrated that a ratCAVE VR system for freely moving animals can be <lb/>successfully applied to a number of behavioral paradigms not possible with <lb/>conventional rVR systems. Untrained rats freely behaved and spontaneously <lb/>interacted with virtual environment by approaching, exploring and leaving virtual <lb/>objects and walls, displaying thigmotaxis along virtual walls and avoiding a virtual <lb/>cliff. We further used ratCAVE system to illustrate how contribution of the virtual <lb/>visual input to hippocampal spatial representation can be strong upon first exposure to <lb/>VE mismatched with the physical world, but becomes negligible after repeated <lb/></body>

			<page>11 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<body>exposure of the rat to cross-sensory conflict. These experiments and design features of <lb/>ratCAVE described above pave the way to a large body of future applications. <lb/>First, high spatio-temporal resolution of 3D tracking of the rodent&apos;s head, which can <lb/>be extended to include the full body, enables quantitative analysis of the natural <lb/>behaviors of the rodent during VE exploration, which significantly extends level of <lb/>analysis possible using two-dimensional locomotion information provided by <lb/>conventional tracking in 2D space or the treadmill measurement in rVR. Second, <lb/>ratCAVE&apos;s &quot;trackable&quot; arena also enables vestibular perturbations during VR <lb/>experiments via arena movement, enabling studies on vestibular system function and <lb/>visuo-vestibular binding in behaving rodents. Third, fmVR&apos;s ability to incorporate a <lb/>three-dimensional element into operant conditioning tasks increases the range of <lb/>motor affordances of digitally-rendered learning stimuli, which have their own <lb/>benefits of flexibility and timing control 49 . Integrating these improvements into VR <lb/>setups will enable new methods in research areas such as learning and memory, <lb/>perceptual decision-making, and 3D-rotation and object perception 39 . Fourth, the <lb/>automated nature of head tracking allowing for online behavior analysis, operant <lb/>conditioning, and fmVR enable high-throughput and automatic behavioral testing in a <lb/>colony of animals 50 across a large variety of tasks, such as perceptual, incidental and <lb/>motor learning, spatial memory paradigms, to name a few. Importantly, use of <lb/>automated fmVR behavioral paradigms allows their standardization, reproducibility <lb/>of results independent of experimenters or setup. Finally, combined with neural <lb/>recording and manipulation ratCAVE enables the detailed investigation of the <lb/>mechanisms of spatial coding. Manipulation of the arena boundaries provides a <lb/>powerful tool to study for multisensory nature, remapping and attractor properties of <lb/>the spatial representation 51 . <lb/>Low latency, unmatched by any other system for freely moving subjects, and rich <lb/>visual features make ratCAVE appealing for use in human subjects. Translation of <lb/>experimental paradigms and physiological validation of psychophysical experiments <lb/>from humans to animals and back could enable validation and further development of <lb/>diagnostic and rehabilitation procedures for the vestibular or neurodegenerative <lb/>disorders in animal models 52,53 . ratCAVE opens new ways to study sensory-motor <lb/>systems in their natural dynamics while having flexibility in manipulating the sensory <lb/>feedback not possible in real life. <lb/></body>

			<div type="acknowledgement">ACKNOWLEDGMENTS <lb/></div>

			<page>12 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="acknowledgement">We thank Esmi Zajaczkowski for help in experiments and design of the 3D virtual <lb/>objects, Nicolas Kuske for help with programming API for Optitrack camera control. <lb/>This work was supported by the Deutsche Forschungsgemeinschaft (DFG) via the <lb/>Werner Reichardt Centre for Integrative Neuroscience (CIN, Tuebingen), Munich <lb/>Cluster for Systems Neurology (SyNergy, Munich), Priority Programs 1665 and 1392 <lb/>and by the German Ministry for Education and Research (BMBF) via Grant Number <lb/>01GQ0440 (Bernstein Center for Computational Neuroscience Munich). <lb/></div>

			<div type="annex">AUTHOR CONTRIBUTIONS <lb/>N.A.D.G. designed and implemented the ratCAVE system, designed and performed <lb/>behavioral experiments in the VR; J.J.G. and E.B.H. performed electrophysiological <lb/>experiments recordings in the VR and behavioral experiments with real objects <lb/>exploration; all authors contributed to data analysis; N.A.D.G., E.B.H and A.S. wrote <lb/>the paper. All authors discussed the results and commented on the manuscript. <lb/></div>

			<div type="annex">COMPETING INTERESTS STATEMENT <lb/>The authors declare that they have no competing financial interests. <lb/></div>

			<div type="annex">ONLINE METHODS <lb/>ratCAVE VR system <lb/>Hardware setup. Our setup consisted of a rectangular arena with dimensions 115cm <lb/>x 65cm (L, W) and walls 40cm high, angled at 70 degrees to increase the projected <lb/>image&apos;s surface area and brightness. A set of 12 cameras (OptiTrack, NaturalPoint <lb/>Inc. U.S) was used to record the 3D position of retro-reflective spheres, six Prime <lb/>17W (360 fps) and six Prime 13W (240 fps). A projector with 240 fps frame rate <lb/>(VPixx Technologies Inc., Saint-Bruno, Canada) was mounted to the ceiling. An <lb/>optically-flat aluminum-foil projection mirror (100cm x 75 cm, Screen-Tech), slanted <lb/>45 degrees, was suspended from the ceiling on an adjustable frame for accurately <lb/>fitting the projected image onto the whole surface of the arena. This setup was <lb/>installed inside an isolating acoustic chamber (Supplementary Fig. 4). <lb/>Software. The ratCAVE VR system depends on many pieces of software to work; <lb/>interactions between each software component are diagrammed in Supplementary <lb/>Figure 3. Virtual environments are modeled and exported to file in a 3D modeling <lb/>program, Blender 3D (Supplementary Fig. 3a). Coregistration of the arena and <lb/>projector with the tracking coordinate system is performed via a custom Python <lb/></div>

			<page>13 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">command-line program package called &quot;ratcave_calibration&quot;, which uses a custom <lb/>Python API called &quot;MotivePy&quot; to access and controlling our Optitrack camera array <lb/>while using a custom Python 3D graphics utility package called &quot;Fruitloop&quot; to render <lb/>the point cloud from the projector (Supplementary Fig. 3, &quot;Grey Zone&quot;). Fruitloop <lb/>provides a user-friendly interface for modern OpenGL rendering techniques, and its <lb/>&quot;Get Data, Update Camera, Render VE&quot; event loop forms the core engine of a <lb/>ratCAVE virtual reality session. Cubemapping, lighting, and antialiasing are done via <lb/>OpenGL FrameBuffer objects and shader scripts supplied with Fruitloop. VR <lb/>Experiment scripts are written in Python, using a custom network client called <lb/>&quot;NatNetClient&quot; to obtain Optitrack camera data in real-time and Fruitloop to render <lb/>the virtual scene (Supplementary Fig. 3, &quot;Blue Zone&quot;). Because all software used in <lb/>the ratCAVE VR setup is comprised of loosely-connected specialized parts, the <lb/>software developed by the lab is generalizable to a variety of different setups, <lb/>enabling other labs to substitute like-components to build a VR setup that matches <lb/>their hardware. <lb/></div>

			<div type="availability">Code availability. All code used in implementation of the ratCAVE is freely <lb/>available for use and modification via Github and installable via the Python Package <lb/>Repository. The Fruitloop package can be found at <lb/>https://github.com/neuroneuro15/fruitloop, MotivePy at <lb/>https://github.com/neuroneuro15/motivepy, NatNetClient at <lb/>https://github.com/neuroneuro15/natnetclient, and ratcave_calibrate at <lb/>https://github.com/neuroneuro15/ratcave_calibrate. Associated documentation and <lb/>usage tutorials are available at fruitloop.readthedocs.io and in Supplementary <lb/>Documentation. All custom-written code used for analysis of results presented in this <lb/>study are available from the corresponding author on reasonable request. <lb/></div>

			<div type="availability">Data availability. The datasets generated during and/or analyzed during the current <lb/>study are available from the corresponding author on reasonable request. <lb/></div>

			<div type="annex">VR implementation. We tracked the rodents&apos; head position and orientation by <lb/>imaging a head-mounted, 3D-printed plastic skeleton of four retro-reflective spheres <lb/>(6-8mm in diameter). Commercial software (Motive, NaturalPoint Inc., USA) <lb/>isolated these spheres&apos; positions in each camera&apos;s imaging data and reconstructed the <lb/>3D position and orientation of the rigid body (Supplementary Fig. 3, &quot;3D Tracking <lb/></div>

			<page>14 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">Software&quot;). Rodent head position was then logged for offline analysis and sent over <lb/>the network to the VR system&apos;s experiment script via a custom python package <lb/>(NatNetClient) for visual stimulus update (Supplementary Fig. 3, &quot;Python Optitrack <lb/>Client&quot;). The ratCAVE VR engine Fruitloop receives the current position of the rat&apos;s <lb/>head from NatNetClient, updating the virtual scene from the rat&apos;s perspective, <lb/>generates the projected image using a cube-mapping algorithm (Supplementary Fig. <lb/>2a-c), performs per-fragment lighting calculations (Supplementary Fig. 2d), and <lb/>antialiases the resultant video output via custom OpenGL shaders (Supplementary <lb/>Fig. 3). The resultant image is then projected onto the arena via the video projector. <lb/>Latency measurement. Motion-to-photon latency was explicitly measured using the <lb/>following setup 54 . A reference point, representing a VR observer, formed by a set of <lb/>three retro-reflecting markers and a small LED, were attached to a bar that was <lb/>rotated in the horizontal plain around a fixed point inside an arena by an AC motor <lb/>and was tracked as described above. The VR system was programmed to generate a <lb/>white spot that was offset in the horizontal plain from the reference point that would <lb/>follow a reference marker. VR spot was thus rotating in the horizontal plain following <lb/>the rotation of the reference LED point. Both LED and VR spots were imaged using <lb/>high-speed-camera (Prime, Photometrics) at 250 Hz. The image stack was processed <lb/>to detect both spots (Supplementary Fig. 1a) and temporal trajectories of X and Y <lb/>coordinates of both reference and VR spots, which were analyzed to detect temporal <lb/>offset between them using cross-correlation function (Supplementary Fig. 1b). The <lb/>angular speed of rotation was varied between trials, and the resulting linear speed <lb/>(tangential) was computed and used for latency-speed analysis (Supplementary Fig. <lb/>1c). <lb/>Animal experiments methods <lb/>All procedures complied with the European Communities Council Directive <lb/>2010/63/EC and the German Law for Protection of Animals and were approved by the <lb/>local authorities, following appropriate ethics review. <lb/>Subjects. Three 6-month-old male Long-Evans rats (Charles-River, Germany) were <lb/>used for the analysis of spontaneous exploratory behavior in virtual environments, <lb/>and three rats were used for analysis of spontaneous exploration of real-world objects. <lb/>An additional rat was used to record hippocampal neural activity in a virtual <lb/></div>

			<page>15 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">environment, as described in the &quot;VE Shift Experiment&quot; section. All rats were <lb/>allowed ad libitum access to water and food. All rats were extensively handled by the <lb/>experimenter prior to behavioral experiments in order to minimize stress. <lb/>Behavioral experiments <lb/>We recorded the spontaneous behaviors of three rats in three virtual environments. <lb/>Each session, conducted twice per day over one week, consisted of two phases: a one-<lb/>minute visual cliff session and a ten-minute arena exploration session, between which <lb/>the rat was removed from the arena. Same-day sessions were separated by a <lb/>minimum of 5 hours; the first in the middle of the rat&apos;s light cycle, and the second at <lb/>the beginning of its dark cycle (labeled in Supp. Figure 5 as &quot;Midday&quot; and &quot;Evening&quot; <lb/>sessions, respectively). Arena exploration sessions could contain either virtual objects <lb/>for exploration, virtual walls, or no virtual objects at all (control condition; the <lb/>projector remained on and a stationary checkerboard pattern remained projected on <lb/>the arena). <lb/>Virtual cliff experiment. In each virtual cliff session, the experimenter placed the rat <lb/>on a 14 cm-wide board suspended 13 cm above the arena&apos;s floor, bisecting the arena <lb/>into two halves. Two virtual floors were randomly assigned and projected onto the <lb/>arena halves, either 1.5 meters below the actual floor level (the cliff side) or at the <lb/>same level as the floor (the safe side). We enhanced the motion parallax stimulus by <lb/>adding a slight height variation to the floor texture; this also helped reduce the chance <lb/>of side selection by the presence of virtual motion. After some visual exploration of <lb/>their environments, rats jumped down from the suspended board to the arena floor on <lb/>one of two sides and were allowed to explore for 15 seconds, after which the <lb/>experimenter removed the rat from the arena. Each session lasted a maximum of 90 <lb/>seconds. Cliff avoidance behavior was interpreted by observation of the rat jumping <lb/>down from the board on the safe side, after a period of visual exploration of the arena, <lb/>with jump side observation counts calculated as a discrimination index {(safe -cliff) / <lb/>(safe + cliff)}. <lb/>Virtual wall experiment. During the virtual wall sessions, rats were allowed to freely <lb/>explore the arena for 10 minutes. A virtual wall extended from the center of the arena, <lb/>dividing it across its length (short wall) or its width (long wall). Each rat was exposed <lb/>to both walls for five minutes (long followed by short wall) in a single session. <lb/>Virtual object exploration. During object exploration, rats were allowed to freely <lb/>explore three different virtual objects, each roughly 6 cm in diameter and randomly <lb/></div>

			<page>16 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">selected from a pool of 11 custom-designed 3D models (Supplementary Fig. 8a). The <lb/>objects were placed in the nodes of triangular configuration &quot;corner&quot;, &quot;wall&quot;, and <lb/>&quot;center&quot; (Fig. 4a), which was pseudo-randomly rotated between trials. In some of <lb/>these sessions, the objects displayed either a shrinking, rotating, jumping, or running <lb/>animation when the rat came within 15 centimeters of the object&apos;s center, with the <lb/>goal of increasing rodent engagement with the objects, although this factor was <lb/>ignored in the analyses due to low sample size. <lb/>Virtual environment shift experiment. During the VE shift experiments, the rat was <lb/>allowed to explore the arena for ~10 minutes. In consecutive sessions, the rat <lb/>experienced two conditions: a &quot;Normal&quot; condition, where the arena had virtual walls <lb/>(checkerboard pattern) matching entirely in space with the real walls, and a &quot;Shifted&quot; <lb/>condition, where the VE was shifted 20 cm along the arena&apos;s length. This effectively <lb/>resulted in the shift of one virtual wall to the outside of the arena and another to the <lb/>inside of the arena, similar to the virtual wall experiment. The rat was exposed to the <lb/>arena and these two conditions twice for the first time on Day 1 (Normal1, Shift1, <lb/>Normal2, Shift2 in Fig. 5 b-c) and then repeatedly to the same conditions, as well as <lb/>other VR manipulations in several sessions on Days 2 and 3 (data not shown). On <lb/>Day 4 two sessions were recorded under Normal and Shifted conditions and an extra <lb/>session under condition &quot;Star field&quot; was introduced (Normal, Shift and Start field in <lb/>Fig. 5d-e). This condition consisted a 3D grid of repeating white cubes, which <lb/>extended 1 meter beyond the walls and floor of the arena. <lb/>Surgery and electrophysiological recordings. Rats were anesthetized with a three-<lb/>component mixture (Fentanyl .005mg/kg, Midazolam 2mg/kg, Medetomidine <lb/>.15mg/kg); this compound also provided analgesia for the first part of the procedure. <lb/>A 1.5% concentration of isoflurane in oxygen was used to maintain depth of <lb/>anesthesia for the rest of the surgery. In animals used for behavioral assays, a small <lb/>screw was fixed into the skull to provide support for our head post. In one rat, a <lb/>silicon probe (NeuroNexus, Buzsaki 32 design, 4 shanks, 8 sites ~25um vertically <lb/>spaced) was implanted following procedures described elsewhere 55 . Briefly, a cranial <lb/>window of ~2 mm 2 was opened, centered on the following coordinates from bregma: <lb/>-3.36 mm AP and +2.6 mm ML. The silicon probe, mounted on a custom-made <lb/>microdrive, was inserted in the center of the craniotomy with the shanks aligned <lb/>parallel to the septo-temporal axis of the hippocampus (45 degrees parasagittal). The <lb/>probe was lowered to a distance 1mm from the surface, and the drive was affixed to <lb/></div>

			<page>17 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">the skull. After the rat recovered from surgery (1 week), the probe was lowered 50-<lb/>150 microns daily until we observed the typical profile of activity of CA1/CA2 cell <lb/>layer; namely, spiking activity and ripple oscillation signal in the LFP. Histological <lb/>verification of the location of the recording electrodes was done after the conclusion <lb/>of the experiments (data not shown). <lb/>Data acquisition and processing. Extracellular signals were amplified and filtered <lb/>by multi-channel preamplifiers (Plexon, 20x, 1-5000 Hz). Wide-band extracellular <lb/>and intracellular signals were digitized at a 20 kHz sampling rate with 16-bit <lb/>resolution and stored for offline analysis using a multichannel acquisition system <lb/>(DigiLynx, Neuralynx). Raw data were preprocessed using a custom-developed suite <lb/>of programs (neurosuite.sourceforge.net ). The wide-band signal was downsampled to <lb/>1.25 kHz and used as the local field potential signal. For spike detection, the wide-<lb/>band signal was high-pass filtered (&gt;0.8 kHz). Single units were isolated semi-<lb/>automatically by a open-source spike-sorting program KlustaKwik <lb/>(http://klustawik.sourceforge.net) 56 and refined manually using open-source GUI <lb/>software (http://klusters.sourceforge.net; http://neuroscope.sourceforge.net) 57 . The <lb/>quality of isolated single units was confirmed by an isolation distance metric and a <lb/>clean refractory period. <lb/>Data Analysis <lb/>All data analysis was performed using custom-written code in Python and Matlab <lb/>(Mathworks, Inc.). <lb/>Data representation and statistics. If not described in the figure legends data <lb/>summary are plotted using Matlab boxplot (whisker plot) showing median, 25/75 <lb/>percentile bar and whiskers extending to +/-2.7σ outliers are shown with small <lb/>crosses. Non-parametric and resampling statistical analysis methods were used in all <lb/>cases and are specified in figure legends and methods sections below. Due to use of <lb/>non-parametric methods assumptions (e.g. normality of the distribution), associated <lb/>with parametric tests were not tested. Size of experimental animal sample to ensure <lb/>adequate power could not be determined prior to the study, since no parameters of <lb/>analysis could be predicted a priory. Instead of increasing animal sample size, we <lb/>repeated individual experimental sessions (virtual cliff and object exploration; virtual <lb/>wall condition was not repeated due to recognized interference between VR sessions <lb/></div>

			<page>18 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">due to potential memory effect). No animals or sessions were excluded from the <lb/>analysis. Randomization and blinding was not performed as all animals were <lb/>subjected to all tests conditions as well as control sessions. <lb/>Behavioral state classification. Behavioral state of the rat was classified based on the <lb/>speed and height of the head. Using data-derived thresholds for these variables, we <lb/>defined running (speed &gt; 3cm/s &amp; height &lt; 13.4cm), immobility (speed &lt;= 3cm/s &amp; <lb/>height &lt; 13.4cm) and rearing (speed &lt;= 3cm/s &amp; height &gt; 15.4cm). <lb/>Virtual cliff avoidance analysis. Rat behavior was segmented based on head-<lb/>tracking data into supported rearing on the arena walls and general exploratory <lb/>behavior In addition, visual exploration was associated with head dips, which were <lb/>detected as trajectories of the head extended within 1cm from the board. Jumps were <lb/>detected as trajectories that depart from the board and land on the floor. A rat&apos;s <lb/>landing after jumping down from the board was detected based on the height of its <lb/>head (threshold &lt; 7 cm). Example sessions&apos; time courses are shown in Supplementary <lb/>Fig. 5a. Since rats spent a variable amount of time across sessions performing <lb/>supported rearing (M=24.8% of trial, SD=15.5%), likely trying to escape the arena or <lb/>look outside the arena, we chose to remove these periods from the decision time <lb/>estimation analysis, yielding an exploration time measure before the jump event, <lb/>which we used to analyze the effect of time spent exploring the VE prior to the jump <lb/>side decision behavior (Fig. 2b, Supplementary Fig. 5d). Excluding supported rearing <lb/>did not make any qualitative difference in the outcome of the statistical analysis. Cliff <lb/>avoidance behavior was interpreted by observation of the rat jumping down from the <lb/>board on the safe side, after a period of visual exploration of the arena, with jump side <lb/>observation counts calculated as a discrimination index [{(safe -cliff) / (safe + <lb/>cliff)]}. <lb/>Virtual wall experiment analysis. Only periods of continuous locomotion were <lb/>considered in the analysis of trajectories. When rats locomoted to the vicinity (within <lb/>7.5 cm) of one side of the virtual wall, the trajectory was counted as a &quot;Crossing&quot; if <lb/>they continued through the wall and reached the threshold on the other side; if they <lb/>returned to the same side of the arena where they entered the vicinity, the trajectory <lb/>was counted as a &quot;Deflection&quot;. A chi-square test of independence on deflection and <lb/>crossing counts was used to test for an increase in total number of deflection <lb/>trajectories for the VR wall condition over control sessions in the same locations. To <lb/>test if the proportion of deflecting trajectories exceeded chance level for each <lb/></div>

			<page>19 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">condition (Long / Short), we permuted (6000 permutations of 3-session means) the <lb/>proportion of deflections for VR (n=3) and Control (n=11) sessions combined to <lb/>generate a null distribution, and computed an empirical one-sided p-value for the <lb/>actually-found percentage of mean for trajectories deflecting off the virtual wall for <lb/>both Long and Short VR conditions. <lb/>Thigmotaxis behavior, i.e. locomotion along and near the wall, was quantified using <lb/>analysis of the angle of instantaneous velocity vector with respect to the virtual wall <lb/>and same axis for control conditions with no wall present. Instantaneous 2D velocity <lb/>vector was smoothed (500 msec boxcar) and the angle between this vector and the <lb/>wall was extracted. Joint probability density function between distance of the animal <lb/>instantaneous position and angle to the wall was computed (Supplementary Fig. 7b,c). <lb/>Presence of the mode at 0 radians for animals positions close to the virtual wall (blue <lb/>line) and real wall (black line) indicated comparable thigmotaxis behavior. Control <lb/>conditions, in contrast to virtual wall conditions lacked this mode. A second mode <lb/>around pi/2 for distances spanning large range from the virtual wall indicates <lb/>trajectories crossing and deflecting from the virtual wall, as analyzed and represented <lb/>in Figure 3. <lb/>Virtual object exploration analysis. Object exploration was quantified using a set of <lb/>metrics aimed to measure rats&apos; exploration of the virtual objects&apos; locations. We used <lb/>progressively more refined measures to quantify animals&apos; exploration of the virtual <lb/>object. First, an occupancy of the object vicinity, i.e. probability that rat is located <lb/>within 15 cm from the object, was used as a crude measure to assess the general <lb/>preference of the animal to be near the virtual objects. Second, the occupancy density <lb/>at the object location, computed as a ratio of occupancy within 5 cm to that within <lb/>15cm of the object&apos;s center, was used to measure the selective localization of <lb/>increased occupancy within the direct vicinity of the object. Third, we analyzed the <lb/>proportion of trajectories that entered the vicinity of the object (10cm radius) that <lb/>reached within 3cm of the virtual object&apos; center. To control for the significance of this <lb/>effect against random locomotor activity, which is naturally constrained and interacts <lb/>with the arena walls, we first considered using control sessions that contained no <lb/>objects within the arena. Surprisingly, we found an increased occupancy at virtual <lb/>object locations compared to the rest of the arena in these sessions (Supplementary <lb/>Fig. 8e), potentially reflecting a memory effect of the animals for the location of the <lb/>objects. To avoid these inter-session interactions, all further trajectory analyses were <lb/></div>

			<page>20 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">done against a within-session control &quot;sham&quot; location, paired to each virtual object on <lb/>the opposite side of the arena (Fig. 4a; Supplementary Fig. 8b, top). For all measures <lb/>of object exploration we constructed a discrimination index [DI = (VR -Sham) / (VR <lb/>+ Sham)] and tested using a Wilcoxon signed-rank test for significant differences <lb/>from zero. Consistent with observations in both other studies utilizing real-object <lb/>discrimination tasks and our own analysis (Supplementary Fig. 7), animal behavior in <lb/>the vicinity of both the arena and virtual wall boundaries was heavily biased to <lb/>thigmotaxis. In addition, we observed a high rate of supported rearing next to the <lb/>walls and, especially, in the corners (Supplementary Fig. 6b). These factors heavily <lb/>contaminated and made insensitive most measures of spontaneous exploration of the <lb/>objects located next to the wall and in the corner. Consistently, we found that <lb/>occupancy times for the object vs sham were significantly higher for the center object <lb/>(Z=2.70, p&lt;.01, Fig. 4b), but not the wall object (Z=1.42, p=.08, data not shown) nor <lb/>the corner object (Z=-0.52, p=.70, data not shown). Occupancy density was <lb/>significantly different from sham for the center object (Z=3.55, p&lt;.001, Fig. 4b) and <lb/>corner object (Z=2.13, p&lt;.05, data not shown), but not the wall object (Z=0.56, p=.29, <lb/>data not shown). Locomotion trajectories approaching the object (within 10 cm) <lb/>were also more likely to pass through the VR objects than their sham pairs for the <lb/>center object (Z=2.889, p&lt;.01, Fig. 4b) and wall object ( Z=2.130, p&lt;.05, <lb/>Supplementary Figure 8b,c), but not the corner object ( Z=1.008, p=.15, <lb/>Supplementary Figure 8b,c). <lb/>The rats sometimes interacted with the virtual objects and then changed their running <lb/>direction. To quantify this behavior, we introduced a notion of trajectory &quot;deflections&quot; <lb/>from the object (see Fig. 4c for trajectories examples). We analyzed the relationship <lb/>of the arc angle made by trajectories entering and leaving the 10 cm circle around the <lb/>object, a &quot;deflection angle&quot;, with the shortest distance between the trajectory to the <lb/>object. If a trajectory approached the object closely and its deflection angle was acute <lb/>(&lt; 90 degrees), we qualified it as a &quot;deflecting&quot;, while obtuse (&gt; 90 degrees) <lb/>deflection angles were qualified as &quot;crossing&quot;. As trajectories not reaching the <lb/>proximity of the object are progressively associated with smaller deflection angles, <lb/>we set a conservative cut-off distance of 3 cm to define a trajectory as deflecting. <lb/>Thus, deflecting trajectories are those that fall in the region of less than 3 cm and less <lb/>than 1.56 radians, displayed in Supplementary Fig. 8d. We compared the proportion <lb/>of &quot;deflecting&quot; trajectories for sham and object-containing locations using an object-<lb/></div>

			<page>21 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">label-shuffling permutation test (Supplementary Fig. 8d, right column). As the arena <lb/>wall blocked trajectories for the other two object positions (&quot;wall&quot; and &quot;corner&quot;), <lb/>deflection trajectory analysis was only possible for the center object. To compare this <lb/>newly-introduced measure of rat interaction with the virtual object with that for the <lb/>real objects, we performed an identical analysis in a separate set of data from 3 rats <lb/>exploring real objects in a cylindrical arena. <lb/>In the fraction of sessions where virtual objects were programmed to be interactive, <lb/>i.e. displayed either a shrinking, rotating, jumping, or running animation upon the <lb/>rat&apos;s approach, we observed increased exploration in the vicinity of the objects (data <lb/>not shown). As this behavior was variable across animals, our data lacked sufficient <lb/>power to statistically assess this effect. <lb/>Brain state segmentation. Hippocampal activity was segmented into two states: theta <lb/>and non-theta. An HMM Gaussian mixture model based on the hippocampal CA1 <lb/>pyramidal layer spectral power ratio between the 6-12 Hz band and the sum of the 1-5 <lb/>Hz and 15-18 Hz bands of the whitened LFP was used to separate theta and non-theta <lb/>states. All further analysis of the hippocampal place cells was constrained to theta-<lb/>associated periods. <lb/>Place cells analysis. Only hippocampal pyramidal cells with place fields that were <lb/>active in the arena were included in the analysis. Spike width and firing rate were <lb/>used to separate pyramidal cells from interneurons. In the sessions used in this paper, <lb/>309 of 367 cells were classified as putative pyramidal cells (Day 1: 166 of 168 cells; <lb/>Day 4: 143 of 182 cells). Place cells were defined as putative pyramidal units with a <lb/>place field peak firing rate of at least 3 Hz, having less than three spatially-separated <lb/>firing rate peaks in all trials, and maintaining a stable spike waveshape across all <lb/>sessions of the day. After filtering based on these selection criteria, 39 total pyramidal <lb/>cells for the two days (20 and 19 cells, respectively) remained. <lb/>Place fields were calculated based on a k-nearest neighbor algorithm, which selected <lb/>for periods in which the speed of the rat&apos;s head was greater than 5 cm/s and <lb/>intersected with periods of theta oscillation state. The k-nearest neighbor estimate of <lb/>the mean firing rate was calculated given the position of the rats head and each unit&apos;s <lb/>smoothed firing rate. The unit firing rate was smoothed using a 800 ms rectangular <lb/>window, convolved with the time-resolved spike histogram and downsampled to 30 <lb/>Hz. The maze was binned with 2 cm square bins. For each bin, the smoothed unit <lb/>firing rate was sorted by its distance to the bin center. The first 300 nearest-neighbor <lb/></div>

			<page>22 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<div type="annex">time bins were collected and averaged to derive the mean rate of that bin. Bins with <lb/>less than 300 neighbors within a radius of 12.5cm were assigned to be empty. This <lb/>procedure provides a data-adaptive and robust estimate of the spatial rate map in <lb/>contrast to conventional estimation methods (ratio between spatially smoothed spike <lb/>count and rat occupancy maps). Qualitatively, though, both measures gave the same <lb/>results. <lb/>The procedure used for place field map estimation has additional benefits, as it allows <lb/>robust estimation of the parameters of the place field based on the bootstrap <lb/>procedure. The variance of the place field center was estimated by bootstrapping each <lb/>unit&apos;s 1000 random subsets of two-second chunks of the rodent&apos;s trajectories (75% of <lb/>total Trial time). The place field center at each iteration was calculated by <lb/>thresholding the rate map by the firing rate at the 95 percentile of all iterations. All <lb/>bins above the threshold were assigned a 1, and all other values were assigned a 0. <lb/>The above-threshold bins were segmented using Matlab&apos;s bwboundaries function into <lb/>spatially contiguous patches, each of which represented a place field. The area, the <lb/>rate-weighted center of mass, and the maximum and minimum firing rate were <lb/>calculated for each patch. Only the main (largest and highest firing rate) field was <lb/>used for further analysis. The location of the peak rate within the patch was computed <lb/>for each bootstrap sample, and the resultant mean estimate was used as an unbiased <lb/>estimate of the x-y position of the center of the place field and used for the further <lb/>analysis of place field remapping. To quantify the effect of the VE shift on the place <lb/>fields of the active population of place cells, we computed the displacement of the <lb/>place field center between consecutive sessions (Normal to Shift, Shift to Normal <lb/>etc). The Kruskall-Wallis test was used to find an overall difference in population <lb/>means between sessions along each axis of the arena, and significant axes were <lb/>probed for individual differences between sessions using a Wilcoxon paired-rank test <lb/>with p-values corrected for multiple comparisons using Benjamini/Hochberg False <lb/>Discovery Rate method. <lb/></div>

			<listBibl>REFERENCES <lb/>1. Gibson, J. The senses considered as perceptual systems (Houghton Mifflin <lb/>Company, Boston, 1966). <lb/></listBibl>

			<page>23 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<listBibl>2. Kleinfeld, D., Deschênes, M., Wang, F. &amp; Moore, J. D. More than a rhythm of <lb/>life: breathing as a binder of orofacial sensation. Nat Neurosci 17, 647-651 (2014). <lb/>3. Morillon, B., Hackett, T. A., Kajikawa, Y. &amp; Schroeder, C. E. Predictive motor <lb/>control of sensory dynamics in auditory active sensing. Curr Opin Neurobiol 31, 230-<lb/>238 (2015). <lb/>4. Gegenfurtner, K. R. The Interaction Between Vision and Eye Movements. <lb/>Perception (2016). <lb/>5. Angelaki, D. E. &amp; Cullen, K. E. Vestibular system: the many facets of a <lb/>multimodal sense. Annu Rev Neurosci 31, 125-150 (2008). <lb/>6. Engel, A. K., Maye, A., Kurthen, M. &amp; König, P. Where&apos;s the action? The <lb/>pragmatic turn in cognitive science. Trends Cogn Sci 17, 202-209 (2013). <lb/>7. Adams, R. A., Shipp, S. &amp; Friston, K. J. Predictions not commands: active <lb/>inference in the motor system. Brain Structure and Function 218, 611-643 (2013). <lb/>8. Feldman, A. G. Referent control of action and perception: Challenging <lb/>conventional theories in behavioral neuroscience (Springer, 2015). <lb/>9. Schwarz, C., Hentschke, H., Butovas, S., Haiss, F., et al. The head-fixed behaving <lb/>rat--procedures and pitfalls. Somatosens Mot Res 27, 131-148 (2010). <lb/>10. Dombeck, D. A. &amp; Reiser, M. B. Real neuroscience in virtual worlds. Curr Opin <lb/>Neurobiol 22, 3-10 (2012). <lb/>11. Minderer, M., Harvey, C. D., Donato, F. &amp; Moser, E. I. Neuroscience: Virtual <lb/>reality explored. Nature 533, 324-325 (2016). <lb/>12. Guo, Z. V., Hires, S. A., Li, N., O&apos;Connor, D. H., et al. Procedures for behavioral <lb/>experiments in head-fixed mice. PLoS One 9, e88678 (2014). <lb/>13. Carandini, M. &amp; Churchland, A. K. Probing perceptual decisions in rodents. <lb/>Nature Neuroscience 16, 824-831 (2013). <lb/>14. Hölscher, C., Schnee, A., Dahmen, H., Setia, L. &amp; Mallot, H. A. Rats are able to <lb/>navigate in virtual environments. J Exp Biol 208, 561-569 (2005). <lb/>15. Jacobs, J., Weidemann, C. T., Miller, J. F., Solway, A., et al. Direct recordings <lb/>of grid-like neuronal activity in human spatial navigation. Nature neuroscience 16, <lb/>1188-1190 (2013). <lb/>16. Thurley, K., Henke, J., Hermann, J., Ludwig, B., et al. Mongolian gerbils learn <lb/>to navigate in complex virtual spaces. Behav Brain Res 266, 161-168 (2014). <lb/>17. Chen, G., King, J. A., Burgess, N. &amp; O&apos;Keefe, J. How vision and movement <lb/>combine in the hippocampal place code. Proc Natl Acad Sci U S A 110, 378-383 <lb/>(2013). <lb/>18. Schmidt-Hieber, C. &amp; Häusser, M. Cellular mechanisms of spatial navigation in <lb/>the medial entorhinal cortex. Nat Neurosci (2013). <lb/></listBibl>

			<page>24 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<listBibl>19. Domnisoru, C., Kinkhabwala, A. A. &amp; Tank, D. W. Membrane potential <lb/>dynamics of grid cells. Nature 495, 199-204 (2013). <lb/>20. Harvey, C. D., Coen, P. &amp; Tank, D. W. Choice-specific sequences in parietal <lb/>cortex during a virtual-navigation decision task. Nature 484, 62-68 (2012). <lb/>21. Aghajan, Z. M., Acharya, L., Moore, J. J., Cushman, J. D., et al. Impaired spatial <lb/>selectivity and intact phase precession in two-dimensional virtual reality. Nat <lb/>Neurosci (2014). <lb/>22. Cushman, J. D., Aharoni, D. B., Willers, B., Ravassard, P., et al. Multisensory <lb/>control of multimodal behavior: do the legs know what the tongue is doing? PLoS <lb/>One 8, e80465 (2013). <lb/>23. Aronov, D. &amp; Tank, D. W. Engagement of Neural Circuits Underlying 2D <lb/>Spatial Navigation in a Rodent Virtual Reality System. Neuron 84, 442-456 (2014). <lb/>24. Ravassard, P., Kees, A., Willers, B., Ho, D., et al. Multisensory Control of <lb/>Hippocampal Spatiotemporal Selectivity. Science (New York, N.Y.) 1342, 1342-1346 <lb/>(2013). <lb/>25. Thurley, K. &amp; Ayaz, A. Virtual reality systems for rodents. Current Zoology <lb/>zow070 (2016). <lb/>26. Scarfe, P. &amp; Glennerster, A. Using high-fidelity virtual reality to study <lb/>perception in freely moving observers. J Vis 15, 3 (2015). <lb/>27. Cruz-Neira, C., Sandin, D. J., DeFanti, T. A., Kenyon, R. V. &amp; Hart, J. C. The <lb/>CAVE: audio visual experience automatic virtual environment. Communications of <lb/>the ACM 35, 64-72 (1992). <lb/>28. Strauss, R., Schuster, S. &amp; Götz, K. G. Processing of artificial visual feedback in <lb/>the walking fruit fly Drosophila melanogaster. J Exp Biol 200, 1281-1296 (1997). <lb/>29. Fry, S. N., Rohrseitz, N., Straw, A. D. &amp; Dickinson, M. H. Visual control of <lb/>flight speed in Drosophila melanogaster. Journal of Experimental Biology 212, 1120-<lb/>1130 (2009). <lb/>30. Orger, M. B., Kampff, A. R., Severi, K. E., Bollmann, J. H. &amp; Engert, F. Control <lb/>of visually guided behavior by distinct populations of spinal projection neurons. Nat <lb/>Neurosci 11, 327-333 (2008). <lb/>31. Stowers, J. R., Fuhrmann, A., Hofbauer, M., Streinzer, M., et al. Reverse <lb/>engineering animal vision with virtual reality and genetics. Computer 47, 38-45 <lb/>(2014). <lb/>32. Ash, A., Palmisano, S. &amp; Kim, J. Vection in depth during consistent and <lb/>inconsistent multisensory stimulation. Perception 40, 155-174 (2011). <lb/>33. Kim, J., Chung, C. Y., Nakamura, S., Palmisano, S. &amp; Khuu, S. K. The Oculus <lb/>Rift: a cost-effective tool for studying visual-vestibular interactions in self-motion <lb/>perception. Front Psychol 6, 248 (2015). <lb/></listBibl>

			<page>25 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<listBibl>34. Hu, H. H., Gooch, A. A., Thompson, W. B., Smits, B. E., et al. Visual cues for <lb/>imminent object contact in realistic virtual environment. Proceedings of the <lb/>conference on Visualization&apos;00 179-185 (2000). <lb/>35. Walk, R. D. &amp; Gibson, E. J. A comparative and analytical study of visual depth <lb/>perception. Psychological Monographs: General and Applied 75, 1 (1961). <lb/>36. Barry, C., Lever, C., Hayman, R., Hartley, T., et al. The boundary vector cell <lb/>of place cell firing and spatial memory. Rev Neurosci 17, 71-97 (2006). <lb/>37. Aronov, D. &amp; Tank, D. W. Engagement of Neural Circuits Underlying 2D <lb/>Spatial Navigation in a Rodent Virtual Reality System. Neuron 84, 442-456 (2014). <lb/>38. Blaser, R. &amp; Heyser, C. Spontaneous object recognition: a promising approach to <lb/>the comparative study of memory. Front Behav Neurosci 9, 183 (2015). <lb/>39. Zoccolan, D. Invariant visual object recognition and shape processing in rats. <lb/>Behav Brain Res 285, 10-33 (2015). <lb/>40. Knierim, J. J. &amp; Hamilton, D. A. Framing spatial cognition: neural <lb/>representations of proximal and distal frames of reference and their roles in <lb/>navigation. Physiol Rev 91, 1245-1279 (2011). <lb/>41. Jeffery, K. J. &amp; O&apos;Keefe, J. M. Learned interaction of visual and idiothetic cues <lb/>in the control of place field orientation. Exp Brain Res 127, 151-161 (1999). <lb/>42. Geva-Sagiv, M., Romani, S., Las, L. &amp; Ulanovsky, N. Hippocampal global <lb/>remapping for different sensory modalities in flying bats. Nat Neurosci 19, 952-958 <lb/>(2016). <lb/>43. Hubona, G. S., Wheeler, P. N., Shirah, G. W. &amp; Brandt, M. The relative <lb/>contributions of stereo, lighting, and background scenes in promoting 3D depth <lb/>visualization. ACM Transactions on Computer-Human Interaction (TOCHI) 6, 214-<lb/>242 (1999). <lb/>44. Warren, W. H. Perception of space and motion 1995). <lb/>45. Taube, J. S. The head direction signal: origins and sensory-motor integration. <lb/>Annu Rev Neurosci 30, 181-207 (2007). <lb/>46. Smear, M., Resulaj, A., Zhang, J., Bozza, T. &amp; Rinberg, D. Multiple perceptible <lb/>signals from a single olfactory glomerulus. Nat Neurosci (2013). <lb/>47. Sofroniew, N. J., Vlasov, Y. A., Andrew Hires, S., Freeman, J. &amp; Svoboda, K. <lb/>Neural coding in barrel cortex during whisker-guided locomotion. Elife 4, (2015). <lb/>48. Seeber, B. U., Kerber, S. &amp; Hafter, E. R. A system to simulate and reproduce <lb/>audio-visual environments for spatial hearing research. Hear Res 260, 1-10 (2010). <lb/>49. Furtak, S. C., Cho, C. E., Kerr, K. M., Barredo, J. L., et al. The Floor Projection <lb/>Maze: A novel behavioral apparatus for presenting visual stimuli to rats. J Neurosci <lb/>Methods 181, 82-88 (2009). <lb/></listBibl>

			<page>26 <lb/></page>

			<note place="headnote">Del Grosso et al <lb/></note>

			<listBibl>50. Schaefer, A. T. &amp; Claridge-Chang, A. The surveillance state of behavioral <lb/>automation. Curr Opin Neurobiol 22, 170-176 (2012). <lb/>51. Jezek, K., Henriksen, E. J., Treves, A., Moser, E. I. &amp; Moser, M. B. Theta-paced <lb/>flickering between place-cell maps in the hippocampus. Nature (2011). <lb/>52. Bergeron, M., Lortie, C. L. &amp; Guitton, M. J. Use of Virtual Reality Tools for <lb/>Vestibular Disorders Rehabilitation: A Comprehensive Analysis. Adv Med 2015, <lb/>916735 (2015). <lb/>53. Fritz, N. E., Cheek, F. M. &amp; Nichols-Larsen, D. S. Motor-Cognitive Dual-Task <lb/>Training in Persons With Neurologic Disorders: A Systematic Review. J Neurol Phys <lb/>Ther 39, 142-153 (2015). <lb/>54. Di Luca, M. New Method to Measure End-to-End Delay of Virtual Reality. <lb/>Presence: Teleoperators and Virtual Environments 19, 569-584 (2010). <lb/>55. Sirota, A., Montgomery, S., Fujisawa, S., Isomura, Y., et al. Entrainment of <lb/>neocortical neurons and gamma oscillations by the hippocampal theta rhythm. Neuron <lb/>60, 683-697 (2008). <lb/>56. Harris, K. D., Henze, D. A., Csicsvari, J., Hirase, H. &amp; Buzsáki, G. Accuracy of <lb/>tetrode spike separation as determined by simultaneous intracellular and extracellular <lb/>measurements. J Neurophysiol 84, 401-414 (2000). <lb/>57. Hazan, L., Zugaro, M. &amp; Buzsáki, G. Klusters, NeuroScope, NDManager: a free <lb/>software suite for neurophysiological data processing and visualization. J Neurosci <lb/>Methods 155, 207-216 (2006). <lb/></listBibl>

			<body>a <lb/>a <lb/>b <lb/>c <lb/>d <lb/>Figure 1. Schematic of the ratCAVE VR components. (a) Projector-arena mapping scheme. A dot pattern is projected onto the <lb/>arena&apos;s surface, scanned using a multi-camera 3D tracking system. A digital model of the arena is then fitted to a 3D point cloud. <lb/>The projector&apos;s position with respect to arena and tracking system is calibrated in a similar manner. (b) The VE projection on <lb/>the arena surface, drawn from the perspective of the current position of the rodent&apos;s head, is then front-projected via the mirror <lb/>onto the arena surface using a high-speed (240 fps) projector. (c) The 3D position a freely-moving rodent&apos;s head is tracked by <lb/>means of head-mounted array of retro-reflective spheres using a multi-camera 3D tracking system, which is used as a feedback <lb/>signal for continuously updating the VE projection from the rodent&apos;s perspective (gray arrow). By rendering the virtual environ-<lb/>ment in a 360o arc about the rodent&apos;s head at a high frame rate (240 fps) and warping the image to match the arena&apos;s shape, <lb/>the rodent is given the illusion of a fully 3D virtual space (blue arrow). (d) A close-up rendering of a mouse&apos;s perspective inside <lb/>a virtual environment, overlooking a virtual cliff. <lb/></body>

			<note place="headnote">% Safe side jumps <lb/></note>

			<page>100 <lb/></page>

			<body>80 <lb/>90 <lb/>70 <lb/>60 <lb/>Exploration time <lb/>before jump (sec) <lb/>10 <lb/>30 <lb/>50 <lb/>70 <lb/>a <lb/>b <lb/>c <lb/>Head Dip <lb/>Jumping <lb/>Rearing <lb/>Unclassified <lb/>Cliff <lb/>side <lb/>Safe <lb/>side <lb/>Cliff <lb/>side <lb/>Safe <lb/>side <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>-0.5 <lb/>-1.0 <lb/>Jump discrimination index <lb/>d <lb/>13 cm <lb/>Figure 2. Virtual cliff avoidance experiment. (a) Virtual environment for the cliff avoidance test. The board was <lb/>suspended 13 cm over the arena floor in the center of the arena. One half of the arena emulated a virtual cliff, a virtual <lb/>floor that appeared to be 1.5 m below the arena&apos;s actual floor. (b) Example trajectory and segmentation of the rat&apos; <lb/>behavior in a single session, with three behaviors indicated by color. (c) Cumulative probability of jumps to safe side <lb/>as a function of exploration time before the jump. Shorter exploration times were associated with a safe-side <lb/>preference (see Supplementary Fig. 5). (d) Population jump direction statistics for sessions with jump latencies less <lb/>than 18 seconds, by location of the virtual cliff. Discrimination index quantifies bias to the left or right side of the arena <lb/>(see Online Methods); error bars represent a 68% confidence interval of bootstrapped means (n=25, p &lt; .05, Fisher&apos;s <lb/>two-sided exact test). <lb/>a <lb/>b <lb/>10 <lb/>30 <lb/>50 <lb/>70 <lb/>% Deflected <lb/>10 <lb/>30 <lb/>50 <lb/>70 <lb/>Width <lb/>Length <lb/>Control <lb/>Virtual Wall <lb/>% Deflected <lb/>* <lb/>c <lb/>* <lb/>Length <lb/>Width <lb/>Width <lb/>Length <lb/>Trajectory direction <lb/>d <lb/>Long wall <lb/>Short wall <lb/>Figure 3. Virtual wall interaction experiment. (a-b) Virtual environment layout. Virtual wall in the center of arena, <lb/>oriented along either the width (a, Long wall, first 5 min. of a session) or the length of arena (b, Short wall, last <lb/>5 min. of a session). (c) Proportion of trajectories deflected from the long virtual wall across directions of <lb/>trajectories analyzed (length and width) for session conditions with the virtual wall (median, session-wise data, <lb/>blue) and empty arena (control, whisker plot, black). Inset, examples of two types of trajectories: deflected <lb/>trajectory (blue) and crossing trajectory (red). (d) Same as c, for virtual wall across length axis. Session-wise <lb/>permutation test for significance of differences in proportions of deflected trajectories between VR (n=3 <lb/>sessions) and Control (n=11 sessions) conditions in c : p=0.006 in width and p=0.68 in length, in d: p=0.65 in <lb/>width and p=0.025 in length directions. <lb/>Object <lb/>Control <lb/>c <lb/>10 cm <lb/>Virtual <lb/>Real <lb/>a <lb/>b <lb/>Virtual <lb/>Real <lb/>% Deflections <lb/>Trajectories <lb/>-1 <lb/>-0.6 <lb/>-0.2 <lb/>0.2 <lb/>0.6 <lb/>1 <lb/>Discrimination index <lb/>Density <lb/>Occupancy <lb/>d <lb/>Control <lb/>Object <lb/>Control <lb/>Object <lb/>10 <lb/>30 <lb/>50 <lb/>70 <lb/>90 <lb/>** <lb/>*** <lb/>** <lb/>Location <lb/>Figure 4. Virtual object exploration experiment. (a) Design of the virtual environment. Three random virtual <lb/>objects were presented in a fixed relative position (&quot;center&quot;, &quot;corner&quot; and &quot;wall&quot;), but in a randomized arena <lb/>orientation. (b) Object interaction analysis for the center object. Group statistics (whisker plot and individual data <lb/>points, n=17 sessions) across sessions for different measures of object exploration: occupancy, occupancy <lb/>density and number of trajectories converging to the center object, all expressed as a discrimination ratio <lb/>between object and sham locations away from the object (dotted gray circles in a) (n=17, Wilcoxon signed-rank <lb/>test, ** p&lt;.01, *** p&lt;.001). (c) All locomotion trajectories (gray) in the vicinity (10cm radius) of the virtual (left two <lb/>plots) and real (right two plots) objects for object and sham locations. Examples of crossing (black lines) and <lb/>deflection (blue lines) trajectories are shown. Trajectories have been rotated to align entry points with the bottom <lb/>of the circle. Trajectories approaching within 3 cm to the object and departing at a less-than-90-degree arc from <lb/>their approach vector were qualified as &quot;deflections&quot; (Supplementary Fig. 8d, Online Methods). (d) Population <lb/>statistics of deflection trajectories for all conditions in c), session-wise bootstrap mean (bars) and standard <lb/>deviation (error bars) of the percentage of deflected trajectories. Deflections were significantly more likely for <lb/>object than sham locations exploration in both virtual (n-VRobj=72, n-sham=41, p&lt; .02) and real sessions <lb/>(n-obj=22, n-sham=16, p&lt; .005). See Online Methods and Supplementary Fig. 8d for more information. <lb/>&lt;-Normal to virtual <lb/>x axis place cell shift (cm) <lb/>y axis place cell shift (cm) <lb/>0 20 <lb/>-20 <lb/>y Axis place cell shift (cm) <lb/>Star field <lb/>Virtual to normal -&gt; <lb/>x Axis place cell shift (cm) <lb/>0 20 <lb/>-20 <lb/>-20 <lb/>0 <lb/>20 <lb/>-20 <lb/>0 <lb/>20 <lb/>-20 <lb/>0 <lb/>20 <lb/>&lt;-Virtual to normal <lb/>-20 <lb/>0 <lb/>20 <lb/>-20 <lb/>0 <lb/>20 <lb/>&lt;-Normal to virtual <lb/>Normal <lb/>Shifted <lb/>Star field <lb/>20 cm <lb/>a <lb/>Unit 1 <lb/>Unit 2 <lb/>Unit 3 <lb/>b <lb/>c <lb/>Normal 1 <lb/>Shifted 1 <lb/>Normal 2 <lb/>Shifted 2 <lb/>60 cm <lb/>0 Hz <lb/>8 Hz <lb/>Firing rate <lb/>Unit 4 <lb/>Unit 5 <lb/>Unit 6 <lb/>Normal <lb/>Shifted <lb/>Star field <lb/>0 Hz <lb/>15 Hz <lb/>Firing rate <lb/>60 cm <lb/>d <lb/>e <lb/>Figure 5. Impact of VR environment on the hippocampal spatial representation. (a) Schematics of the virtual <lb/>environments used in the experiment. Normal (left) physical boundaries of the arena and virtual boundaries of the VE <lb/>are aligned; Shifted (center) VE is displaced leftwards by 20 cm; Star Field (right), no virtual boundaries, 3D array of <lb/>white virtual cubes expanding beyond arena walls. (b) Individual examples of place fields of hippocampal pyramidal <lb/>cells showing the center position of the field (asterisk) across four sequential conditions: Normal 1 -Shifted 1 -Normal <lb/>2 -Shifted 2. The center of the virtual arena is shown as blue bars for reference. White numbers indicate the peak <lb/>firing rate of the cell (spikes/s). (c) Analysis of the place field center shift between conditions. Scatterplots showing X-<lb/>and Y-axis shift of the location of center place fields across conditions: Normal 1 to Shifted 1 (top), Shifted 1 to Normal <lb/>2 (middle), Normal 2 to Shifted 2 (bottom). Gray shadows, 95% confidence interval of population shift estimate (n=20). <lb/>Non-overlap of gray bar with dotted line indicates a significant place field shift in the given axis. (d) Examples, as in b, <lb/>for different units recorded three days later across conditions: Normal-Shifted-Star field. e) Same as c for conditions <lb/>in d. Shift of place field centers between Normal and Shifted conditions was significant for length, but not width <lb/>directions (Kruskall-Wallis n= 20, H=35.40, p &lt;.001 and H=5.92, p=.21, length and width, correspondingly) and <lb/>between consecutive session shifts were tested with post hoc Wilcoxon paired-rank test (n=20, in c : Normal 1 to Shift <lb/>1, W=12, p &lt; .01; Shift 1 to Normal 2, W=15, p &lt; .01; Normal 2 to Shift 2, W=56, p=.11; in d: Normal to Shifted , W=90, <lb/>p=.67; Shifted to Star field, W=97, p=.77). <lb/> 1000 1200 1400 1600 1800 <lb/>-300 <lb/>-200 <lb/> -100 <lb/>0 <lb/>100 <lb/>3000 <lb/>3500 <lb/>4000 <lb/>4500 <lb/>time (msec) <lb/>-1 <lb/>-0.5 <lb/>0 <lb/>0.5 <lb/>1 <lb/>x position (a.u.) <lb/>reference <lb/>VR <lb/>100 <lb/>200 <lb/>300 <lb/>400 <lb/>500 <lb/>speed (cm/s) <lb/>14 <lb/>16 <lb/>18 <lb/>20 <lb/>22 <lb/>time lag (msec) <lb/>x position <lb/>y position <lb/>a <lb/>b <lb/>d <lb/>Rat head velocity (cm/sec) <lb/></body>

			<page>10 20 30 40 50 60 <lb/></page>

			<div type="annex">c <lb/>Supplementary Figure 1. Motion-to-photon lag measurement of ratCAVE system. (a) Raw image of the latency <lb/>data collection procedure. A tracked object (reference point, left spot, orange) and its x-axis offset <lb/>VR-represented projection (virtual point, right spot, blue) were recorded using high-speed camera as the <lb/>reference point was rotated about a central point at different speeds. Arrows show the direction and shape of the <lb/>reference and virtual points&apos; trajectories. (b) Example of the time courses of the x-axis project of the reference <lb/>(blue) and virtual (red) points. Inset, magnification of a section of time course. Note the delay between the two <lb/>time courses, scale bar 10 msec. (c) Time lag between the reference and virtual points as a function of linear <lb/>(tangential) velocity of the reference point motion. Note slow increase of time lag with speed of the reference <lb/>point above 100 cm/s, with a minimum latency of 15 msecs in the range of the head velocity of rats. (d) <lb/>Distribution of head velocity in rats, note that all movement are contained within 60cm/sec (doted line in c). <lb/>b <lb/>d <lb/>a <lb/>c <lb/>Supplementary Figure 2. Cube-mapping and image rendering. (a-b) Schematic of the image-warping <lb/>transformation of the rat&apos;s perspective view of the virtual environment to the projection on the arena. The image <lb/>warping algorithm involves three steps: (a) The virtual world (consisting, in this example, of four colored 3D objects) <lb/>is rendered 360 degrees about the rat&apos;s head position on the faces of the cube using a cube-mapping algorithm, (b) <lb/>each wall&apos;s relative position to the rat is mapped to this 3D virtual world,and (c) all arena surfaces (walls and floor) <lb/>are then warped from the perspective of a video projector mounted above the arena. This process is repeated every <lb/>frame, maintaining the VR-rodent-arena despite movement of virtual objects, the rat, or the arena itself. (d) 3D <lb/>lighting algorithms employed by ratCAVE to increase spatial visual cues and visual richness of the virtual <lb/>environment. Improvements are successively applied to the object, from left to right. First, diffuse reflections <lb/>increase object brightness on parts of the object facing the light. Second, high-resolution objects are used, with <lb/>smoothed surfaces, to further increase object detail. Third, specular reflections are added to provide <lb/>subject-object-light triangulation cues. Finally, shadows are added to provide inter-object distance cues. <lb/>a <lb/>b <lb/>c <lb/>d <lb/>e <lb/>Supplementary Figure 3. RatCAVE hardware-software components flowchart. Each component, depicted as vertical <lb/>parenthesis, takes information from one source and sends information to another source; information flow is depicted in <lb/>direction of arrows. Detailed operations of each component are depicted as blocks, and software components are labeled by <lb/>letter. (a) Blender 3D. The virtual environment is created before the experiment using 3D modeling software (right-center <lb/>module) for loading into the VR experiment script. (Gray Zone) Tracking and Setup Coregistration. A Multi-camera array <lb/>sends imaging data of the rodent&apos;s position on each camera to 3D tracking software, which combines the data from each <lb/>camera image into a single 3D location and sends that position to the ratCAVE environment (left, &quot;Optical 3D Tracking <lb/>System&quot;). (b) MotivePy. The cameras&apos; settings can be modified directly in a Python environment to make visible-light collection <lb/>possible, a necessary step for arena scanning and projector calibration. (c) ratcave_calibrate. Two command-line programs <lb/>are used for arena scanning and projector calibration. The arena scanning program projects a moving grid of white dots on the <lb/>arena surface, collects the 3D positions of the projected points via the camera array, and fits the resultant point cloud to a 3D <lb/>mesh model of the arena. The projector calibration program maps single points displayed from the projector onto the 3D <lb/>position of the arena, one at a time. It then uses OpenCV&apos;s camera_calibrate tool to use these mappings to find the position <lb/>of the projector in the camera array&apos;s coordinate space. (Blue Zone) VR Engine. (d) NatNetClient. Rat position data is collected <lb/>in real-time from the camera array and brought into the Python environment, for use in VR experiment scripts. (e) Fruitloop. <lb/>The virtual environment (VE) is rendered in a Python 3D graphics engine. The VE is loaded from file (created in Blender 3D), <lb/>and on each display frame, using the rodent position data to move a virtual camera to the rodent&apos;s position in a virtual <lb/>environment (blue zone). This process, encompassing the core of the VR engine, (get rodent position, move camera, update <lb/>and render scene) occurs in a loop, repeated each frame, with the frames themselves sent to the GPU for arena mapping and <lb/>shading (examples on Supp. Figure 1d) and then to the video projector (bottom-right corner). See the &quot;Software&quot; section in <lb/>Online Methods for more details. <lb/>a <lb/>b <lb/>c <lb/>Supplementary Figure 4. Photographs of the ratCAVE setup. (a) Photograph of the full system showing <lb/>arena, projector, mirror, cameras (our system uses twelve cameras, arranged about the recording chamber <lb/>and above the arena; only five (shown with blue lighted rings, normally turned off) are visible here). (b) <lb/>Close-up on the projector, mirror, and cameras. (c) Close-up on the arena showing retro-reflective markers <lb/>attached; the increased brightness of the markers is created in the photo by the camera&apos;s flash, and is <lb/>brightened during VR sessions by the infra-red lighting of the tracking cameras. This increased brightness is <lb/>beyond the visual spectrum of rodent vision. <lb/>Head height (normalized) <lb/>-20 <lb/>Head height (cm) <lb/>Correct jump side <lb/>Time to landing (sec) <lb/>Exploration time before jump (sec) <lb/>Position (cm) Cliff side <lb/>Safe side <lb/>-15 -10 -5 0 <lb/>5 10 15 20 <lb/>46 <lb/>42 <lb/>38 <lb/>34 <lb/>30 <lb/>1.6 <lb/>1.2 <lb/>0.8 <lb/>0.4 <lb/>0 <lb/>-0.4 <lb/>-0.2 <lb/>0 <lb/>0.2 <lb/>-0.4 <lb/>True <lb/>False <lb/>0 10 20 30 40 50 60 70 <lb/>Evening <lb/>Midday <lb/>Cliff side jumps <lb/>Safe side jumps <lb/>Time (sec) <lb/>Session number <lb/>20 <lb/>15 <lb/>10 <lb/>5 <lb/>0 <lb/>10 20 30 40 50 60 70 80 <lb/>a <lb/>b <lb/>c <lb/>d <lb/>Safe side dips <lb/>Cliff side dips <lb/>Supplementary Figure 5. Visual cliff behavior trajectories. (a) Head-dipping and jumping events for each <lb/>session. Rodents peeked over the edge of the board several times before choosing a side to jump off <lb/>(example shown in Figure 2b), demonstrating risk assessment or exploration behavior. Head-dip events <lb/>(rectangle markers) and jumps (circles) over time for each session. (b) Jump trajectories. Temporal <lb/>dynamics of the rat&apos;s head height (scaled by maximum height of jump over time relative to landing) is plotted <lb/>as a function of time centered on the landing time. Individual jumps (gray lines), mean (black line) and <lb/>standard deviation (gray shadow) are shown. (c) Head-dipping trajectories during rats&apos; visual inspection of <lb/>the arena floors from the board (board cross-section shown in black, x-axis flipped with cliff on right side). <lb/>We found no significant relationship between statistics of the head-dips and decision side. (d) Factors <lb/>affecting jump side preference. Jump decision as a function of exploration time before jump. Note the <lb/>difference in accuracy between first and second sessions recorded each day. Logistic regression found a <lb/>significant correlation between exploration time and safe side preference (b = -.06, p &lt; .05, solid black line, <lb/>68% CI as gray shading). Evening sessions with jump latencies greater than 18 secs (7 out of 33 sessions) <lb/>were, as a result, excluded from the analysis of jump preference. <lb/>Head height (cm) <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>5 <lb/>Behavioral state <lb/>Rearing Running Standing <lb/>Rat position along width (cm) <lb/>Rearing <lb/>Walking <lb/>Standing <lb/>Long wall <lb/>Control <lb/>-40 <lb/>0.0 <lb/>40 <lb/>-40 <lb/>0.0 <lb/>40 <lb/>-40 <lb/>0.0 <lb/>40 <lb/>a <lb/>b <lb/>Supplementary Figure 6. Virtual wall interaction. (a) Distribution of head height across di erent behav-<lb/>ioral states (see Online Methods). (b) Occupancy along the arena width across di erent behaviors, in <lb/>control and virtual wall conditions. Note that increased occupancy around the virtual wall location (arena <lb/>center) occurs only when the virtual wall was present. <lb/>a <lb/>b <lb/>0 10 20 30 40 50 60 70 <lb/>0 10 20 30 40 50 60 70 <lb/>Trajectory angle from virtual wall (rad) <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>Distance from virtual wall (cm) <lb/>Distance from virtual wall (cm) <lb/>Wall <lb/>Control <lb/>1.5 <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>1.5 <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>1.5 <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>1.5 <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>Trajectory angle from virtual wall (rad) <lb/>c <lb/>Supplementary Figure 7. Thigmotaxis-like behavior along the virtual walls. (a) Example of a long <lb/>trajectory parallel to the virtual wall. (b) Joint probability density of the orientation of locomotion <lb/>trajectories with respect to the virtual wall (y axis, 0 rad means parallel to virtual wall) and distance <lb/>to the location of the long virtual wall along arena width (x-axis) for sessions with the wall (left) and <lb/>control sessions without a wall (right). Location of virtual (blue) and real arena (black) walls is <lb/>indicated by thick lines on the plot. (c) Same as b for the short wall. Note the mode at 0 radians close <lb/>to the wall (&lt;10cm from virtual wall), resembling thigmotaxis behavior near the real walls for both <lb/>conditions. The mode close to 1.5 rad corresponds both to trajectories that &quot;deflect from&quot; and those <lb/>that &quot;cross&quot; the virtual wall (Fig. 3). <lb/>-1 <lb/>-0.6 <lb/>-0.2 <lb/>0.2 <lb/>0.6 <lb/>1 <lb/>Wall <lb/>Corner <lb/>Discrimination index <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1 <lb/>Sham <lb/>VR Obj Sham <lb/>VR Obj Sham <lb/>VR Obj <lb/>3 cm <lb/>In <lb/>Out <lb/>% Inner trajectories <lb/>Trajectories <lb/>10 cm <lb/>Distance from object (cm) <lb/>b <lb/>c <lb/>d <lb/>Center <lb/>Wall <lb/>Corner <lb/>-50 <lb/>-30 <lb/>-10 <lb/>10 <lb/>30 <lb/>50 <lb/>0.030 <lb/>0.035 <lb/>0.040 <lb/>0.045 <lb/>0.050 <lb/>0.055 <lb/>e <lb/>0.5 <lb/>1 <lb/>1.5 <lb/>2 <lb/>2.5 <lb/>1 <lb/>3 <lb/>5 <lb/>7 <lb/>9 <lb/>0.2 0.4 0.6 0.8 <lb/>1 <lb/>3 <lb/>5 <lb/>7 <lb/>9 <lb/>0.5 <lb/>1 <lb/>1.5 <lb/>2 <lb/>2.5 <lb/>1 <lb/>3 <lb/>5 <lb/>7 <lb/>9 <lb/>0.2 0.4 0.6 0.8 <lb/>1 <lb/>3 <lb/>5 <lb/>7 <lb/>9 <lb/>VR <lb/>Real <lb/>Deflection angle (rad) <lb/>Proportion <lb/>** <lb/>* <lb/>Center <lb/>Object <lb/>Sham <lb/>Probability <lb/>Arena length (cm) <lb/>a <lb/>Supplementary Figure 8. Object exploration analysis. (a) Virtual objects used in the task. Object shape and coloring were <lb/>randomized between sessions. (b) Fine direction of trajectories towards virtual objects. Diagram on the left shows the <lb/>areas used to calculate fine trajectories: percentage between inner (3 cm) and outer (10 cm) radius from the center of <lb/>objects where consider as trajectories. Sham (dotted circles) and virtual object locations (filled circles) used in the <lb/>calculations are shown for all objects (top). Percentage of inner trajectories for sham (Sham) and virtual object (VR Obj) <lb/>locations (c) Discriminatory index from same data shown in b, significant discrimination index is observed only in center <lb/>and wall object (Center: Z=2.889, n=17, ** p&lt;.01; Wall: Z=2.130, n=17, * p&lt;.05; Corner Z=1.008, n=17, p=.08, Wilcoxon <lb/>sign-rank test). (d) Deflection analysis for trajectories around the object. Scatter plots showing the relationship of arc angle <lb/>made by trajectories entering and leaving the 10 cm circle around the object (deflection angle) and shortest distance <lb/>between the trajectory to the object, for virtual (&quot;VR&quot;, top) and real (&quot;Real&quot;, bottom) objects for sham locations (blue) and <lb/>object locations (red). Note that there is a concentration of trajectories in the vicinity of the object (~3 cm) with low arc angle <lb/>value (&lt;1.56 rad, 90 degrees), indicated by a dotted rectangle . These trajectories were used in the analysis in Fig. 4D, as <lb/>they corresponded to trajectories that deflected from the object. Right column, cumulative proportion of deflecting <lb/>trajectories (&lt;1.56 rad) as function of distance from the object. Distances showing significant difference between object <lb/>and sham conditions are indicated by thick black line on the right (p&lt;.05, p-value calculated from a permutation distribution <lb/>shuffled between conditions). (e) Example of the occupancy (projected on arena length) distribution of empty arena for the <lb/>trials that followed object exploration trials, note the peaks that appear at the center of the arena (0, location of the center <lb/>object), +/-30cm (location of the wall object), and +/-50 cm (location of the corner object), indicating a bias of rat behavior <lb/>by potential spatial memory of the previous virtual object experience in these locations. Error bars above the modes at 0, <lb/>30 and 50cm show bootstrap confidence intervals of the mode position computed across control sessions. </div>

	</text>
</tei>
