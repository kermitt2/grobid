<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Proceedings of the Workshop on BioNLP: Shared Task, pages 19–27, <lb/>Boulder, Colorado, June 2009. c <lb/> 2009 Association for Computational Linguistics <lb/> Event Extraction from Trimmed Dependency Graphs <lb/> Ekaterina Buyko, Erik Faessler, Joachim Wermter and Udo Hahn <lb/> Jena University Language &amp; Information Engineering (JULIE) Lab <lb/>Friedrich-Schiller-Universität Jena <lb/>Fürstengraben 30, 07743 Jena, Germany <lb/> {ekaterina.buyko|erik.faessler|joachim.wermter|udo.hahn}@uni-jena.de <lb/> Abstract <lb/> We describe the approach to event extrac-<lb/>tion which the JULIELab Team from FSU <lb/>Jena (Germany) pursued to solve Task 1 in <lb/>the &quot; BioNLP&apos;09 Shared Task on Event Ex-<lb/>traction &quot; . We incorporate manually curated <lb/>dictionaries and machine learning method-<lb/>ologies to sort out associated event triggers <lb/>and arguments on trimmed dependency graph <lb/>structures. Trimming combines pruning ir-<lb/>relevant lexical material from a dependency <lb/>graph and decorating particularly relevant lex-<lb/>ical material from that graph with more ab-<lb/>stract conceptual class information. Given <lb/>that methodological framework, the JULIELab <lb/>Team scored on 2nd rank among 24 competing <lb/>teams, with 45.8% precision, 47.5% recall and <lb/>46.7% F1-score on all 3,182 events. <lb/></front>

			<body> 1 Introduction <lb/> Semantic forms of text analytics for the life sciences <lb/>have long been equivalent with named entity recog-<lb/>nition and interpretation, i.e., finding instances of se-<lb/>mantic classes such as proteins, diseases, or drugs. <lb/>For a couple of years, this focus has been comple-<lb/>mented by analytics dealing with relation extraction, <lb/>i.e., finding instances of relations which link one or <lb/>more (usually two) arguments, the latter being in-<lb/>stances of semantic classes, such as the interaction <lb/>between two proteins (PPIs). <lb/>PPI extraction is a complex task since cascades <lb/>of molecular events are involved which are hard to <lb/>sort out. Many different approaches have already <lb/>been tried – pattern-based ones (e.g., by Blaschke <lb/>et al. (1999), Hakenberg et al. (2005) or Huang et <lb/>al. (2004)), rule-based ones (e.g., by Yakushiji et al. <lb/>(2001), ˇ <lb/>Sari´ét al. (2004) or Fundel et al. (2007)), <lb/>and machine learning-based ones (e.g., by Katrenko <lb/>and Adriaans (2006), Saetre et al. (2007) or Airola et <lb/>al. (2008)), yet without conclusive results. <lb/>In the following, we present our approach to solve <lb/>Task 1 within the &quot; BioNLP&apos;09 Shared Task on Event <lb/>Extraction &quot; . 1 Task 1 &quot; Event detection and charac-<lb/>terization &quot; required to determine the intended rela-<lb/>tion given a priori supplied protein annotations. Our <lb/>approach considers dependency graphs as the cen-<lb/>tral data structure on which various trimming oper-<lb/>ations are performed involving syntactic simplifica-<lb/>tion but also, even more important, semantic enrich-<lb/>ment by conceptual overlays. A description of the <lb/>component subtasks is provided in Section 2, while <lb/>the methodologies intended to solve each subtask <lb/>are discussed in Section 3. The system pipeline for <lb/>event extraction reflecting the task decomposition is <lb/>described in Section 4, while Section 5 provides the <lb/>evaluation results for our approach. <lb/> 2 Event Extraction Task <lb/> Event extraction is a complex task that can be sub-<lb/>divided into a number of subtasks depending on <lb/>whether the focus is on the event itself or on the ar-<lb/>guments involved: <lb/> Event trigger identification deals with the large <lb/>variety of alternative verbalizations of the same <lb/>event type, i.e., whether the event is expressed in <lb/>

			<note place="footnote"> 1  http://www-tsujii.is.s.u-tokyo.ac.jp/ <lb/> GENIA/SharedTask/ <lb/></note>

			<page> 19 <lb/></page>

			a verbal or in a nominalized form (e.g., &quot; A is ex-<lb/>pressed &quot;  and &quot; the expression of A &quot;  both refer to the <lb/>same event type, viz. expression(A)). Since the <lb/>same trigger may stand for more than one event type, <lb/>event trigger ambiguity has to be resolved as well. <lb/> Event trigger disambiguation selects the correct <lb/>event name from the set of alternative event triggers. <lb/> Event typing, finally, deals with the semantic <lb/>classification of a disambiguated event name and the <lb/>assignment to an event type category. 2 <lb/> Argument identification is concerned with find-<lb/>ing all necessary participants in an event, i.e., the <lb/>arguments of the relation. <lb/> Argument typing assigns the correct semantic <lb/>category (entity class) to each of the determined par-<lb/>ticipants in an event (which can be considered as in-<lb/>stances of that class). <lb/> Argument ordering assigns each identified par-<lb/>ticipant its functional role within the event, mostly <lb/>Agent (and Patient/Theme). <lb/>The sentence &quot; Regulation of jun and fos gene ex-<lb/>pression in human monocytes by the macrophage <lb/>colony-stimulating factor &quot; , e.g., contains mentions <lb/>of two Gene Expression events with respective <lb/>THEME arguments &quot; jun &quot; and &quot; fos &quot; , triggered in the <lb/>text by the literal phrase &quot; gene expression &quot; . <lb/> Task 1 of the &quot; BioNLP&apos;09 Shared Task on Event <lb/>Extraction &quot; was defined in such a way as to iden-<lb/>tify a proper relation (event) name and link it with <lb/>its type, plus one or more associated arguments de-<lb/>noting proteins. To focus on relation extraction only <lb/>no automatic named entity recognition and interpre-<lb/>tation had to be performed (subtask &apos;argument typ-<lb/>ing&apos; from above); instead candidate proteins were <lb/>already pre-tagged. The complexity of Task 1 was <lb/>raised by the condition that not only proteins were <lb/>allowed to be arguments but also were events. <lb/> 3 Event Extraction Solution <lb/> Our event extraction approach is summarized in Fig-<lb/>ure 1 and consists of three major streams – first, the <lb/>detection of lexicalized event triggers (cf. Section <lb/>3.1), second, the trimming of dependency graphs <lb/>which involves pruning irrelevant and semantically <lb/>enriching relevant lexical material (cf. Section 3.2), <lb/>

			<note place="footnote">2 In our approach, event trigger disambiguation already im-<lb/>plies event typing. <lb/></note>

			Pre-processing <lb/> Argument Identification with <lb/>Ensemble of Classifiers <lb/> Event Detection <lb/> Post-processing <lb/>Trimming of <lb/>Dependency Graphs <lb/>Typing of Putative <lb/>Event Triggers <lb/> Figure 1: General Architecture of the Event Extraction <lb/>Solution of the JULIELab Team. <lb/> and, third, the identification of arguments for the <lb/>event under scrutiny (cf. Section 3.3). Event typ-<lb/>ing results from proper event trigger identification <lb/>(see Section 3.1.2), which is interlinked with the out-<lb/>come of the argument identification. We talk about <lb/> putative triggers because we consider, in a greedy <lb/>manner, all relevant lexical items (see Section 3.1.1) <lb/>as potential event triggers which might represent an <lb/>event. Only those event triggers that can eventually <lb/>be connected to arguments, finally, represent a true <lb/>event. To achieve this goal we preprocessed both the <lb/>original training and test data such that we enrich the <lb/>original training data with automatically predicted <lb/>event triggers in order to generate more negative ex-<lb/>amples for a more effective learning of true events. 3 <lb/> 3.1 Event Trigger Identification <lb/> Looking at the wide variety of potential lexicalized <lb/>triggers for an event, their lacking discriminative <lb/>power relative to individual event types and their <lb/>inherent potential for ambiguity, 4 we decided on <lb/>a dictionary-based approach whose curation princi-<lb/>ples are described in Section 3.1.1. Our disambigua-<lb/>tion policy for the ambiguous lexicalized event trig-<lb/>

			<note place="footnote">3 Although the training data contains cross-sentence event <lb/>descriptions, our approach to event extraction is restricted to <lb/>the sentence level only. <lb/></note> 
			
			<note place="footnote">4 Most of the triggers are neither specific for molecular event <lb/>descriptions, in general, nor for a special event type. &quot; Induc-<lb/>tion &quot; , e.g., occurs 417 times in the training data. In 162 of these <lb/>cases it acts as a trigger for Positive regulation, 6 times as a <lb/>trigger for Transcription, 8 instances trigger Gene expression, <lb/> while 241 occurrences do not trigger an event at all. <lb/></note>

			<page> 20 <lb/></page>

			gers assembled in this suite of dictionaries, one per <lb/>event type, is discussed in Section 3.1.2. <lb/> 3.1.1 Manual Curation of the Dictionaries <lb/> We started collecting our dictionaries from the <lb/>original GENIA event corpus (Kim et al., 2008a). <lb/>The extracted event triggers were then automatically <lb/>lemmatized 5 and the resulting lemmata were subse-<lb/>quently ranked by two students of biology according <lb/>to their predictive power to act as a trigger for a par-<lb/>ticular event type. This expert assessment led us to <lb/>four trigger groups (for each event type these groups <lb/>were determined separately): <lb/>(1) Triggers are important and discriminative for <lb/>a specific event type. This group contains event trig-<lb/>gers such as &quot; upregulate &quot; for Positive regulation. <lb/> (2) Triggers are important though not fully dis-<lb/>criminative for a particular event type; yet, this defi-<lb/>ciency can be overcome by other lexical cues within <lb/>the context of the same sentence. This group with in-<lb/>context disambiguators contains lexical items such <lb/>as &quot; proteolyse &quot; for Protein catabolism. <lb/> (3) Triggers are non-discriminative for an event <lb/>type and even cannot be disambiguated by linguistic <lb/>cues within the context of the same sentence. This <lb/>group contains lexical items such as &quot; presence &quot; for <lb/> Localization and Gene expression. <lb/> (4) Triggers are absolutely non-discriminative for <lb/>an event. This group holds general lexical triggers <lb/>such as &quot; observe &quot; , &quot; demonstrate &quot; or &quot; function &quot; . <lb/>The final dictionaries used for the detection of <lb/>putative event triggers are a union of the first two <lb/>groups. They were further extended by biologists <lb/>with additional lexical material of the first group. <lb/>The dictionaries thus became event type-specific – <lb/>they contain all morphological forms of the original <lb/>lemma, which were automatically generated using <lb/>the Specialist NLP Tools (2008 release). <lb/>We matched the entries from the final set of dic-<lb/>tionaries with the shared task data using the Ling-<lb/>pipe Dictionary Chunker. 6 After the matching pro-<lb/>cess, some cleansing had to be done. 7 <lb/>

			<note place="footnote">5 We used the lemmatizer from the Specialist NLP Tools <lb/>(http://lexsrv3.nlm.nih.gov/SPECIALIST/ <lb/> index.html, 2008 release). <lb/></note> 
			
			<note place="footnote">6  http://alias-i.com/lingpipe/ <lb/></note> 
			
			<note place="footnote">7 Event triggers were removed which (1) were found within <lb/>sentences without any protein annotations, (2) occurred within <lb/></note>

			3.1.2 Event Trigger Disambiguation <lb/> Preliminary experiments indicated that the dis-<lb/>ambiguation of event triggers might be beneficial <lb/>for the overall event extraction results since events <lb/>tend to be expressed via highly ambiguous triggers. <lb/>Therefore, we performed a disambiguation step pre-<lb/>ceding the extraction of any argument structures. <lb/>It is based on the importance of an event trig-<lb/>ger t  i  for a particular event type T as defined by <lb/> Imp(t  T <lb/>i  ) := <lb/> f (t  T <lb/> i  ) <lb/> P <lb/> i  f (t  T <lb/>i  ) <lb/> , where f (t  T <lb/>i  ) is the frequency <lb/>of the event trigger t  i  of the selected event type T <lb/> in a training corpus divided by the total amount of <lb/>all event triggers of the selected event type T in <lb/>that training corpus. The frequencies are measured <lb/>on stemmed event triggers. For example, Imp for <lb/>the trigger stem &quot; depend &quot; amounts to 0.013 for the <lb/>event type Positive regulation, while for the event <lb/>type Regulation it yields 0.036 . If a text span con-<lb/>tains several event triggers with the same span off-<lb/>set, the event trigger with max(Imp) is selected and <lb/>other putative triggers are discarded. The trigger <lb/>stem &quot; depend &quot; remains thus only for Regulation. <lb/> 3.2 Trimming Dependency Graphs <lb/> When we consider event (relation) extraction as a se-<lb/>mantic interpretation task, plain dependency graphs <lb/>as they result from deep syntactic parsing might not <lb/>be appropriate to directly extract semantic informa-<lb/>tion from. This is due to two reasons -they contain <lb/>a lot of apparently irrelevant lexical nodes (from the <lb/>semantic perspective of event extraction) and they <lb/>also contain much too specific lexical nodes that <lb/>might better be grouped and further enriched se-<lb/>mantically. Trimming dependency graphs for the <lb/>purposes of event extraction, therefore, amounts to <lb/>eliminate semantically irrelevant and to semantically <lb/>enrich relevant lexical nodes (i.e., overlay with con-<lb/>cepts). This way, we influence the final representa-<lb/>tion for the machine learners we employ (in terms of <lb/>features or kernel-based representations) — we may <lb/>avoid an overfitting of the feature or kernel spaces <lb/>with syntactic and lexical data and thus reduce struc-<lb/>tural information in a linguistically motivated way. <lb/>

			<note place="footnote"> a longer event trigger, (3) overlapped with a longer trigger of <lb/>the same event type, (4) occurred inside an entity mention an-<lb/>notation. <lb/></note>

			<page> 21 <lb/></page>

			3.2.1 Syntactic Pruning <lb/> Pruning targets auxiliary and modal verbs which <lb/>govern the main verb in syntactic structures such as <lb/>passives, past or future tense. We delete the aux-<lb/>iliars/modals as govenors of the main verbs from <lb/>the dependency graph and propagate the semantics-<lb/>preserving dependency relations of these nodes di-<lb/>rectly to the main verbs. Adhering to the depen-<lb/>dency tree format and labeling conventions set up <lb/>for the 2006 and 2007 CONLL shared tasks on de-<lb/>pendency parsing main verbs are usually connected <lb/>with the auxiliar by the VC dependency relation (see <lb/>Figure 2). Accordingly, in our example, the verb <lb/> &quot; activate &quot; is promoted to the ROOT in the depen-<lb/>dency graph and governs all nodes that were origi-<lb/>nally governed by the modal &quot; may &quot; . <lb/> Figure 2: Trimming of Dependency Graphs. <lb/> 3.2.2 Conceptual Decoration <lb/> Lexical nodes in the (possibly pruned) depen-<lb/>dency graphs deemed to be important for argument <lb/>extraction were then enriched with semantic class <lb/>annotations, instead of keeping the original lexical <lb/>(stem) representation (see Figure 2). The rationale <lb/>behind this decision was to generate more powerful <lb/>kernel-based or features representations (see Section <lb/>3.3.2 and 3.3.1). <lb/>The whole process is based on a three-tier task-<lb/>specific semantic hierarchy of named entity classes. <lb/>The top rank is constituted by the equivalent classes <lb/> Transcription factor, Binding site, and Promoter. <lb/> The second rank is occupied by MESH terms, and <lb/>the third tier assembles the named entity classes <lb/> Gene and Protein. Whenever a lexical item is cat-<lb/>egorized by one of these categories, the associated <lb/>node in the dependency graph is overlaid with that <lb/>category applying the ranking in cases of conflicts. <lb/>We also enriched the gene name mentions with <lb/>their respective Gene Ontology Annotations from <lb/>GOA. 8 For this purpose, we first categorized GO <lb/>terms both from the &quot; molecular function &quot; and from <lb/>the &quot; biological process &quot; branch with respect to <lb/>their matching event type, e.g., Phosphorylation <lb/> or Positive regulation. We then mapped all gene <lb/>name mentions which occurred in the text to their <lb/>UNIPROT identifier using the gene name normalizer <lb/>GENO (Wermter et al., 2009). This identifier links a <lb/>gene with a set of (curated) GO annotations. <lb/>In addition, we inserted semantic information in <lb/>terms of the event trigger type and the experimen-<lb/>tal methods. As far as experimental methods are <lb/>concerned, we extracted all instances of them an-<lb/>notated in the GENIA event corpus. One student <lb/>of biology sorted the experimental methods relative <lb/>to the event categories under scrutiny. For example <lb/> &quot; affinity chromatography &quot;  was assigned both to the <lb/> Gene expression and to the Binding category. For <lb/>our purposes, we only included those GO annota-<lb/>tions and experimental methods which matched the <lb/>event types to be identified in a sentence. <lb/> 3.3 Argument Identification and Ordering <lb/> The argument identification task can be subdivided <lb/>into three complexity levels. Level (1) incorpo-<lb/>rates five event types (Gene expression, Transcrip-<lb/>tion, Protein catabolism, Localization, Phosphory-<lb/>lation) which involve a single participant with a <lb/>THEME role only. Level (2) is concerned with one <lb/>event type (Binding) that provides an n-ary argument <lb/>structure where all arguments occupy the THEME(n) <lb/>role. Level (3) comprises three event types (Posi-<lb/> tive regulation, Negative regulation, or an unspeci-<lb/>fied Regulation) that represent a regulatory relation <lb/>between the above-mentioned event classes or pro-<lb/>teins. These events have usually a binary structure, <lb/>with a THEME argument and a CAUSE argument. <lb/>For argument extraction, we built sentence-wise <lb/>pairs of putative triggers and their putative argu-<lb/>ment(s), the latter involving ontological informa-<lb/>tion about the event type. For Level (1), we built <lb/>pairs only with proteins, while for Level (3) we al-<lb/>

			<note place="footnote">8  http://www.ebi.ac.uk/GOA <lb/></note>

			<page> 22 <lb/></page>

			lowed all events as possible arguments. For Level <lb/>(2), Binding events, we generated binary (trigger, <lb/>protein) pairs as well as triples (trigger, protein  1  , <lb/>protein  2  ) to adequately represent the binding be-<lb/>tween two proteins. 9 Pairs of mentions not con-<lb/>nected by a dependency path could not be detected. <lb/>For the argument extraction we chose two ma-<lb/>chine learning-based approaches, feature-based and <lb/>a kernel-based one, as described below. 10 <lb/> 3.3.1 Feature-based Classifier <lb/> We distinguished three groups of features. First, <lb/> lexical features (covering lexical items before, af-<lb/>ter and between both mentions (of the event trigger <lb/>and an argument) as described by Zhou and Zhang <lb/>(2007)); second, chunking features (concerned with <lb/>head words of the phrases between two mentions as <lb/>described by Zhou and Zhang (2007)); third, de-<lb/>pendency parse features (considering both the se-<lb/>lected dependency levels of the arguments (parents <lb/>and least common subsumer) as discussed by Ka-<lb/>trenko and Adriaans (2006), as well as a shortest de-<lb/>pendency path structure between the arguments as <lb/>used by Kim et al. (2008b) for walk features). <lb/>For the feature-based approach, we chose the <lb/>Maximum Entropy (ME) classifier from MALLET. 11 <lb/> 3.3.2 Graph Kernel Classifier <lb/> The graph kernel uses a converted form of depen-<lb/>dency graphs in which each dependency node is rep-<lb/>resented by a set of labels associated with that node. <lb/>The dependency edges are also represented as nodes <lb/>in the new graph such that they are connected to the <lb/>nodes adjacent in the dependency graph. Subgraphs <lb/>which represent, e.g., the linear order of the words <lb/>in the sentence can be added, if required. The entire <lb/>graph is represented in terms of an adjacency matrix <lb/>which is further processed to contain the summed <lb/>weights of paths connecting two nodes of the graph <lb/>(see Airola et al. (2008) for details). <lb/> 
			
			<note place="footnote">9 We did not account for the binding of more than two pro-<lb/>teins as this would have led to a combinatory explosion of pos-<lb/>sible classifications. <lb/></note> 
			
			<note place="footnote">10 In our experiments, we used full conceptual overlaying <lb/>(see Section 3.2) for the kernel-based representation and partial <lb/>overlaying for the dependency parse features (only gene/protein <lb/>annotation was exploited here). Graph representations allow for <lb/>many semantic labels to be associated with a node. <lb/></note> 
			
			<note place="footnote">11  http://mallet.cs.umass.edu/index.php/ <lb/> Main_Page <lb/></note> 
			
			Figure 3: Graph Kernel Representation for a Trimmed <lb/>Dependency Graph — (1) original representation, (2) <lb/>representation without graph dependency edge nodes <lb/>(weights (0.9, 0.3) taken from Airola et al. (2008)). <lb/> For our experiments, we tried some variants of the <lb/>original graph kernel. In the original version each <lb/>dependency graph edge is represented as a node. <lb/>That means that connections between graph token <lb/>nodes are expressed through graph dependency edge <lb/>nodes (see Figure 3; (1)). To represent the connec-<lb/>tions between original tokens as direct connections <lb/>in the graph, we removed the edge nodes and each <lb/>token was assigned the edge label (its dependency <lb/>label; see Figure 3; (2)). Further variants included <lb/>encodings for (1) the shortest dependency path (sp) <lb/>between two mentions (argument and trigger) 12 (2) <lb/>the complete dependency graph (sp-dep), and (3) the <lb/>complete dependency graph and linear information <lb/>(sp-dep-lin) (the original configuration from Airola <lb/>et al. (2008)). <lb/>For the graph kernel, we chose the LibSVM <lb/>(Chang and Lin, 2001) Support Vector Machine as <lb/>classifier. <lb/> 3.4 Postprocessing <lb/> The postprocessing step varies for the three different <lb/>Levels (see Section 3.3). For every event trigger of <lb/>Level (1) (e.g., Gene expression), we generate one <lb/>event per relation comprising a trigger and its argu-<lb/>ment. For Level (2) (Binding), we create a Binding <lb/> event with two arguments only for triples (trigger, <lb/>protein  1  , protein  2  ). For the third Level, we create <lb/>for each event trigger and its associated arguments <lb/> e = n × m events, for n CAUSE arguments and m <lb/> THEME arguments. <lb/>

			<note place="footnote">12 For Binding we extracted the shortest path between two <lb/>protein mentions if we encounter a triple (trigger, protein1, <lb/>protein2). <lb/></note>

			<page> 23 <lb/></page>

			4 Pipeline <lb/> The event extraction pipeline consists of two ma-<lb/>jor parts, a pre-processor and the dedicated event <lb/>extractor. As far as pre-processing is concerned, <lb/>we imported the sentence splitting, tokenization and <lb/>GDep parsing results (Sagae and Tsujii, 2007) as <lb/>prepared by the shared task organizers for all data <lb/>sets (training, development and test). We processed <lb/>this data with the OpenNLP POS tagger and Chun-<lb/>ker, both re-trained on the GENIA corpus (Buyko et <lb/>al., 2006). Additionally, we enhanced the original <lb/>tokenization by one which includes hyphenization <lb/>of lexical items such as in &quot; PMA-dependent &quot; . 13 <lb/> The data was further processed with the gene nor-<lb/>malizer GENO(Wermter et al., 2009) and a num-<lb/>ber of regex-and dictionary-based entity taggers <lb/>(covering promoters, binding sites, and transcrip-<lb/>tion factors). We also enriched gene name men-<lb/>tions with their respective Gene Ontology annota-<lb/>tions (see Section 3.2.2). The MESH thesaurus (ex-<lb/>cept chemical and drugs branch) was mapped on the <lb/>data using the Lingpipe Dictionary Chunker. 14 <lb/> After preprocessing, event extraction was started <lb/>distinguishing between the event trigger recognition <lb/>(cf. Section 3.1), the trimming of the dependency <lb/>graphs (cf. Section 3.2), and the argument extrac-<lb/>tion proper (cf. Section 3.3). 15 We determined in <lb/>our experiments on the development data the perfor-<lb/>mance of every classifier type and its variants (for <lb/>the graph kernel), and of ensembles of the most per-<lb/>formant (F-Score) graph kernel variant and an ME <lb/>model. 16 We present here the argument extraction <lb/>configuration used for the official run. 17 For the <lb/>prediction of Phosphorylation, Localization, Pro-<lb/>tein catabolism types we used the graph kernel in <lb/> 
			
			<note place="footnote">13 This tokenization is more advantageous for the detection <lb/>of additional event triggers as it allows to generate depen-<lb/>dency relations from hyphenated terms. For example, in &quot; PMA-<lb/> dependent &quot; ,  &quot; PMA &quot; will be a child of &quot; dependent &quot; linked by <lb/>the AMOD dependency relation, and &quot; dependent &quot; receives the <lb/>original dependency relation of the &quot; PMA-dependent &quot; token. <lb/></note> 
			
			<note place="footnote">14  http://alias-i.com/lingpipe/ <lb/></note> 
			
			<note place="footnote">15 For the final configurations of the graph kernel, we opti-<lb/>mized the C parameter in the spectrum between 2  −3  and 2  3  on <lb/>the final training data for every event type separately. <lb/></note> 
			
			<note place="footnote">16 In the ensemble configuration we built the union of positive <lb/>instances. <lb/></note> 
			
			<note place="footnote">17 We achieved with this configuration the best performance <lb/>on the development set. <lb/></note> 
			
			its &quot; sp without dependency-edge-nodes &quot;  configura-<lb/>tion, while for the prediction of Transcription and <lb/> Gene expression events we used an ensemble of the <lb/>graph kernel in its &quot; sp with dependency-edge-nodes &quot; <lb/> variant, and an ME model. For the prediction of <lb/> Binding we used an ensemble of the graph kernel <lb/>( &quot; sp-dep with dependency-edge-nodes &quot; ) and an ME <lb/>model. For the prediction of regulatory events we <lb/>used ME models for each regulatory type. <lb/> 5 Results <lb/> The baseline against which we compared our ap-<lb/>proach can be captured in a single rule. We extract <lb/>for every pair of a putative trigger and a putative ar-<lb/>gument the shortest dependency path between them. <lb/>If the shortest dependency path does not contain any <lb/>direction change, i.e., the argument is either a direct <lb/>child or a direct parent of the trigger, and if the path <lb/>does not contain any other intervening event trig-<lb/>gers, the argument is taken as the THEME role. <lb/>We performed evaluations on the shared task de-<lb/>velopment and test set. Our baseline achieved com-<lb/>petitive results of 36.0% precision, 34.0% recall, <lb/>35.0% F-score on the development set (see Table <lb/>1), and 30.4% precision, 35.7% recall, 32,8% F-<lb/>score on the test set (see Table 2). In particular <lb/>the one-argument events, i.e., Gene expression, Pro-<lb/>tein catabolism, Phosphorylation are effectively ex-<lb/>tracted with an F-score around 70.0%. More com-<lb/>plex events, in particular events of Level (3), i.e., <lb/>(Regulation) were less properly dealt with because <lb/>of their strong internal complexity. <lb/> Event Class <lb/>gold recall prec. F-score <lb/> Localization <lb/> 53 <lb/>75.47 30.30 43.24 <lb/> Binding <lb/> 248 <lb/>33.47 20.80 25.66 <lb/> Gene expression <lb/> 356 <lb/>76.12 75.07 75.59 <lb/> Transcription <lb/> 82 <lb/>68.29 40.58 50.91 <lb/> Protein catabolism 21 <lb/>76.19 66.67 71.11 <lb/> Phosphorylation <lb/> 47 <lb/>76.60 72.00 74.23 <lb/> Regulation <lb/> 169 <lb/>14.20 15.09 14.63 <lb/> Positive regulation 617 <lb/>15.40 20.83 17.71 <lb/> Negative regulation 196 <lb/>11.73 13.22 12.43 <lb/>TOTAL <lb/>1789 36.00 34.02 34.98 <lb/>Table 1: Baseline results on the shared task development <lb/>data. Approximate Span Matching/Approximate Recur-<lb/>sive Matching. <lb/>

			<page> 24 <lb/></page>

			Event Class <lb/>gold recall prec. F-score gold recall prec. F-score <lb/> Localization <lb/> 174 <lb/> 42.53 44.85 43.66 <lb/>174 <lb/>42.53 44.85 43.66 <lb/> Binding <lb/> 347 <lb/>32.28 37.09 34.51 <lb/>398 <lb/>44.22 58.28 50.29 <lb/> Gene expression <lb/> 722 <lb/>61.36 80.55 69.65 <lb/>722 <lb/>61.36 80.55 69.65 <lb/> Transcription <lb/> 137 <lb/>39.42 35.06 37.11 <lb/>137 <lb/>39.42 35.06 37.11 <lb/> Protein catabolism 14 <lb/>71.43 66.67 68.97 <lb/>14 <lb/>71.43 66.67 68.97 <lb/> Phosphorylation <lb/> 135 <lb/>65.93 90.82 76.39 <lb/>135 <lb/>65.93 90.82 76.39 <lb/>EVT-TOTAL <lb/>1529 51.14 60.90 55.60 <lb/>1580 53.54 65.89 59.08 <lb/> Regulation <lb/> 291 <lb/>9.62 <lb/>11.72 10.57 <lb/>338 <lb/>9.17 <lb/>12.97 10.75 <lb/> Positive regulation 983 <lb/>10.38 11.33 10.83 <lb/>1186 14.67 19.33 16.68 <lb/> Negative regulation 379 <lb/>14.25 19.22 16.36 <lb/>416 <lb/>14.18 21.00 16.93 <lb/>REG-TOTAL <lb/>1653 11.13 12.96 11.98 <lb/>1940 13.61 18.59 15.71 <lb/>ALL-TOTAL <lb/>3182 30.36 35.72 32.82 <lb/>3520 31.53 41.05 35.67 <lb/>Table 2: Baseline results on the shared task test data. Approximate Span Matching/Approximate Recursive Matching <lb/>(columns 3-5). Event decomposition, Approximate Span Matching/Approximate Recursive Matching (columns 7-9). <lb/> The event extraction approach, in its final config-<lb/>uration (see Section 4), achieved a performance of <lb/>50.4% recall, 45.8% precision and 48.0% F-score on <lb/>the development set (see Table 4), and 45.8% recall, <lb/>47.5% precision and 46.7% F-score on the test set <lb/>(see Table 3). This approach clearly outperformed <lb/>the baseline with an increase of 14 percentage points <lb/>on the test data. In particular, the events of Level (2) <lb/>and (3) were more properly dealt with than by the <lb/>baseline. In the event decomposition mode (argu-<lb/>ment detection is evaluated in a decomposed event) <lb/>we achieved a performance of 49.4% recall, 56.2% <lb/>precision, and 52.6% F-score (see Table 3). <lb/>Our experiments on the development set showed <lb/>that the combination of the feature-based and the <lb/>graph kernel-based approach can boost the results up <lb/>to 6 percentage points F-score (for the Binding event <lb/>type). It is interesting that the combination for Bind-<lb/>ing increased recall without dropping precision. The <lb/>original graph kernel approach for Binding events <lb/>performs with 38.3% recall, 27.9% precision and <lb/>32.3% F-score on the development set. The com-<lb/>bined approach comes with a remarkable increase <lb/>of 14 percentage points in recall. The combination <lb/>could also boost the recall of the Gene expression <lb/> and Transcription by 15 percentage points and 5 per-<lb/>centage points, respectively, without seriously drop-<lb/>ping the precision (4 points for every type). For <lb/>the other event types, no improvements were found <lb/>when we combined both approaches. <lb/> 5.1 Error Discussion <lb/> One expert biologist analyzed 30 abstracts randomly <lb/>extracted from the development error data. We de-<lb/>termined seven groups of errrors based on this anal-<lb/>ysis. The first group contains examples for which <lb/>an event should be determined, but a false argument <lb/>was found (e.g., Binding arguments were not prop-<lb/>erly sorted, or correct and false arguments were de-<lb/>tected for the same trigger) (44 examples). The sec-<lb/>ond group comprised examples where no trigger was <lb/>found (23 examples). Group (3) stands for cases <lb/>where no events were detected although a trigger <lb/>was properly identified (14 examples). Group (4) <lb/>holds examples detected in sentences which did not <lb/>contain any events (12 examples). Group (5) lists bi-<lb/>ologically meaningful analyses, actually very close <lb/>to the gold annotation, especially for the cascaded <lb/>regulatory events (12 examples), while Group (6) in-<lb/>corporates examples of a detected event with incor-<lb/>rect type (1 example). Group (7) gathers misleading <lb/>gold annotations (10 examples). <lb/>This assessment clearly indicates that a major <lb/>source of errors can be traced to the level of argu-<lb/>ment identification, in particular for Binding events. <lb/>The second major source has its offspring at the <lb/>level of trigger detection (we ignored, for exam-<lb/>ple, triggers such as &quot; in the presence of  &quot; , &quot; when &quot; , <lb/> &quot; normal &quot; ). About 10% of the errors are due to a <lb/>slight difference between extracted events and gold <lb/>events. For example, in the phrase &quot; role for NF-<lb/>kappaB in the regulation of FasL expression &quot;  we <lb/></body>

			<page> 25 <lb/></page>

			<div type="annex"> Event Class <lb/>gold recall prec. <lb/>F-score gold recall prec. F-score <lb/> Localization <lb/> 174 <lb/> 43.68 77.55 55.88 <lb/>174 <lb/>43.68 77.55 55.88 <lb/> Binding <lb/> 347 <lb/>49.57 35.25 41.20 <lb/>398 <lb/>63.57 54.88 58.91 <lb/> Gene expression <lb/> 722 <lb/>64.82 80.27 71.72 <lb/>722 <lb/>64.82 80.27 71.72 <lb/> Transcription <lb/> 137 <lb/>35.77 62.03 45.37 <lb/>137 <lb/>35.77 62.03 45.37 <lb/> Protein catabolism 14 <lb/>78.57 84.62 81.48 <lb/>14 <lb/>78.57 84.62 81.48 <lb/> Phosphorylation <lb/> 135 <lb/>76.30 91.15 83.06 <lb/>135 <lb/>76.30 91.15 83.06 <lb/>EVT-TOTAL <lb/>1529 57.49 63.97 60.56 <lb/>1580 60.76 71.27 65.60 <lb/> Regulation <lb/> 291 <lb/>31.27 30.13 30.69 <lb/>338 <lb/>35.21 37.54 36.34 <lb/> Positive regulation 983 <lb/>34.08 37.18 35.56 <lb/>1186 40.64 49.33 44.57 <lb/> Negative regulation 379 <lb/>40.37 31.16 35.17 <lb/>416 <lb/>42.31 39.11 40.65 <lb/>REG-TOTAL <lb/>1653 35.03 34.18 34.60 <lb/>1940 40.05 44.55 42.18 <lb/> ALL-TOTAL <lb/>3182 45.82 <lb/>47.52 46.66 <lb/>3520 49.35 56.20 52.55 <lb/></div>

			<body> Table 3: Offical Event Extraction results on the shared task test data of the JULIELab Team. Approximate <lb/>Span Matching/Approximate Recursive Matching (columns 3-5). Event decomposition, Approximate Span Match-<lb/>ing/Approximate Recursive Matching (columns 7-9). <lb/> could not extract the gold event Regulation of Regu-<lb/>lation (Gene expression (FasL)) associated with the <lb/>trigger &quot; role &quot; , but we were able to find the (inside) <lb/>event Regulation (Gene expression (FasL)) associ-<lb/>ated with the trigger &quot; regulation &quot; . Interestingly, the <lb/>typing of events is not an error source in spite of <lb/>the simple disambiguation approach. Still, our dis-<lb/>ambiguation strategy is not appropriate for the anal-<lb/>ysis of double-annotated triggers such as &quot; overex-<lb/>pression &quot; , &quot; transfection &quot; , etc., which are annotated <lb/>as Gene expression and Positive regulation and are <lb/>a major source of errors in Group (2). As Group <lb/>(6) is an insignificant source of errors in our ran-<lb/>domly selected data, we focused our error analysis <lb/>on the especially ambiguous event type Transcrip-<lb/>tion. We found from 34 errors that 14 of them were <lb/>due to the disambiguation strategy (in particular for <lb/>triggers &quot; (gene) expression &quot; and &quot; induction &quot; ). <lb/> 6 Conclusion <lb/> Our approach to event extraction incorporates man-<lb/>ually curated dictionaries and machine learning <lb/>methodologies to sort out associated event triggers <lb/>and arguments on trimmed dependency graph struc-<lb/>tures. Trimming combines pruning irrelevant lexi-<lb/>cal material from a dependency graph and decorat-<lb/>ing particularly relevant lexical material from that <lb/>graph with more abstract conceptual class informa-<lb/>tion. Given that methodological framework, the <lb/>JULIELab Team scored on 2nd rank among 24 com-<lb/> Event Class <lb/>gold recall prec. <lb/>F-score <lb/> Localization <lb/> 53 <lb/>71.70 74.51 73.08 <lb/> Binding <lb/> 248 <lb/>52.42 29.08 37.41 <lb/> Gene expression <lb/> 356 <lb/>75.28 81.46 78.25 <lb/> Transcription <lb/> 82 <lb/>60.98 73.53 66.67 <lb/> Protein catabolism 21 <lb/>90.48 79.17 84.44 <lb/> Phosphorylation <lb/> 47 <lb/>82.98 84.78 83.87 <lb/> Regulation <lb/> 169 <lb/>37.87 36.78 37.32 <lb/> Positive regulation 617 <lb/>34.36 35.99 35.16 <lb/> Negative regulation 196 <lb/>41.33 33.61 37.07 <lb/> TOTAL <lb/>1789 50.36 45.76 47.95 <lb/> Table 4: Event extraction results on the shared task <lb/>development data of the official run of the JULIELab <lb/>Team. Approximate Span Matching/Approximate Recur-<lb/>sive Matching. <lb/> peting teams, with 45.8% precision, 47.5% recall <lb/>and 46.7% F1-score on all 3,182 events. <lb/> 
			
		</body>
			
		<back>	
			
			<div type="acknowledgement">7 Acknowledgments <lb/> We wish to thank Rico Landefeld for his technical <lb/>support, Tobias Wagner and Rico Pusch for their <lb/>constant help and great expertise in biological is-<lb/>sues. This research was partially funded within the <lb/>BOOTSTREP project under grant FP6-028099 and <lb/>the CALBC project under grant FP7-231727. <lb/></div>

			<listBibl> References <lb/> Antti Airola, Sampo Pyysalo, Jari Björne, Tapio <lb/>Pahikkala, Filip Ginter, and Tapio Salakoski. 2008. A <lb/>

			<page> 26 <lb/></page>

			graph kernel for protein-protein interaction extraction. <lb/>In Proceedings of the Workshop on Current Trends in <lb/>Biomedical Natural Language Processing, pages 1–9. <lb/>Christian Blaschke, Miguel A. Andrade, Christos Ouzou-<lb/>nis, and Alfonso Valencia. 1999. Automatic ex-<lb/>traction of biological information from scientific text: <lb/>Protein-protein interactions. In ISMB&apos;99 – Proceed-<lb/>ings of the 7th International Conference on Intelligent <lb/>Systems for Molecular Biology, pages 60–67. <lb/>Ekaterina Buyko, Joachim Wermter, Michael Poprat, and <lb/>Udo Hahn. 2006. Automatically adapting an NLP <lb/>core engine to the biology domain. In Proceedings <lb/>of the Joint BioLINK-Bio-Ontologies Meeting. A Joint <lb/>Meeting of the ISMB Special Interest Group on Bio-<lb/>Ontologies and the BioLINK Special Interest Group on <lb/>Text Data M ining in Association with ISMB, pages <lb/>65–68. Fortaleza, Brazil, August 5, 2006. <lb/>Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-<lb/>SVM: a library for support vector machines. Soft-<lb/>ware available at http://www.csie.ntu.edu. <lb/> tw/ ˜ cjlin/libsvm. <lb/> Katrin Fundel, Robert Küffner, and Ralf Zimmer. <lb/>2007. Relex-relation extraction using dependency <lb/>parse trees. Bioinformatics, 23(3):365–371. <lb/>Jörg Hakenberg, Ulf Leser, Conrad Plake, Harald Kirsch, <lb/>and Dietrich Rebholz-Schuhmann. 2005. LLL&apos;05 <lb/>challenge: Genic interaction extraction -identifica-<lb/>tion of language patterns based on alignment and finite <lb/>state automata. In Proceedings of the 4th Learning <lb/>Language in Logic Workshop (LLL05), pages 38–45. <lb/>Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-<lb/>bin Qu, and Ming Li. 2004. Discovering patterns <lb/>to extract protein-protein interactions from full texts. <lb/> Bioinformatics, 20(18):3604–3612. <lb/>Sophia Katrenko and Pieter W. Adriaans. 2006. Learn-<lb/>ing relations from biomedical corpora using depen-<lb/>dency trees. In Karl Tuyls, Ronald L. Westra, Yvan <lb/>Saeys, and Ann Nowé, editors, KDECB 2006 – Knowl-<lb/>edge Discovery and Emergent Complexity in Bioin-<lb/>formatics. Revised Selected Papers of the 1st Inter-<lb/>national Workshop., volume 4366 of Lecture Notes <lb/>in Computer Science, pages 61–80. Ghent, Belgium, <lb/>May 10, 2006. Berlin: Springer. <lb/>Jin-Dong Kim, Tomoko Ohta, and Jun&apos;ichi Tsujii. 2008a. <lb/>Corpus annotation for mining biomedical events from <lb/>literature. BMC Bioinformatics, 9(10). <lb/>Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008b. <lb/>Kernel approaches for genic interaction extraction. <lb/> Bioinformatics, 24(1):118–126. <lb/>Rune Saetre, Kenji Sagae, and Jun&apos;ichi Tsujii. 2007. Syn-<lb/>tactic features for protein-protein interaction extrac-<lb/>tion. In Christopher J. O. Baker and Jian Su, editors, <lb/> LBM 2007, volume 319, pages 6.1–6.14. <lb/>Kenji Sagae and Jun&apos;ichi Tsujii. 2007. Dependency pars-<lb/>ing and domain adaptation with LR models and par ser <lb/>ensembles. In Proceedings of the CoNLL Shared Task <lb/>Session of EMNLP-CoNLL 2007, pages 1044–1050. <lb/>Jasmiň <lb/>Sari´ ars J. Jensen, Rossitza Ouzounova, Isabel <lb/>Rojas, and Peer Bork. 2004. Extracting regulatory <lb/>gene expression networks from pubmed. In ACL &apos;04: <lb/>Proceedings of the 42nd Annual Meeting on Associa-<lb/>tion for Computational Linguistics, page 191, Morris-<lb/>town, NJ, USA. Association for Computational Lin-<lb/>guistics. <lb/>Joachim Wermter, Katrin Tomanek, and Udo Hahn. <lb/>2009. High-performance gene name normalization <lb/>with GeNo. Bioinformatics, 25(6):815–821. <lb/>Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and <lb/>Jun&apos;ichi Tsujii. 2001. Event extraction from biomed-<lb/>ical papers using a full parser. In Russ B. Altman, <lb/>A. Keith Dunker, Lawrence Hunter, Kevin Lauderdale, <lb/>and Teri E. Klein, editors, PSB 2001 – Proceedings <lb/>of the 6th Pacific Symposium on Biocomputing, pages <lb/>408–419. Maui, Hawaii, USA. January 3-7, 2001. Sin-<lb/>gapore: World Scientific Publishing. <lb/>Guodong Zhou and Min Zhang. 2007. Extracting re-<lb/>lation information from text documents by exploring <lb/>various types of knowledge. Information Processing <lb/>&amp; Management, 43(4):969–982. <lb/></listBibl>

			<page> 27 </page>

		</back>
	</text>
</tei>
