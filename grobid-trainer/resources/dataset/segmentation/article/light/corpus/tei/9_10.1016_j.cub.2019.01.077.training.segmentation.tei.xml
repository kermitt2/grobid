<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<cover>Article <lb/>A Computational Model of Visual Recognition <lb/>Memory via Grid Cells <lb/>Highlights <lb/>d Visual grid cells can encode saccade vectors between salient <lb/>stimulus features <lb/>d A sequence of (memory-guided) saccades confirms stimulus <lb/>identity <lb/>d Known properties of grid cells confer size and position <lb/>invariance onto the model <lb/>d Grid cell lesions may impair relational memory and could <lb/>contribute to prosopagnosia <lb/>Authors <lb/>Andrej Bicanski, Neil Burgess <lb/>Correspondence <lb/>a.bicanski@ucl.ac.uk (A.B.), <lb/>n.burgess@ucl.ac.uk (N.B.) <lb/>In Brief <lb/>Bicanski and Burgess propose that grid <lb/>cells support visual recognition memory, <lb/>by encoding translation vectors between <lb/>salient stimulus features. They provide an <lb/>explicit neural mechanism for the role of <lb/>directed saccades in hypothesis-driven, <lb/>constructive perception and recognition, <lb/>and of the hippocampal formation in <lb/>relational visual memory. <lb/>Bicanski &amp; Burgess, 2019, Current Biology 29, 979-990 <lb/>March 18, 2019 ª 2019 The Author(s). Published by Elsevier Ltd. <lb/>https://doi.org/10.1016/j.cub.2019.01.077 <lb/></cover>

		<front>Current Biology <lb/>Article <lb/>A Computational Model of Visual <lb/>Recognition Memory via Grid Cells <lb/>Andrej Bicanski 1,2, * and Neil Burgess 1, * <lb/>1 Institute of Cognitive Neuroscience, University College London, Alexandra House, 17 Queen Square, WC1N 3AZ London, UK <lb/>2 Lead Contact <lb/>*Correspondence: a.bicanski@ucl.ac.uk (A.B.), n.burgess@ucl.ac.uk (N.B.) <lb/>https://doi.org/10.1016/j.cub.2019.01.077 <lb/>SUMMARY <lb/>Models of face, object, and scene recognition tradi-<lb/>tionally focus on massively parallel processing of <lb/>low-level features, with higher-order representations <lb/>emerging at later processing stages. However, visual <lb/>perception is tightly coupled to eye movements, <lb/>which are necessarily sequential. Recently, neurons <lb/>in entorhinal cortex have been reported with grid <lb/>cell-like firing in response to eye movements, i.e., in <lb/>visual space. Following the presumed role of grid <lb/>cells in vector navigation, we propose a model of <lb/>recognition memory for familiar faces, objects, and <lb/>scenes, in which grid cells encode translation vec-<lb/>tors between salient stimulus features. A sequence <lb/>of saccadic eye-movement vectors, moving from <lb/>one salient feature to the expected location of the <lb/>next, potentially confirms an initial hypothesis (accu-<lb/>mulating evidence toward a threshold) about stim-<lb/>ulus identity, based on the relative feature layout <lb/>(i.e., going beyond recognition of individual features). <lb/>The model provides an explicit neural mechanism <lb/>for the long-held view that directed saccades <lb/>support hypothesis-driven, constructive perception <lb/>and recognition; is compatible with holistic face pro-<lb/>cessing; and constitutes the first quantitative pro-<lb/>posal for a role of grid cells in visual recognition. <lb/>The variance of grid cell activity along saccade tra-<lb/>jectories exhibits 6-fold symmetry across 360 de-<lb/>grees akin to recently reported fMRI data. The model <lb/>suggests that disconnecting grid cells from occipito-<lb/>temporal inputs may yield prosopagnosia-like symp-<lb/>toms. The mechanism is robust with regard to partial <lb/>visual occlusion, can accommodate size and posi-<lb/>tion invariance, and suggests a functional explana-<lb/>tion for medial temporal lobe involvement in visual <lb/>memory for relational information and memory-<lb/>guided attention. <lb/></front>

			<body>INTRODUCTION <lb/>How the brain implements recognition of familiar faces, objects, <lb/>and scenes at the neural level is a complex problem that has <lb/>engendered a multitude of different approaches. Both unsuper-<lb/>vised and supervised learning systems have been proposed for <lb/>the classification of visual stimuli into categories, and for recog-<lb/>nition of specific familiar stimuli within a category [1-3]. These <lb/>approaches often focus on the parallel processing of low-level <lb/>visual features, inspired by the landmark findings of Hubel and <lb/>Wiesel [4], and on how higher-level representations emerge at <lb/>later processing stages. <lb/>However, since the pioneering studies of eye movements <lb/>by Yarbus [5], perception has been known to also depend on <lb/>motor acts, which are necessarily sequential. The notion that <lb/>sequences of saccades (rapid target-driven eye movements) <lb/>might underlie complex pattern recognition and constructive, <lb/>hypothesis-driven perception is an idea with a long history in <lb/>neuroscience (e.g., focal attention and figural synthesis [6]; <lb/>Scanpath theory [7, 8]), and has been taken up repeatedly since <lb/>(e.g., [9]). It is also supported by rare pathologies in which pa-<lb/>tients incapable of performing eye movements emulate sac-<lb/>cades with head movements [10]. According to this account, <lb/>the currently attended part of a stimulus is foveated and used <lb/>to calculate a saccade to where the next feature of the stimulus <lb/>should lie according to the hypothesized stimulus identity. For <lb/>example, on foveating the nose of a familiar face, the facial <lb/>identity is confirmed by generating a saccade to where that <lb/>person&apos;s left eye should be and, if detected, thence on to where <lb/>their mouth should be, and so on. This implies consistency <lb/>of saccade targets (though not necessarily order) between <lb/>encoding and retrieval conditions, which is supported by behav-<lb/>ioral data [11, 12], in particular, by memory-guided saccades, <lb/>which depend on the return of gaze to previously encoded (i.e., <lb/>fixated) locations [13, 14]. <lb/>Decoupling depth perception (i.e., ocular focus) from yaw and <lb/>pitch movements of the eye, a sequence of saccades can be <lb/>viewed as a complex trajectory on a 2-dimensional plane <lb/>(the field of view). This allows us to exploit analogies to <lb/>studies of spatial navigation in which experimental animals <lb/>move freely on a 2D plane while neuronal activity is recorded. <lb/>These paradigms have revealed neuronal responses in and <lb/>near the hippocampus that are well suited to represent locations <lb/>and trajectories in two dimensions. In fact, so-called place <lb/>cells, which exhibit firing specific to a single location in an <lb/>environment [15], and grid cells, which exhibit multiple regularly <lb/>arranged firing fields [16], have become cornerstones of our <lb/>growing understanding of spatial cognition. <lb/>Several recent studies have suggested that entorhinal cortex <lb/>cells can exhibit grid cell-like responses in visual space <lb/>[17-20]. Here, we propose that these visually driven grid cells <lb/></body>

			<front>Current Biology 29, 979-990, March 18, 2019 ª 2019 The Author(s). Published by Elsevier Ltd. 979 <lb/>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/). <lb/></front>

			<body>support recognition memory by encoding inter-feature move-<lb/>ment vectors, capturing the layout of compound stimuli in a stim-<lb/>ulus-specific coordinate system. This model might also account <lb/>for medial temporal lobe interactions with the visual system dur-<lb/>ing visual recognition of relational information or object-location <lb/>binding [21-24] and memory-guided attention [14, 15], and en-<lb/>coding of saccade direction within entorhinal cortex [25]. <lb/>A <lb/>B <lb/>C <lb/>D <lb/>E <lb/>Figure 1. Grid Cell-Based Vector Computa-<lb/>tions and Visual Recognition Memory <lb/>(A) Grid cells in medial entorhinal cortex (MEC) <lb/>exhibit periodic, hexagonally arranged firing fields, <lb/>originally characterized as spatially selective cells <lb/>in rodent experiments. Right: Spiking locations <lb/>(red dots) superimposed on a rodent&apos;s trajectory <lb/>during foraging; Bottom left: A stereotypical, <lb/>smoothed firing rate map. <lb/>(B) Vectorial coding in one dimension. 1D grid cells <lb/>with appropriate phases across modules (top: 4 <lb/>cells with different scales S1-4) project to distance <lb/>cells. Distance cell arrays (2 for each direction <lb/>along the 1D axis) project with monotonically <lb/>increasing weights (W) to two readout cells (for up <lb/>and down, respectively). The difference in output <lb/>between readout cells is a measure of the distance <lb/>and direction between start (blue) and goal (green) <lb/>locations. <lb/>(C) Replicating distance cells and readout cells for <lb/>a second, non-co-linear axis allows computation <lb/>of 2D vectors in the stimulus (e.g., between facial <lb/>features, to guide eye movements, e.g., in D). <lb/>(D) A face with superimposed saccade trajectories. <lb/>(E) Model schematic: Grayscale images are <lb/>sampled by a square fovea (blue square). Feature <lb/>detectors drive feature label cells, each coding for <lb/>a particular salient feature. During training each <lb/>feature label cell has been associated with a grid <lb/>cell population vector (current position grid cells, <lb/>blue cell and dashed arrows). All feature label cells <lb/>of a given stimulus are bi-directionally connected <lb/>to a single cell coding for the identity of the at-<lb/>tended stimulus. Upon firing of an identity cell the <lb/>currently active feature label cell is inhibited and <lb/>identity cells select the next feature label cell to be <lb/>active (short green arrow and cell), which is <lb/>associated with its own grid cell population <lb/>vector (target grid cells). Current and target posi-<lb/>tion grid cell representations yield the next <lb/>saccade vector (red arrow on image). Note, the <lb/>selection of the feature label cell corresponds to a <lb/>prediction of the next sensory discrimination (see <lb/>also Figure S1). Image credit: Mr. Spock: public <lb/>domain image; Grid cell rate map and rodent <lb/>trajectory adapted from Barry and Bush Neural <lb/>Systems &amp; Circuits 2012 2:6, Attribution 2.0 <lb/>Generic Creative Commons CC-BY 3.0; Saccade <lb/>trajectories reproduces from Wikimedia Com-<lb/>mons; Attribution 2.0 Generic Creative Commons <lb/>CC-BY 3.0). <lb/>Grid cells have been suggested to pro-<lb/>vide a spatial metric that supports path <lb/>integration (by integrating self-motion in-<lb/>puts [26]) and vector navigation [27-29]. <lb/>Located in medial entorhinal cortex, grid <lb/>cells form modules of different spatial scales (Figure 1A). Within <lb/>a module, different offsets (or phases) are present and the firing <lb/>fields of relatively few grid cells evenly cover an environment. <lb/>The spatial periodicity of grid cells at different scales suggests <lb/>that they provide a compact code for location. A set of phases <lb/>of grid cells across multiple spatial scales can uniquely encode <lb/>locations within a space much larger than the largest grid scale <lb/></body>

		<page>980</page>

		<note place="footnote">Current Biology 29, 979-990, March 18, 2019 <lb/></note>

			<body>[29] (see also STAR Methods). Several potential neural network <lb/>architectures can be built to compute the direct vector between <lb/>any two locations in 2D space. Here, we suggest that visually <lb/>driven grid cells encode vectors between salient stimulus fea-<lb/>tures in visual space to drive saccades in the service of visual <lb/>recognition. Intriguingly, the known properties of grid cells can <lb/>confer size and position invariance onto the model and help it <lb/>deal with occlusions. The model suggests clear predictions, <lb/>and the proposed recognition mechanism conforms to a large <lb/>body of literature on visual recognition (in particular holistic <lb/>face processing). <lb/>For a detailed description of the model, see the STAR <lb/>Methods. Briefly, a population vector across the grid cell <lb/>ensemble uniquely defines a position in the visual field (Figures <lb/>1A and 1B). Any two such positions can be used as inputs to a <lb/>distance cell model [29], which yields the displacement vector <lb/>between those two locations (Figures 1B and 1C). This vector <lb/>calculation is integrated with visual processing in the following <lb/>manner. Grayscale images (440 3 440 pixels) represent pre-pro-<lb/>cessed visual input to the model. A small square array of cells <lb/>represents output from a simplified fovea (61 3 61 cells) with <lb/>associated sensory cells (feature detectors). In a training phase, <lb/>the model learns Hebbian associations between the following <lb/>cell types. First, as an individual component feature (e.g., the <lb/>nose, eyes, lips, etc. of a given face) of a stimulus is foveated, <lb/>it generates a characteristic response among the feature detec-<lb/>tors (i.e., individual features are evaluated; Figure 2). A Hebbian <lb/>association between the array of feature detectors and an indi-<lb/>vidual (newly recruited) feature label cell (representing the <lb/>foveated component feature of a stimulus) is then learned. Sec-<lb/>ond, a connection between feature label cells and the locations <lb/>of those features in the visual field (represented by grid cell pop-<lb/>ulation vectors) is learned. Finally, all feature label cells belonging <lb/>to a stimulus are associated-bi-directionally-to a stimulus <lb/>identity cell, representing the abstract identity of a stimulus. <lb/>That is, each stimulus identity cell receives connections from a <lb/>small number of feature label cells representing the component <lb/>features of that stimulus and has a return projection to the <lb/>same feature label cells (Figure 1E). <lb/>Once the model has learned the necessary associations, its <lb/>recognition memory is tested by presenting stimuli from the <lb/>training set. An action-perception cycle then consists of the <lb/>following steps: the foveal array is centered on a given feature <lb/>(we assume that the first feature is selected by bottom-up atten-<lb/>tional mechanisms, which are not modeled here). Feature detec-<lb/>tors (perception) drive the feature label cells, which (partially) <lb/>match the attended feature. Thresholding and a subsequent <lb/>softmax operation ensure a sparse code among feature label <lb/>cells. Active feature label cells drive their associated stimulus <lb/>identity cells, generating competing hypotheses about the stim-<lb/>ulus identity. The most active identity cell then determines the <lb/>computation of the vector for the next saccade (action) in the <lb/>following way. The current location of the fovea is represented <lb/>by a population vector of grid cell activity, which is updated by <lb/>eye movements analogously to how grid cell firing is updated <lb/>by self-motion during navigation [26]. This yields the starting <lb/>point of the saccade vector. Previously active feature label cells <lb/>(including the one for the currently foveated feature) are reset to <lb/>zero, and the most active stimulus identity cell (representing the <lb/>leading hypothesis) randomly selects the next feature label cell <lb/>via its return projection. Randomness is given by weak noise <lb/>on the back projection, and winner-take-all dynamics select <lb/>A <lb/>C <lb/>D <lb/>E <lb/>B <lb/>Figure 2. Feature Detection and Ambiguity <lb/>(A) The first (randomly chosen) stimulus feature is <lb/>assumed to attract attention in a bottom-up <lb/>manner. <lb/>(B) The foveated feature is compared to imprecise <lb/>reference values by banks of sensory cells (hexa-<lb/>gons). Each pixel drives a given set of sensory <lb/>cells maximally (filled hexagons). Here, the re-<lb/>sponses for 3 pixels are illustrated (red Gaussians <lb/>and pixels). During training, a blurred version of the <lb/>stimulus is presented (purple pixels and bars <lb/>[preferred values] under Gaussians), resulting in <lb/>feature ambiguity. Connections between sensory <lb/>cells and feature label cells are learned (black <lb/>arrows, only one set of connections shown for <lb/>clarity). <lb/>(C) Feature ambiguity could lead to a feature label <lb/>cell from an incorrect stimulus identity being the <lb/>most active, or multiple feature label cells from an <lb/>incorrect stimulus being partially active (red). <lb/>(D) In either case, the corresponding (incorrect, <lb/>red) stimulus identity cell will receive more input <lb/>than the correct one (Spock, blue), and the system <lb/>starts with an incorrect hypothesis. <lb/>(E) Thus, the incorrect stimulus identity cell (red) <lb/>determines the next saccade(s), which cannot <lb/>bring the memorized features of the stimulus into <lb/>foveal focus. Image credit: Mr. Spock: public <lb/>domain image. <lb/></body>

			<note place="footnote">Current Biology 29, 979-990, March 18, 2019</note>

		<page>981 <lb/></page>

			<body>the feature label cell, which is in turn associated with its own grid <lb/>cell population vector (yielding the end point of the next saccade <lb/>vector). Given the starting and end points of the next saccade, <lb/>the distance cell system outputs the vector required to update <lb/>foveal position (see Figure 1E for an overview; details in STAR <lb/>Methods), allowing the system to sample another portion of <lb/>the visual field. The cycle then repeats while stimulus identity <lb/>cells accumulate firing across cycles until a firing rate &apos;&apos;decision <lb/>threshold&apos;&apos; has been reached (at which point the leading hypoth-<lb/>esis about the identity of the stimulus is accepted and identity <lb/>cells reset to zero before the next stimulus is presented). <lb/>Importantly, in addition to specifying the endpoint of the next <lb/>saccade via associated grid cells, the activated feature label <lb/>cell that has been selected by the return projection of the leading <lb/>stimulus identity neuron also represents a prediction. Once the <lb/>fovea relocates, and the next sensory discrimination is carried <lb/>out, the maximally active feature label cell should be the pre-<lb/>dicted one. This prediction is incorporated as a facilitatory effect, <lb/>boosting the firing of the predicted feature label cell in the next <lb/>cycle by a factor of two, prior to the application of the softmax <lb/>operation across all feature label cells. If the predicted feature la-<lb/>bel cell is not the most active one after the next sensory discrim-<lb/>ination, a mismatch is registered. At the third mismatch event, <lb/>the system resets, beginning with different component feature. <lb/>This procedure allows for a fast rejection of false hypotheses, <lb/>which will otherwise produce saccades that do not take the <lb/>fovea to expected features. Figure S1 details the effect of sen-<lb/>sory predictions. <lb/>During learning, each feature of a given stimulus is anchored to <lb/>the grid cell representation, and the relative locations of features <lb/>encoded across all grids are mutually consistent. Paralleling <lb/>experimental results [17, 20] we encode all stimuli in the same <lb/>grid ensemble (as if anchored to a presentation screen). However, <lb/>if connections from the most active feature label cell can re-align <lb/>the grid cell ensemble (foveal array 0 feature label cell 0 grid <lb/>cells) to the phases specific to a given stimulus, then recognition <lb/>irrespective of the position of a stimulus in the field of view (position <lb/>invariance) follows from the fact that the grid system encodes the <lb/>relative (rather than absolute) locations of the features within a <lb/>stimulus. This is consistent with grid cell firing patterns being sta-<lb/>ble across visits to the same environment but shifting coherently <lb/>across different environments [30]. This predicts that grid cell <lb/>phases would follow the position of the stimulus in the visual field, <lb/>in the absence of environmental anchor points, e.g., during recog-<lb/>nition of illuminated faces in darkness. <lb/>Regarding size invariance, pre-processing of visual inputs <lb/>could generate a size invariant representation prior to input to <lb/>the system described here. However, the present model could <lb/>accommodate size invariance. All circuit level models of grid <lb/>cells require velocity inputs to update their firing (locomotor ve-<lb/>locity during spatial navigation or eye-velocity in visual para-<lb/>digms). The coupling between &apos;&apos;neural space&apos;&apos; (i.e., distance on <lb/>the grid pattern) and self-motion has been shown to be plastic <lb/>in spatial navigation paradigms [31]. Hence, we assume a given <lb/>saccade length is subject to gain-modulation to give an appro-<lb/>priately scaled distance on the grid cell ensemble, and, <lb/>conversely, a given distance on the grid cell ensemble can be <lb/>scaled to yield a saccade of appropriate length. This requires a <lb/>change in gain with the estimated size (distance) of the stimulus. <lb/>The estimation of stimulus size during recognition could reflect <lb/>the size of the segmented retinal image or ocular focus <lb/>(compared to a memorized baseline), which is related to the dis-<lb/>tance and hence size of the stimulus. <lb/>RESULTS <lb/>Following one-shot learning trials (where the stimulus identity, <lb/>feature label, and grid cell associations have been encoded; <lb/>see Figure 3A), the model is tested on the stimuli it has learned. <lb/>Once a starting feature has been selected (at random, in the <lb/>absence of a model for bottom-up attention), grid cell-driven <lb/>recognition memory takes over, calculating saccade vectors to <lb/>predicted locations of other stimulus features. The firing of iden-<lb/>tity cells in response to the first perceived feature signifies the <lb/>generation of hypotheses about the stimulus being observed, <lb/>and the leading hypothesis (i.e., the most active identity neuron) <lb/>determines successive saccades to confirm that hypothesis (Fig-<lb/>ures 3B-3D). Each saccade represents an attempt to move a <lb/>different part of the stimulus into foveal focus, based on the lead-<lb/>ing hypothesis (e.g., attending the eyebrow of Mr. Spock, moving <lb/>the eyes in a certain direction and distance, should bring his nose <lb/>into focus). Once a stimulus identity cell reaches the threshold <lb/>for recognition, the next stimulus is presented. Figures 3B-3D <lb/>show examples of successfully recognized stimuli. In Figure 3B, <lb/>panel 3 (Earhart), the system started off with a wrong hypothesis <lb/>(black arrows in line plot) but subsequently recovered. Because <lb/>the relative arrangement of features can be similar between the <lb/>competing stimuli, successive saccades (relative displacement <lb/>vectors) eventually let the correct stimulus identity accumulate <lb/>more evidence because of partial matches with encoded fea-<lb/>tures. That is, the model does not rely exclusively on resets if <lb/>the initial hypothesis is wrong. The correct hypothesis can over-<lb/>take an initially leading, incorrect hypothesis. A reset on the other <lb/>hand is illustrated in Figure 3C, panel 2 (Dido Building Carthage) <lb/>where the initially incorrect hypothesis led to more than two mis-<lb/>matches between the expected and actual outcome of feature <lb/>discrimination and hence triggered an early reset. On the second <lb/>try, starting with a different feature the system recognizes the <lb/>stimulus within 5 fixations. Figures 3B-3D show examples for <lb/>face, scene, and object stimuli, respectively. A total of 99 stimuli <lb/>were tested (33 faces, 33 scenes, 33 objects; see Table S1). <lb/>Most stimuli are recognized within 4-6 saccades (from last <lb/>reset; Figure 3E), and 98 out of 99 stimuli were successfully <lb/>recognized (see Figure 5G for summary statistics). These <lb/>numbers reflect the amount of thresholding among feature label <lb/>cells, and the strength of the connection between feature label <lb/>cells and stimulus identity cells (which could be variable for var-<lb/>iable numbers of features per stimulus; see STAR Methods). A <lb/>high threshold for feature label cell firing means fewer co-active <lb/>feature label cells at a given time (and hence fewer competing <lb/>hypotheses), with the consequence that the leading feature label <lb/>cell accounts for a higher fraction of overall firing after the soft-<lb/>max operation, leading to fewer saccades on average. Scaling <lb/>down the strengths (i.e., gain) of connections from feature label <lb/>cells to stimulus identity neurons increases the average number <lb/>of saccades. <lb/>The activity of grid cells along each saccade vector (or just at <lb/>the start and end locations) can be recorded and binned <lb/></body>

			<page>982</page>

		<note place="footnote"> Current Biology 29, 979-990, March 18, 2019 <lb/></note>

			<body>A <lb/>B <lb/>E <lb/>F <lb/>G <lb/>D <lb/>C <lb/>Figure 3. Recognition of Stimuli <lb/>(A) Salient features are associated with locations in the visual field via grid cells, and each location (red crosses in black circles) is encoded by the phases across <lb/>the entire grid cell ensemble (4 out of 9 scales shown) as a population vector (i.e., a given pixel value across all grid cell rate maps). <lb/>(B) Saccade sequences (red arrows) superimposed on face stimuli (left). Cyan circles indicate the centers of all encoded local features. With each sampled <lb/>feature, a firing rate of the corresponding identity cell is incremented (right). The dashed line indicates the decision threshold. Note, in panel 3, an initially wrong <lb/>hypothesis (black arrows) is overtaken by the correct one. <lb/>(C) Saccades superimposed on scene stimuli. In panel 2, an initially wrong hypothesis yields misdirected saccades (relative to the true stimulus), which leads to an <lb/>early reset (black arrow) because the predicted and actual outcome of the feature discrimination differ persistently. Starting from different initial features <lb/>eventually leads to recognition. <lb/>(D) Recognition of object stimuli. <lb/>(E) Histogram of the number of saccades necessary for recognition across all stimuli. <lb/>(F) Variance of the activity across grid cells along each saccade vector (red bars) or just at the start and end locations (dark gray bars), binned according to the <lb/>direction of the saccade (10-degree bins). 6-fold symmetry akin to fMRI data arises from the underlying symmetry of grid cells. <lb/>(G) All saccade vectors used for the analysis in (F). Image credit: Mr. Spock, Amelia Earhart, Dido Building Carthage: public domain images; Nelson Mandela, <lb/>Schloss Charlottenburg, Brain, Perseus: Creative Commons Attribution. <lb/></body>

			<note place="footnote">Current Biology 29, 979-990, March 18, 2019</note>

		<page>983 <lb/></page>

			<body>according to the direction of the saccade. Plotting the firing rate <lb/>variance across cells (normalized and baseline corrected, using <lb/>10 o bins; Figure 3F) reveals a 6-fold symmetry (a few cells fire a <lb/>lot if saccades align to grid axes, whereas many cells fire at a <lb/>lower rate if they misalign), a direct consequence of the use of <lb/>grid cells. Hence, the model is consistent with 6-fold modulation <lb/>of the fMRI signal by eye-movement direction [18-20] if genera-<lb/>tion of the signal includes history-dependent factors such as <lb/>seen in &apos;&apos;fMRI adaptation&apos;&apos; [32, 33]. The present model thus sug-<lb/>gests a functional explanation for the 6-fold symmetry in fMRI <lb/>signal amplitude in visual experimental paradigms, following <lb/>those seen during navigation in virtual and cognitive spaces <lb/>[34-36]. Figure 3G shows all saccade vectors used for the <lb/>analysis. <lb/>Further advantages of coupling internally generated hypothe-<lb/>ses to eye-movement vectors via grid cells become apparent <lb/>when partially occluded stimuli are considered. The model is <lb/>able to generate a meaningful succession of eye movements, <lb/>even if the stimulus is partially occluded. Two types of simula-<lb/>tions were conducted. In the first simulation, occlusions are <lb/>modeled as regions of random intensity that weakly and indis-<lb/>criminately drive feature label cells. In the second simulation, <lb/>real-world occlusions were used. We randomly selected the <lb/>occlusion from a set of 33 stimuli that consisted of approxi-<lb/>mately equal proportions of faces, scenes, objects, and generic <lb/>textures (e.g., a brick wall, tree bark, etc.). Figure 4 shows ex-<lb/>amples of successful recognition of partially occluded stimuli. <lb/>Starting with an un-occluded feature, the model generates sac-<lb/>cades. If a saccade lands on the occlusion, the output of <lb/>feature label cells is often weak and/or noisy (due to partial <lb/>matches with many features) and does not necessarily pass <lb/>the first threshold, which would allow it to contribute toward ev-<lb/>idence accumulation (firing 2.8 SD above the mean, see Model <lb/>Description), so the firing of the associated identity neuron <lb/>does not increase (see step patterns in Figure 4). Thus, when <lb/>the next saccade is performed, it is based on the currently <lb/>most active stimulus identity cell (which is likely the previously <lb/>most active one) without refinement of the hypothesis. The next <lb/>target is simply selected by the return projection from the iden-<lb/>tity cell to its associated feature label cells, and the starting <lb/>point of the saccade is given by current eye position (given <lb/>by eye-motion updating of grid cells). Thus, saccades to and <lb/>from expected but occluded features can still be performed. <lb/>Moreover, the system can make use of the encoded stimulus <lb/>layout by visiting non-occluded features from different <lb/>(occluded or un-occluded) starting locations, though resets <lb/>may be triggered more often, reflected in an increase number <lb/>of total fixations (Figure 4G). With white noise occlusions, 97 <lb/>out 99 stimuli were recognized. This number dropped to 86 <lb/>with real-world occlusions. However, restricting the number of <lb/>consecutive fixations on the occlusion to 1 improves perfor-<lb/>mance to 92 recognized stimuli (see Figure 5G for summary <lb/>statistics). We hypothesize that saccades to occlusions should <lb/>be rare because they cannot be expected to contribute to ev-<lb/>idence accumulation. In fact, such saccades may occur only <lb/>if subjects are explicitly instructed to infer the location of hidden <lb/>features. To the best of our knowledge, this ability has not been <lb/>tested in the literature, although memory-guided saccades do <lb/>provide indirect evidence. Lucas et al. [14] show that subjects <lb/>can correctly place features (previously encountered as an <lb/>array) on an empty screen, accompanied by fixations of the <lb/>target areas; i.e., the locations of the features are somehow in-<lb/>ferred. Importantly, re-running the same simulations with a new <lb/>random seed does not lead to failed recognition in the same <lb/>stimuli. In other words, a recognition failure is specific to a <lb/>given order of saccade targets, and most possible orders of <lb/>saccades produces correct recognition. <lb/>As outlined in the model overview, size invariance can be <lb/>achieved by allowing a variable gain factor relating the magni-<lb/>tude of grid cell displacement vectors and the magnitude of <lb/>oculo-motor output. Correctly estimating this gain depends on <lb/>a size estimate of the stimulus relative to some memorized base-<lb/>line value. This allows the model to recognize a previously en-<lb/>coded stimulus at different distances (i.e., a downscaled image; <lb/>Figure 5A). In analogy to the effect of environmental change on <lb/>grid cells in spatial memory [30], the first (foveal) sensory input <lb/>is assumed to align the grid cell ensemble (accounting for posi-<lb/>tional variation). Between full-size and half-size versions of the <lb/>same stimulus, no changes to the vector computation performed <lb/>by the grid cell ensemble or the distance cell model are needed. <lb/>Only the extent of the foveal array is downscaled to match the <lb/>smaller image (e.g., corresponding to an attentional modulation <lb/>determining the extent of the retinal image used). Simply scaling <lb/>the gain of the magnitude of all saccade vectors by the same <lb/>amount suffices to generate a successful sequence of saccades <lb/>(Figures 5A and 5B). 98 out 99 stimuli were recognized when <lb/>downscaled (Figure 5G), with a similar median number of fixa-<lb/>tions (Figure 5B) compared to default size stimuli. <lb/>Finally, disconnecting the grid system from the rest of the <lb/>model leads to a sharp drop in recognition performance. We con-<lb/>ducted two simulations, assuming that in the absence of grid cell-<lb/>guided saccades bottom-up attention will select fixation targets <lb/>randomly among all available targets. As a consequence, sensory <lb/>predictions rarely match the outcome of feature discriminations, <lb/>leading to poor evidence accumulation and/or a sharp increase <lb/>in the number of resets. We define 10 resets without recognition <lb/>as a failure to recognize the stimulus. Under these conditions, <lb/>recognition performance drops to 40 recognized stimuli. Since it <lb/>is unlikely that all potential targets for bottom-up attention are <lb/>memorized, we conducted a second simulation where we <lb/>included 5 additional targets that can attract attention (i.e., dis-<lb/>tractors) in each image. Attention can then select the next fixation <lb/>target among 14 targets (9 memorized + 5 distractors). This re-<lb/>duces recognition performance further to 16 out of 99 items. <lb/>Thus, lesions to the grid system may produce prosopagnosia-<lb/>like symptoms (see Discussion). <lb/>DISCUSSION <lb/>We have presented a simple model of how the brain may calcu-<lb/>late saccade vectors during recognition memory. The model is <lb/>the first to suggest a specific role for grid cells in visual recogni-<lb/>tion and consistent with recent evidence of grid cells driven by <lb/>eye movements [17-20]. The core predictions are that visual <lb/>recognition memory will engage grid cells whenever the relative <lb/>layout of multiple features must be taken into account, that le-<lb/>sions to the grid cell system will preclude this relational (i.e., con-<lb/>figural) processing, and that the system allows an agent to infer <lb/></body>

			<page>984</page>

		<note place="footnote"> Current Biology 29, 979-990, March 18, 2019 <lb/></note>

			<body>A <lb/>C <lb/>D <lb/>B <lb/>E <lb/>F <lb/>G <lb/>Figure 4. Occlusions <lb/>The (expected) locations of occluded features can still serve as start and end points of saccades, however, occluded features rarely increment the output of the <lb/>associated identity cells (stair patterns in line plots). <lb/>(A) Face stimuli with white noise occlusions. Saccades sequences (red arrows) superimposed on face stimuli (left). Cyan circles indicate the centers of all encoded <lb/>local features. With each sampled feature, a firing rate of the corresponding identity cell is incremented (right). <lb/>(B) Same as (A) with real-world occlusions. <lb/>(C and D) Scene stimuli with white noise (C) and real-world (D) occlusions. <lb/>(E and F) Object stimuli with white noise (E) and real-world (F) occlusions. <lb/>(G) The total number of fixations (including resets) for white noise (left) and real-world occlusions. <lb/>Image credit: F.S., S.B.P., M.E.F.M.: used with permission; Boat, La Pelosa Beach: supplied by author, with permission; Shuttle: public domain image; Carl <lb/>Sagan, Nina Simone, Orion, Sigourney Weaver, Angela Merkel, brick wall, Picasso, Emmy Noether plaque: Creative Commons Attribution. <lb/></body>

			<note place="footnote">Current Biology 29, 979-990, March 18, 2019</note>

		<page>985 <lb/></page>

			<body>the locations of occluded features. The model suggests how <lb/>one-shot learning of exemplars may occur (paralleling one-shot <lb/>learning of episodic memories in the hippocampal formation). <lb/>Following this analogy, if deliberate recall is viewed as a top-<lb/>down reconstruction of a previously experienced sensory repre-<lb/>sentation, then the model also shows how relational information <lb/>(sensory information tied to an arrangement of locations) could <lb/>be made available for reinstatement. The specific visual mecha-<lb/>nism presented here may extend to object recognition in other <lb/>modalities [37]. <lb/>In viewing tasks where whole scenes are presented within a <lb/>constant border [17, 20], grid cells appear to be anchored to <lb/>the borders, as are spatial grids in rodents across multiple ses-<lb/>sions of navigation within a constant enclosure. In these experi-<lb/>ments, the offset of grid cells relative to the enclosure only shifts <lb/>in extreme circumstances, in which place cells &apos;&apos;remap,&apos;&apos; like <lb/>moving the entire enclosure to a different experimental room <lb/>[30]. We suggest that in the absence of environmental anchor <lb/>points, grid-like firing patterns would be anchored to a salient <lb/>foreground stimulus that has to be recognized (e.g., when recog-<lb/>nizing illuminated faces or objects in darkness). A strong predic-<lb/>tion is that the grid patterns shift with the to-be-recognized <lb/>object when presented in a new location. Grid cells could be <lb/>anchored to the sensory scene by connections from place cells <lb/>in the spatial case [38] and similarly by connections from feature <lb/>label cells during visual recognition. <lb/>A <lb/>B <lb/>C <lb/>D <lb/>G <lb/>F <lb/>E <lb/>Figure 5. Size Invariance and Grid Cell Le-<lb/>sions <lb/>(A) Top: Illustration of the relationship between <lb/>distances in &apos;&apos;neural space&apos;&apos; (on grid patterns) and <lb/>visual space. Panels 2-4: Scaled down (50%) <lb/>stimuli within the presentation frame of default <lb/>size. Scaling the gain between the displacement <lb/>vector on the grid pattern and the oculo-motor <lb/>output uniformly for all saccades allows the model <lb/>to sample all features of downscaled stimuli, <lb/>leading to successful recognition irrespective of <lb/>size. Saccade sequences (red arrows) are super-<lb/>imposed on stimuli (left). Cyan circles indicate the <lb/>centers of all encoded local features. With each <lb/>sampled feature, a firing rate of the corresponding <lb/>identity cell is incremented (right). <lb/>(B) Histogram of the number of saccades (from last <lb/>reset) necessary for recognition across all stimuli. <lb/>(C) Upon disconnecting grid cells from the model, it <lb/>is assumed bottom-up attention will select among <lb/>possible targets randomly. Panel 1: The next pre-<lb/>dicted feature (red circle) and the foveated feature <lb/>rarely match, leading to poor evidence accumula-<lb/>tion. Panel 2: By chance the next predicted feature <lb/>(red circle) can be selected for foveation. Panel 3: <lb/>The absence of confirmatory predictions (and thus <lb/>facilitation among feature label cells) increases the <lb/>number of false hypotheses of stimulus identity, <lb/>leading to predicted features that are not present in <lb/>the stimulus being viewed (purple circle). The <lb/>behavior in panels 1 and 3 reduces recognition <lb/>performance, while that in panel 2 contributes to <lb/>residual recognition ability. <lb/>(D) Cyan circles indicate memorized features. <lb/>Adding additional potential targets for bottom-up <lb/>attention (i.e., distractors, filled green circles) ex-<lb/>acerbates the performance drop. <lb/>(E-G) The average number of fixations from the last <lb/>reset across conditions (E). The average number of <lb/>fixations including resets (F). Recognition and fail-<lb/>ure rates (no recognition within 10 resets, &apos;&apos;not rec.&apos;&apos;) <lb/>for all tested conditions (G). Default condition, blue; <lb/>white noise and real-world occlusions, red and <lb/>yellow, respectively; real-world occlussions limited <lb/>to one consecutive saccade towards the occlusion, <lb/>purple; 50% shrunken stimuli, green; grid cell lesion <lb/>with and without distractors, light blue and dark red, <lb/>respectively. <lb/>Image credit: B.B., P.A.: used with permission; Mr. <lb/>Spock, Ai Wei Wei, Kiwi, bathroom: public domain <lb/>images. <lb/></body>

			<page>986</page>

		<note place="footnote"> Current Biology 29, 979-990, March 18, 2019 <lb/></note>

			<body>The implementation of the model is deliberately simplistic so <lb/>as to focus on the basic mechanism proposed. Grid cells are im-<lb/>plemented as a look-up table (firing rate maps). The remaining <lb/>components of the model are also fairly conventional (visual <lb/>feature detectors, identity or concept cells). The model suggests <lb/>a parsimonious encoding of relative locations. The distance cell <lb/>and grid cell systems are not stimulus specific. The number of <lb/>stimulus identity cells and feature label cells per stimulus is of <lb/>the order of the number of identifying (high-level) features, <lb/>though the model does not preclude that visual features could <lb/>be also encoded in a distributed manner. Despite this simplicity, <lb/>tentative anatomical links can be established. The stimulus iden-<lb/>tity cells in the model are reminiscent of concept cells found in <lb/>the human hippocampus [39]. The use of hippocampal cells in <lb/>conjunction with entorhinal grid cells would be consistent with <lb/>the involvement of the hippocampal formation in some percep-<lb/>tual tasks [14, 40, 41]. It may also account for medial temporal <lb/>lobe interactions with the visual system during visual recognition <lb/>of relational information and object-location binding [22-24], and <lb/>for memory-guided attention [13, 14], as well as during naviga-<lb/>tion [42, 43]. <lb/>The hippocampus and the parahippocampal gyrus also <lb/>appear to support retrieval of pre-experimental knowledge about <lb/>stimulus location within a scene [13] or pre-experimental knowl-<lb/>edge about specific faces [44]. The fact that the hippocampus is <lb/>required to recognize familiar faces, but not new, recently seen <lb/>faces [45], suggests that the grid cell system may be predomi-<lb/>nantly required to support long-term relational memory for <lb/>familiar identifiable stimuli. <lb/>The presence of object, scene, and face-specific processing <lb/>streams in anterior temporal lobe suggests that recognition <lb/>memory for all these stimulus categories could benefit from <lb/>anatomically close grid cell representations in entorhinal cortex. <lb/>Intriguingly, face processing in Macaques engages entorhinal <lb/>cortex in addition to homologs of human face-processing areas <lb/>[46]. The prominent role of perirhinal cortex in the processing and <lb/>memory of objects and faces [47, 48] suggests that neural pop-<lb/>ulations akin to feature label cells may reside there. Face-selec-<lb/>tive patches have also been identified in the ventral anterior <lb/>temporal lobes, adjacent to perirhinal and entorhinal cortices <lb/>[49, 50]. Similarly, the anatomical adjacency of the parahippo-<lb/>campal areas to entorhinal cortex makes it a candidate structure <lb/>for cells that may represent salient parts of a scene, similar to <lb/>feature label cells in the model. Supporting this view, anatomical <lb/>projections have been reported between entorhinal cortex and <lb/>parahippocampal areas TH and TF, as well as IT/TE and perirhi-<lb/>nal cortex [51-53]. Intriguingly, Blatt et al. [54] report that these <lb/>same areas (TF, TE) are connected to LIP, a structure crucial <lb/>for saccade execution, which is directly connected to the supe-<lb/>rior colliculus (SC) and the frontal eye fields (FEFs). It has been <lb/>suggested that memory-guided saccades rely on SC being sub-<lb/>ject to top-down influence via FEF and LIP [55]. A saccade could <lb/>then be executed to the location in the visual field indicated by <lb/>the SC, which most closely matches the endpoint of a grid <lb/>cell-derived saccade vector [25]. Grid cell-based computations <lb/>in and near entorhinal cortex for memory-guided saccades <lb/>should then precede motor output and thus corollary discharge <lb/>mediated by thalamic pathways involving the SC (for review, see <lb/>[56, 57],). <lb/>We note that categorical object recognition (e.g., distinguish-<lb/>ing a car from an elephant) need not make use of the proposed <lb/>mechanism (the different categories can be distinguished by <lb/>their constituent features irrespective of their layout). However, <lb/>when fine within-category judgments of feature layout are <lb/>required, relational processing may be necessary, for which <lb/>grid cells can provide the neural substrate. Focal lesions to <lb/>grid cell systems should then disrupt relational memory process-<lb/>ing (with bottom-up attentional saccade guidance preserved), <lb/>yielding deficits in the recognition of exemplars with spared cate-<lb/>gory recognition. Interestingly, Damasio et al. [58] report that true <lb/>prosopagnosics are still able to recognize a face as a face and <lb/>name it as such (when the query is presented similarly to objects) <lb/>but are unable to report specific identity. Additionally, recog-<lb/>nizing object exemplars can also be impaired (e.g., a bird <lb/>watcher being unable to recognize individual birds [58]). We <lb/>have shown that prosopagnosia-like symptoms may arise from <lb/>disconnecting grid cells (and/or stimulus identity cells) from neu-<lb/>rons (akin to feature label cells in the model) in upstream areas. <lb/>Although sequences of saccades can in principle be solely <lb/>driven by bottom-up attention to salient features in the visual <lb/>field, the relational content is essential for deliberate recall. <lb/>Further support comes from experiments showing that recogni-<lb/>tion of multi-featured items is more difficult when stimuli are <lb/>scrambled [12, 59]. Moreover, bottom-up attention alone <lb/>cannot account for eye movements during acts of visual imagery <lb/>[60, 61], i.e., when specific eye-movement patterns are induced <lb/>from memory [21, 62, 63]. <lb/>On a purely behavioral level, an interesting prediction is that <lb/>idiosyncratic differences between subjects could transfer be-<lb/>tween apparently radically different tasks, if they employ grid <lb/>cells. For example, bad navigators could also be bad at recog-<lb/>nizing exemplars, with the strongest effect likely occurring for <lb/>familiar, non-famous faces and difficulty-matched groups of ob-<lb/>ject or scene exemplars [64]. Although configural processing has <lb/>mainly been associated with faces [65], studies employing <lb/>&apos;&apos;greebles&apos;&apos; show that the hallmarks of holistic, face-like process-<lb/>ing can be observed for objects, potentially related to expertise <lb/>in distinguishing individual (object) exemplars [66, 67]. <lb/>Since successful saccades in the model depend on the rela-<lb/>tive arrangement of features, the model is compatible with the <lb/>notion of holistic processing, as exemplified by the &apos;&apos;composite <lb/>face effect&apos;&apos; [68], the &apos;&apos;part-whole recognition effect&apos;&apos; [12], and <lb/>the difficulty to process and recognize upside-down faces [69], <lb/>for which saccades would be guided in the wrong direction. <lb/>Notably, Tanaka and Sengco [12] have shown that displacement <lb/>of one facial feature (e.g., increasing eye separation) reduces the <lb/>recognition rate for other features within the same face, a finding <lb/>that maps well onto the present account in which individual fea-<lb/>tures determine the next saccade vector. The successful recog-<lb/>nition of stimuli that are partially occluded is also a consequence <lb/>of the vector-based, relational mechanism and may form the ba-<lb/>sis of our ability to infer the locations of occluded features. <lb/>If size invariance is accomplished within the grid cell system, <lb/>then the present account predicts a rescaling of visual grids <lb/>with the size of the stimulus in the visual field (cf. [31].). Regarding <lb/>rotations, classic models of object recognition have proposed <lb/>that either viewpoint-invariant (3D) representations of objects <lb/>are stored [70], or that multiple canonical views of an object <lb/></body>

			<note place="footnote">Current Biology 29, 979-990, March 18, 2019</note>

		<page>987 <lb/></page>

			<body>are stored, with intermediate views synthesized by interpolation <lb/>[71, 72]. It has been suggested [73] that both view-invariant <lb/>(structural) as well as view-based approaches are implemented <lb/>in the brain, and that structural-descriptions might support cate-<lb/>gory level classification, whereas view-based mechanisms could <lb/>support item-specific recognition. The mechanism we have <lb/>proposed here could, for instance, operate on one or several in-<lb/>dividual view-based, canonical representations (possibly with <lb/>separate feature label cells for each view, but the same identity <lb/>neuron(s) across views) in anterior temporal regions, corre-<lb/>sponding to different rotation angles, and interpolate saccade <lb/>length between the two closest views. Attentional mechanisms <lb/>could facilitate small corrective saccades (i.e., microsaccades) <lb/>if the calculated saccade lands sufficiently close to the target. <lb/>Finally, the present account can accommodate a broader <lb/>Bayesian interpretation of perception. For instance, information <lb/>about context could be integrated (as prior beliefs), similar to <lb/>the facilitatory bias of predictions in the current model, applied <lb/>to a subset of stimuli one is expecting to encounter. For example, <lb/>at a workplace one would expect to encounter colleagues and <lb/>hence firing of cells representing their features and identities <lb/>may be preferentially incremented. However, note that individual <lb/>memory-dependent saccades must be guided by one and only <lb/>one hypothesis at a time in order to move the fovea to the loca-<lb/>tion of an expected feature, rather than, e.g., between two <lb/>competing locations. Also note, already the first firing of an iden-<lb/>tity cell represents the formation of a hypothesis or belief. Inte-<lb/>grated with contextual and gist information, a very small number <lb/>of saccades may suffice to reach sufficient confidence. <lb/>Conclusions <lb/>We have presented a mechanistic model of visual recognition <lb/>memory via grid cells, better known for their role in spatial navi-<lb/>gation. However, grid-like activity in visual paradigms suggests <lb/>that the same neural circuit could also contribute to visual pro-<lb/>cessing. Vestibular and bodily motor efference signals could <lb/>drive grid cells during path integration and large-scale spatial <lb/>navigation, and occulomotor inputs could update the same cells <lb/>when an agent is engaged in a visual discrimination task. Simi-<lb/>larly, by extension grid cells could provide a compact code for <lb/>locations in any continuous space, e.g., in conceptual [36] or <lb/>auditory [74] spaces. The present model offers an explanation <lb/>as to why medial temporal structures are sometimes involved <lb/>in recognition memory and supports the emerging notion that <lb/>grid cells are part of a universal representational system, where <lb/>the inputs determine the exact response properties of grid cells <lb/>to amend their neural code to a wide range of tasks. <lb/>STAR+METHODS <lb/>Detailed methods are provided in the online version of this paper <lb/>and include the following: <lb/>d KEY RESOURCE TABLE <lb/>d CONTACT FOR REAGENT AND RESOURCE SHARING <lb/>d EXPERIMENTAL MODEL AND SUBJECT DETAILS <lb/>d METHOD DETAILS <lb/>B Model overview <lb/>B Action/Perception cycles <lb/>B Sensory predictions and resets <lb/>B Grid Cells and Vector Computations <lb/>B Training phase <lb/>B Feature detection and ambiguity <lb/>B Position Invariance <lb/>B Size Invariance <lb/>d QUANTIFICATION AND STATISTICAL ANALYSIS <lb/>d DATA AND SOFTWARE AVAILABILITY <lb/>SUPPLEMENTAL INFORMATION <lb/>Supplemental Information includes one figure and two tables and can be found <lb/>with this article online at https://doi.org/10.1016/j.cub.2019.01.077. <lb/></body>

			<div type="acknowledgement">ACKNOWLEDGMENTS <lb/>We acknowledge funding by the ERC Advanced grant NEUROMEM, Well-<lb/>come, and the European Union&apos;s Horizon 2020 research and innovation pro-<lb/>gramme under grant agreement no. 785907 Human Brain Project SGA2. We <lb/>thank Daniel Bush, James Bisby, Sofie Meyer, Alexandra Constantinescu, <lb/>Jesse Geerts, and Talfan Evans for comments and useful discussions. <lb/></div>

			<div type="annex">AUTHOR CONTRIBUTIONS <lb/>A.B. conceptualized, designed, and implemented the model in code, created <lb/>visualizations, and wrote the paper. N.B conceptualized the model, wrote the <lb/>paper, and secured funding. <lb/>DECLARATION OF INTERESTS <lb/>The authors declare no competing interests. <lb/></div>

			<front>Received: September 28, 2018 <lb/>Revised: December 23, 2018 <lb/>Accepted: January 30, 2019 <lb/>Published: March 7, 2019 <lb/></front>

			<listBibl>REFERENCES <lb/>1. Fukushima, K., and Miyake, S. (1982). Neocognitron: A new algorithm for <lb/>pattern recognition tolerant of deformations and shifts in position. Pattern <lb/>Recognit. 15, 455-469. <lb/>2. Riesenhuber, M., and Poggio, T. (1999). Hierarchical models of object <lb/>recognition in cortex. Nat. Neurosci. 2, 1019-1025. <lb/>3. LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature 521, <lb/>436-444. <lb/>4. Hubel, D.H., and Wiesel, T.N. (1962). Receptive fields, binocular interac-<lb/>tion and functional architecture in the cat&apos;s visual cortex. J. Physiol. 1, <lb/>106-154. <lb/>5. Yarbus, A.L. (1967). Eye Movements During Perception of Complex <lb/>Objects (Springer). <lb/>6. Neisser, U. (1967). Cognitive Psychology. (Appleton-Century-Crofts). <lb/>7. Noton, D., and Stark, D. (1970). Scanpaths in eye movements during <lb/>Paftern perception. Science 171, 308-311. <lb/>8. Gregory, R.L. (1980). Perceptions as hypotheses. Philos. Trans. R. Soc. <lb/>Lond. B Biol. Sci. 290, 181-197. <lb/>9. Friston, K., Adams, R.A., Perrinet, L., and Breakspear, M. (2012). <lb/>Perceptions as hypotheses: Saccades as experiments. Front. Psychol. <lb/>28, 151. <lb/>10. Gilchrist, I.D., Brown, V., and Findlay, J.M. (1997). Saccades without eye <lb/>movements. Nature 390, 130-131. <lb/>11. Henderson, J.M., Williams, C.C., and Falk, R.J. (2005). Eye movements are <lb/>functional during face learning. Mem. Cognit. 33, 98-106. <lb/></listBibl>

			<page>988</page>

		<note place="footnote">Current Biology 29, 979-990, March 18, 2019 <lb/></note>

			<listBibl>12. Tanaka, J.W., and Sengco, J.A. (1997). Features and their configuration in <lb/>face recognition. Mem. Cognit. 25, 583-592. <lb/>13. Summerfield, J.J., Lepsien, J., Gitelman, D.R., Mesulam, M.M., and <lb/>Nobre, A.C. (2006). Orienting attention based on long-term memory expe-<lb/>rience. Neuron 49, 905-916. <lb/>14. Lucas, H.D., Duff, M.C., and Cohen, N.J. (2018). The hippocampus pro-<lb/>motes effective saccadic information gathering in humans. J. Cogn. <lb/>Neurosci. 2018, 1-6. <lb/>15. O&apos;Keefe, J., and Conway, D.H. (1978). Hippocampal place units in the <lb/>freely moving rat: Why they fire where they fire. Exp. Brain Res. 31, <lb/>573-590. <lb/>16. Hafting, T., Fyhn, M., Molden, S., Moser, M.B., and Moser, E.I. (2005). <lb/>Microstructure of a spatial map in the entorhinal cortex. Nature 436, <lb/>801-806. <lb/>17. Killian, N.J., Jutras, M.J., and Buffalo, E.A. (2012). A map of visual space in <lb/>the primate entorhinal cortex. Nature 491, 761-764. <lb/>18. Nau, M., Navarro Schro ¨der, T., Bellmund, J.L.S., and Doeller, C.F. (2018). <lb/>Hexadirectional coding of visual space in human entorhinal cortex. Nat. <lb/>Neurosci. 21, 188-190. <lb/>19. Nau, M., Julian, J.B., and Doeller, C.F. (2018). How the brain&apos;s navigation <lb/>system shapes our visual experience. Trends Cogn. Sci. 22, 810-825. <lb/>20. Julian, J.B., Keinath, A.T., Frazzetta, G., and Epstein, R.A. (2018). Human <lb/>entorhinal cortex represents visual space using a boundary-anchored <lb/>grid. Nat. Neurosci. 21, 191-194. <lb/>21. Hannula, D.E., Ryan, J.D., Tranel, D., and Cohen, N.J. (2007). Rapid onset <lb/>relational memory effects are evident in eye movement behavior, but not in <lb/>hippocampal amnesia. J. Cogn. Neurosci. 19, 1690-1705. <lb/>22. Olsen, R.K., Lee, Y., Kube, J., Rosenbaum, R.S., Grady, C.L., Moscovitch, <lb/>M., and Ryan, J.D. (2015). The role of relational binding in item memory: <lb/>Evidence from face recognition in a case of developmental amnesia. <lb/>J. Neurosci. 35, 5342-5350. <lb/>23. Eichenbaum, H., Yonelinas, A.P., and Ranganath, C. (2007). The medial <lb/>temporal lobe and recognition memory. Annu. Rev. Neurosci. 30, <lb/>123-152. <lb/>24. Pertzov, Y., Miller, T.D., Gorgoraptis, N., Caine, D., Schott, J.M., Butler, C., <lb/>and Husain, M. (2013). Binding deficits in memory following medial tempo-<lb/>ral lobe damage in patients with voltage-gated potassium channel com-<lb/>plex antibody-associated limbic encephalitis. Brain 136, 2474-2485. <lb/>25. Killian, N.J., Potter, S.M., and Buffalo, E.A. (2015). Saccade direction en-<lb/>coding in the primate entorhinal cortex during visual exploration. Proc. <lb/>Natl. Acad. Sci. USA 112, 15743-15748. <lb/>26. McNaughton, B.L., Battaglia, F.P., Jensen, O., Moser, E.I., and Moser, <lb/>M.B. (2006). Path integration and the neural basis of the &apos;cognitive map&apos;. <lb/>Nat. Rev. Neurosci. 7, 663-678. <lb/>27. Kubie, J.L., and Fenton, A.A. (2012). Linear look-ahead in conjunctive <lb/>cells: An entorhinal mechanism for vector-based navigation. Front. <lb/>Neural Circuits. 6, 20. <lb/>28. Erdem, U.M., and Hasselmo, M. (2012). A goal-directed spatial navigation <lb/>model using forward trajectory planning based on grid cells. Eur. J. <lb/>Neurosci. 35, 916-931. <lb/>29. Bush, D., Barry, C., Manson, D., Burgess, N., Barry, C., Manson, D., and <lb/>Burgess, N. (2015). Using grid cells for navigation. Neuron 87, 507-520. <lb/>30. Fyhn, M., Hafting, T., Treves, A., Moser, M.-B., and Moser, E.I. (2007). <lb/>Hippocampal remapping and grid realignment in entorhinal cortex. <lb/>Nature 446, 190-194. <lb/>31. Barry, C., Hayman, R., Burgess, N., and Jeffery, K.J. (2007). Experience-<lb/>dependent rescaling of entorhinal grids. Nat. Neurosci. 10, 682-684. <lb/>32. Grill-Spector, K., and Malach, R. (2001). fMR-adaptation: A tool for study-<lb/>ing the functional properties of human cortical neurons. Acta Psychol. <lb/>(Amst.) 107, 293-321. <lb/>33. Doeller, C.F., Barry, C., and Burgess, N. (2010). Evidence for grid cells in a <lb/>human memory network. Nature 463, 657-661. <lb/>34. Bellmund, J.L., Deuker, L., Navarro Schro ¨der, T., and Doeller, C.F. (2016). <lb/>Grid-cell representations in mental simulation. Elife 30, e17089. <lb/>35. Horner, A.J., Bisby, J.A., Zotow, E., Bush, D., and Burgess, N. (2016). Grid-<lb/>like processing of imagined navigation. Curr. Biol. 21, 842-847. <lb/>36. Constantinescu, A.O., O&apos;Reilly, J.X., and Behrens, T.E.J. (2016). <lb/>Organizing conceptual knowledge in humans with a gridlike code. <lb/>Science 352, 1464-1468. <lb/>37. Hawkins, J., Lewis, M., Klukas, M., Purdy, S., and Ahmad, S. (2019). A <lb/>framework for intelligence and cortical function based on grid cells in <lb/>the neocortex. Front. Neural Circuits 12, 121. <lb/>38. O&apos;Keefe, J., and Burgess, N. (2005). Dual phase and rate coding in hippo-<lb/>campal place cells: Theoretical significance and relationship to entorhinal <lb/>grid cells. Hippocampus 15, 853-866. <lb/>39. Quiroga, R.Q. (2012). Concept cells: The building blocks of declarative <lb/>memory functions. Nat. Rev. Neurosci. 13, 587-597. <lb/>40. Lee, A.C.H., Buckley, M.J., Pegman, S.J., Spiers, H., Scahill, V.L., Gaffan, <lb/>D., Bussey, T.J., Davies, R.R., Kapur, N., Hodges, J.R., and Graham, K.S. <lb/>(2005). Specialization in the medial temporal lobe for processing of objects <lb/>and scenes. Hippocampus 15, 782-797. <lb/>41. Barense, M.D., Henson, R.N.A., and Graham, K.S. (2011). Perception and <lb/>conception: Temporal lobe activity during complex discriminations of <lb/>familiar and novel faces and objects. J. Cogn. Neurosci. 23, 3052-3067. <lb/>42. Sherrill, K.R., Chrastil, E.R., Ross, R.S., Erdem, U.M., Hasselmo, M.E., and <lb/>Stern, C.E. (2015). Functional connections between optic flow areas and <lb/>navigationally responsive brain regions during goal-directed navigation. <lb/>Neuroimage 118, 386-396. <lb/>43. Saleem, A.B., Diamanti, E.M., Fournier, J., Harris, K.D., and Carandini, M. <lb/>(2018). Coherent encoding of subjective spatial position in visual cortex <lb/>and hippocampus. Nature 562, 124-127. <lb/>44. Trinkler, I., King, J.A., Doeller, C.F., Rugg, M.D., and Burgess, N. (2009). <lb/>Neural bases of autobiographical support for episodic recollection of <lb/>faces. Hippocampus 19, 718-730. <lb/>45. Bird, C.M., and Burgess, N. (2008). The hippocampus supports recogni-<lb/>tion memory for familiar words but not unfamiliar faces. Curr. Biol. 18, <lb/>1932-1936. <lb/>46. Ku, S.P., Tolias, A.S., Logothetis, N.K., and Goense, J. (2011). fMRI of <lb/>the face-processing network in the ventral temporal lobe of awake and <lb/>anesthetized macaques. Neuron 70, 352-362. <lb/>47. Brown, M.W., and Aggleton, J.P. (2001). Recognition memory: What are <lb/>the roles of the perirhinal cortex and hippocampus? Nat. Rev. Neurosci. <lb/>2, 51-61. <lb/>48. Murray, E.A., Bussey, T.J., and Saksida, L.M. (2007). Visual perception <lb/>and memory: A new view of medial temporal lobe function in primates <lb/>and rodents. Annu. Rev. Neurosci. 30, 99-122. <lb/>49. Jonas, J., Jacques, C., Liu-Shuang, J., Brissart, H., Colnat-Coulbois, S., <lb/>Maillard, L., and Rossion, B. (2016). A face-selective ventral occipito-tem-<lb/>poral map of the human brain with intracerebral potentials. Proc. Natl. <lb/>Acad. Sci. USA 113, E4088-E4097. <lb/>50. Collins, J.A., and Olson, I.R. (2014). Beyond the FFA: The role of the ventral <lb/>anterior temporal lobes in face processing. Neuropsychologia 61, 65-79. <lb/>51. Kosel, K.C., Van Hoesen, G.W., and Rosene, D.L. (1982). Non-hippocampal <lb/>cortical projections from the entorhinal cortex in the rat and rhesus monkey. <lb/>Brain Res. 244, 201-213. <lb/>52. Insausti, R., Herrero, M.T., and Witter, M.P. (1997). Entorhinal cortex of the <lb/>rat: Cytoarchitectonic subdivisions and the origin and distribution of <lb/>cortical efferents. Hippocampus 7, 146-183. <lb/>53. Saleem, K.S., and Tanaka, K. (1996). Divergent projections from the ante-<lb/>rior inferotemporal area TE to the perirhinal and entorhinal cortices in the <lb/>macaque monkey. J. Neurosci. 16, 4757-4775. <lb/>54. Blatt, G.J., Andersen, R.A., and Stoner, G.R. (1990). Visual receptive field <lb/>organization and cortico-cortical connections of the lateral intraparietal <lb/>area (area LIP) in the macaque. J. Comp. Neurol. 299, 421-445. <lb/></listBibl>

			<note place="headnote">Current Biology 29, 979-990, March 18, 2019</note>

		<page>989 <lb/></page>

			<listBibl>55. Pierrot-Deseilligny, C., Rivaud, S., Gaymard, B., and Agid, Y. (1991). <lb/>Cortical control of memory-guided saccades in man. Exp. Brain Res. 83, <lb/>607-617. <lb/>56. Wurtz, R.H., McAlonan, K., Cavanaugh, J., and Berman, R.A. (2011). <lb/>Thalamic pathways for active vision. Trends Cogn. Sci. 15, 177-184. <lb/>57. Sommer, M.A., and Wurtz, R.H. (2008). Brain circuits for the internal moni-<lb/>toring of movements. Annu. Rev. Neurosci. 31, 317-338. <lb/>58. Damasio, A.R., Damasio, H., and Van Hoesen, G.W. (1982). Prosopagnosia: <lb/>Anatomic basis and behavioral mechanisms. Neurology 32, 331-341. <lb/>59. Collishaw, S.M., and Hole, G.J. (2000). Featural and configurational pro-<lb/>cesses in the recognition of faces of different familiarity. Perception 29, <lb/>893-909. <lb/>60. Brandt, S.A., and Stark, L.W. (1997). Spontaneous eye movements during <lb/>visual imagery reflect the content of the visual scene. J. Cogn. Neurosci. 9, <lb/>27-38. <lb/>61. Bone, M.B., St-Laurent, M., Dang, C., McQuiggan, D.A., Ryan, J.D., and <lb/>Buchsbaum, B.R. (2018). Eye movement reinstatement and neural reacti-<lb/>vation during mental imagery. Cereb. Cortex. Published online February 3, <lb/>2018. https://doi.org/10.1093/cercor/bhy014. <lb/>62. Meister, M.L.R., and Buffalo, E.A. (2016). Getting directions from the <lb/>hippocampus: The neural connection between looking and memory. <lb/>Neurobiol. Learn. Mem. 134 (Pt A), 135-144. <lb/>63. Urgolites, Z.J., Smith, C.N., and Squire, L.R. (2018). Eye movements <lb/>support the link between conscious memory and medial temporal lobe <lb/>function. Proc. Natl. Acad. Sci. USA 115, 7599-7604. <lb/>64. Zachariou, V., Safiullah, Z.N., and Ungerleider, L.G. (2017). The fusiform <lb/>and occipital face areas can process a nonface category equivalently to <lb/>faces. J. Cogn. Neurosci. 30, 1-8. <lb/>65. Maurer, D., Grand, R.L., and Mondloch, C.J. (2002). The many faces of <lb/>configural processing. Trends Cogn. Sci. 6, 255-260. <lb/>66. Gauthier, I., Tarr, M.J., Anderson, A.W., Skudlarski, P., and Gore, J.C. <lb/>(1999). Activation of the middle fusiform &apos;face area&apos; increases with exper-<lb/>tise in recognizing novel objects. Nat. Neurosci. 2, 568-573. <lb/>67. Campbell, A., and Tanaka, J.W. (2018). Inversion impairs expert budger-<lb/>igar identity recognition: A face-like effect for a nonface object of exper-<lb/>tise. Perception 47, 647-659. <lb/>68. Young, A.W., Hellawell, D., and Hay, D.C. (2013). Configurational informa-<lb/>tion in face perception. Perception 42, 1166-1178. <lb/>69. Yin, R.K. (1969). Looking at upide-down faces. J. Exp. Psychol. 81, <lb/>141-145. <lb/>70. Marr, D., and Nishihara, H.K. (1978). Representation and recognition of the <lb/>spatial organization of three-dimensional shapes. Proc. R. Soc. Lond. B <lb/>Biol. Sci. 200, 269-294. <lb/>71. Logothetis, N.K., Pauls, J., Bu ¨lthoff, H.H., and Poggio, T. (1994). View-<lb/>dependent object recognition by monkeys. Curr. Biol. 4, 401-414. <lb/>72. Blanz, V., Tarr, M.J., and Bu ¨lthoff, H.H. (1999). What object attributes <lb/>determine canonical views? Perception 28, 575-599. <lb/>73. Tarr, M.J., and Bu ¨lthoff, H.H. (1998). Image-based object recognition in <lb/>man, monkey and machine. Cognition 67, 1-20. <lb/>74. Aronov, D., Nevers, R., and Tank, D.W. (2017). Mapping of a non-spatial <lb/>dimension by the hippocampal-entorhinal circuit. Nature 543, 719-722. <lb/>75. Bush, D., and Burgess, N. (2014). A hybrid oscillatory interference/contin-<lb/>uous attractor network model of grid cell firing. J. Neurosci. 34, 5065-<lb/>5079. <lb/>76. Burak, Y., and Fiete, I.R. (2009). Accurate path integration in continuous <lb/>attractor network models of grid cells. PLoS Comput. Biol. 5, e1000291. <lb/>77. Hasselmo, M.E., Rolls, E.T., and Baylis, G.C. (1989). The role of expression <lb/>and identity in the face-selective responses of neurons in the temporal <lb/>visual cortex of the monkey. Behav. Brain Res. 32, 203-218. <lb/>78. Perrett, D.I., Smith, P.A., Potter, D.D., Mistlin, A.J., Head, A.S., Milner, <lb/>A.D., and Jeeves, M.A. (1984). Neurones responsive to faces in the tempo-<lb/>ral cortex: Studies of functional organization, sensitivity to identity and <lb/>relation to perception. Hum. Neurobiol. 3, 197-208. <lb/>79. Fiete, I.R., Burak, Y., and Brookings, T. (2008). What grid cells convey <lb/>about rat location. J. Neurosci. 28, 6858-6871. <lb/>80. Mathis, A., Herz, A.V.M., and Stemmler, M. (2012). Optimal population co-<lb/>des for space: Grid cells outperform place cells. Neural Comput. 24, 2280-<lb/>2317. <lb/>81. Stensola, H., Stensola, T., Solstad, T., Frøland, K., Moser, M.B., and <lb/>Moser, E.I. (2012). The entorhinal grid map is discretized. Nature 492, <lb/>72-78. <lb/>82. Chen, Q., and Verguts, T. (2010). Beyond the mental number line: A neural <lb/>network model of number-space interactions. Cognit. Psychol. 60, <lb/>218-240. <lb/>83. Poletti, M., and Rucci, M. (2016). A compact field guide to the study of <lb/>microsaccades: Challenges and functions. Vision Res. 118, 83-97. <lb/>84. Peel, T.R., Hafed, Z.M., Dash, S., Lomber, S.G., and Corneil, B.D. (2016). A <lb/>causal role for the cortical frontal eye fields in microsaccade deployment. <lb/>PLoS Biol. 14, e1002531. <lb/>85. Willeke, K., Tian, X., Bellet, J., Ramirez-Cardenas, A., and Hafed, Z.M. <lb/>(2017). Memory-guided microsaccades. bioRxiv. https://doi.org/10. <lb/>1101/539205. <lb/>86. Kagan, I., and Hafed, Z.M. (2013). Active vision: Microsaccades direct the <lb/>eye to where it matters most. Curr. Biol. 23, R712-R714. <lb/>87. Wilming, N., Ko ¨nig, P., Ko ¨nig, S., and Buffalo, E.A. (2018). Entorhinal cor-<lb/>tex receptive fields are modulated by spatial attention, even without move-<lb/>ment. eLife 7, e31745. <lb/>88. Baddeley, R.J., and Tatler, B.W. (2006). High frequency edges (but not <lb/>contrast) predict where we fixate: A Bayesian system identification anal-<lb/>ysis. Vision Res. 46, 2824-2833. <lb/>89. Samonds, J.M., Geisler, W.S., and Priebe, N.J. (2018). Natural image and <lb/>receptive field statistics predict saccade sizes. Nat. Neurosci. 21, 1591-<lb/>1599. <lb/>90. Nadasdy, Z., Nguyen, T.P., To ¨ro ¨k, A ´., Shen, J.Y., Briggs, D.E., Modur, <lb/>P.N., and Buchanan, R.J. (2017). Context-dependent spatially periodic ac-<lb/>tivity in the human entorhinal cortex. Proc. Natl. Acad. Sci. USA 114, <lb/>E3516-E3525. <lb/></listBibl>

			<page>990</page>

		<note place="footnote">Current Biology 29, 979-990, March 18, 2019 <lb/></note>

			<div type="annex">STAR+METHODS <lb/>KEY RESOURCE TABLE <lb/>CONTACT FOR REAGENT AND RESOURCE SHARING <lb/>Further information and requests for resources/code should be directed to the Lead Contact, Andrej Bicanski (andrej.bicanski@ <lb/>gmail.com). <lb/>EXPERIMENTAL MODEL AND SUBJECT DETAILS <lb/>Ten human subjects, personal acquaintances of the author (eight male and two female; ages 30-45) agreed to have their picture used <lb/>in the study (as input to the model). Subjects provided informed written consent before donating a picture. <lb/>METHOD DETAILS <lb/>Model overview <lb/>All non-grid cells in the model are simple connectionist neurons with rectified linear output. Since grid cells exist as canonical firing <lb/>rate maps (see below for details), their firing rates can be looked up as a function of eye position (for related models of grid cell firing <lb/>dynamics see e.g., [75, 76]). <lb/>Grayscale images (440x440 pixels) represent pre-processed visual input to the model. In addition to the grid cell and distance cell <lb/>components, the model consists of: a small square array of cells, representing pre-processed outputs from a simplified fovea (61x61 <lb/>cells/pixels) with associated sensory cells (see next paragraph); a small number of &apos;feature label cells&apos; (one for each salient feature <lb/>within a familiar stimulus, here 9 per stimulus); and a single stimulus identity cell per image (representing the abstract identity of the <lb/>stimulus, see e.g., [39, 77, 78]). <lb/>Banks of sensory neurons (playing the role of feature detectors, one bank associated with each foveal pixel) are implemented as <lb/>cells with Gaussian tuning curves across possible (grayscale) pixel values. Individual cell exhibit a FWHM (full width at half maximum) <lb/>of approximately 10% of the range of possible pixel values [0,255]. Thus, when a given input is presented by the fovea, sensory neu-<lb/>rons express a characteristic response, indicative of the attended feature, though subject to noise (see below). <lb/>Stimuli (the grayscale images) are first presented in a training phase (see below for details). the model learns Hebbian associations <lb/>between the following cell types. First, as an individual component feature (e.g., for the nose, eyes, lips etc. of a given face) of a stim-<lb/>ulus is foveated it generates a characteristic response among the feature detectors. A Hebbian association between the array of <lb/>feature detectors and an individual (newly recruited) feature label cell (representing the foveated component feature of a stimulus) <lb/>is then learned. Second, a connection between feature label cells and the locations of those features in the visual field (represented <lb/>by grid cell population vectors) is learned. Finally, all feature label cells belonging to a stimulus are associated -bi-directionally -to a <lb/>stimulus identity cell, representing the abstract identity of a stimulus. That is, each stimulus identity cell receives connections from a <lb/>small number of feature label cells representing the component features of that stimulus, and has a return projection to the same <lb/>feature label cells (Figure 1E). Once the model has learned the necessary associations, its recognition memory is tested by presenting <lb/>stimuli from the training set. <lb/>Action/Perception cycles <lb/>An action-perception cycle consists of the following steps: The foveal array is centered on a given feature (we assume that the first <lb/>feature is selected by bottom-up attentional mechanisms, which are not modeled here). Feature detectors (perception) drive the <lb/>feature label cells which (partially) match the attended feature. Feature label cells must exceed 2.8 standard deviations with respect <lb/>to the firing rates of all feature label cells to be eligible to contribute evidence. After this thresholding a softmax operation is applied to <lb/>ensure a sparse code among feature label cells. Active feature label cells drive their associated stimulus identity cells, generating <lb/>competing hypotheses about the stimulus identity. The most active identity cell then determines the computation of the vector for <lb/>the next saccade (action) in the following way. The current location of the fovea is represented by a population vector of grid cell <lb/>activity, which is updated by eye-movements analogously to how grid cell firing is updated by self-motion during navigation <lb/>[26, 76]. This yields the starting point of the saccade vector. Previously active feature label cells (including the one for the currently <lb/>REAGENT or RESOURCE <lb/>SOURCE <lb/>IDENTIFYIER <lb/>Software and Algorithms <lb/>MATLAB R2017b <lb/>https://www.mathworks.com/ <lb/>https://www.mathworks.com/ <lb/>Custom MATLAB Code <lb/>This Article <lb/>https://github.com/bicanski <lb/></div>

		<note place="footnote">Current Biology 29, 979-990.e1-e4, March 18, 2019</note>

		<page>e1 <lb/></page>

			<div type="annex">foveated feature) are reset to zero and the most active stimulus identity cell (representing the leading hypothesis) randomly selects <lb/>the next feature label cell via its return-projection (feature label cells representing already visited features are permanently inhibited). <lb/>Randomness is given by weak noise on the back-projection and winner-take-all dynamics select the feature label cell, which is in turn <lb/>associated with its own grid cell population vector (yielding the end point of the next saccade vector). Given the starting and end <lb/>points of the next saccade, the distance cell system outputs the vector required to update foveal position (see below for vector com-<lb/>putations), allowing the system to sample another portion of the visual field. The cycle then repeats while stimulus identity cells accu-<lb/>mulate firing across cycles until a firing rate &apos;decision threshold&apos; has been reached (at which point the leading hypothesis about the <lb/>identity of the stimulus is accepted and identity cells reset to zero before the next stimulus is presented). The increment in firing of a <lb/>stimulus identity cell (i.e., the gain on weights from feature label cells to stimulus identity cells; parameter FLC2ID in Table S2), and the <lb/>decision threshold are free parameters. They could be adapted in situations where sensory input is more or less reliable, setting a <lb/>lower recognition threshold (or a larger increment) would facilitate faster recognition, potentially at the expense of accuracy. It could <lb/>also be a function of the number of available component features, thus accounting for variable numbers of available features between <lb/>stimuli. If the decision threshold is not reached once all component features have been visited (which happens rarely), all permanently <lb/>inhibited feature label cells (i.e., coding for already visited features) are disinhibited and the process continues. <lb/>Sensory predictions and resets <lb/>In addition to specifying the endpoint of the next saccade via associated grid cells, the feature label cell that has been selected by the <lb/>return projection of the leading stimulus identity neuron also represents a prediction. Once the fovea relocates, and the next sensory <lb/>discrimination is carried out, the maximally active feature label cell should be the predicted one. This prediction is incorporated as a <lb/>facilitatory effect, boosting the firing of the predicted feature label cell in the next cycle by a factor (two), prior to the application of the <lb/>softmax operation across all feature label cells. If the predicted feature label cell is not the most active one after the next sensory <lb/>discrimination, a mismatch is registered. At the third mismatch event the system resets (i.e., the current hypotheses are all rejected), <lb/>beginning with different component feature. This procedure allows for early rejection of false hypotheses, which will otherwise pro-<lb/>duce saccades that do not take the fovea to expected features. Figure S1 details the effect of sensory predictions. Note that multiple <lb/>failures to reach the decision threshold could also be used to infer that the attended stimulus is unfamiliar. <lb/>Grid Cells and Vector Computations <lb/>Grid cells have been suggested to provide a spatial metric that supports path integration (by integrating self-motion inputs) and vec-<lb/>tor navigation [27-29]. The spatial periodicity of grid cells at different scales suggests that they provide a compact code for location, <lb/>and that they can uniquely encode locations within a space much larger than the largest grid scale [29, 79, 80]. Grid cells are imple-<lb/>mented as canonical firing rate maps which act as a look-up table. Each map consists of a matrix of the same dimensions as the PC <lb/>sheet (440x440 pixels) and is computed as 60 degrees offset, superimposed cosine waves using the following set of equations. <lb/>b 0 = <lb/>cosð0Þ <lb/>sinð0Þ <lb/>b 1 = <lb/>0 <lb/>B <lb/>@ <lb/>cos <lb/>p <lb/>3 <lb/>sin <lb/>p <lb/>3 <lb/>1 <lb/>C <lb/>Ab2 = <lb/>0 <lb/>B <lb/>B <lb/>B <lb/>@ <lb/>cos <lb/>2p <lb/>3 <lb/>sin <lb/>2p <lb/>3 <lb/>1 <lb/>C <lb/>C <lb/>C <lb/>A <lb/>(1) <lb/>z i = b i ðF x <lb/>! + x <lb/>! <lb/>offset Þ <lb/>(2) <lb/>r GC = maxð0; cosðz 0 Þ + cosðz 1 Þ + cosðz 2 ÞÞ <lb/>(3) <lb/>Here b0, b1 and b2 are the normal vectors for the cosine waves. 9 modules with constant orientation are used. F is the spatial fre-<lb/>quency of the grids, starting at 0.0028*2p. The scales of successive grids are related by the scaling factor <lb/>ffiffiffi <lb/>2 <lb/>p <lb/>[81]. The grid patterns of <lb/>different cells in a module/scale are offset relative to each other [16], collectively covering the entire visual field evenly. For each grid <lb/>scale 100 offsets are sampled uniformly along the principle axes of two adjacent equilateral triangles on the grid (i.e., the rhomboid <lb/>made of 4 grid vertices). Thus the grid cell ensemble consists of 9 modules/scales with 100 cells each. <lb/>To calculate displacement vectors between locations encoded by grid cell population vectors we employ a distance-cell model, <lb/>following Bush et al. [29] and Chen and Verguts [82]. Briefly, a given location on a 2D plane is uniquely represented by a set of grid cell <lb/>phases (Figure 1B; [30]). Grid cells with appropriate phases in each module project to a single cell encoding the corresponding dis-<lb/>tance in each of four distance cells arrays, two for each of two non-co-linear axes. The two distance cell arrays belonging to the same <lb/>axis project to two readout cells. One readout cell receives monotonically increasing weights from one distance cell array and mono-<lb/>tonically decreasing weights from the other. For the second readout cell the connections increase/decrease in the opposite direction <lb/>along the distance axis. The relative difference in firing rate between the two readout neurons encodes the displacement between <lb/>start and goal locations along the given axis (Figure 1B). The connections between grid cells and distance cells are universal (i.e., <lb/>not stimulus specific) and could be set up during development. <lb/></div>

		<page>e2</page>

			<note place="footnote">Current Biology 29, 979-990.e1-e4, March 18, 2019 <lb/></note>

			<div type="annex">Since the resolution of images and grid maps is restricted to 440x440 pixels we allow for a small tolerance of 1% in pixel coordi-<lb/>nates for translation vectors derived from vector computations. This allows for the compensation of small rounding errors for discrete <lb/>pixel targets. It has been suggested that microsaccades might serve to tweak the alignment of the fovea [83]. Interestingly, Hafed and <lb/>co-workers [84, 85] raise the possibility that a mechanism similar to the present model may also inform microsaccades, e.g., for fine <lb/>feature discrimination ([83]; reviewed in [86]), or recognition at a distance. However, it remains to be seen if memory-guided micro-<lb/>saccades can occur at delays which exclude working memory sources. <lb/>Rather than eye-movement vectors, the location of the focus of attention might be calculated [87], although we do not distinguish <lb/>the focus of attention from the target of fixation here. However, covert attention may be involved in recognition under restricted <lb/>viewing conditions [11], which could have implications for critiques of scan-path theory. <lb/>Training phase <lb/>We assume that during learning of a new stimulus, salient locations on an image (e.g., regions of high contrast) are foveated via bot-<lb/>tom-up attention, without knowledge of the stimulus identity, consistent with the typical eye-movements performed by human sub-<lb/>jects as they encounter new faces [11]. This bottom-up processing would also include figure-ground segmentation. For simplicity this <lb/>processing stage is not modeled and salient features are selected manually (9 per stimulus). In reality an analysis of the scene sta-<lb/>tistics may help select maximally de-correlated inputs [88, 89]. For face stimuli the selected locations include the corners of eyes, the <lb/>tip or sides of the nose, corners of the lips, etc. That is, regions of the stimulus that exhibit strong gradients in contrast under most <lb/>lighting conditions, and similar criteria are applied for scenes and objects. At each of the locations foveated during the training phase <lb/>a cell coding for the attended feature (feature label cell, see Feature detection, below) is associated with the current grid cell popu-<lb/>lation vector and with a single stimulus identity cell. Identity cells represent the abstract identity of a stimulus and are recruited on the <lb/>fly during the first exposure to a stimulus. They might reside in face-, object-or scene specific neocortical area or the hippocampus <lb/>(please see Discussion). <lb/>Feature detection and ambiguity <lb/>Feature detection is accomplished by banks of sensory neurons (with Gaussian tuning with regard to preferred gray-scale values) <lb/>responding to the content of each pixel in the foveal array (Figure 2A,B). Preferred pixel values for Gaussian tuning curves of sensory <lb/>cells are taken from blurred images during training (Mathworks, MATLAB function integral filter, range 5 pixels). This reflects (to a first <lb/>approximation) the fact that a large variance in absolute pixel values would be encountered in reality, and the fact that stimuli of <lb/>different identity may contain some similar-looking individual features (feature ambiguity). Depending on the content of the fovea <lb/>a different subset of these sensory neurons is maximally active. During the training phase connections from these sensory cells to <lb/>a given feature label cell are learned. That is, for a given foveal content the sum of all sensory cells maximally drives a given feature <lb/>label cell, thus implementing a simple toy model of feature detection. The ambiguity of feature perception can lead to an incorrect <lb/>initial hypothesis regarding the attended stimulus, i.e., an incorrect identity neuron being the most active. Such a hypothesis will pro-<lb/>duce saccades which do not take the fovea to expected features (Figure 2). As a consequence, sensory cells can drive many feature <lb/>label cells due to incidental partial overlap between their preferred features and the content of the fovea. Those feature label cells <lb/>impart some activity onto their associated identity neurons. That is a random subset of identity cells is weakly driven, leading to a <lb/>flattening of the distribution of firing rates among stimulus identity cells. By contrast, when the output of the feature detectors primar-<lb/>ily drives the correct feature label cells, and successive saccades confirm the initial hypothesis, the distribution of firing rates among <lb/>stimulus identity cells becomes progressively more peaked. Alternatively to tracking mismatch events between predictions and sen-<lb/>sory discriminations (see above) the absence of convergence to a specific hypothesis could be detected by the increased total <lb/>amount of firing among stimulus identity cells. <lb/>Position Invariance <lb/>During learning, each feature of a given stimulus is anchored to the grid cell representation, and the relative locations of features en-<lb/>coded across all grids are mutually consistent. Paralleling experimental results [17, 20] we encode all stimuli in the same grid <lb/>ensemble (as if anchored to a presentation screen). However, if connections from the most active feature label cell can re-align <lb/>the grid cell ensemble (foveal array R feature label cell R grid cells) to the phases specific to a given stimulus, then recognition irre-<lb/>spective of the position of a stimulus in the field of view (position invariance) follows from the fact that the grid system encodes the <lb/>relative (rather than absolute) locations of the features within a stimulus. This parallels observations from spatial navigation studies <lb/>that show grid cell firing patterns are stable across visits to the same environment but shift coherently upon a change to the environ-<lb/>ment [30]. This predicts that grid cell phases will follow the position of the stimulus in the visual field, as suggested by recognition of <lb/>illuminated faces in darkness (i.e., in the absence of environmental anchor points). Recognition irrespective of the position of a stim-<lb/>ulus in the field of view (stimulus position invariance) follows from the fact that the grid system encodes the relative (rather than ab-<lb/>solute) locations of the features within a stimulus. Associating each stimulus with a randomly shifted initial distribution of grid cell <lb/>phases could reduce interference. However, because of the multitude of individual differences between stimuli (even those with a <lb/>stereotypical layout of component features, e.g., in faces all eyes have roughly similar, but usually not identical, distances), all stimuli <lb/>can be encoded in the same grid cell ensemble. Individual differences ensure that individual features map onto different grid cell <lb/>population vectors. <lb/></div>

			<note place="footnote">Current Biology 29, 979-990.e1-e4, March 18, 2019</note>

		<page>e3 <lb/></page>

			<div type="annex">Size Invariance <lb/>Pre-processing of visual inputs may conceivably generate a size invariant representation of stimuli prior to input to the system <lb/>described here. However, the present model could accommodate size invariance. All circuit level models of grid cells require velocity <lb/>inputs to update their firing (locomotor velocity during spatial navigation or eye-velocity in visual paradigms). The coupling between <lb/>&apos;&apos;neural space&apos;&apos; (i.e., distance on the grid pattern) and self-motion has been shown to be plastic in spatial navigation paradigms <lb/>[31, 90]. Hence we assume a given saccade length is subject to gain-modulation to give an appropriately scaled distance on the <lb/>grid cell ensemble, and conversely, a given distance on the grid cell ensemble can be scaled to yield a saccade of appropriate length. <lb/>This requires a change in gain with the estimated size (distance) of the stimulus. The estimation of stimulus size during recognition <lb/>could reflect the size of the retinal image or ocular focus (compared to a memorized baseline), which is related to the distance and <lb/>hence size of the stimulus (Thales theorem). <lb/>Note that the estimation of stimulus size during recognition does not necessitate prior recognition of a particular exemplar. The gain <lb/>could be set dynamically against some baseline reference size (e.g., the size of the segmented retinal image at the time of the first <lb/>encounter) to dynamically scale saccades. <lb/>QUANTIFICATION AND STATISTICAL ANALYSIS <lb/>Recognition rates were computed as the percentage of recognized stimuli. If a stimulus was not recognized within 10 resets (see <lb/>Method Details) a fail was registered (labeled &apos;not rec.&apos; in Figure 5). <lb/></div>

		<div type="availability">DATA AND SOFTWARE AVAILABILITY <lb/>Code will be made available at https://github.com/bicanski <lb/></div>

		<page>e4</page>

		<note place="footnote">Current Biology 29, 979-990.e1-e4, March 18, 2019 </note>


	</text>
</tei>
