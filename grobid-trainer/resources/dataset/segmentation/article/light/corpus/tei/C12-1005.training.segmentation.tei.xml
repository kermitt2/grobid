<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="__C12-1005"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Proceedings of COLING 2012: Technical Papers, pages 67–82, <lb/>COLING 2012, Mumbai, December 2012. <lb/> Experiments with Term Translation <lb/> M ihael ARCAN  1  Chr ist ian F E DERM AN N  2  Paul BU I T E LAAR  1 <lb/> (1) Unit for Natural Language Processing, Digital Enterprise Research Institute <lb/>National University of Ireland Galway <lb/>Galway, Ireland <lb/>(2) Language Technology Lab, German Research Center for AI <lb/>Saarbrücken, Germany <lb/> mihael.arcan@deri.org, paul.buitelaer@deri.org, <lb/>cfedermann@dfki.de <lb/> Abstract <lb/> In this article we investigate the translation of financial terms from English into German in the <lb/>isolation of an ontology vocabulary. For this study we automatically built new domain-specific <lb/>resources from the translation search engine Linguee and from the online encyclopaedia Wikipedia. <lb/>Due to the fact that we performed the translation approach on a monolingual ontology, we ran <lb/>several sub-experiments to find the most appropriate model to translate the financial vocabulary. <lb/>The findings from these experiments lead to the conclusion that a hybrid translation system, a <lb/>combination of bilingual terminological resources and statistical machine translation, can help <lb/>to improve translation of domain-specific terms. Finally we undertook a manual cross-lingual <lb/>evaluation on the monolingual ontology to get a better understanding on this specific short text <lb/>translation task. <lb/> Keywords: Ontologies and terminology, Empirical machine translation. <lb/></front>

			<page> 67 <lb/></page>

			<body> 1 Introduction <lb/> Our research on the translation of ontology vocabularies is motivated by the challenge of translating <lb/>domain-specific terms with restricted or no additional textual context that in other cases may be <lb/>used to improve the translation. For our experiment we started by translating financial terms with <lb/>the baseline systems trained on the JRC-Acquis (Steinberger et al., 2006) corpus and the European <lb/>Central Bank Corpus (Tiedemann, 2009). Although both resources contain a large amount of <lb/>parallel data, the translations were not satisfactory. To improve the translations of the financial <lb/>ontology vocabulary we built a new parallel resource, which was generated using Linguee, an online <lb/>translation query service. With this data, we could train a small model, which produced better <lb/>translations than the baseline model using only general resources. <lb/>Since the manual development of terminological resources is a time intensive and expensive task, <lb/>we used Wikipedia as a background knowledge base and examined the articles tagged with domain-<lb/>specific categories. With this extracted domain-specific data we built a specialised English-German <lb/>lexicon to store translations of domain-specific terms. These terms were then used in a pre-processing <lb/>method in the decoding approach. This approach incorporates the work by (Aggarwal et al., 2011), <lb/>where the authors use the ontology structure to calculate the similarity between the labels. They <lb/>combine the semantic, terminological and linguistic information for monolingual ontology matching, <lb/>which can be extended to the multilingual scenario. We split the financial terms into n-grams and <lb/>queried for financial sub-terms in Wikipedia, which we used to query Wikipedia. <lb/>The remainder of the paper is organised as follows: In Section 2 we give an overview on the related <lb/>work. In Section 3 we describe the ontology and the existing parallel resources, which were used <lb/>for generating the translation and language model. Section 4 presents the new resources which <lb/>were used for improving the term translation. Furthermore we discuss the results of exploiting the <lb/>different resources. We conclude with a summary and give an outlook on future work. <lb/> 
			
			2 Related Work <lb/> The related research focusses on different aspects relevant to our work: domain-specific term <lb/>translation. Firstly we have to understand the structure of these specific terms and the variations <lb/>which come when dealing with these terms. Kerremans (2010) discusses in detail the issue of <lb/>terminological variation in the context of specialised translation on a parallel corpus of biodiversity <lb/>texts. He shows that a term often cannot be aligned to any term in the target language. As a result, <lb/>he proposes that specialised translation dictionaries should store different translation possibilities or <lb/>term variants. In addition to that, Weller et al. (2011) describe methods for terminology extraction <lb/>and bilingual term alignment from comparable corpora. In their compound translation task, they <lb/>use a dictionary to avoid out-of-domain translation. In contrast, to address this problem, which <lb/>frequently arises in domain-specific translation we decided to generate our own customised lexicon; <lb/>which we constructed from the multilingual Wikipedia and its dense inter-article link structure. <lb/>Erdmann et al. (2008) also extracted terms from Wikipedia articles; however, they assumed that <lb/>two articles connected by an Interlanguage link are likely to have the same content and thus an <lb/>equivalent title. We likewise build a lexicon from Wikipedia, but instead of collecting all of the titles <lb/>from Wikipedia, we target only the domain-specific titles and their translated equivalents. Vivaldi <lb/>and Rodriguez (2010) proposed a methodology for term extraction in the biomedical domain with <lb/>the help of Wikipedia. As a starting point, they manually selected a set of seed words for a domain, <lb/>which were then used to find the corresponding nodes in this resource. For cleaning their collected <lb/>data, they used thresholds to avoid storing undesirable categories. Müller and Gurevych (2008) used <lb/>

			<page> 68 <lb/></page>

			Wikipedia and Wiktionary as knowledge bases to integrate semantic knowledge into Information <lb/>Retrieval. Their models, text semantic relatedness (for Wikipedia) and word semantic relatedness <lb/>(for Wiktionary), are compared to a statistical model implemented in Lucene. In their approach to <lb/>bilingual retrieval, they use the cross-language links in Wikipedia, which improved the retrieval <lb/>performance in their experiment, especially when the machine translation system generated incorrect <lb/>translations. Zesch et al. (2008) address the issues in accessing the largest collaborative resources: <lb/>Wikipedia and Wiktionary. They describe several modules and APIs for converting a Wikipedia <lb/>XML Dump into a more suitable format. Instead of parsing the large Wikipedia XML Dump, they <lb/>suggest to store the Dump into a database, which significantly increases the performance in retrieval <lb/>time of queries. <lb/> 
			
			3 Experimental Data <lb/> We are investigating the problem of translating a domain-specific vocabulary, therefore our exper-<lb/>iments started with an analysis of the financial terms stored in the investigated ontology. With <lb/>these extracted terms we built different multilingual resources, which were used for financial term <lb/>translation. Firstly, we used the encyclopaedia Wikipedia, where we extracted the titles from <lb/>domain-specific Wikipedia articles. Secondly, we used the same financial labels to build a parallel <lb/>resource for the financial domain. For this approach we used the Linguee Web service. <lb/>In this section, we present several types of data. Section 3.1 gives an overview of the data that was <lb/>used in translation. In Sections 3.2 and 3.3 we describe existing multilingual resources, which were <lb/>used to train the translation and language model. For our current research we used JRC-Acquis and <lb/>the European Central Bank (ECB) corpus, respectively. In the end we describe the procedure to <lb/>obtain domains-specific resources by Linguee 3.4 and Wikipedia 3.5. <lb/> 3.1 The Financial Ontology <lb/> For our study we used the UK GAAP 1 financial ontology, prepared by the XBRL 2 European <lb/>Business Registers (xEBR) Working Group. This financial ontology is a framework for describing <lb/>financial accounting and profile information of business entities across Europe; see also Declerck <lb/>et al. (2010). The ontology holds 142 concepts and is partially aligned into German, Dutch, Spanish, <lb/>French and Italian. We identified only 16 English financial terms and their German equivalents, <lb/>which were used as reference translations for automatic evaluation. <lb/>The financial terms are not really terms from a linguistic point of view, but they are used in financial <lb/>or accounting reports as unique financial expressions or tags to organize and retrieve automatically <lb/>reported information. Therefore it is important to translate these financial terms exactly. Table 1 <lb/>illustrates the structure of xEBR terms. <lb/>It is obvious that they are not comparable to general language, but instead are more like headlines <lb/>in newspapers, which are often short, very informative, and written in a telegraphic style. xEBR <lb/>terms are often only noun phrases without any determiner. The length of the financial terms varies, <lb/>e.g. the longest financial term considered for translation has a length of 11 tokens, while others may <lb/>consist of 1 or 2 (Figure 1). <lb/>

			<note place="footnote"> 1 GAAP -Generally Accepted Accounting Practice <lb/> 2 XBRL -eXtensible Business Reporting Language,  http://www.xbrl.org/ <lb/></note>

			<page> 69 <lb/></page>

			Term Length Term Examples <lb/> 11 <lb/> Taxes Remuneration And Social Security Payable After More Than One Year <lb/>10 <lb/>Amounts Owed To Credit Institutions After More Than One Year . . . <lb/>. . . <lb/>2 <lb/>Net Turnover, Liquid Assets, Income Taxes, Financial Charges . . . <lb/>1 <lb/>Assets, Capital, Equity, Securities, Charges, Balance, Capital, Reserves . . . <lb/> Table 1: Examples for financial labels in the UK GAAP <lb/> 1 <lb/> 2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>11 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/> Length of a label <lb/># of labels <lb/> Figure 1: Label length of the UK GAAP ontology <lb/> 3.2 JRC-Acquis <lb/> The general parallel corpus JRC-Acquis 3 was used as baseline training data. This corpus is available <lb/>in almost every EU official language (except Irish), and is a collection of legislative texts written <lb/>between 1950 and now. <lb/>Although previous research showed, that a training model built by using a general resource cannot <lb/>be used to translate domain-specific terms (Wu et al., 2008), we decided to evaluate the translations <lb/>on these resources to illustrate any improvement steps from a general resource to specialised domain <lb/>resources. <lb/> 3.3 European Central Bank Corpus <lb/> For comparison with JRC-Acquis, we also did experiments using the European Central Bank <lb/>Corpus 4 , which contains a financial vocabulary. The multilingual corpus is generated by extracting <lb/>the website and documentation from the European Central Bank and is aligned among 19 European <lb/>languages. For our research we used the English-German language pair, which consists of 113,171 <lb/>sentence pairs or 2.8 million English and 2.5 million German tokens. <lb/> 3.4 Linguee -Dictionary and Translation Search Engine <lb/> Alongside these existing resources, we built a new parallel resource based on the ontology vocabulary <lb/>that we want to translate. Therefore we used Linguee, 5 a combination of a dictionary and a search <lb/>engine, which indexes around 100 million bilingual texts on words and expressions. The search <lb/>results show example sentences that depict how the searched expression has been translated in <lb/>context. The bilingual dataset was gathered from the web, particularly from multilingual websites <lb/>of companies, organisations or universities. Other sources include EU documents and patent <lb/>

			<note place="footnote"> 3  http://langtech.jrc.it/JRC-Acquis.html <lb/> 4  http://opus.lingfil.uu.se/ECB.php <lb/> 5  http://www.linguee.com/ <lb/></note>

			<page> 70 <lb/></page>

			specifications. Since Linguee includes EU documents, they also use parallel sentences from JRC-<lb/>Acquis, whereby the proportion of sentences returned by Linguee is very low, only 131 sentences or <lb/>0.54% overlap with the corpus. <lb/>In contrast to translation engines like Google Translate and Bing Translator, which give you the most <lb/>probable translation of a source text, every entry in the Linguee database was translated manually. <lb/> Domain-specific parallel corpus generation <lb/> To build a new training model that is specialised for our xEBR ontology, we used the Linguee search <lb/>engine. This resource can be queried on single words and on word expressions with or without <lb/>quotation marks. We stored the HTML output of the Linguee queries of our financial terms and <lb/>parsed these files to extract plain parallel text. From this, we built a financial parallel corpus with <lb/>24,247 translation pairs, including single words, multi-word expressions and sentences (Table 2). <lb/>The English part of the parallel resource contained 1,032,676 tokens and the German part 865,460. <lb/> Single terms Enterprise, share, reserve, debtor, expenses, . . . <lb/>Multi-words at a specific amount, credit institute, in the amount of, doubled over the last year <lb/>Sentences Finally, the European Parliament called for social and cultural aspects of immigration <lb/>to receive equal treatment than economic and security aspects of the issue. <lb/> Table 2: Examples of extracted text from the translation search engine Linguee <lb/> 3.5 Wikipedia <lb/> Wikipedia 6 is a multilingual, freely available encyclopaedia that was built by a collaborative effort <lb/>of voluntary contributors. All combined Wikipedias hold approximately 19 million articles or more <lb/>than 8 billion words in more than 270 languages, making it the largest collection of freely available <lb/>knowledge. 7 <lb/> With the heavily interlinked information base, Wikipedia forms a rich lexical and semantic resource. <lb/>Besides a large number of articles, it also holds a hierarchy of categories that Wikipedia articles are <lb/>tagged with. It includes knowledge about named entities, domain-specific terms and word senses. <lb/>Furthermore, the redirect system of Wikipedia articles can be used as a dictionary for synonyms, <lb/>spelling variations and abbreviations. <lb/> Domain-specific lexicon generation <lb/> To improve translations, based on the domain-specific parallel corpus, we built a cross-lingual <lb/>terminological lexicon. From the Wikipedia articles we used different information units: the title, <lb/>the category (or categories) of the title and the internal Interwiki\Interlanguage links of the title. <lb/>The concept of Interwiki links can be used to make links to other Wikipedia articles in the same <lb/>language or to another Wikipedia language i.e. Interlanguage links. The domain-specific lexicon <lb/>was generated by two approaches: <lb/>a) domain detection of the ontology (bottom-up approach); <lb/>b) extraction of cross-lingual terminology (top-down approach). <lb/>

			<note place="footnote"> 6  http://www.wikipedia.org <lb/> 7  http://en.wikipedia.org/wiki/Wikipedia:Size_comparison <lb/></note>

			<page> 71 <lb/></page>

			In our first approach, we used Wikipedia to determine the domain (or several domains) of the <lb/>ontology. The bottom-up approach (a) is to represent this domain by the most frequent categories <lb/>associated with the vocabulary we want to translate. For this approach, the financial terms, which <lb/>were extracted from the ontology, were used to query the Wikipedia knowledge base. 8 Initially a <lb/>Wikipedia article was considered for further examination if its title is equivalent to our financial <lb/>terms. In this first step, 7 terms from our ontology were identified in the Wikipedia knowledge base, <lb/>i.e.: <lb/> Income tax, Earnings before interest and taxes, Asset, Stocks, Debtor, Gross profit, Income <lb/> We then collected the categories of the articles associated with these titles. Since a category can <lb/>appear with different financial term, we also stored the frequency of these categories. 9 In a second <lb/>round, we split our financial terms into all possible n-grams and repeated the query again to find <lb/>additional categories based on the split n-grams. Table 3 shows the collected categories of the first <lb/>approach and how often they appeared with respect to the extracted financial terms. <lb/> Collected Wikipedia Categories <lb/>Frequency Name <lb/>8 <lb/>Generally Accepted Accounting Principles <lb/>4 <lb/>Debt <lb/>. . . <lb/>1 <lb/>Political science terms <lb/>1 <lb/>Physical punishments <lb/> Table 3: Collected Wikipedia Categories based on the extracted financial terms <lb/>After storing all categories, the only categories considered were the ones that had a frequency value <lb/>more than the calculated arithmetic mean of all the frequencies (&gt; 3.15). For the calculation of the <lb/>arithmetic mean only the categories that had a frequency larger than 1 were considered, since 2,262 <lb/>of 3,615 collected categories (62.6%) had a frequency of 1. Using this threshold we avoided the <lb/>extraction of a vocabulary that is not related to the ontology. Without this threshold, out-of-domain <lb/>categories would be stored, which would extend the lexicon with vocabulary that would not benefit <lb/>the ontology translation, e.g. Physical punishments, which was a category associated with the <lb/>financial term Stocks. <lb/> In the next step, we further extended the list of the previous collected categories with the use of full <lb/>and split terms. This was done by storing new categories based on the Wikipedia Interwiki links of <lb/>each article which was tagged with a category from Table 3. For example, we collected all categories <lb/>of the Wikipedia article Balance sheet.  10 In addition to that, we examined all Interwiki links of the <lb/>article Balance sheet and also stored the categories of articles which have an incoming link from <lb/>this article. 11 For example, we stored all categories of the 106 articles which are linked with the <lb/>article Balance sheet. The frequencies of these categories were summed up again to re-calculate the <lb/>geometric mean. Finally a new category was added to the final category list, if the new category <lb/>frequency exceeds the arithmetic mean threshold (&gt; 18.40). <lb/> 
			
			<note place="footnote">8 For the Wikipedia Query we used the Wikipedia XML dump;  enwiki-20120702-pages-articles <lb/> 9 The Wikipedia titles Operating Income, Income, Gross profit, Income statement, Debtor . . . are tagged with the category <lb/> Generally Accepted Accounting Principles <lb/> 10  Financial statements, Accounting terminology <lb/> 11  Balance sheet <lb/></note>

			<page> 72 <lb/></page>

			Final Category List <lb/>Frequency Name <lb/>95 <lb/>Economics terminology <lb/>62 <lb/>Generally Accepted Accounting Principles <lb/>61 <lb/>Macroeconomics <lb/>. . . <lb/> Table 4: Most frequent Categories based on the xEBR terms and their Interwiki links <lb/>The final category list contained 33 financial Wikipedia categories (Table 4), which were used to <lb/>extract the financial terms and their translations. <lb/>With the final list of categories, we started an investigation of all Wikipedia articles tagged with <lb/>these financial categories. Each Wikipedia title was considered as a useful domain-specific term and <lb/>was stored in our lexicon if a German title in the Wikipedia knowledge base also existed. As an <lb/>example, we examined the category Accounting terminology and stored the English Wikipedia title <lb/> Balance sheet with the German equivalent Wikipedia title Bilanz. <lb/> At the end of the lexicon generation we examined 5,228 Wikipedia articles that were tagged with <lb/>one or more financial categories. From this set of articles we were able to generate a terminological <lb/>lexicon with 3,228 English-German entities. The difference between the number of examined titles <lb/>and the lexicon items is attributed to the fact that not all English Wikipedia titles are linked to a <lb/>German one. These translation pairs were used to suggest the SMT system to choose the extracted <lb/>translations by annotating the decoder input using the XML input markup scheme. <lb/>

			<note place="footnote"> 4 Experiments and Evaluation <lb/></note>

			Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for <lb/>automatic evaluation. Therefore we performed several experiments to find the best approach to <lb/>translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings <lb/>(Section 4.1). If reference translations were available, we undertook an automatic evaluation using <lb/>the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and <lb/>Meteor 12 (Denkowski and Lavie, 2011) algorithms. <lb/>With the first evaluation experiment we translated 16 aligned English-German labels with different <lb/>translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see <lb/>which translation model performs best regarding the 2794 financial labels that are stored in this <lb/>ontology (Section 4.3). We also compared the perplexity between several language models and the <lb/>vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation <lb/>model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six <lb/>annotators (Section 4.5). <lb/> 4.1 Translation System: Moses Toolkit <lb/> For generating the translations from English into German, we used the statistical translation toolkit <lb/>Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the <lb/>surface level, and therefore no part-of-speech information was taken into account. Word and phrase <lb/>alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language <lb/>

			<note place="footnote"> 12 Meteor configuration: exact, stem, paraphrase <lb/></note>

			<page> 73 <lb/></page>

			model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). <lb/> 4.2 Translating aligned UK – German GAAP labels <lb/> The UK GAAP is a monolingual ontology which holds 142 financial labels. With the help of the <lb/>German equivalent, i.e. German GAAP, we aligned 16 German labels with the English ones, stored <lb/>in the UK GAAP. This allowed us to do a small automatic evaluation, regardless of the low number <lb/>of labels to be translated. <lb/> Scoring Metric <lb/>Source <lb/># correct BLEU-2 BLEU-4 <lb/>NIST <lb/>TER <lb/>Meteor <lb/>JRC-Acquis <lb/>3 <lb/>0.2629 <lb/>0.2747 1.8112 0.6969 0.1579 <lb/>ECB <lb/>3 <lb/>0.2572 <lb/>0.2725 1.5282 0.7878 0.1707 <lb/>Linguee+Wikipedia <lb/>5 <lb/>0.3623 <lb/>0.2922 2.3259 0.6363 0.4085 <lb/> Table 5: Evaluation scores for aligned UK–German GAAP translations <lb/>Despite the small amount of translations Table 5 shows the Linguee + Wikipedia resource produces <lb/>the best BLEU score. <lb/> # <lb/>Source Label <lb/>Linguee+Wikipedia Model <lb/>Reference Translation <lb/>1 <lb/>Fixed assets <lb/>Anlagevermögen <lb/>Anlagevermögen <lb/>2 <lb/>Tangible fixed assets <lb/>Sachanlagen <lb/>Sachanlagen <lb/>3 <lb/>Other tangible fixed assets sonstige Sachanlagen <lb/>sonstige Sachanlagen <lb/>4 <lb/>Equity <lb/>Eigenkapital <lb/>Eigenkapital <lb/>5 <lb/>Income statement <lb/>Gewinn-und Verlustrechnung <lb/>Gewinn-und Verlustrechnung <lb/>6 <lb/>Intangible fixed assets <lb/>immaterielle Vermögenswerte <lb/>Immaterielle Vermögensgegenstände <lb/>7 <lb/>Other intangible fixed <lb/>sonstige immaterielle <lb/>sonstige immaterielle <lb/>Vermögenswerte <lb/>Vermögensgegenstände <lb/>8 <lb/>Social security cost <lb/>Sozialbeiträge <lb/>soziale Abgaben <lb/>9 <lb/>Other provisions <lb/>die sonstigen Rückstellungen <lb/>sonstige Rückstellungen <lb/>10 Other operating income <lb/>die sonstigen betrieblichen Erträge <lb/>sonstige betriebliche Erträge <lb/>11 Wages and salaries <lb/>die Löhne und Gehälter <lb/>Löhne und Gehälter <lb/>12 <lb/>Current assets <lb/>kurzfristige Vermögenswerte <lb/>Umlaufvermögen <lb/>13 <lb/>Work in progress <lb/>angefangene Arbeiten <lb/>unfertige Erzeugnisse <lb/>14 <lb/>Work in progress <lb/>angefangene Arbeiten <lb/>unfertige Leistungen <lb/>15 <lb/>Extraordinary income <lb/>das außerordentliche Ergebnis <lb/>außerordentliche Erträge <lb/>16 <lb/>Equity and Liabilities <lb/>Eigenkapital und Zur <lb/>Bilanzsumme, Summe Passiva <lb/> Table 6: Results of financial translations generated by Linguee+Wikipedia translation model <lb/>Table 6 shows the translations of the 16 financial labels which were aligned between the UK and the <lb/>German GAAP. The first part of the table, examples 1 to 5, represents the correct translations, which <lb/>match exactly with the reference provided by the xEBR Working Group. <lb/>The next block represents translations which do not match completely with the reference translations. <lb/>Examples 6 and 7 illustrate the problem of translating the label fixed assets  13 that can be translated <lb/>into near synonyms Vermögenswerte or Vermögensgegenstände. Example 8 shows where the <lb/>translation model generated a compound, but the reference translation consists of two separate <lb/>tokens. If we de-compound the translation Sozialbeträge into soziale Beträge, we get a synonym to <lb/>

			<note place="footnote"> 13  Fixed assets and Other fixed assets <lb/></note>

			<page> 74 <lb/></page>

			 the reference translation. Examples 9 to 11 represent translations with over-specification, since the <lb/>ontology labels do not require the German article 14 at the beginning of a label. <lb/>The last part of the table illustrates incorrect translations. Examples 12 to 14 are translated into <lb/>idiomatic expressions, whereby example 15 shows a wrong lexical choice. The word Income was <lb/>translated intoErgebnis, whereas it should have been translated into Erträge. In example 16 a part <lb/>of the source label, i.e. Liabilities is missed in the target translation. <lb/> 4.3 Translating the German GAAP with different models <lb/> Since we built a financial parallel resource (see Section 3.4 and 3.5) and generated a translation <lb/>model based on this financial vocabulary, we tested how well the model performs on a similar <lb/>ontology. Therefore we translated the aforementioned German GAAP ontology, which holds 2,794 <lb/>labels 15 . <lb/> Scoring Metric <lb/>Source <lb/># correct BLEU-2 BLEU-4 <lb/>NIST <lb/>TER <lb/>Meteor <lb/>JRC-Acquis <lb/>47 <lb/>0.2276 <lb/>0.1122 2.7022 0.9337 0.1761 <lb/>ECB <lb/>24 <lb/>0.1715 <lb/>0.0596 2.1921 0.9834 0.1321 <lb/>Linguee+Wikipedia <lb/>79 <lb/>0.3397 <lb/>0.2292 3.9383 0.8291 0.2917 <lb/> Table 7: Evaluation scores for German GAAP term translations <lb/>Table 7 illustrates the automatic metrics used to evaluate the translation of the German GAAP, <lb/>where the best BLEU results are generated by the Linguee+Wikipedia translation model. We can <lb/>deduce from this experiment that even though JRC-Acquis has a larger number of tokens than the <lb/>Linguee+Wikipedia corpus, it does not generate better translations of financial labels. The ECB <lb/>corpus also does not generate better translations, although it is considered a domain-specific corpus. <lb/> 4.4 Perplexity of different language models <lb/> The automatic evaluation with the small amount of translation and their references cannot demon-<lb/>strate the quality of the translation model with regard to the whole UK GAAP ontology. Therefore <lb/>we compared the perplexity 16 of different language models and the vocabulary of the UK GAAP <lb/>ontology. Since a better language model should assign a higher probability to its test set, we tested <lb/>which generated language model gives the highest probability on the UK GAAP vocabulary. <lb/>The perplexity (1) is a reformulation of cross-entropy (2). <lb/> P P = 2  H(p LM  ) <lb/> (1) <lb/> H(p L M  ) = − <lb/> 1 <lb/> n <lb/> n <lb/> i=1 <lb/> log p L M  (w  i  |, . . . , w i−1  ) <lb/> (2) <lb/>Table 8 illustrates that the ECB language model generates the worst perplexity on the UK GAAP <lb/>vocabulary. On the other hand, the best probability is calculated by the Linguee+Wikipedia language <lb/> 
			 
			 <note place="footnote"> 14 German articles: die, der, das <lb/> 15 For comparison, the monolingual UK GAAP holds only 142 financial labels <lb/> 16 The perplexity was calculated with the SRILM ngram tool <lb/></note>

			<page> 75 <lb/></page>

			 model, which is not a surprise, since the resource is generated from the same vocabulary. Besides <lb/>that, the best perplexity is generated by the German GAAP language model, which indicates that <lb/>the vocabulary is most similar to the UK GAAP in comparison to other languages models. <lb/> logprob <lb/>Perplexity <lb/>JRC-Acquis LM <lb/>-1,656.39 <lb/>243.625 <lb/>ECB LM <lb/>-1,871.33 <lb/>497.098 <lb/>German GAAP LM <lb/>-1,528.92 <lb/>159.608 <lb/>Linguee + Wikipedia LM -1,277.15 <lb/>69.226 <lb/> Table 8: Perplexity of the language models <lb/> 4.5 Manual Evaluation of Translation Quality -UK GAAP <lb/> We have undertaken a manual evaluation campaign to assess the translation quality of our terminol-<lb/>ogy translation system, which was performed with the Appraise Toolkit.(Federmann, 2012) <lb/>In this section, we will a) describe the annotation setup and task presented to the human annotators, <lb/>b) report on the translation quality achieved by the Linguee+Wikipedia approach, and c) present <lb/>inter-annotator agreement scores that allow us to judge the reliability of the human rankings. <lb/> 4.5.1 Annotation Setup <lb/> In order to manually assess the translation quality of the different systems under investigation, we <lb/>designed a simple classification scheme consisting of three distinct classes: <lb/>1. Acceptable (A): terms classified as acceptable are either fully identical to the reference term <lb/>or semantically equivalent; <lb/>2. Can easily be fixed (C): terms in this class require some minor correction (such as fixing of <lb/>typos, removal of punctuation, etc.) but are nearly acceptable. The general semantics of the <lb/>reference term are correctly conveyed to the reader. <lb/>3. None of both (N): the translation of the term does not match the intended semantics or it is <lb/>plain wrong. Items in this class are considered severe errors which cannot easily be fixed and <lb/>hence should be avoided wherever possible. <lb/> 4.5.2 Annotation Data <lb/> We set up an evaluation task containing 142 term translations and the corresponding source term. <lb/>The set was then given to a total of six human annotators who classified the observed translation <lb/>output according to the classification scheme described above. The human annotators were lay users <lb/>without in-depth knowledge of the terms&apos; domain. <lb/>In total, we collected 852 classification items from six annotators. Table 9 shows the results from <lb/>the manual evaluation for term translations into German. We report the distribution of classes per <lb/>evaluation task which are displayed in best-to-worst order. <lb/> Classes <lb/>System <lb/>A <lb/>C <lb/>N <lb/>Linguee+Wikipedia Model 59.15% 29.34% 11.50% <lb/> Table 9: Results from the manual evaluation for German <lb/>

			<page> 76 <lb/></page>

			In order to better be able to interpret these rankings, we computed the inter-annotator agreement <lb/>between human annotators. We report the scores generated with the following agreement metrics: <lb/> • S (Bennett et al., 1954); <lb/> •  π  (Scott, 1955); <lb/> •  κ  (Fleiss, 1971); <lb/> •  α  (Krippendorff, 2004). <lb/>Table 10 presents the aforementioned metrics&apos; scores for German term translations. <lb/> Agreement Metric <lb/>System <lb/>S <lb/> π <lb/>κ <lb/>α <lb/> Linguee+Wikipedia Model 0.467 0.355 0.357 0.355 <lb/> Table 10: Annotator agreement scores for German <lb/>Overall, we achieve an average  κ  score of 0.357, which can be interpreted as fair agreement <lb/> following (Landis and Koch, 1977). Given the observed inter-annotator agreement, we expect <lb/>the reported ranking results to be meaningful. The inclusion of domain experts into the manual <lb/>evaluation campaign will be an interesting extension of the work presented. <lb/> 4.6 Manual error analysis of UK GAAP <lb/> In addition to the manual evaluation we performed with six annotators on the UK GAAP ontology <lb/>monolingual (Section 4.5), we also performed a closer analysis of each label. <lb/>In the first step, we extracted 36 labels from the manual evaluation campaign, where all evaluators <lb/>annotated the translation as &quot;Acceptable&quot;. Examples 1 to 7 (Table 11) depict a small set of the <lb/>acceptable translations. <lb/> # <lb/>Source label <lb/>Target label <lb/>1 <lb/>Equity <lb/>Eigenkapital <lb/>2 <lb/>Stocks <lb/>Wertpapiere <lb/>3 <lb/>Key Balance Sheet Figures <lb/>Bilanzkennzahlen <lb/>4 <lb/>Revaluation Reserve <lb/>Neubewertungsrücklage <lb/>5 <lb/>Interest And Similar Charges <lb/>Zinsen und ähnliche Aufwendungen <lb/>6 <lb/>Debenture Loans After More <lb/>Schuldscheindarlehen nach mehr <lb/>Than One Year <lb/>als einem Jahr <lb/>7 <lb/>Profit Or Loss On Ordinary <lb/>Gewinn oder Verlust aus der gewöhnlichen <lb/>Activities Before Taxes <lb/>Geschäftstätigkeit vor Steuern <lb/>8 <lb/>Net Operating Income <lb/>Ergebnis aus der <lb/>9 <lb/>Equity And Liabilities <lb/>Und Passiva <lb/>10 <lb/>Profit Loss For The Period <lb/>Ergebnis der <lb/> Table 11: Translations which all annotators considered as &quot;Acceptable&quot; (1-7) and &quot;None of both&quot; <lb/>(8-10) <lb/>We also extracted financial labels where all evaluators annotated the translations of the labels as <lb/>&quot;None of both&quot;, which indicates a low quality of the translations. These labels are shown in the <lb/>

			<page> 77 <lb/></page>

			last part of Table 11, examples 8 to 10. The reason for the low quality of the translations is that the <lb/>target label omits part of the source label. In example 8 we miss the translation for the segment Net <lb/> operating, in 9 Equity is not translated and in example 10 Loss for the period is missing. <lb/> 4.7 Interpretation of the evaluation time and the quality of translations <lb/> In addition to the evaluation of the quality of financial label translation, we also measured the <lb/>evaluation time regarding different criteria, i.e. regarding the length of the label, the quality of the <lb/>translation and the evaluation time for all labels. <lb/> Evaluation time regarding the length of the source labels <lb/> Figure 2 illustrates the evaluation time regarding the length of a source label. We learned that, on <lb/>average, the evaluation time increased with the length of the source label, e.g. the evaluators spent <lb/>more than 9 seconds to evaluate unigram label. 17 On the other hand, it took more than 26 seconds to <lb/>evaluate the longest financial label. 18 <lb/> 1 <lb/> 2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>11 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/> Evaluation Time [s] <lb/> Figure 2: Evaluation time per length of the source labels <lb/> Evaluation time with respect to the quality of the translations <lb/> The evaluation task asked the evaluators to evaluate the translation quality based on three classes, <lb/>&quot;Acceptable&quot;, &quot;Can easily be fixed&quot; and &quot;None of both&quot; (cf. Section 4.5). To get a more fine-<lb/>grained classification with a broader span of data, we gave each label a numeric value regarding the <lb/>translation quality set by the six evaluators, e.g. the financial label Charges and its translation Kosten <lb/> was annotated by all evaluators as &quot;Acceptable&quot;; analogously, the financial label Financial Charges <lb/> and its translation finanziellen Belastungen was annotated by four evaluators with &quot;Acceptable&quot;, <lb/>whereas two evaluators annotated it as &quot;Can easily be fixed&quot;. Since we know how each evaluator <lb/>annotated a translation, we interpret the three evaluation classes into a numerical value evaluation <lb/>score, i.e. if an translation was annotated with &quot;Acceptable&quot; we add the value 3 to the evaluation <lb/>score, if it was annotated with &quot;Can easily be fixed&quot; we add 2, and if it was annotated with &quot;None of <lb/>both&quot;, we do not add any value to the score. With this reformulation, the financial label Charges-<lb/>Kosten gets an evaluation score of 18, 19 and the Financial Charges-finanziellen Belastungen gets <lb/>an evaluation score of 16. 20 With this additional classification we get a broader variety with 18 <lb/>different quality classes, compared to the three classes set by the evaluators. <lb/>Figure 3 depicts the evaluation time regarding the translation quality of the financial labels. For <lb/> 
			
			<note place="footnote"> 17  Assets, Reserves, Equity, Stocks . . . <lb/> 18  Taxes Remuneration And Social Security Payable After More Than One Year <lb/> 19 3+3+3+3+3+3 = 18 <lb/> 20 3+3+3+3+2+2 = 16 <lb/></note>

			<page> 78 <lb/></page>

			0 <lb/> 2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>12 <lb/>14 <lb/>16 <lb/>18 <lb/>0 <lb/>10 <lb/>20 <lb/> Evaluation  Time [s] <lb/> Figure 3: Evaluation time per quality of the translation <lb/>labels, which have an evaluation score of zero 21 the evaluation time is more than 20 seconds. The <lb/>evaluation time decreases for labels with an evaluation score between two and five, but starts to <lb/>increase when the evaluation score is equal six or more. For labels that have an evaluation score <lb/>between six and thirteen, the evaluation time is higher than for labels with a lower or higher score. <lb/>At the end the evaluation time decreases again. We can deduce from this experiment that it is easier <lb/>to evaluate good and weak translations, but on the other hand it is harder to evaluate translations that <lb/>do not belong to these two evaluation classes. <lb/> Evaluation time for the financial 142 labels <lb/> Figure 4 shows the evaluation time for all 142 labels stored in the UK GAAP ontology. We can see <lb/>that the longest evaluation time to evaluate one term was more than 62 seconds, namely for the label <lb/> Operating Bach Ratios. On the other hand, the fastest time to evaluate a label was less than 3 second <lb/>for the label Staff Costs which was translated into Personalkosten. <lb/> 20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>120 <lb/>140 <lb/>0 <lb/>20 <lb/>40 <lb/>60 <lb/> Evaluation Time [s] <lb/> Figure 4: Evaluation time for the financial 142 labels <lb/> # <lb/>Source label <lb/>Time [s] <lb/>1 <lb/>Staff Costs <lb/>2.946 <lb/>2 <lb/>Capital <lb/>3.177 <lb/>3 <lb/>Extraordinary Charges <lb/>3.421 <lb/>. . . <lb/>. . . <lb/>. . . <lb/>140 <lb/>Deferred Charges And Accrued Income <lb/>44.213 <lb/>141 Depreciation On Intangible And Tangible Fixed Assets <lb/>47.211 <lb/>142 <lb/>Operating Bach Ratios <lb/>62.943 <lb/> Table 12: Financial labels with the fastest (above) and the slowest (below) evaluation time <lb/>Table 12 shows the five fastest and slowest evaluation for the financial labels. <lb/>

			<note place="footnote"> 21  Net Operating Income, Equity And Liabilities, Profit Loss For The Period <lb/></note>

			<page> 79 <lb/></page>

			Conclusion and Future Work <lb/> We presented our work on the translation of a monolingual financial ontology. We performed smaller <lb/>sub-experiments to determine the most appropriate translation model to translate financial labels in <lb/>isolation. Hence we evaluated the translations on a small subset of aligned labels between different <lb/>financial ontologies. Furthermore, we evaluated different translation models on a comparable <lb/>ontology from the financial domain and compared the perplexity of the ontology to be translated <lb/>with different resources. All these sub-experiments proved that the approach of building new, <lb/>specific resources showed a large impact on the translation quality. Therefore, generating specialised <lb/>resources for different specific domains will be the focus of our future work. On the one hand, <lb/>building appropriate translation models is important, but our experiment also highlighted the <lb/>importance of additional non-parallel resources, like Wikipedia, Wiktionary, 22 and DBpedia. 23 In <lb/>addition to extracting Wikipedia articles with their multilingual equivalents, Wikipedia holds much <lb/>more information in the articles themselves. Therefore, exploiting these non-parallel resources, <lb/>as shown by (Fišer et al., 2011), would clearly help to improve the performance of the translation <lb/>system. Future work needs to include the Wikipedia redirect system, which would allow a better <lb/>understanding of the synonymy and spelling variations of specific terms. <lb/>In addition to exploiting new resources for statistical machine translation, the manual evaluation <lb/>for monolingual resources needs to become the focus of our future work. The manual evaluation <lb/>campaign was time consuming, but provided a closer look into the translation errors. It indicates <lb/>that the evaluation classes for manual evaluation have to be reformulated into more fine-grained <lb/>decisions. We learned that we may distinguish between translations with &quot;one grammatical error&quot; <lb/>or &quot;several grammatical errors&quot;. It might also be interesting to classify the types of grammatical <lb/>error, e.g. number, gender or case, e.g. Betriebsstoffen vs. Betriebsstoffe. During the evaluation <lb/>we also observed over-specification, where the translation into German Die Forderungen ...,  24 <lb/> does not require the German article die at the beginning. Specifically to the German language we <lb/>further observed some compound errors, e.g. Ergebnis Verlust should be merged into a compound <lb/>expression. Another major issue were errors of omissions, where we miss some information from the <lb/>source side, e.g. the translation Und Passiva omits the source part Equity. Further to the linguistic <lb/>error classification, the type of the translation mismatch might be interesting to investigate, i.e. <lb/>cultural, linguistic or domain-specific. Also it is important to know if a translation is too broad or <lb/>too narrow. Especially for GAAP national differences are important as financial concepts largely <lb/>depend on the legal system of the country. <lb/>In summary, the work presented in this paper outlines an initial approach to domain-specific ontology <lb/>translation. It provides an indication that external resources are useful for overcoming the sparsity <lb/>of data, as well as a wealth of challenges to fuel future work on this task. <lb/> 
		
		</body> 
		
		<back>	
			
			<div type="acknowledgement">Acknowledgments <lb/> This work has been funded under the Seventh Framework Programme for Research and Techno-<lb/>logical Development of the European Commission through the T4ME contract (grant agreement <lb/>no.: 249119) and in part by the European Union under Grant No. 248458 for the Monnet project <lb/>as well as by the Science Foundation Ireland under Grant No. SFI/08/CE/I1380 (Lion-2). The <lb/>authors would like to thank Noëmi Aepli, Tobias Wunner and Thierry Declerck for their help with <lb/>the manual evaluation. We are grateful to the anonymous reviewers for their valuable feedback. <lb/></div>

			<note place="footnote">22  en.wiktionary.org/wiki/Wiktionary:Main_Page <lb/> 23  dbpedia.org/About <lb/> 24 generated from Trade Debtors <lb/> </note>
			
			<page> 80 <lb/></page>

			<listBibl> References <lb/> Aggarwal, N., Wunner, T., Arcan, M., Buitelaar, P., and O&apos;Riain, S. (2011). A similarity measure <lb/>based on semantic, terminological and linguistic information. In The Sixth International Workshop <lb/>on Ontology Matching collocated with the 10th International Semantic Web Conference (ISWC&apos;11). <lb/> Bennett, E. M., Alpert, R., and Goldstein, A. C. (1954). Communications Through Limited-<lb/>response Questioning. Public Opinion Quarterly, 18(3):303–308. <lb/>Declerck, T., Krieger, H.-U., Thomas, S. M., Buitelaar, P., O&apos;Riain, S., Wunner, T., Maguet, G., <lb/>McCrae, J., Spohr, D., and Montiel-Ponsoda, E. (2010). Ontology-based multilingual access to <lb/>financial reports for sharing business knowledge across europe. In Internal Financial Control <lb/>Assessment Applying Multilingual Ontology Framework. <lb/> Denkowski, M. and Lavie, A. (2011). Meteor 1.3: Automatic Metric for Reliable Optimization <lb/>and Evaluation of Machine Translation Systems. In Proceedings of the Sixth Workshop on <lb/>Statistical Machine Translation, pages 85–91, Edinburgh, Scotland. Association for Computational <lb/>Linguistics. <lb/>Doddington, G. (2002). Automatic evaluation of machine translation quality using n-gram co-<lb/>occurrence statistics. In Proceedings of the second international conference on Human Language <lb/>Technology Research, HLT &apos;02, pages 138–145. <lb/>Erdmann, M., Nakayama, K., Hara, T., and Nishio, S. (2008). An approach for extracting bilingual <lb/>terminology from wikipedia. Lecture Notes in Computer Science, (4947):380–392. Springer. <lb/>Federmann, C. (2012). Appraise: An open-source toolkit for manual evaluation of machine <lb/>translation output. The Prague Bulletin of Mathematical Linguistics, 98:25–35. <lb/>Fišer, D., Vintar, v., Ljubeši´ ánd Pollak, S. (2011). Building and using comparable corpora <lb/>for domain-specific bilingual lexicon extraction. In Proceedings of the 4th Workshop on Building <lb/>and Using Comparable Corpora: Comparable Corpora and the Web, BUCC &apos;11, pages 19–26. <lb/>Fleiss, J. (1971). Measuring Nominal Scale Agreement among Many Raters. Psychological <lb/>Bulletin, 76(5):378–382. <lb/>Kerremans, K. (2010). A comparative study of terminological variation in specialised translation. <lb/>In Reconceptualizing LSP Online proceedings of the XVII European LSP Symposium 2009, pages <lb/>1–14. <lb/>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, <lb/>W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open <lb/>Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of <lb/>the ACL, ACL &apos;07, pages 177–180. <lb/>Krippendorff, K. (2004). Reliability in Content Analysis. Some Common Misconceptions and <lb/>Recommendations. Human Communication Research, 30(3):411–433. <lb/>Landis, J. and Koch, G. (1977). Measurement of Observer Agreement for Categorical Data. <lb/> Biometrics, 33(1):159–174. <lb/>Müller, C. and Gurevych, I. (2008). Using wikipedia and wiktionary in domain-specific information <lb/>retrieval. In Working Notes for the CLEF 2008 Workshop. <lb/></listBibl>

			<page> 81 <lb/></page>

			<listBibl> Och, F. J. and Ney, H. (2003). A systematic comparison of various statistical alignment models. <lb/> Computational Linguistics, 29. <lb/>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: A Method for Automatic <lb/>Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association <lb/>for Computational Linguistics, ACL &apos;02, pages 311–318, Stroudsburg, PA, USA. Association for <lb/>Computational Linguistics. <lb/>Scott, W. A. (1955). Reliability of Content Analysis: The Case of Nominal Scale Coding. The <lb/>Public Opinion Quarterly, 19(3):321–325. <lb/>Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. (2006). A study of translation <lb/>edit rate with targeted human annotation. In Proceedings of Association for Machine Translation <lb/>in the Americas, pages 223–231. <lb/>Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., and Varga, D. (2006). <lb/>The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the <lb/>5th International Conference on Language Resources and Evaluation (LREC&apos;2006). <lb/> Stolcke, A. (2002). Srilm-an extensible language modeling toolkit. In Proceedings International <lb/>Conference on Spoken Language Processing, pages 257–286. <lb/>Tiedemann, J. (2009). News from OPUS -A collection of multilingual parallel corpora with <lb/>tools and interfaces. In Nicolov, N., Bontcheva, K., Angelova, G., and Mitkov, R., editors, <lb/> Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, <lb/>Amsterdam/Philadelphia, Borovets, Bulgaria. <lb/>Vivaldi, J. and Rodriguez, H. (2010). Using wikipedia for term extraction in the biomedical domain: <lb/>first experiences. Procesamiento del Lenguaje Natural, 45:251–254. <lb/>Weller, M., Gojun, A., Heid, U., Daille, B., and Harastani, R. (2011). Simple methods for dealing <lb/>with term variation and term alignment. In Proceedings of the 9th International Conference on <lb/>Terminology and Artificial Intelligence, pages 87–93. <lb/>Wu, H., Wang, H., and Zong, C. (2008). Domain adaptation for statistical machine translation with <lb/>domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference <lb/>on Computational Linguistics -Volume 1, COLING &apos;08, pages 993–1000. <lb/>Zesch, T., Müller, C., and Gurevych, I. (2008). Extracting lexical semantic knowledge from <lb/>wikipedia and wiktionary. In Proceedings of the Sixth International Conference on Language <lb/>Resources and Evaluation (LREC&apos;08). <lb/></listBibl>

			<page> 82 </page>

		</back>
	</text>
</tei>
