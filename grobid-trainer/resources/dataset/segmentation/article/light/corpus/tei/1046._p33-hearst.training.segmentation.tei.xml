<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>TextTiling: Segmenting Text into <lb/>Multi-paragraph Subtopic Passages <lb/>Marti A. Hearst* <lb/>Xerox PARC <lb/>TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, <lb/>or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical <lb/>co-occurrence and distribution. The algorithm is fully implemented and is shown to produce <lb/>segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. <lb/>Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including <lb/>information retrieval and summarization. <lb/></front>

			<body>1. Introduction <lb/>Most work in discourse processing, both theoretical and computational, has focused <lb/>on analysis of interclausal or intersentential phenomena. This level of analysis is im-<lb/>portant for many discourse-processing tasks, such as anaphor resolution and dialogue <lb/>generation. However, important and interesting discourse phenomena also occur at <lb/>the level of the paragraph. This article describes a paragraph-level model of discourse <lb/>structure based on the notion of subtopic shift, and an algorithm for subdividing <lb/>expository texts into multi-paragraph &quot;passages&quot; or subtopic segments. <lb/>In this work, the structure of an expository text is characterized as a sequence of <lb/>subtopical discussions that occur in the context of one or more main topic discussions. <lb/>Consider a 21-paragraph science news article, called Stargazers, whose main topic is the <lb/>existence of life on earth and other planets. Its contents can be described as consisting <lb/>of the following subtopic discussions (numbers indicate paragraphs): <lb/>lm3 Intro -the search for life in space <lb/>4--5 The moon&apos;s chemical composition <lb/>6m8 How early earth-moon proximity shaped the moon <lb/>9--12 How the moon helped life evolve on earth <lb/>13 Improbability of the earth-moon system <lb/>14--16 <lb/>Binary/trinary star systems make life unlikely <lb/>17--18 The low probability of nonbinary/trinary systems <lb/>19--20 Properties of earth&apos;s sun that facilitate life <lb/>21 Summary <lb/>Subtopic structure is sometimes marked in technical texts by headings and sub-<lb/>headings. Brown and Yule (1983, 140) state that this kind of division is one of the most <lb/>basic in discourse. However, many expository texts consist of long sequences of para-<lb/>graphs with very little structural demarcation, and for these a subtopical segmentation <lb/>can be useful. <lb/></body>

			<front>* 3333 Coyote Hill Rd, Palo Alto, CA. 94304. E-mail: hearst@parc.xerox.com <lb/>(~) 1997 Association for Computational Linguistics <lb/>Computational Linguistics <lb/>Volume 23, Number 1 <lb/></front>

			<body>This article describes fully implemented techniques for the automatic detection <lb/>of multi-paragraph subtopical structure. Because the goal is to partition texts into <lb/>contiguous, nonoverlapping subtopic segments, I call the general approach TextTiling <lb/>(Hearst, 1993, 1994a, 1994b). 1 Subtopic discussions are assumed to occur within the <lb/>scope of one or more overarching main topics, which span the length of the text. <lb/>This two-level structure is chosen for reasons of computational feasibility and for the <lb/>purposes of the application types described below. <lb/>TextTiling makes use of patterns of lexical co-occurrence and distribution. The <lb/>algorithm has three parts: tokenization into terms and sentence-sized units, determi-<lb/>nation of a score for each sentence-sized unit, and detection of the subtopic bound-<lb/>aries, which are assumed to occur at the largest valleys in the graph that results from <lb/>plotting sentence-units against scores. Three methods for score assignment have been <lb/>explored: blocks, vocabulary introductions, and chains, although only the first two <lb/>are evaluated in this article (the third is discussed in Hearst [1994b]). All three scoring <lb/>methods make use only of patterns of lexical co-occurrence and distribution within <lb/>texts, eschewing other kinds of discourse cues. <lb/>The ultimate goal of passage-level structuring is not just to identify the subtopic <lb/>units, but also to identify and label their subject matter. This article focuses only <lb/>on the discovery of the segment boundaries, but there is extensive ongoing research <lb/>on automated topic classification (Lewis and Hayes 1994). Most classification work <lb/>focuses on identifying main topic(s), as opposed to TextTiling&apos;s method of finding <lb/>both globally distributed main topics and locally occurring subtopics; nevertheless, <lb/>variations on some existing algorithms should be applicable to subtopic classification. <lb/>The next section argues for the need for algorithms that can detect multi-paragraph <lb/>subtopic structure (referred to here interchangeably as passages and subtopic seg-<lb/>ments), and discusses application areas that should benefit from such structure. Sec-<lb/>tion 3 describes in more detail what is meant in this article by &quot;subtopic&quot; and presents <lb/>a description of the discourse model that underlies this work. Section 4 introduces the <lb/>general framework of using lexical co-occurrence information for detecting subtopic <lb/>shift, and describes other related work in empirical discourse analysis. The TextTil-<lb/>ing algorithms are described in more detail in Section 5 and their performance is <lb/>assessed in Section 6. Finally, Section 7 summarizes the work and describes future <lb/>directions. <lb/>2. Why Multi-paragraph Units? <lb/>In school we are taught that paragraphs are to be written as coherent, self-contained <lb/>units, complete with topic sentence and summary sentence. In real-world text, these <lb/>expectations are often not met. Paragraph markings are not always used to indicate <lb/>a change in discussion, but instead can sometimes be invoked just to break up the <lb/>physical appearance of the text in order to aid reading (Stark 1988). A conspicuous <lb/>example of this practice can be found in the layout of the columns of text in many <lb/>newspapers (Longacre 1979). Brown and Yule (1983, 95-96) note that text genre has <lb/>a strong influence on the role of paragraph markings, and that markings differ for <lb/>different languages. Hinds (1979, 137) also suggests that different discourse types have <lb/>different organizing principles. <lb/>Although most discourse segmentation work is done at a finer granularity than <lb/></body>

			<note place="footnote">1 A free version of the code, written in C, is available for research purposes. Contact the author for more <lb/>information. <lb/></note>

			<page>34 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>that suggested here, multi-paragraph segmentation has many potential applications. <lb/>TextTiling is geared towards expository text; that is, text that explicitly explains or <lb/>teaches, as opposed to, say, literary texts, since expository text is better suited to the <lb/>main target applications of information retrieval and summarization. More specifi-<lb/>cally, TextTiling is meant to apply to expository text that is not heavily stylized or <lb/>structured, and for simplicity does not make use of headings or other kinds of or-<lb/>thographic information. A typical example is a 5-page science magazine article or a <lb/>20-page environmental impact report. <lb/>This section concentrates on two application areas for which the need for multi-<lb/>paragraph units has been recognized: hypertext display and information retrieval. <lb/>There are also potential applications in some other areas, such as text summarization. <lb/>Some summarization algorithms extract sentences directly from the text. These meth-<lb/>ods make use of information about the relative positions of the sentences in the text <lb/>(Kupiec, Pedersen, and Chen 1995; Chen and Withgott 1992). However, these methods <lb/>do not use subtopic structure to guide their choices, focusing more on the beginning <lb/>and ending of the document and on position within paragraphs. Paice (1990) recog-<lb/>nizes the need for taking topical structure into account but does not suggest a method <lb/>for determining such structure. <lb/>Another area that models the multi-paragraph unit is automated text generation. <lb/>Mooney, Carberry, and McCoy (1990) present a method centered around the notion of <lb/>Basic Blocks: multi-paragraph units of text, each of which consists of (1) an organiza-<lb/>tional focus such as a person or a location, and (2) a set of concepts related to that <lb/>focus. Their scheme emphasizes the importance of organizing the high-level structure <lb/>of a text according to its topical content, and afterwards incorporating the necessary <lb/>related information, as reflected in discourse cues, in a finer-grained pass. <lb/>2.1 Online Text Display and Hypertext <lb/>Research in hypertext and text display has produced hypotheses about how textual <lb/>information should be displayed to users. One study of an on-line documentation <lb/>system (Girill 1991) compares display of fine-grained portions of text (i.e., sentences), <lb/>full texts, and intermediate-sized units. Girill finds that divisions at the fine-grained <lb/>level are less efficient to manage and less effective in delivering useful answers than <lb/>intermediate-sized units of text. <lb/>Girill does not make a commitment about exactly how large the desired text unit <lb/>should be, but talks about &quot;passages&quot; and describes passages in terms of the com-<lb/>municative goals they accomplish (e.g., a problem statement, an illustrative example, <lb/>an enumerated list). The implication is that the proper unit is the one that groups <lb/>together the information that performs some communicative function; in most cases, <lb/>this unit will range from one to several paragraphs. (Girill also finds that using doc-<lb/>ument boundaries is more useful than ignoring document boundaries, as is done in <lb/>some hypertext systems, and that premarked sectional information, if available and <lb/>not too long, is an appropriate unit for display.) <lb/>Tombaugh, Lickorish, and Wright (1987) explore issues relating to ease of read-<lb/>ability of long texts on CRT screens. Their study explores the usefulness of multiple <lb/>windows for organizing the contents of long texts, hypothesizing that providing read-<lb/>ers with spatial cues about the location of portions of previously read texts will aid <lb/>in their recall of the information and their ability to quickly locate information that <lb/>has already been read once. In the experiment, the text is divided using premarked <lb/>sectional information, and one section is placed in each window. They conclude that <lb/>segmenting the text by means of multiple windows can be very helpful if readers are <lb/>familiar with the mechanisms supplied for manipulating the display. <lb/></body>

			<page>35 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>Converting text to hypertext, in what is called post hoc authoring (Marchionini, <lb/>Liebscher, and Lin 1991), requires division of the original text into meaningful units (a <lb/>task noted by these authors to be a challenging one) as well as meaningful intercon-<lb/>nection of the units. Automated multi-paragraph segmentation should help with the <lb/>first step of this process, and is more important than ever now that pre-existing docu-<lb/>ments are being put up for display on the World Wide Web. Salton et al. (1996) have <lb/>recognized the need for multi-paragraph units in the automatic creation of hypertext <lb/>links as well as theme generation (this work is discussed in Section 5). <lb/>2.2 Information Retrieval <lb/>In the field of information retrieval, there has recently been a surge of interest in <lb/>the role of passages in full text. Until very recently, most information retrieval ex-<lb/>periments made use only of titles and abstracts, bibliographic entries, or very short <lb/>newswire articles, as opposed to full text. When long texts are available, there arises <lb/>the question: can retrieval results be improved if the query is compared against only a <lb/>passage or subpart of the text, as opposed to the text as a whole? And if so, what size <lb/>unit should be used? In this context, &quot;passage&quot; refers to any segment of text isolated <lb/>from the full text. This includes author-determined segments, marked orthographi-<lb/>cally (paragraphs, sections, and chapters) (Hearst and Plaunt 1993; Salton, Allan, and <lb/>Buckley 1993; Moffat et al. 1994) and/or automatically derived units of text, includ-<lb/>ing fixed-length blocks (Hearst and Plaunt 1993; Callan 1994), segments motivated <lb/>by subtopic structure (TextTiles) (Hearst and Plaunt 1993), or segments motivated by <lb/>properties of the query (Mittendorf and Sch~iuble 1994). <lb/>Hearst and Plaunt (1993), in some early passage-based retrieval experiments, re-<lb/>port improved results using passages over full-text documents, but do not find a <lb/>significant difference between using motivated subtopic segments and arbitrarily cho-<lb/>sen block lengths that approximated the average subtopic segment length. Salton, Al-<lb/>lan, and Buckley (1993), working with encyclopedia text, find that comparing a query <lb/>against orthographically marked sections and then paragraphs is more successful than <lb/>comparing against full documents alone. <lb/>Moffat et al. (1994) find, somewhat surprisingly, that manually supplied section-<lb/>ing information may lead to poorer retrieval results than techniques that automatically <lb/>subdivide the text. They compare two methods of subdividing long texts. The first con-<lb/>sists of using author-supplied sectioning information. The second uses a heuristic in <lb/>which small numbers of paragraphs are grouped together until they exceed a size <lb/>threshold. The results are that the small, artificial multi-paragraph groupings seemed <lb/>to perform better than the author-supplied sectioning information (which usually con-<lb/>sisted of many more paragraphs than Moffet et al.&apos;s subdivision algorithm or Text-<lb/>Tiling would create). More experiments in this vein are necessary to firmly establish <lb/>this result, but it does lend support to the conjecture that multi-paragraph subtopic-<lb/>sized segments, such as those produced by TextTiling, are useful for similarity-based <lb/>comparisons in information retrieval. <lb/>It will not be surprising if motivated subtopic segments are not found to per-<lb/>form significantly better than appropriately sized, but arbitrarily segmented, units in <lb/>a coarse-grained information retrieval evaluation. At TREC, the most prominent in-<lb/>formation retrieval evaluation platform (Harman 1993), the top 1,000 documents are <lb/>evaluated for each query, and the best-performing systems tend to use very simple <lb/>statistical methods for ranking documents. In this kind of evaluation methodology, <lb/>subtle distinctions in analysis techniques tend to be lost, whether those distinctions be <lb/>how accurately words are reduced to their roots (Hull and Grefenstette 1995; Harman <lb/>1991), or exactly how passages are subdivided. The results of Hearst and Plaunt (1993), <lb/></body>

			<page>36 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>Salton, Allan, and Buckley (1993) and Moffat et al. (1994) suggest that it is the nature <lb/>of the intermediate size of the passages that matters. <lb/>Perhaps a more appropriate use of motivated segment information is in the display <lb/>of information to the user. One obvious way to use segmentation information is to have <lb/>the system display the passages with the closest similarity to the query, and to display <lb/>a passage-based summary of the documents&apos; contents. <lb/>As a more elaborate example of using segmentation in full-text information ac-<lb/>cess, I have used the results of TextTiling in a new paradigm for display of retrieval <lb/>results (Hearst 1995). This approach, called TileBars, allows the user to make informed <lb/>decisions about which documents and which passages of those documents to view, <lb/>based on the distributional behavior of the query terms in the documents. TileBars <lb/>allows users to specify different sets of query terms, as discussed later. The goal is to <lb/>simultaneously and compactly indicate: <lb/>1. <lb/>the relative length of the document, <lb/>2. <lb/>the frequency of the term sets in the document, and <lb/>3. <lb/>the distribution of the term sets with respect to the document and to <lb/>each other. <lb/>TextTiling is used to partition each document, in advance, into a set of multi-paragraph <lb/>subtopical segments. <lb/>Figure 1 shows an example query about automated systems for medical diagno-<lb/>sis, run over the ZIFF portion of the TIPSTER collection (Harman 1993). Each large <lb/>rectangle next to a title indicates a document, and each square within the rectangle <lb/>represents a TextTile in the document. The darker the tile, the more frequent the term <lb/>(white indicates 0, black indicates 8 or more hits; the frequencies of all the terms within <lb/>a term set are added together). The top row of each rectangle corresponds to the hits <lb/>for Term Set 1, the middle row to hits for Term Set 2, and the bottom row to hits for <lb/>Term Set 3. The first Column of each rectangle corresponds to the first TextTile of the <lb/>document, the second column to the second TextTile, and so on. The patterns of gray-<lb/>level are meant to provide a compact summary of which passages of the document <lb/>matched which topics of the query. <lb/>Users&apos; queries are written as lists of words, where each list, or term set, is meant <lb/>to correspond to a different component of the query. 2 This list of words is then trans-<lb/>lated into conjunctive normal form. For example, the query in the Figure is translated <lb/>by the system as: (patient OR medicine OR medical) AND (test OR scan OR cure OR <lb/>diagnosis) AND (software OR program). This formulation allows the interface to reflect <lb/>each conceptual part of the query: the medical terms, the diagnosis terms, and the <lb/>software terms. The document whose title begins &quot;VA automation means faster ad-<lb/>missions&quot; is quite likely to be relevant to the query, and has hits on all three term sets <lb/>throughout the document. By contrast, the document whose title begins &quot;It&apos;s hard to <lb/>ghostbust a network ...&quot; is about computer-aided diagnosis, but has only a passing <lb/>reference to medical diagnosis, as can be seen by the graphical representation. <lb/>This version of the TileBars interface allows the user to filter the retrieved doc-<lb/>uments according to which aspects of the query are most important. For example, if <lb/>the user decides that medical terms should be better represented, the Min Hits or Min <lb/></body>

			<note place="footnote">2 This query format was found to be unproblematic for users in a separate study (Hearst et al. 1996), and <lb/>is also used in the Grateful Med medical information system (Hersh et al. 1995). <lb/></note>

			<page>37 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>~t <lb/>=¢== wz= dLi~gumsi,, <lb/>Iii,i ii , <lb/>~ <lb/>r <lb/>~ <lb/>&quot;TextPert (So{b~mre Review) (eva{u~on)&quot; <lb/>~ I &quot;PC pdn~ers gain 3;;87 power with pro~col converters~ (Har~var~ <lb/>&quot;VA au~matJon means faster admissions, (US Department ol Veterans A~Iai~ <lb/>&quot;Better ADP could cut VA delays 4D%, of Bcia{s say, (autgma~c 0~a process <lb/>&quot;Card smarts, (smart car~s)* <lb/>&quot;It&apos;s hard to ghostbust a ne~k ~ current diagnostic tools managers s~ <lb/>&quot;Army tests prototype battlefield information ~m,&amp;O * <lb/>• Lack of ]rnagtnat~lon stalls op*dcai-disk appiications.&amp;O&quot; <lb/>&quot;The electrlc cadaver, [computerized anatomy {assorts and digits! dissection <lb/>[ &quot;interesting r~, things, (monitoring and testing equipment) (buyers g <lb/>&quot;MegaDdve 20 is fellah!e, has exceltent sol~ra~e: &apos;mice adjunct to standard h <lb/>Figure 1 <lb/>The TileBars Display on a query about automated systems for medical diagnosis (Hearst 1995 <lb/>(~) ACM). <lb/>Distribution constraint on this term set can be adjusted accordingly. Min Hits indicates <lb/>the minimum number of times words from a term set must appear in the document <lb/>in order for it to be displayed. Similarly, Min Distribution indicates the minimum per-<lb/>centage of tiles that must have a representative from the term set. The setting Min <lb/>Overlap Span refers to the minimum number of tiles that must have at least one hit <lb/>from each of the three term sets. In Figure 1, the user has indicated that the diagnosis <lb/>aspect of the query must be strongly present in the retrieved documents, by setting <lb/>the Min Distribution to 30% for the second term set. 3 <lb/>When the user mouse-clicks on a square in a TileBar, the corresponding document <lb/>is displayed beginning at the selected TextTile. Thus the user can also view the subtopic <lb/>structure within the document itself. <lb/></body>

			<note place="footnote">3 Most likely this setting information is too complicated for a typical user; I have performed some <lb/>experiments to determine how to set these constraints automatically (Hearst 1996) to be used in future <lb/>versions of the interface. <lb/></note>

			<page>38 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>This section has discussed why multi-paragraph segmentation is important and <lb/>how it might be used. The next section elaborates on what is meant by multi-paragraph <lb/>subtopic structure, casting the problem in terms of detection of topic or subtopic shift. <lb/>3. Coarse-Grained Subtopic Structure <lb/>3.1 What is Subtopic Structure? <lb/>In order to describe the detection of subtopic structure, it is important to define the <lb/>phenomenon of interest. The use of the term subtopic here is meant to signify pieces <lb/>of text &quot;about&quot; something and is not to be confused with the topic/comment distinc-<lb/>tion (Grimes 1975), also known as the given/new contrast (Kuno 1972), found within <lb/>individual sentences. <lb/>The difficulty of defining the notion of topic is discussed at length in Brown and <lb/>Yule (1983, Section 3). They note: <lb/>The notion of &apos;topic&apos; is clearly an intuitively satisfactory way of de-<lb/>scribing the unifying principle which makes one stretch of discourse <lb/>&apos;about&apos; something and the next stretch &apos;about&apos; something else, for it <lb/>is appealed to very frequently in the discourse analysis literature .... <lb/>Yet the basis for the identification of &apos;topic&apos; is rarely made explicit. <lb/>(pp. 69-70) <lb/>After many pages of attempting to pin the concept down, they suggest, as one <lb/>alternative, investigating topic-shift markers instead: <lb/>It has been suggested.., that instead of undertaking the difficult task <lb/>of attempting to define &apos;what a topic is&apos;, we should concentrate on <lb/>describing what we recognize as topic shift. That is, between two con-<lb/>tiguous pieces of discourse which are intuitively considered to have <lb/>two different &apos;topics&apos;, there should be a point at which the shift from <lb/>one topic to the next is marked. If we can characterize this marking <lb/>of topic-shift, then we shall have found a structural basis for dividing <lb/>up stretches of discourse into a series of smaller units, each on a sep-<lb/>arate topic .... The burden of analysis is consequently transferred to <lb/>identifying the formal markers of topic-shift in discourse. (pp. 94-95) <lb/>This notion of looking for a shift in content bears a close resemblance to Chafe&apos;s notion <lb/>of The Flow Model of discourse in narrative texts (Chafe 1979), in description of which <lb/>he writes: <lb/>Our data ... suggest that as a speaker moves from focus to focus (or <lb/>from thought to thought) there are certain points at which there may <lb/>be a more or less radical change in space, time, character configuration, <lb/>event structure, or, even, world .... At points where all of these change <lb/>in a maximal way, an episode boundary is strongly present. But often <lb/>one or another will change considerably while others will change less <lb/>radically, and all kinds of varied interactions between these several <lb/>factors are possible. 4 (pp. 179-80) <lb/></body>

			<note place="footnote">4 Interestingly, Chafe arrived at the Flow Model after working extensively with, and then becoming <lb/>dissatisfied with, a hierarchical model of paragraph structure like that of Longacre (1979). <lb/></note>

			<page>39 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>Thus, rather than identifying topics (or subtopics) per se, several theoretical dis-<lb/>course analysts have suggested that changes or shifts in topic can be more readily <lb/>identified and discussed. TextTiling adopts this stance. The problem remains, then, of <lb/>how to detect subtopic shift. Brown and Yule (1983) consider in detail two markers: <lb/>adverbial clauses and certain kinds of prosodic markers. By contrast, the next sub-<lb/>section will show that lexical co-occurrence patterns can be used to identify subtopic <lb/>shift. <lb/>3.2 Relationship to Segmentation in Hierarchical Discourse Models <lb/>Much of the current work in empirical discourse processing makes use of hierarchical <lb/>discourse models, and several prominent theories of discourse assume a hierarchical <lb/>segmentation model. Foremost among these are the attentional/intentional structure of <lb/>Grosz and Sidner (1986) and the Rhetorical Structure Theory of Mann and Thompson <lb/>(1987). The building blocks for these theories are phrasal or clausal units, and the <lb/>targets of the analyses are usually very short texts, typically one to three paragraphs in <lb/>length. 5 Many problems in discourse analysis, such as dialogue generation and turn-<lb/>taking (Moore and Pollack 1992; Walker and Whittaker 1990), require fine-grained, <lb/>hierarchical models that are concerned with utterance-level segmentation. Progress is <lb/>being made in the automatic detection of boundaries at this level of granularity using <lb/>machine learning techniques combined with a variety of well-chosen discourse cues <lb/>(Litman and Passonneau 1995). <lb/>In contrast, TextTiling has the goal of identifying major subtopic boundaries, at-<lb/>tempting only a linear segmentation. We should expect to see, in grouping together <lb/>paragraph-sized units instead of utterances, a decrease in the complexity of the fea-<lb/>ture set and algorithm needed. The work described here makes use only of lexical <lb/>distribution information, in lieu of prosodic cues such as intonational pitch, pause, <lb/>and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well, <lb/>ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolu-<lb/>tion (Passonneau and Litman 1993; Webber 1988) and tense and aspect (Webber 1987; <lb/>Hwang and Schubert 1992). From a computational viewpoint, deducing textual topic <lb/>structure from lexical occurrence information alone is appealing, both because it is <lb/>easy to compute, and because discourse cues are sometimes misleading with respect <lb/>to the topic structure (Brown and Yule 1983, Section 3). <lb/>4. Detecting Subtopic Change via Lexical Co-occurrence Patterns <lb/>TextTiling assumes that a set of lexical items is in use during the course of a given <lb/>subtopic discussion, and when that subtopic changes, a significant proportion of the <lb/>vocabulary changes as well. The algorithm is designed to recognize episode boundaries <lb/>by determining where thematic components like those listed by Chafe (1979) change <lb/>in a maximal way. However, unlike other researchers who have studied setting, time, <lb/>characters, and the other thematic factors that Chafe mentions, I attempt to determine <lb/>where a relatively large set of active themes changes simultaneously, regardless of <lb/>the type of thematic factor. This is especially important in expository text in which <lb/>the subject matter tends to structure the discourse more so than characters, setting, <lb/>and so on. For example, in the Stargazers text introduced in Section 1, a discussion of <lb/></body>

			<note place="footnote">5 Discourse work at the multi-paragraph level has been mainly in the theoretical, as opposed to <lb/>computational, realm, notably the work on macrostructures (van Dijk 1980, 1981) and story grammars <lb/>(Lakoff 1972; Rumelhart 1975). <lb/></note>

			<page>40 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>continental movement, shoreline acreage, and habitability gives way to a discussion <lb/>of binary and unary star systems. This is not so much a change in setting or character <lb/>as a change in subject matter. <lb/>The flow of subtopic structure as determined by lexical co-occurrence is illustrated <lb/>graphically in Figure 2. This figure shows the distribution, by sentence number, of se-<lb/>lected terms from the Stargazers text. The number of times a given word occurs in a <lb/>given sentence is shown, with blank spaces indicating zero occurrences. Words that <lb/>occur frequently throughout the text (e.g., life, moon) are often indicative of the main <lb/>topic(s) of the text. Words that are less frequent but more uniform in distribution, such <lb/>as form and scientist, tend to be neutral and do not provide much information about <lb/>the divisions within the discussions. The remaining words are what are of interest <lb/>here. They are &quot;clumped&quot; together, and it is these clumps or groups that TextTiling <lb/>assumes are indicative of the subtopic structure. The problem of segmentation there-<lb/>fore becomes the problem of detecting where these clumps begin and end. <lb/>For example, words binary through planet have considerable overlap in sentences <lb/>58 to 78, and correspond to the subtopic discussion Binary/trinary star systems make <lb/>life unlikely shown in the (manually produced) outline in Section 1. There is also a <lb/>well-demarcated cluster of terms between sentences 35 and 50, corresponding to the <lb/>grouping together of paragraphs 10, 11, and 12 by human judges who have read the <lb/>text, and to the subtopic discussion in Section 1 of How the moon helped life evolve on <lb/>earth. <lb/>These observations suggest that a very simple take on lexical cohesion relations <lb/>(Halliday and Hasan 1976) can be used to determine subtopic boundaries. However, <lb/>from the diagram it is evident that simply looking for chains of repeated terms (as <lb/>suggested by Morris and Hirst [1991]) is not sufficient for determining subtopic breaks. <lb/>Even combining terms that are closely related semantically into single chains is insuf-<lb/>ficient, since often several different themes are active within the same segment. For <lb/>example, sentences 37 to 51 contain dense interactions among the terms move, conti-<lb/>nent, shoreline, time, species, and life, and all but the latter occur only in this region. (It <lb/>is, however, the case that the interlinked terms of sentences 57 to 71, space, star, binary, <lb/>trinary, astronomer, orbit, are closely related semantically, assuming the appropriate <lb/>senses of the words.) <lb/>Because groups of words that are not necessarily closely related conceptually seem <lb/>to work together to indicate subtopic structure, I adopt a technique that can take into <lb/>account the occurrences of multiple simultaneous themes rather than use chains of <lb/>lexical cohesion relations alone. This viewpoint is also advocated by Skorochod&apos;ko <lb/>(1972), who suggests discovering a text&apos;s structure by dividing it up into sentences <lb/>and seeing how much word-overlap appears among the sentences. The overlap forms <lb/>a kind of intrastructure; fully connected graphs might indicate dense discussions of a <lb/>topic, while long spindly chains of connectivity might indicate a sequential account. <lb/>The central idea is that of defining the structure of a text as a function of the con-<lb/>nectivity patterns of the terms that comprise it, in contrast with segmentation guided <lb/>primarily by fine-grained discourse cues such as register change and cue words. <lb/>Many researchers, (e.g., Halliday and Hasan [1976], Tarmen [1989], and Walker <lb/>[1992]), have noted that term repetition is a strong cohesion indicator. Phillips (1985) <lb/>suggests performing &quot;an analysis of the distribution of the selected text elements rela-<lb/>tive to each other in some suitable text interval ... for whatever patterns of association <lb/>they may contract with each other as a function of repeated co-occurrence&quot; (p. 59). <lb/>Perhaps surprisingly, however, the results in Section 6 show that term repetition alone, <lb/>independent of other discourse cues, can be a very useful indicator of subtopic struc-<lb/>ture. This may be less true in the case of narrative texts, which tend to use more <lb/></body>

			<page>41 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>Ob <lb/>0 <lb/>O~ <lb/>LO <lb/>O0 <lb/>0 <lb/>~0 <lb/>LO <lb/>0 <lb/>l&quot;-<lb/>It) <lb/>0 <lb/>L.O <lb/>0 <lb/>&quot;,~ <lb/>LO <lb/>LO <lb/>0 <lb/>0 <lb/>m <lb/>0 <lb/>0 <lb/>Figure 2 <lb/>,¢-I <lb/>,r.t <lb/>,r-t <lb/>,¢--I <lb/>(&apos;,1 <lb/>T-t <lb/>,r-t <lb/>.wt <lb/>,¢-t <lb/>P. <lb/>P. <lb/>,r--t <lb/>C4 <lb/>CO <lb/>.i-I <lb/>¢&apos;1 <lb/>,¢-t <lb/>,¢-t <lb/>v-I <lb/>• ,--I ~ <lb/>0 <lb/>¢g r--I <lb/>-~ <lb/>• <lb/>I~ <lb/>• <lb/>-,-I <lb/>~&apos;~ ~ <lb/>I~l 0 <lb/>O <lb/>~ <lb/>0 <lb/>,:::1~ O0 u~ LO LO ~1~ O0 b.-¢,0 ¢.0 1~ ,~ Ob 1,~. O0 b.. O0 ¢.000 <lb/>¢,0 ~ <lb/>tO <lb/>0~ <lb/>O <lb/>O~ <lb/>tO <lb/>~0 <lb/>O <lb/>00 <lb/>u3 <lb/>t&apos;-<lb/>o <lb/>O <lb/>¢.O <lb/>LO <lb/>O <lb/>I.O <lb/>,q4 <lb/>CO <lb/>O <lb/>03 <lb/>O,I <lb/>O <lb/>v-I <lb/>O <lb/>O <lb/>O <lb/>Distribution of selected terms from the Stargazer text, with a single digit frequency per <lb/>sentence number (blanks indicate a frequency of zero). <lb/></body>

			<page>42 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>variation in the way concepts are expressed, and so may require that thesaural rela-<lb/>tions be used as well, as in (Kozima 1993). <lb/>It should be noted that other researchers have experimented with the display of <lb/>patterns of cohesion cues other than lexical cohesion as tools for analyzing discourse <lb/>structure. Grimes (1975, Chapter 6) introduces span charts to show the interaction <lb/>of various thematic devices such as character identification, setting, and tense. Stod-<lb/>dard (1991) creates cohesion maps by assigning to each word a location on a two-<lb/>dimensional grid corresponding to the word&apos;s position in the text. <lb/>To summarize, many discourse analysis tasks require a fine-grained, hierarchical <lb/>model, and consequently require many kinds of discourse cues for segmentation in <lb/>practice. TextTiling attempts a coarser-grained analysis and so gets away with using <lb/>a simpler feature set. Additionally, if we think of subtopic segmentation in terms of <lb/>detection of shift from one discussion to the next, we can simplify the task to one <lb/>of detecting where the use of one set of terms ends and another set begins. Figure 2 <lb/>illustrates that lexical distribution information can be used to discover such subtopic <lb/>shifts. <lb/>The next subsections describe three different strategies for detecting subtopic shift. <lb/>The first is based on the observations of this subsection, that subtopics can be viewed <lb/>as &quot;clumps&quot; of vocabulary, and the problem of segmentation is one of detecting these <lb/>clumps. The following two subsections describe alternative techniques, derived by <lb/>recasting other researchers&apos; algorithms into a more appropriate framework for the <lb/>TextTiling task. <lb/>4.1 Comparing Adjacent Blocks of Text <lb/>In the block comparison algorithm, adjacent pairs of text blocks are compared for <lb/>overall lexical similarity. The TextTiling algorithm requires that a score, called the <lb/>lexical score, be computed for every sentence, or more precisely, for the gap between <lb/>every pair of sentences (since this is where paragraph breaks take place). <lb/>The sketch in Figure 3(a) illustrates the scores computed for the block comparison <lb/>algorithm. In this figure is shown a sequence of eight hypothetical sentences, their <lb/>contents represented as columns of letters, where each letter represents a term or <lb/>word. The sentences are grouped into blocks of size k, where in this illustration k = 2. <lb/>The more words the blocks have in common, the higher the lexical score at the gap <lb/>between them. If a low lexical score is preceded by and followed by high lexical scores, <lb/>this is assumed to indicate a shift in vocabulary corresponding to a subtopic change. <lb/>The blocks act as moving windows over the text. Several sentences can be con-<lb/>tained within a block, but the blocks shift by only one sentence at a time. Thus if there <lb/>are k sentences within a block, each sentence occurs in k, 2 score computations (except <lb/>for sentences at the extreme ends of the text). <lb/>The current version of the block algorithm computes scores in a very simple man-<lb/>ner, as the inner product of two vectors, where a vector contains the number of times <lb/>each lexical item occurs in its corresponding block. The inner product is normalized <lb/>to make the score fall between 0 and 1, inclusive. <lb/>Figure 3(a) shows the computation of the scores at the gaps between sentences 2 <lb/>and 3, between 4 and 5, and between 6 and 7. The scores shown are simple, unnor-<lb/>malized inner products of the frequencies of the terms in the blocks. For example the <lb/>gap between sentences 2 and 3 gets assigned a score of 8 computed as 2 • 1 (for A) <lb/>+1 • 1 (for B) +2 * 1 (for C) +1 * 1 (for D) +1 * 2 (for E). Results for this approach are <lb/>reported in Section 6. <lb/>After these scores are computed, the blocks are shifted by one sentence (sentences <lb/>1 and 8 need to be handled as boundary conditions). So, for example, in addition <lb/></body>

			<page>43 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>1 2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>A <lb/>A <lb/>co <lb/>c <lb/>B <lb/>E <lb/>E <lb/>I <lb/>,,,o/,,,./-,,./ <lb/>g <lb/>3 <lb/>9 <lb/>(a) <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>c B <lb/>B <lb/>B <lb/>) <lb/>H 0 <lb/>5 <lb/>+ <lb/>0 <lb/>+ <lb/>3 <lb/>+ <lb/>1 <lb/>\/N/&apos;,,, <lb/>/ <lb/>(b) <lb/>5 <lb/>3 <lb/>4 <lb/>1 2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>A-AI <lb/>B <lb/>C-C.4--C <lb/>D <lb/>E <lb/>A I <lb/>I <lb/>I B <lb/>I <lb/>B-I-B <lb/>I <lb/>I <lb/>D I <lb/>I <lb/>E-E.--I.-E <lb/>I <lb/>I F~F-I-<lb/>F <lb/>I G <lb/>I G-G <lb/>I H~H-I-<lb/>H-H <lb/>I <lb/>I <lb/>I <lb/>I <lb/>I <lb/>s <lb/>1 <lb/>4 <lb/>(c) <lb/>Figure 3 <lb/>Illustration of three ways to compute the lexical score at gaps between sentences. Numbers <lb/>indicate a numbered sequence of sentences, columns of letters signify the terms in the given <lb/>sentence. (a) Blocks -dot product of vectors of word counts in the block on the left and the <lb/>block on the right. (b) Vocabulary introduction -the number of words that occur for the first <lb/>time within the interval centered at the sentence gap. (c) Chains -the number of active chains, <lb/>or terms that repeat within threshold sentences and span the sentence gap. <lb/>to comparing sentences 3 and 4 against sentences 5 and 6, the algorithm compares <lb/>sentences 4 and 5 against sentences 6 and 7. <lb/>An earlier version of the algorithm (Hearst 1993; Hearst and Plaunt 1993) weighted <lb/>terms according to tf.idf weights from Information Retrieval (Salton 1989). This weight-<lb/>ing function computes, for each word, the number of times it occurs in the document <lb/>tf, times the inverse of the number of documents that the term occurs in, in a large col-<lb/>lection idf, or as in this case, with some normalizing constants. The idea is that terms <lb/>that commonly occur throughout a collection are not necessarily good indicators of <lb/>relevance to a query because they are so common, and so their importance is down-<lb/>weighted. Hearst (1993) posited that this argument should also apply to determining <lb/>which words best distinguish one subtopic from another. However, the estimates of <lb/>importance that tf.idf makes seem not to be accurate enough within the scope of com-<lb/>paring adjacent pieces of text to justify using this measure, and the results seem more <lb/>robust weighting the words according to their frequency within the block alone. <lb/></body>

			<page>44 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>4.2 Vocabulary Introductions <lb/>Another recent analytic technique that makes use of lexical information is described in <lb/>Youmans (1991), which introduces a variant on type/token curves, called the Vocabu-<lb/>lary-Management Profile. Type/token curves are simply plots of the number of unique <lb/>words against the number of words in a text, starting with the first word and pro-<lb/>ceeding through the last. Youmans modifies this algorithm to keep track of how many <lb/>first-time uses of words occur at the midpoint of every 35-word window in a text. <lb/>Youmans&apos; goal is to study the distribution of vocabulary in discourse rather than to <lb/>segment it along topical lines, but upon examining many English narratives, essays, <lb/>and transcripts he notices that sharp upturns after deep valleys in the curve &quot;correlate <lb/>closely to constituent boundaries and information flow&quot; (p. 788). <lb/>Youmans&apos; analysis of the graphs is descriptive in nature, mainly attempting to <lb/>identify the cause of each peak or valley in terms of a principle of narrative structure, <lb/>and is done at a very fined-grained level. He discusses one text in detail, describ-<lb/>ing changes at the single-word level, and focusing on within-paragraph and within-<lb/>sentence events. Examples of events are changes in characters, occurrences of dialogue, <lb/>and descriptions of places, each of which ranges in length from one clause to a few sen-<lb/>tences. He also finds that paragraph boundaries are not always predicted--sometimes <lb/>the onset of a new paragraph is signaled by the occurrence of a valley in the graph, <lb/>but often paragraph onset is not signaled until one or two sentences beyond onset. 6 <lb/>One of Youmans&apos; main foci is an attempt to cast the resulting peaks in terms of <lb/>co-ordination and subordination relations. However, in the discussion he notes that <lb/>this does not seem like an appropriate use of the graphs. No systematic evaluation of <lb/>the algorithm is presented, nor is there any discussion of how one might automatically <lb/>determine the significance of the peaks and valleys. <lb/>Nomoto and Nitta (1994) attempt to use Youmans&apos; algorithm for distinguishing <lb/>entire articles from one another when they are concatenated into a single file. They find <lb/>that it &quot;fails to detect any significant pattern in the corpus&quot; (p. 1148). I recast Youmans&apos; <lb/>algorithm into the TextTiling framework, renaming it the vocabulary introduction <lb/>method. Figure 3(b) illustrates. The text is analyzed, and the positions at which terms <lb/>are first introduced are recorded (shown in black circles in the figure). A moving <lb/>window is used again, as in the blocks algorithm, and this window corresponds to <lb/>Youmans&apos; interval. The number of new terms that occur on either side of the midpoint, <lb/>or the sentence gap of interest, are added together and plotted against sentence gap <lb/>number. <lb/>This approach differs from that of Youmans (1991) and Nomoto and Nitta (1994) in <lb/>two main ways. First, Nomoto and Nitta (1994) use too large an interval--300 words--<lb/>because this is approximately the average size needed for their implementation of the <lb/>blocks version of TextTiling. Large paragraph-sized intervals for measuring introduc-<lb/>tion of new words seem unlikely to be useful since every paragraph of a given length <lb/>should have approximately the same number of new words, although those at the be-<lb/>ginning of a subtopic segment will probably have slightly more. Instead, I use interval <lb/>lengths of size 40, closer to Youmans&apos; suggestion of 35. <lb/>Second, the granularity at which Youmans takes measurements is too fine, since <lb/>he plots the score at every word. Sampling this frequently yields a very spiky plot <lb/>from which it is quite difficult to draw conclusions at a paragraph-sized granularity. I <lb/></body>

			<note place="footnote">6 This might be explained in part by Stark (1988) who shows that readers disagree measurably about <lb/>where to place paragraph boundaries when presented with texts with those boundaries removed. <lb/></note>

			<page>45 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>plot the score at every sentence gap, thus eliminating the wide variation that is seen <lb/>when measuring after each word. Results for this approach are reported in Section 6. <lb/>4.3 Lexical Chains <lb/>Morris and Hirst&apos;s pioneering work on computing discourse structure from lexical <lb/>relations (Morris and Hirst 1991; Morris 1988) is a precursor to the work reported on <lb/>here. Influenced by Halliday and Hasan&apos;s (1976) theory of lexical coherence, Morris <lb/>developed an algorithm that finds chains of related terms via a comprehensive the-<lb/>saurus (Roget&apos;s Fourth Edition). 7 For example, the words residential and apartment both <lb/>index the same thesaural category and can thus be considered to be in a coherence <lb/>relation with one another. The chains are used to structure texts according to the at-<lb/>tentional/intentional theory of discourse structure (Grosz and Sidner 1986) discussed <lb/>above. The extent of the lexical chains is assumed to correspond to the extent of a <lb/>segment. The algorithm also incorporates the notion of chain returns--repetition of <lb/>terms after a long hiatus--to complete an intention that spans over a digression. The <lb/>boundaries of the segments correspond to the sentences that contain the first and last <lb/>words of the chain. <lb/>Since the Morris and Hirst (1991) algorithm attempts to discover attentional/inten-<lb/>tional structure, its goals are different than those of TextTiling. Specifically, the dis-<lb/>course structure it attempts to discover is hierarchical and more fine-grained than <lb/>that discussed here. Morris (1988) provides five short example texts for which she has <lb/>determined the intentional structure, and states that the lexical chains generated by <lb/>her algorithm provide a good indication of the segment boundaries that Grosz and <lb/>Sidner&apos;s theory assumes. In Morris (1988) and Morris and Hirst (1991), tables are pre-<lb/>sented showing the sentences spanned by the lexical chains and by the corresponding <lb/>segments of the attentional/intentional structure (derived by hand), but no formal <lb/>evaluation is performed. <lb/>This algorithm is not directly applicable for TextTiling for several reasons. First, <lb/>many words are ambiguous and fall into more than one thesaurus class. This is not <lb/>stated as a concern in Morris&apos;s work, perhaps because the texts were short, and pre-<lb/>sumably, if a word were ambiguous, the correct thesaurus class would nevertheless <lb/>be chosen because the chained-to words would share only the correct thesaurus class. <lb/>However, my experimentation with an implemented version of Morris&apos; algorithm that <lb/>made use of Roget&apos;s 1911 thesaurus (which is admittedly less structured than the <lb/>thesaurus used by Morris), when run on longer texts, found ambiguous links to be a <lb/>common occv&apos;,:ence and detrimental to the algorithm. A thesaurus-based disambigua-<lb/>tion algorithm (Yarowsky 1992) may help alleviate this problem (this option is revisited <lb/>in Section 7), but another solution is to move away from thesaurus classes and use <lb/>simple word co-occurrence instead, since within a given text a word is usually used <lb/>with only one sense (Gale, Church, and Yarowsky 1992b). The potential downside of <lb/>this approach is that many useful links may be missed. <lb/>Another limitation of the Morris algorithm is that it does not take advantage of, <lb/>or discuss how to account for, the tendency for multiple simultaneous chains to occur <lb/>over the same intention (each chain corresponds to one intention). Related to this is the <lb/>fact that chains tend to overlap one another in long texts, as can be seen in Figure 2. <lb/>These two types of difficulties can be circumvented by recasting the Morris al-<lb/>gorithm to take advantage of the observations at the beginning of this section. Three <lb/></body>

			<note place="footnote">7 The algorithm was executed by hand since the thesaurus is not generally available online. Current <lb/>extensions to this work make use of WordNet (Miller et al. 1990). <lb/></note>

			<page>46 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>changes are made to the algorithm: First, no thesaurus classes are used (only term <lb/>repetition of morphological variants of the same word); second, multiple chains are <lb/>allowed to span an intention; and third, chains at all levels of intentions are analyzed <lb/>simultaneously. Instead of deciding which chain is the applicable one for a given in-<lb/>tention, it measures how many chains at all levels are active at each sentence gap. <lb/>This approach is illustrated in Figure 3(c). A lexical chain for term t is considered <lb/>active across a sentence gap if instances of t occur within some distance threshold of <lb/>one another. In the figure, all three instances of the word A occur within the distance <lb/>threshold. The third B, however, follows too far after the second B to continue the <lb/>chain. The score for the gap between 2 and 3 is simply the number of active chains <lb/>that span this gap. Boundaries are determined as specified in Section 5. This variation <lb/>of the TextTiling algorithm is explored and evaluated in Hearst (1994b). <lb/>4.4 Vector Space Similarity Comparisons <lb/>As mentioned in Section 2, Salton and Allan (1993) report work in the automatic de-<lb/>tection of hypertext links and theme generation from large documents, focusing pri-<lb/>marily on encyclopedia text. They describe the application of similarity comparisons <lb/>between articles, sections, and paragraphs within an encyclopedia, both for creating <lb/>links among related passages, and for better facilitating retrieval of articles in response <lb/>to user queries. Their approach finds similarities among the paragraphs of large doc-<lb/>uments using normalized tfidf term weighting, scoring text segments according to <lb/>a normalized inner product of vectors of these weights (this algorithm is called the <lb/>vector space model [Salton 1989]). <lb/>Salton and Allan (1993) do not try to determine the extents of passages within <lb/>articles or sections. Instead, all paragraphs, sections, and articles are assigned pair-<lb/>wise similarity scores, and links are drawn between those with the highest scores, <lb/>independent of their position within the text. This distinction is important because <lb/>the difficulty in subtopic segmentation lies in detecting the subtle differences between <lb/>adjacent text blocks. A method that finds blocks with the topmost similarity to one <lb/>another can succeed at finding the equivalent of the center of a subtopic extent, but <lb/>does not distinguish where one subtopic ends and the next begins. <lb/>If the algorithm of Salton and Allan (1993) were transformed so that adjacent text <lb/>units were compared, and a method for determining where the similarity scores are <lb/>low were used, then it would resemble the blocks algorithm with tfidf weighting, <lb/>but without the use of overlapping text windows. However, a consequence of the <lb/>fact that the vector space method is better at distinguishing similarities than differ-<lb/>ences, is that similarity scores alone are probably less effective at finding the transition <lb/>points between subtopic discussions than sequences of similarity scores, using moving <lb/>windows of text, in the manner described above. <lb/>Salton et al. (1996) attempt to address a version of the subtopic segmentation prob-<lb/>lem by extending the algorithm to finding &quot;text pieces exhibiting internal consistency <lb/>that can be distinguished from the remainder of the surrounding text&quot; (p. 55). As one <lb/>part of this goal, they seek what is called the text segment, which is defined as &quot;a <lb/>contiguous piece of text that is linked internally, but largely disconnected from the <lb/>adjacent text. Typically, a segment might consist of introductory material, or cover the <lb/>exposition and development of the text, or contain conclusions and results&quot; (p. 55). <lb/>Thus, they do not address the subtopic detection task because they attempt only to <lb/>find those segments of text that are strongly different than the surrounding text. They <lb/>do this by comparing similarity between a paragraph and its four closest paragraph <lb/>neighbors to the left and the right. If a similarity score between a pair of paragraphs <lb/>does not exceed a threshold, then the link between that pair is removed. If a discon-<lb/></body>

			<page>47 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>nected sequence of paragraphs is found, that sequence is considered a text segment. <lb/>This algorithm is not evaluated. <lb/>4.5 Other Related Approaches <lb/>Kozima (1993) describes an algorithm for the detection of text segments, which are <lb/>defined as &quot;a sequence of clauses or sentences that display local coherence&quot; (p. 286) in <lb/>narrative text. Kozima (1993) presents a very elaborate algorithm for computing the <lb/>lexical cohesiveness of a window of words, using spreading activation in a seman-<lb/>tic network created from an English dictionary. The cohesion score is plotted against <lb/>words and smoothed, and boundaries are considered to fall at the lowest-scoring <lb/>words. This complex computation, as opposed to simple term repetition, may be nec-<lb/>essary when working with narrative texts, but no comparison of methods is done. The <lb/>algorithm&apos;s results are shown on one text, but are not evaluated formally. <lb/>Reynar (1994) describes an algorithm similar to that of Hearst (1993) and Hearst <lb/>and Plaunt (1993) with a difference in the way in which the size of the blocks of <lb/>adjacent regions are chosen. A greedy algorithm is used: the algorithm begins with no <lb/>boundaries, then a boundary b (between two sentences) is chosen which maximizes <lb/>the lexical score resulting from comparing the block on the left whose extent ranges <lb/>from b to the closest existing boundary on the left, and similarly for the right. This <lb/>process is repeated until a prespecified number of boundaries have been chosen. This <lb/>seems problematic, since the initial comparisons are between very large text segments: <lb/>the first boundary is chosen by comparing the entire text to the right and left of the <lb/>initial position. The algorithm is evaluated only in terms of how well it distinguishes <lb/>entire articles from one another when concatenated into one file. The precision/recall <lb/>tradeoffs varied widely: on 660 Wall Street Journal articles, if the algorithm is allowed <lb/>to be off by up to three sentences, it achieves precision of .80 with recall of .30, and <lb/>precision of .30 with recall of .92. <lb/>5. The TextTiling Algorithm <lb/>The TextTiling algorithm for discovering subtopic structure using term repetition has <lb/>three main parts: <lb/>1. <lb/>2. <lb/>3. <lb/>Tokenization <lb/>Lexical Score Determination <lb/>Boundary Identification <lb/>Each is discussed in turn below. The methods for lexical score determination were <lb/>outlined in Section 4, but more detail is presented here. <lb/>5.1 Tokenization <lb/>Tokenization refers to the division of the input text into individual lexical units, and <lb/>is sensitive to the format of the input text. For example, if the document has markup <lb/>information, the header and other auxiliary information is skipped until the body of <lb/>the text is located. Tokens that appear in&apos; the body of the text are converted to all <lb/>lower-case characters and checked against a stop list of closed-classed and other high-<lb/>frequency words, s If the token is a stop word then it is not passed on to the next <lb/></body>

			<note place="footnote">8 &quot;Stop list&quot; is a term commonly used in Information Retrieval (Salton 1989). In this case, the list consists <lb/>of 898 words, developed in a somewhat ad hoc manner. <lb/></note>

			<page>48 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>step. Otherwise, the token is reduced to its root by a morphological analysis function <lb/>based on that of Kartunen, Koskenniemi, and Kaplan (1987), converting regularly and <lb/>irregularly inflected nouns and verbs to their roots. <lb/>The text is subdivided into pseudosentences of a predefined size w (a parameter of <lb/>the algorithm) rather than using &quot;real&quot; syntactically-determined sentences. This is done <lb/>to allow for comparison between equal-sized units, since the number of shared terms <lb/>between two long sentences and between a long and a short sentence would probably <lb/>yield incomparable scores (and sentences are too short to expect normalization to <lb/>really accommodate for the differences). For the purposes of the rest of the discussion <lb/>these groupings of tokens will be referred to as token-sequences. The morphologically <lb/>analyzed token is stored in a table along with a record of the token-sequence number <lb/>it occurred in, and the number of times it appeared in the token-sequence. A record is <lb/>also kept of the locations of the paragraph breaks within the text. Stop words contribute <lb/>to the computation of the size of the token-sequence, but not to the computation of <lb/>the similarity between blocks of text. <lb/>5.2 Determining Scores <lb/>As mentioned above, two methods for determining the score to be assigned at each <lb/>token-sequence gap are explored here. The first, block comparison, compares adjacent <lb/>blocks of text to see how similar they are according to how many words the adjacent <lb/>blocks have in common. The second, the vocabulary introduction method, assigns <lb/>a score to a token-sequence gap based on how many new words were seen in the <lb/>interval in which it is the midpoint. <lb/>5.2.1 Blocks. In the block comparison algorithm, adjacent pairs of blocks of token-<lb/>sequences are compared for overall lexical similarity. The block size, labeled k, is the <lb/>number of token-sequences that are grouped together into a block to be compared <lb/>against an adjacent group of token-sequences. This value is meant to approximate the <lb/>average paragraph length. Actual paragraphs are not used because their lengths can <lb/>be highly irregular, leading to unbalanced comparisons, but perhaps with a clever <lb/>normalizing scheme, &quot;real&quot; paragraphs could be used (analogous to the substitution <lb/>of token-sequences for real sentences). <lb/>Similarity values are computed for every token-sequence gap number; that is, a <lb/>score is assigned to token-sequence gap i corresponding to how similar the token-<lb/>sequences from token-sequence i -k to i are to the token-sequences from i + 1 to <lb/>i + k + 1. Note that this moving window approach means that each token-sequence <lb/>appears in k • 2 similarity computations. <lb/>The lexical score for the similarity between blocks is calculated by a nor-<lb/>malized inner product: given two text blocks bl and b2, each with k token-se-<lb/>quences, where bl = {token-sequencei_k,..., token-sequencei} and b2 ~-{token-sequencei+l, <lb/>.... token-sequencei+k + l }, <lb/>score(i) = <lb/>Y~t Wt,bl Wt,b2 <lb/>V/G w2 G w2 <lb/>t,bl <lb/>t,b2 <lb/>where t ranges over all the terms that have been registered during the tokenization <lb/>step (thus excluding stop words), and Wt,b is the weight assigned to term t in block b. <lb/>As mentioned in Section 4, in this version of the algorithm, the weights on the terms <lb/>are simply their frequency within the block, This formula yields a score between 0 <lb/>and 1, inclusive. <lb/>These scores can be plotted, token-sequence number against similarity score. How-<lb/>ever, since similarity is measured between blocks bl and b2, the score&apos;s x-axis coordi-<lb/></body>

			<page>49 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>nate falls between token-sequences i and i + 1. Rather than plotting a token-sequence <lb/>number on the x-axis, the token-sequence gap number i is plotted instead. <lb/>5.2.2 Vocabulary Introduction. The lexical score assigned in the vocabulary introduc-<lb/>tion version of scoring is the ratio of new words in an interval divided by the length <lb/>of that interval. Tokenization is as described above, eliminating stop words and per-<lb/>forming morphological analysis. A score is then assigned to a token-sequence gap as <lb/>follows: the number of never-yet-seen words in the token-sequence to the left of the <lb/>gap is added to the number of never-yet-seen words in the token-sequence to the right, <lb/>and this number is divided by the total number of tokens in the two token-sequences, <lb/>or w • 2. Since in these experiments w is set to 20, this yields an interval length of 40, <lb/>which is close to the parameter 35 suggested as most useful in (Youmans 1991). As in <lb/>the block version of the algorithm, the score is plotted at the token-sequence gap, and <lb/>scores can range from 0 to 1, inclusive. <lb/>The lexical score is computed as follows. For each token-sequence gap i, create a <lb/>text interval b of length w • 2 (where w is the length of the token-sequences) centered <lb/>around i, and let b be subdivided into two equal-length parts, bl and b2, where bl = <lb/>{ tokensi_w .... , tokensi } and b2 : { tokensi+ l, . . . , tokensi+w+ l }. Then, <lb/>score(i) = NumNewTerms(bl ) + NumNewTerms(b2) <lb/>w,2 <lb/>where NumNewTerms(b) returns the number of terms in interval b seen for the first <lb/>time in the text. <lb/>5.3 Boundary Identification <lb/>Boundary identification is done identically for all lexical scoring methods, and assigns <lb/>a depth score, the depth of the valley (if one occurs), to each token-sequence gap. The <lb/>depth score corresponds to how strongly the cues for a subtopic changed on both sides <lb/>of a given token-sequence gap and is based on the distance from the peaks on both <lb/>sides of the valley to that valley. Figure 4 illustrates. In Figure 4(a), the depth score at <lb/>gap a2 is (Yax -Ya2) q-(Ya3 --Ya2)&quot; Relatively &quot;deeper&quot; valleys receive higher scores than <lb/>shallower ones. More formally, for a given token-sequence gap i, the program records <lb/>the lexical score of the token-sequence gap I to the left of i until the score for I -1 is <lb/>smaller than the score for l (meaning the top of the peak was found at 1). Similarly, for <lb/>token sequences to the right of i, the program monitors the score of token-sequence <lb/>r until the score for r + 1 is less than that of r. Finally, score(r) -score(i) is added to <lb/>score(l) -score(i), and the result is the depth score at i. <lb/>A potential problem with this scoring method is illustrated in Figure 4(b). Here <lb/>we see a small valley at gap b4 that can be said to &quot;interrupt&quot; the score for b2. As one <lb/>safeguard, the algorithm uses smoothing (described below) to help eliminate small <lb/>perturbations of the kind seen at b4. Additionally, because the distance between Yb3 <lb/>and Yb4 is small in these kinds of cases, this gap is less likely to be marked as a <lb/>boundary than gaps like b2, which have large peak distances both to the left and the <lb/>right. This example illustrates the need to take into account the length of both sides <lb/>of the valley, since a valley that has high peaks on both sides indicates that not only <lb/>has the vocabulary on the left decreased in score, but the vocabulary on the right has <lb/>increasing score, thus signaling a strong subtopic change. <lb/>Figure 4(c) shows another potentially problematic case, in which two strong peaks <lb/>flank a long, flat valley. The question becomes which of gaps c2, c3, or both, should <lb/>be assigned a boundary. Such &quot;plateaus&quot; occur when vocabulary changes very grad-<lb/>ually and reflect a poor fit of the corresponding portion of the document to the model <lb/></body>

			<page>50 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>Yb5 <lb/>Ya3 <lb/>Ybl YN~ <lb/>Ya2 <lb/>Yb2 <lb/>Yc2 <lb/>Yc3 <lb/>I <lb/>I <lb/>t <lb/>I <lb/>a2 <lb/>b2 <lb/>c2 <lb/>c3 <lb/>(a) <lb/>(b) <lb/>(c) <lb/>Figure 4 <lb/>A sketch illustrating the computation of depth scores in three different situations. The x-axis <lb/>indicates token sequence gap number and the y-axis indicates lexical score. <lb/>assumed by TextTiling. When the plateau occurs over a longer stretch, usually it is rea-<lb/>sonable to choose both bordering gaps as boundaries. However, when such a plateau <lb/>occurs over a very short stretch of text, the algorithm is forced to make a somewhat <lb/>arbitrary choice. Choices like these are cases in which the algorithm should proba-<lb/>bly make use of additional information, such as more localized lexical distribution <lb/>information, or perhaps more conventional discourse cues. <lb/>Note that the depth scores are based only on relative score information, ignoring <lb/>absolute values. The justification for this is twofold. First, it helps make decisions in the <lb/>cases in which a gap&apos;s lexical score falls into the middle of the lexical score range, but <lb/>is flanked by tall peaks on either side, and this situation happens commonly enough to <lb/>be important. Second, using relative rather than absolute scores helps avoid problems <lb/>associated with situations like that of Figure 4(c), in which all gaps between c2 and c3 <lb/>would be considered boundaries if only absolute scores were taken into account. <lb/>The depth scores are sorted and used to determine segment boundaries. The larger <lb/>the score, the more likely the boundary occurs at that location, modulo adjustments <lb/>as necessary to place the boundaries at orthographically marked paragraphs (if avail-<lb/>able). A proviso check is made to prevent assignment of very close adjacent segment <lb/>boundaries. Currently, at least three intervening token-sequences are required between <lb/>boundaries. This helps control for the fact that many texts have spurious header in-<lb/>formation and single-sentence paragraphs. <lb/>An alternative to this method of computing depth scores is to use the slope of <lb/>the valley&apos;s sides, or the &quot;sharpness&quot; of the vocabulary change. However, because <lb/>deeper valleys with smaller slopes indicate larger, although more gradual, shifts in <lb/>vocabulary usage than shallower valleys with larger slopes, they are preferable for <lb/>detecting subtopic boundaries. Furthermore, steep slopes can sometimes indicate a <lb/>spurious change associated with a very short digression. The depth score is more <lb/>robust for the purposes of subtopic boundary detection. <lb/></body>

			<page>51 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>5.4 Smoothing the Plot <lb/>As mentioned above, the plot is smoothed to remove small dips, using average smooth-<lb/>ing with a width of size s, as follows: <lb/>for each token-sequence gap g and a small even number s <lb/>find the scores of the s/2 gaps to the left of g <lb/>find the scores of the s/2 gaps to the right of g <lb/>find the score at g <lb/>take the average of these scores and assign it to g <lb/>repeat this procedure n times <lb/>The choice of smoothing function is somewhat arbitrary; other low-pass filters could <lb/>be used instead. <lb/>5.5 Determining the Number of Boundaries <lb/>The algorithm must determine how many segments to assign to a document, since <lb/>every paragraph is a potential segment boundary. Any attempt to make an absolute <lb/>cutoff, even one normalized for the length of the document, is problematic since there <lb/>should be some relationship between the structure and style of the text and the number <lb/>of segments assigned to it. As discussed above, a cutoff based on a particular valley <lb/>depth is similarly problematic. <lb/>Instead, I suggest making the cutoff a function of the characteristics of the depth <lb/>scores for a given document, using the average ~ and standard deviation ~ of their <lb/>scores (thus assuming that the scores are normally distributed). One version of this <lb/>function entails drawing a boundary only if the depth score exceeds ~ -cr (the liberal <lb/>measure, LC). This function can be varied to achieve correspondingly varying preci-<lb/>sion/recall trade-offs. A higher precision but lower recall can be found by setting the <lb/>limit to be depth scores exceeding ~ -or/2 (the conservative measure, HC) instead of <lb/>3-o-. <lb/>6. Evaluation <lb/>There are several ways to evaluate a segmentation algorithm, including comparing <lb/>its segmentation against that of human judges, comparing its segmentation against <lb/>author-specified orthographic information, and comparing its segmentation against <lb/>other automated segmentation strategies in terms of how they effect the outcome of <lb/>some computational task. This section presents comparisons of the results of the algo-<lb/>rithm against human judgments and against article boundaries. It is possible to com-<lb/>pare against author-specified markups, but unfortunately, as discussed above, authors <lb/>usually do not specify the kind of subtopic information desired. As mentioned above, <lb/>Hearst (1995) and Hearst and Plaunt (1993) show how to use TextTiles in information <lb/>retrieval tasks, although this work does not show whether or not the results of these <lb/>algorithms produce better performance than the results of some other segmentation <lb/>strategy would. <lb/>6.1 Reader Judgments <lb/>There is a growing concern surrounding issues of intercoder reliability when using <lb/>human judgments to evaluate discourse-processing algorithms (Carletta 1996; Condon <lb/>and Cech 1995). Proposals have recently been made for protocols for the collection of <lb/>human discourse segmentation data (Nakatani et al. 1995) and for how to evaluate the <lb/>validity of judgments so obtained (Carletta 1996; Isard and Carletta 1995; Ros6 1995; <lb/>Passonneau and Litman 1993; Litman and Passonneau 1995). Recently, Hirschberg <lb/></body>

			<page>52 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>and Nakatani (1996) have reported promising results for obtaining higher interjudge <lb/>agreement using their collection protocols. <lb/>For the evaluation of the TextTiling algorithms, judgments were obtained from <lb/>seven readers for each of 12 magazine articles that satisfied the length criteria (between <lb/>1,800 and 2,500 words) 9 and that contained little structural demarcation. The judges <lb/>were asked simply to mark the paragraph boundaries at which the topic changed; they <lb/>were not given more explicit instructions about the granularity of the segmentation. 1° <lb/>Figure 5 shows the boundaries marked by seven judges on the Stargazers text. This <lb/>format helps illustrate the general trends in the judges&apos; assessments, and also helps <lb/>show where and how often they disagree. For instance, all but one judge marked a <lb/>boundary between paragraphs 2 and 3. The dissenting judge did mark a boundary <lb/>after 3, as did two of the concurring judges. The next three major boundaries occur <lb/>after paragraphs 5, 9, 12, and 13. There is some contention in the later paragraphs; <lb/>three readers marked both 16 and 18, two marked 18 alone, and two marked 17 alone. <lb/>The outline in the Introduction gives an idea of what each segment is about. <lb/>Passonneau and Litman (1993) discuss at length considerations about evaluat-<lb/>ing segmentation algorithms according to reader judgment information. As Figure 5 <lb/>shows, agreement among judges is imperfect, but trends can be discerned. In the <lb/>data of Passonneau and Litman (1993), if four or more out of seven judges mark a <lb/>boundary, the segmentation is found to be significant using a variation of the Q-test <lb/>(Cochran 1950). However, in later work (Litman and Passonneau 1995), three out of <lb/>seven judges marking a boundary was considered sufficient to classify, that point as a <lb/>&quot;major&quot; boundary. <lb/>Carletta (1996) and Ros6 (1995) point out the importance of taking into account <lb/>the expected chance agreement among judges when computing whether or not judges <lb/>agree significantly. They suggest using the kappa coefficient (K) for this purpose. Ac-<lb/>cording to Carletta (1996), K measures pairwise agreement among a set of coders <lb/>making category judgments, correcting for expected chance agreement as follows: <lb/>K-P(A) -P(E) <lb/>1 -P(E) <lb/>where P(A) is the proportion of times that the coders agree and P(E) is the proportion <lb/>of times that they would be expected to agree by chance. The coefficient can be com-<lb/>puted by making pairwise comparisons against an expert or by comparing to a group <lb/>decision. Carletta (1996) also states that in the behavioral sciences, K &gt; .8 signals good <lb/>replicability, and .67 &lt; K &lt; .8 allows tentative conclusions to be drawn. The kappa <lb/>coefficients found in Isard and Carletta (1995) ranged from .43 to .68 for four coders <lb/>placing transaction boundaries, and those found in (Ros~ 1995) ranged from .65 to <lb/>.90 for four coders segmenting sentences. Carletta cautions, however, that &quot;... coding <lb/>discourse and dialogue phenomena, and especially coding segment boundaries, may <lb/></body>

			<note place="footnote">9 One longer text of 2,932 words was used since reader judgments had been obtained for it from an <lb/>earlier experiment. Judges were technical researchers. Two texts had three or four short headers, which <lb/>were removed for consistency. One text that was used in Hearst (1994b) is not used here because <lb/>inconsistencies were found in the paragraph break locations. <lb/></note>

			<note place="footnote">10 Specifically, the instructions were in written form and ran as follows: &quot;You will receive three texts. <lb/>Mark where the topics seem to change--draw a line between the paragraphs, where any blank line can <lb/>be considered a paragraph boundary. It&apos;s recommended that you read quickly; no need to understand <lb/>all the nuances. However, you are allowed to go back and look over parts that you&apos;ve already looked <lb/>at and change your markings if desired. If on occasion you can&apos;t decide between two places, definitely <lb/>pick one but indicate that you thought the other one was just as appropriate.&quot; On the rare occasions in <lb/>which the subject picked a secondary boundary, only the primary one was retained for evaluation. <lb/></note>

			<page>53 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>be inherently more difficult than many previous types of content analysis (for instance, <lb/>dividing newspaper articles based on subject matter)&quot; and so implies that the levels of <lb/>agreement needed to indicate good reliability for TextTiling may be justified in being <lb/>lower. <lb/>For my test texts, the judges placed boundaries on average 39.1% of the time, <lb/>and nonboundaries 60.9%. Thus the expected chance agreement P(E) is .524 (since <lb/>P(Boundary) = .391 and P(Nonboundary) ~-.609, (.3912 + .6092) = .524). To compute K, <lb/>each judge&apos;s decision was compared to the group decision, where a paragraph gap was <lb/>considered a &quot;true&quot; boundary if at least three out of seven judges placed a boundary <lb/>mark there, as in Litman and Passonneau (1995). 11 The remaining gaps are considered <lb/>nonboundaries. The average K for these texts was .647. This score is at the low end <lb/>of the stated acceptability range but is comparable with those of other interreliability <lb/>results (with fewer judges) found in discourse segmentation experiments. <lb/>6.2 Parameter Settings <lb/>An unfortunate aspect of the algorithm in its current form is that it requires the set-<lb/>ting of several interdependent parameters, the most important of which are the size of <lb/>the text unit that is compared, and the number of words in a token-sequence (which <lb/>controls the number of times a term appears in a window as well as the number of <lb/>data points that are sampled). The method, width, and number of rounds of smoothing <lb/>must also be chosen. Usually only modest amounts of smoothing can be allowed, since <lb/>more dramatic smoothing tends to obscure the point at which the subtopic transition <lb/>takes place. Finally, the method for determining how many boundaries to assign must <lb/>be specified. The three are interrelated: for example, using a larger text window re-<lb/>quires less smoothing and fewer boundaries will be found, yielding a coarser-grained <lb/>segmentation. <lb/>Initial testing was done on the texts evaluated with several different sets of pa-<lb/>rameter settings and a default configuration that seems to cover many different text <lb/>types was chosen. The defaults set w = 20, k = 10, n = 1, s = 2, for token-sequence <lb/>size, block size, number of rounds of smoothing, and smoothing width, respectively. <lb/>The evaluation presented here shows the results for different setting types to give a <lb/>feeling for the space of results. Because the evaluation collection is very small, these <lb/>results can be seen only as a suggestion; different settings may work better in different <lb/>situations. <lb/>6.3 Results: Qualitative Analysis <lb/>Figure 6 shows a plot of the results of applying the block comparison algorithm to the <lb/>Stargazer text with k set to 10. When the lowermost portion of a valley is not located at <lb/>a paragraph gap, the judgment is moved to the nearest paragraph gap. 12 For the most <lb/>part, the regions of strong similarity correspond to the regions of strong agreement <lb/>among the readers. (The results for this text are among the stronger ones and appear <lb/>in the last line of Table 2.) Note however, that the similarity information around para-<lb/>graph 12 is weak. This paragraph briefly summarizes the contents of the previous <lb/>three paragraphs; much of the terminology that occurred in all of them reappears in <lb/></body>

			<note place="footnote">11 Paragraphs of three or fewer sentences were combined with their neighbor if that neighbor was <lb/>deemed to follow at a &quot;major&quot; boundary, as in paragraphs 2 and 3 of the Stargazers text. <lb/></note>

			<note place="footnote">12 More specifically, if the closest paragraph location (first left, then right) has not been marked as a <lb/>boundary, then mark it. Otherwise, look to the paragraph to the left. If that paragraph has not been <lb/>marked and if it is at least gap_limit = 3 token-sequences away, then mark the paragraph to the left. If <lb/>this fails, try the paragraph to the right in a similar way. If both fail, mark nothing. <lb/></note>

			<page>54 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>Figure 5 <lb/>I <lb/>== <lb/>23 <lb/>13 14 15 16 17 18 19 20 <lb/>4 5 <lb/>6 7 <lb/>8 <lb/>9 10 11 12 <lb/>I <lb/>f <lb/>1; 2; 3; ,; <lb/>3; 6; 7; ~o ~ .... <lb/>Judgments of seven readers on the Stargazer text. Internal numbers indicate location of gaps <lb/>between paragraphs; x-axis indicates token-sequence gap number, y-axis indicates judge <lb/>number, a break in a horizontal line indicates a judge-specified segment break. <lb/>0.7 <lb/>0.6 I <lb/>, <lb/>, I <lb/>4 $ <lb/>67 <lb/>8151 101 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>90 <lb/>100 <lb/>Figure 6 <lb/>Results of the block similarity algorithm on the Stargazer text with k set to 10 and the loose <lb/>boundary cutoff limit. Both the smoothed and unsmoothed plot are shown. Internal numbers <lb/>indicate paragraph numbers, x-axis indicates token-sequence gap number, y-axis indicates <lb/>similarity between blocks centered at the corresponding token-sequence gap. Vertical lines <lb/>indicate boundaries chosen by the algorithm; for example, the leftmost vertical line represents <lb/>a boundary after paragraph 3. Note how these align with the boundary gaps of Figure 5 above. <lb/>this one location (in the spirit of a Grosz and Sidner [1986] &quot;pop&quot; operation). Thus <lb/>it displays low similarity both to itself and to its neighbors. This is an example of a <lb/>breakdown caused by the assumptions about the subtopic structure. <lb/>Because of the depth score cutoff, not all valleys are chosen as boundaries. Al-<lb/>though there is a dip around paragraph gaps 5 and 6, no boundary is marked there. <lb/>From the summary of the text&apos;s contents in Section 1, we know that paragraphs 4 and <lb/>5 discuss the moon&apos;s chemical composition while 6 to 8 discuss how it got its shape; <lb/>these two subtopic discussions are more similar to one another in content than they <lb/>are to the subtopics on either side of them, thus accounting for the small change in <lb/>similarity. <lb/></body>

			<page>55 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>Table 1 <lb/>Average K, precision, and recall scores for 12 test texts. Baseline shows the scores for an <lb/>algorithm that assigns a boundary 39% of the time (the average overall), Tiling (V) indicates <lb/>the vocabulary introduction version of computing lexical scores with token-sequence size <lb/>w = 20, and Tiling (B) indicates the blocks version with token-sequence size w = 20 and block <lb/>size k = 10. Both versions&apos; results are shown at both the low cutoff (LC) and the high cutoff <lb/>(HC) for terminating boundary assignment. Judges shows the average kappa, precision, and <lb/>recall for all judges averaged over all texts. <lb/>Baseline <lb/>Tiling (V) <lb/>Tiling (B) <lb/>Judges <lb/>LC <lb/>HC <lb/>LC <lb/>HC <lb/>P <lb/>R <lb/>K <lb/>P <lb/>K <lb/>P <lb/>K <lb/>75 47 <lb/>K <lb/>P <lb/>K <lb/>50 51 23 52 78 32 58 64 46 66 <lb/>71 59 65 83 71 <lb/>Five out of seven readers indicated a break between paragraphs 18 and 19. The <lb/>algorithm registers a slight, but not significant valley at this point. Upon inspection it <lb/>turns out that paragraph 19 really is a continuation of the discussion in 18, answering <lb/>a question that is posed at the end of 18. However, paragraph 19 begins with an <lb/>introductory phrase type that strongly signals a change in subtopic: For the last two <lb/>centuries, astronomers have studied .... <lb/>The final paragraph is a summary of the entire text; the algorithm recognizes the <lb/>change in terminology from the preceding paragraphs and marks a boundary, but only <lb/>two of the readers chose to differentiate the summary; for this reason the algorithm is <lb/>judged to have made an error even though this sectioning decision is reasonable. This <lb/>illustrates the inherent fallibility of testing against reader judgments, although in part <lb/>this is because the judges were given loose constraints. <lb/>6.4 Results: Quantitative Analysis <lb/>To assess the results of the algorithm quantitatively, I follow the advice of Gale, Church, <lb/>and Yarowsky (1992a), and compare the algorithm against both upper and lower <lb/>bounds. The upper bound in this case is the reader judgment data. The lower bound <lb/>is a baseline algorithm that is a simple, reasonable approach to the problem, which <lb/>can be automated. A simple way to segment the texts is to place boundaries randomly <lb/>in the document, constraining the number of boundaries to equal that of the average <lb/>number of paragraph gaps per document assigned as boundaries by judges. In the <lb/>test data, boundaries are placed in about 39% of the paragraph gaps. A program was <lb/>written that places a boundary at each potential gap 39% of the time (using a random <lb/>number generator), and run 10,000 times for each text, and the average of the scores <lb/>of these runs was found. These scores appear in Table 1. <lb/>The algorithms are evaluated according to the proportion of &quot;true&quot; or majority <lb/>boundaries they select out of the total selected (precision) and the proportion of &quot;true&quot; <lb/>boundaries found out of the total possible (recall) (Salton 1989). Precision also implies <lb/>the number of extraneous boundaries (or false positives, or insertion errors), and recall <lb/>implies the number of missed boundaries (or false negatives, or deletion errors). <lb/>Table 1 shows that both the blocks algorithm for lexical score assignment and the <lb/>vocabulary introduction algorithm fall between the upper and lower bounds. The re-<lb/>sults are shown for making both a liberal (LC) and a conservative (HC) number of <lb/>boundary assignments (see Section 5.5). As is to be expected, when more boundaries <lb/>can be assigned, recall becomes higher at the expense of precision, and conversely, <lb/></body>

			<page>56 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>Table 2 <lb/>Precision for various parameter settings at the recall level obtained on average by the judges <lb/>for 12 texts. NP: number of paragraphs; NB: number of boundaries according to judges&apos; <lb/>consensus; JP: judges&apos; average precision; JR: judges&apos; average recall; K: kappa for the judges for <lb/>each text; Bk: precision for the blocks algorithm with block size k and w = 20; Vw: precision for <lb/>the vocabulary introduction algorithm with token sequence size w. Dashes occur in cases in <lb/>which the algorithm does not produce a recall level equivalent to that of the judges&apos; average. <lb/>NP NB <lb/>JP <lb/>JR <lb/>B9 <lb/>B10 B12 V10 V16 V20 V24 <lb/>1 <lb/>18 <lb/>8 <lb/>2 <lb/>30 <lb/>10 <lb/>3 <lb/>21 <lb/>9 <lb/>4 <lb/>41 <lb/>14 <lb/>5 <lb/>30 <lb/>9 <lb/>6 <lb/>25 <lb/>16 <lb/>7 <lb/>39 <lb/>8 <lb/>8 <lb/>28 <lb/>10 <lb/>9 <lb/>27 <lb/>11 <lb/>10 <lb/>24 <lb/>8 <lb/>11 <lb/>17 <lb/>8 <lb/>12 <lb/>21 <lb/>9 <lb/>.809 .696 <lb/>.897 .714 <lb/>.907 .778 <lb/>.892 .684 <lb/>.716 .619 <lb/>.932 .688 <lb/>.736 .732 <lb/>.793 .657 <lb/>.917 .649 <lb/>.743 .857 <lb/>.812 .768 <lb/>.839 .651 <lb/>K <lb/>B7 <lb/>.56 .580 <lb/>.74 .877 <lb/>.72 .875 <lb/>.68 .593 <lb/>.72 .480 <lb/>.52 <lb/>.75 <lb/>.63 <lb/>1.0 <lb/>.65 .682 <lb/>.67 .695 <lb/>.61 <lb/>--<lb/>.58 .673 <lb/>.580 .611 .524 <lb/>--<lb/>.781 <lb/>.875 .875 .788 <lb/>.577 .614 .790 <lb/>.687 .687 <lb/>--<lb/>1.0 <lb/>.766 .766 <lb/>.854 .781 .683 <lb/>.707 .707 <lb/>.605 .544 --<lb/>.673 .745 .745 <lb/>.480 .500 .442 .442 <lb/>.505 .617 .633 <lb/>--<lb/>.583 .500 .778 .636 <lb/>.528 .558 .633 --<lb/>.478 .649 .500 .581 <lb/>.785 .785 --<lb/>--<lb/>.422 .634 .402 .467 <lb/>.522 .464 .541 .450 <lb/>.460 .416 .704 .588 <lb/>.478 <lb/>--<lb/>.471 <lb/>.380 .458 .591 .662 <lb/>.500 .604 .539 .455 <lb/>when boundary assignment is conservative, better precision is obtained at the ex-<lb/>pense of recall. This table also shows the average K scores for the agreement between <lb/>the algorithm and the judges. The scores for the blocks version of the algorithm are <lb/>stronger than those for the vocabulary introduction version. <lb/>Table 2 shows results in more detail, varying some of the parameter settings. To <lb/>allow for a more direct comparison, the precision for each version of the algorithm <lb/>is shown at the recall level obtained by the judges, on average. This is computed as <lb/>follows for each version of the algorithm: The depth scores are examined in order of <lb/>their strength. For each depth score, if it corresponds to a true boundary, the count <lb/>of correct boundaries is incremented, otherwise the count of incorrect boundaries is <lb/>incremented. Precision and recall are computed after each correct boundary encoun-<lb/>tered. When the recall equals that of the judges&apos; average recall, the corresponding <lb/>precision of the algorithm is returned. If the recall level exceeds that of the judges&apos;, <lb/>then the value of the precision is estimated as a linear interpolation between the two <lb/>precision scores whose recall scores most closely surround that of the judges&apos; average <lb/>recall. (This assumption of a linear interpolation is justified because in most cases, <lb/>although not all, precision changes monotonically.) In some cases the algorithm does <lb/>not produce a recall level as high as that found by the judges, since paragraphs with <lb/>a nonpositive depth score are not eligible for boundary assignment, and these cases <lb/>are marked with a dash. Note that this evaluation does away with the need for LC <lb/>and HC cutoff levels. <lb/>From Table 2 we can see that varying the parameter settings improves the scores for <lb/>some texts while detracting from others. We can also see that the blocks algorithm for <lb/>lexical score determination produces stronger results in most cases than the vocabulary <lb/>introduction method, although the latter seems to do better on the cases where the <lb/>blocks algorithm finds few boundaries (e.g., texts 6, 7, and 11). In almost all cases the <lb/>algorithms are not as accurate as the judges, but the scores for the blocks version of <lb/>the algorithm are very strong in many cases. <lb/>In looking at the results in more detail, one might wonder why the algorithm <lb/>performs better on some texts than on others. Text 7, for example, scores especially <lb/></body>

			<page>57 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>poorly. This may be caused by the fact that this text has a markedly different style <lb/>from the others. It is a chatty article (about how to survive office politics), and con-<lb/>sists of a series of anecdotes about particular individuals. The article is interspersed <lb/>throughout with spoken quotations, and these tend to throw the algorithm off because <lb/>spoken statements usually contain different vocabulary than the surrounding prose. <lb/>This phenomenon occurs in some of the other texts as well, but to a much lesser <lb/>extent. It suggests a need for recognizing and accommodating very short digressions <lb/>more effectively. Another interesting property of this text is that most of the subtopic <lb/>switches occur when switching from one anecdote to another, and by inspection it <lb/>appears that the best cues for these switches are pronouns that appear on the stop list <lb/>and are discarded (for example, the anecdotes alternate between men and women&apos;s <lb/>experiences, and correspondingly alternate between using she and her and using he <lb/>and him). However, in most cases, use of the stop list improves results. <lb/>It should also be noted that the texts used in this study were not chosen to have <lb/>well-defined boundaries, and so pose a difficult test for the algorithm. Perhaps some <lb/>tests against texts with more obvious subtopic boundaries (for which the kappa coef-<lb/>ficient for interjudge agreement is larger) would be illuminating. <lb/>6.5 Detecting Breaks between Consecutive Documents <lb/>One way to evaluate the algorithm is in terms of how well it distinguishes entire <lb/>articles from one another when they are concatenated into one file. Nomoto and Nitta <lb/>(1994) implement the tf.idf version of TextTiling from Hearst (1993) and Hearst and <lb/>Plaunt (1993) and evaluate it this way on Japanese newswire text. 13 Also, as discussed <lb/>in Section 4, Reynar (1994) uses this form of evaluation on a greedy version of the <lb/>blocks algorithm. <lb/>This task violates a major assumption of the TextTiling algorithm. TextTiling as-<lb/>sumes that the similarity comparisons are done within the vocabulary patterns of <lb/>one text, and so a relatively large shift in vocabulary indicates a change in subtopic. <lb/>Because this evaluation method assumes that article boundary changes are more im-<lb/>portant than subtopic boundary changes, it penalizes the algorithm for marking very <lb/>strong subtopic changes that occur within a very cohesive document before relatively <lb/>weaker changes in vocabulary between similar articles. For example, for hypothetical <lb/>articles dl, d2, and d3, assume dl has very strong internal coherence indicators, d2 has <lb/>relatively weak ones, and d3 is in the midrange. The interidr subtopic transition scores <lb/>for dl can swamp out the score for the transition between d2 and d3. <lb/>Nevertheless, because others have used this evaluation method, one such evalu-<lb/>ation is shown here as well. The evaluation set consisted of 44 articles from the Wall <lb/>Street Journal from 1989. Consecutive articles were used, except any article fewer than <lb/>10 sentences was removed. The data consisted of 691 paragraphs, most of which con-<lb/>tained between 1 and 3 sentences, some of which were very short, e.g., article bylines <lb/>(thus making exact assignment of boundary locations more difficult). The text was not <lb/>&quot;clean&quot;: several articles consisted of a sequence of stories, several had tabular data, <lb/>and one article was just a listing of interest rates. <lb/>The blocks version of TextTiling was run over this data using the default param-<lb/>eter settings. The depth scores were sorted and the number of assignments to article <lb/>boundaries that were within three sentences of the correct location were recorded at <lb/>several cutoff levels and are shown in Table 3. B corresponds to the number of bound-<lb/></body>

			<note place="footnote">13 Instead of using fixed-sized blocks, Nomoto and Nitta (1994) take advantage of the fact that Japanese <lb/>provides discourse markers indicating multi-sentence units that participate in a topic/comment <lb/>relationship, and find these motivated units can work slightly better. <lb/></note>

			<page>58 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<body>Table 3 <lb/>Performance for blocks algorithm <lb/>with default settings distinguishing <lb/>between article boundaries in <lb/>newspaper text consisting of 44 <lb/>articles. B: number of boundaries <lb/>chosen; C: number of correct <lb/>boundaries; P: precision; R: recall. <lb/>B <lb/>C <lb/>P <lb/>R <lb/>10 <lb/>8 <lb/>.80 <lb/>.19 <lb/>20 <lb/>16 <lb/>.80 <lb/>.37 <lb/>30 <lb/>22 <lb/>.73 <lb/>.51 <lb/>40 <lb/>27 <lb/>.68 <lb/>.63 <lb/>43* <lb/>29 <lb/>.67 <lb/>.67 <lb/>50 <lb/>31 <lb/>.62 <lb/>.72 <lb/>60 <lb/>36 <lb/>.60 <lb/>.83 <lb/>70 <lb/>41 <lb/>.59 <lb/>.95 <lb/>aries assigned, in sorted order (i.e., the first row shows the precision and recall after <lb/>the first 10 boundaries are assigned), C corresponds to the number of correctly placed <lb/>boundaries, P the precision, R the recall, and the asterisk shows the precision/recall <lb/>break-even point. <lb/>The higher-scoring boundaries are almost always exact hits, but those farther down <lb/>are more likely to be off by one to three sentences. Only one transition is missed <lb/>entirely, and it occurs after a sequence of five isolated sentences and a byline (a weak <lb/>boundary is marked preceding these isolated sentences). The high-scoring boundaries <lb/>that do not correspond to shifts between articles almost always correspond to strong <lb/>subtopic shifts. One exception occurs in the article consisting only of interest rate <lb/>listings. Another occurs in an article associating numerical information with names. <lb/>Overall the scores are much stronger than those reported in Reynar (1994), and are <lb/>comparable to those of Nomoto and Nitta (1994) whose best precision/recall trade-off <lb/>on a collection of approximately 80 articles is approximately .50 precision and .81 recall. <lb/>However, all three studies are done on different test collections and so comparisons <lb/>are at best suggestive. <lb/>7. Summary and Future Work <lb/>This article has described an algorithm that uses changes in patterns of lexical repeti-<lb/>tion as the cue for the segmentation of expository texts into multi-paragraph subtopic <lb/>structure. It has also advocated the investigation and use of the multi-paragraph dis-<lb/>course unit, something that had not been explored .in the computational literature <lb/>until this work was introduced. The algorithms described here are fully implemented, <lb/>and use term repetition alone, without requiring thesaural relations, knowledge bases, <lb/>or inference mechanisms. Evaluation reveals acceptable performance when compared <lb/>against human judgments of segmentation, although there is room for improvement. <lb/>TextTiles have already been integrated into a user interface in an information re-<lb/>trieval system (Hearst 1995) and have been used successfully for segmenting Arabic <lb/>newspaper texts, which have no paragraph breaks, for information retrieval (Has-<lb/>nah 1996). With the increase in importance of multimedia information, especially in <lb/>the context of Digital Library projects, the need for segmentation and summarization <lb/>of alternative media types is becoming increasingly important. For example, the al-<lb/></body>

			<page>59 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<body>gorithms described here should prove useful for topic-based segmentation of video <lb/>transcripts (Christel et al. 1995). In a line of work we call Mixed-Media access (Chen <lb/>et al. 1994), textual subtopic structure is being integrated with other media types, such <lb/>as images and speech. <lb/>TextTiling has been used in innovative ways by other researchers. Karlgren (1996), <lb/>in a study of the effects of stylistic variation in texts on information retrieval results, <lb/>uses TextTiling as one of several ways of characterizing newspaper texts. Overall, he <lb/>finds that relevant documents tend to be more complex than nonrelevant ones in terms <lb/>of length, sentence structure, and other metrics. When examining documents of all <lb/>lengths, he finds that relevant documents tend to have more TextTiles than nonrelevant <lb/>ones (95% significant by a Mann Whitney test). As another example of an innovative <lb/>application, van der Eijk (1994) suggests using TextTiles to align parallel multilingual <lb/>text corpora according to the overlap in their subtopic structure for English, German, <lb/>and French text. This work, along with that of Nomoto and Nitta (1994), on Japanese, <lb/>and Hasnah (1996), on Arabic, also provides evidence that TextTiling is applicable to <lb/>a wide range of natural languages. <lb/>There are several ways that the algorithms could be modified to attempt to im-<lb/>prove the results. One way is to use thesaural relations in addition to term repetition to <lb/>make better estimates about the cohesiveness of the discussion. Earlier work (Hearst <lb/>1993) incorporated thesaural information into the &apos; algorithms, but later experiments <lb/>found that this information degrades the performance. This could very well be due to <lb/>problems with the thesaurus and assignment algorithm used. A simpler algorithm that <lb/>just posits relations among terms that are a small distance apart according to Word-<lb/>Net (Miller et al. 1990), modeled after Morris and Hirst&apos;s heuristics, might work better. <lb/>Therefore, the issue should not be considered closed, but rather as an area for future <lb/>exploration, with this work as a baseline for comparison. The approach to similarity <lb/>comparison suggested by Kozima (1993), while very expensive to compute, might also <lb/>prove able to improve results. Other ways of computing semantic similarity, such as <lb/>those of Sch~itze (1993) or Resnik (1995), may also prove useful. As a related point, ex-<lb/>perimentation should be done with variations in tokenization strategies, and it may be <lb/>especially interesting to incorporate phrase or bigram information into the similarity <lb/>computation. <lb/>The methods for computing lexical score also have the potential to be improved. <lb/>Some possibilities are weighting terms according to their prior probabilities, weight-<lb/>ing terms according to the distance from the location under scrutiny according to a <lb/>Gaussian distribution, or treating the plot as a probabilistic time series and detecting <lb/>the boundaries based on the likelihood of a transition from nontopic to topic. Another <lb/>alternative is to devise a good normalization strategy that would allow for meaningful <lb/>comparisons of &quot;real&quot; paragraphs, rather than regular-sized windows of text. <lb/>The question arises as to how to extend the algorithm to capture hierarchical struc-<lb/>ture. One solution is to use the coarse subtopic structure to guide the more fine-grained <lb/>methods. Another is to make several passes through the text, using the results of one <lb/>round as the input, in terms of which blocks of text are compared, in the next round. <lb/>Finally, it may prove fruitful to use localized discourse cue information or other <lb/>specialized processing around potential boundary locations to help better determine <lb/>exactly where segmentation should take place. The use of discourse cues for detection <lb/>of segment boundaries and other discourse purposes has been extensively researched, <lb/>although predominantly on spoken text (see Hirschberg and Litman [1993] for a sum-<lb/>mary of six research groups&apos; treatments of 64 cue words). It is possible that incorpo-<lb/>ration of such information may improve the cases where the algorithm is off by one <lb/>paragraph. <lb/></body>

			<page>60 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<div type="acknowledgement">Acknowledgments <lb/>This article was enormously improved as a <lb/>result of the careful comments of four <lb/>anonymous reviewers, the editors of this <lb/>special issue, and Christine Nakatani and <lb/>Andreas StOlcke. Earlier writeups of this <lb/>work benefited from the comments of Jan <lb/>Pedersen, Per-Kristian Halvorsen, Ken <lb/>Church, Bill Gale, David Yarowsky, Graeme <lb/>Hirst, Jeff Siskind, Michael Braverman, <lb/>Narciso Jaramillo, Dan Jurafsky, Mike Schiff, <lb/>Dekai Wu, Penni Sibun, John Maxwell, <lb/>Hinrich Sch~itze, and Christine Nakatani. I <lb/>would like to thank Anne Fontaine for her <lb/>interest and help in the early stages of this <lb/>work, and Robert Wilensky for supporting <lb/>this line of research as my thesis advisor. <lb/>This work was sponsored in part by the <lb/>Advanced Research Projects Agency under <lb/>Grant No. MDA972-92-J-1029 with the <lb/>Corporation for National Research <lb/>Initiatives (CNRI), the University of <lb/>California and Digital Equipment <lb/>Corporation under Digital&apos;s flagship <lb/>research project Sequoia 2000: Large <lb/>Capacity Object Servers to Support Global <lb/>Change Research, and by the Xerox Palo <lb/>Alto Research Center. <lb/></div>

			<listBibl>References <lb/>Brown, Gillian and George Yule. 1983. <lb/>Discourse Analysis. Cambridge Textbooks <lb/>in Linguistics Series. Cambridge <lb/>University Press. <lb/>Callan, James P. 1994. Passage-level <lb/>evidence in document retrieval. In <lb/>Proceedings of the 17th Annual International <lb/>ACM/SIGIR Conference, pages 302-310, <lb/>Dublin, Ireland. <lb/>Carletta, Jean. 1996. Assessing agreement on <lb/>classification tasks: The kappa statistic. <lb/>Computational Linguistics, 22(2):249-254. <lb/>Chafe, Wallace L. 1979. The flow of thought <lb/>and the flow of language. In Talmy Giv6n, <lb/>editor, Syntax and Semantics: Discourse and <lb/>Syntax, volume 12. Academic Press, New <lb/>York, pages 159-182. <lb/>Chen, Francine, Marti A. Hearst, Julian <lb/>Kupiec, Jan O. Pedersen, and Lynn <lb/>Wilcox. 1994. Metadata in mixed-media <lb/>access. SIGMOD Record, 23(4):64-71. <lb/>Chen, Francine R. and Margaret Withgott. <lb/>1992. The use of emphasis to <lb/>automatically summarize a spoken <lb/>discourse. In ICASSP-92:1992 IEEE <lb/>International Conference on Acoustics, Speech <lb/>and Signal Processing, volume 1, pages <lb/>229-232. <lb/>Christel, M., T. Kanade, M. Mauldin, <lb/>R. Reddy, M. Sirbu, S. Stevens, and <lb/>H. Wactlar. 1995. Informedia digital video <lb/>library. Communications of the ACM, <lb/>38(4):57-58, April. <lb/>Cochran, W, G. 1950. The comparison of <lb/>percentages in matched samples. <lb/>Biometrika, 37:256-266. <lb/>Condon, Sherri L. and Claude G. Cech. <lb/>1995. Problems for reliable discourse <lb/>coding systems. In Johanna Moore and <lb/>Marilyn Walker, editors, Empirical Methods <lb/>in Discourse: Interpretation &amp; Generation, <lb/>AAAI Technical Report SS-95-06, Menlo <lb/>Park, CA. AAA! Press. <lb/>Gale, William A., Kenneth W. Church, and <lb/>David Yarowsky. 1992a. Estimating upper <lb/>and lower bounds on the performance of <lb/>word-sense disambiguation programs. In <lb/>Proceedings of the 30th Meeting, pages <lb/>249-256. Association for Computational <lb/>Linguistics. <lb/>Gale, William A., Kenneth W. Church, and <lb/>David Yarowsky. 1992b. One sense per <lb/>discourse. In Proceedings of the DARPA <lb/>Speech and Natural Language Workshop. <lb/>Girill, T. R. 1991. Information chunking as <lb/>an interface design issue for full-text <lb/>databases. In Martin Dillon, editor, <lb/>Interfaces for Information Retrieval and <lb/>Online Systems. Greenwood Press, New <lb/>York, NY, pages 149-158. <lb/>Grimes, J. 1975. The Thread of Discourse. <lb/>Mouton, The Hague. <lb/>Grosz, Barbara J. and Candace L. Sidner. <lb/>1986. Attention, intention, and the <lb/>structure of discourse. Computational <lb/>Linguistics, 12(3):172-204. <lb/>Halliday, M. A. K. and R. Hasan. 1976. <lb/>Cohesion in English. Longman, London. <lb/>Harman, Donna. 1991. How effective is <lb/>suffixing? Journal of the American Society for <lb/>Information Science (JASIS), 42(1):7-15. <lb/>Harman, Donna. 1993. Overview of the first <lb/>Text REtrieval Conference. In Proceedings <lb/>of the 16th Annual International ACM/SIG1R <lb/>Conference, pages 36-48, Pittsburgh, PA. <lb/>Hasnah, Ahmad. 1996. Full Text Processing <lb/>and Retrieval: Weight Ranking, Text <lb/>Structuring, and Passage Retrieval for Arabic <lb/>Documents. Ph.D. thesis, Illinois Institute <lb/>of Technology. <lb/>Hearst, Marti A. 1993. TextTiling: A <lb/>quantitative approach to discourse <lb/>segmentation. Technical Report Sequoia <lb/>93/24, Computer Science Division, <lb/>University of California, Berkeley. <lb/>Hearst, Marti A. 1994a. Context and Structure <lb/>in Automated Full-Text Information Access. <lb/>Ph.D. thesis, University of California at <lb/></listBibl>

			<page>61 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<listBibl>Berkeley. (Computer Science Division <lb/>Technical Report UCB/CSD-94/836). <lb/>Hearst, Marti A. 1994b. Multi-paragraph <lb/>segmentation of expository text. In <lb/>Proceedings of the 32nd Meeting, pages 9-16, <lb/>Las Cruces, NM, June. Association for <lb/>Computational Linguistics. <lb/>Hearst, Marti A. 1995. TileBars: <lb/>Visualization of term distribution <lb/>information in full text information <lb/>access. In Proceedings of the ACM SIGCHI <lb/>Conference on Human Factors in Computing <lb/>Systems, Denver, CO, May. <lb/>Hearst, Marti A. 1996. Improving full-text <lb/>precision using simple query constraints. <lb/>In Proceedings of the Fifth Annual Symposium <lb/>on Document Analysis and Information <lb/>Retrieval (SDAIR), Las Vegas, NV. <lb/>Hearst, Marti, Jan Pedersen, Peter Pirolli, <lb/>Hinrich Sch(ietze, Gregory Grefenstette, <lb/>and David Hull. 1996. Four TREC-4 <lb/>Tracks: The Xerox site report. In Donna <lb/>Harman, editor, Proceedings of the Fourth <lb/>Text Retrieval Conference TREC-4. National <lb/>Institute of Standards and Technology <lb/>Special Publication. (To appear). <lb/>Hearst, Marti A. and Christian Plaunt. 1993. <lb/>Subtopic structuring for full-length <lb/>document access. In Proceedings of the 16th <lb/>Annual International ACM/SIGIR <lb/>Conference, pages 59-68, Pittsburgh, PA. <lb/>Hersh, William R., Diane L. Elliot, David H. <lb/>Hickam, Stephanie L. Wolf, and Anna <lb/>Molnar. 1995. Towards new measures of <lb/>information retrieval evaluation. In <lb/>Proceedings of the 18th Annual International <lb/>ACM/SIGIR Conference, pages 164-170, <lb/>Seattle, WA. <lb/>Hinds, John. 1979. Organizational patterns <lb/>in discourse. In Talmy Giv6n, editor, <lb/>Syntax and Semantics: Discourse and Syntax, <lb/>volume 12. Academic Press, New York, <lb/>pages 135-158. <lb/>Hirschberg, Julia and Diane Litman. 1993. <lb/>Empirical studies on the disambiguation <lb/>of cue phrases. Computational Linguistics, <lb/>19(3):501-530. <lb/>Hirschberg, Julia and Christine H. Nakatani. <lb/>1996. A prosodic analysis of discourse <lb/>segments in direction-giving monologues. <lb/>In Proceedings of the 34th Annual Meeting, <lb/>pages 286-293, Santa Cruz, CA. <lb/>Association for Computational <lb/>Linguistics. <lb/>Hull, David and Gregory Grefenstette. 1995. <lb/>Stemming algorithms--A case study for <lb/>detailed evaluation. Journal of the American <lb/>Society for Information Science (JASIS), 46(9). <lb/>Hwang, Chung Hee and Lenhart K. <lb/>Schubert. 1992. Tense trees as the &apos;fine <lb/>structure&apos; of discourse. In Proceedings of the <lb/>30th Meeting, pages 232-240. Association <lb/>for Computational Linguistics. <lb/>Isard, Amy and Jean Carletta. 1995. <lb/>Replicability of transaction and action <lb/>coding in the map task corpus. In <lb/>Johanna Moore and Marilyn Walker, <lb/>editors, Empirical Methods in Discourse: <lb/>Interpretation &amp; Generation, AAAI Technical <lb/>Report SS-95~06. AAAI Press, Menlo Park, <lb/>CA. <lb/>Karlgren, Jussi. 1996. Stylistic variation in <lb/>an information retrieval experiment. In <lb/>Proceedings of the NeMLaP-2 Conference, <lb/>Ankara, Turkey, September. <lb/>Karttunen, Lauri, Kimmo Koskenniemi, and <lb/>Ronald M. Kaplan. 1987. A compiler for <lb/>two-level phonological rules. In Mary <lb/>Dalrymple, editor, Tools for Morphological <lb/>Analysis. Center for the Study of <lb/>Language and Information, Stanford, CA. <lb/>Kozima, Hideki. 1993. Text segmentation <lb/>based on similarity between words. In <lb/>Proceedings of the 31th Annual Meeting <lb/>(Student Session), pages 286-288, <lb/>Columbus, OH. Association for <lb/>Computational Linguistics. <lb/>Kuno, Susumo. 1972. Functional sentence <lb/>perspective: A case study from Japanese <lb/>and English. Linguistic Inquiry, <lb/>3(3):269-320. <lb/>Kupiec, Julian, Jan Pedersen, and Francine <lb/>Chen. 1995. A trainable document <lb/>summarizer. In Proceedings of the 18th <lb/>Annual International ACM/SIGIR <lb/>Conference, pages 68-73, Seattle, WA. <lb/>Lakoff, George P. 1972. Structural <lb/>complexity in fairy tales. The Study of Man, <lb/>1:128-150. <lb/>Lewis, David D. and Philip J. Hayes. 1994. <lb/>Special issue on text categorization. <lb/>Transactions of Office Information Systems, <lb/>12(3). <lb/>Litman, Diane J. and Rebecca J. Passonneau. <lb/>1995. Combining multiple knowledge <lb/>sources for discourse segmentation. In <lb/>Proceedings of the 33rd Meeting, pages <lb/>108-115, June. Association for <lb/>Computational Linguistics. <lb/>Longacre, R. E. 1979. The paragraph as a <lb/>grammatical unit. In Talmy Giv6n, editor, <lb/>Syntax and Semantics: Discourse and Syntax, <lb/>volume 12. Academic Press, New York, <lb/>pages 115-134. <lb/>Mann, William C. and Sandra A. <lb/>Thompson. 1987. Rhetorical structure <lb/>theory: A theory of text organization. <lb/>Technical Report ISI/RS 87-190, ISI. <lb/>Marchionini, Gary, Peter Liebscher, and Xia <lb/>Lin. 1991. Authoring hyperdocuments: <lb/>Designing for interaction. In Martin <lb/>Dillon, editor, Interfaces for Information <lb/></listBibl>

			<page>62 <lb/></page>

			<note place="headnote">Hearst <lb/>TextTiling <lb/></note>

			<listBibl>Retrieval and Online Systems. Greenwood <lb/>Press, New York, NY, pages 119-131. <lb/>Miller, George A., Richard Beckwith, <lb/>Christiane Fellbaum, Derek Gross, and <lb/>Katherine J. Miller. 1990. Introduction to <lb/>WordNet: An on-line lexical database. <lb/>Journal of Lexicography, 3(4):235-244. <lb/>Mittendorf, Elke and Peter Scha&apos;uble. 1994. <lb/>Document and passage retrieval based on <lb/>Hidden Markov Models. In Proceedings of <lb/>the 17th Annual International ACM/SIGIR <lb/>Conference, pages 318-327, Dublin, Ireland. <lb/>Moffat, Alistair, Ron Sacks-Davis, Ross <lb/>Wilkinson, and Justin Zobel. 1994. <lb/>Retrieval of partial documents. In Donna <lb/>Harman, editor, Proceedings of the Second <lb/>Text Retrieval Conference TREC-2. National <lb/>Institute of Standards and Technology <lb/>Special Publication 500-215, pages <lb/>181-190. <lb/>Mooney, David J., M. Sandra Carberry, and <lb/>Kathleen E McCoy. 1990. The generation <lb/>of high-level structure for extended <lb/>explanations. In Proceedings of the <lb/>Thirteenth International Conference on <lb/>Computational Linguistics, volume 2, pages <lb/>276-281, Helsinki. <lb/>Moore, Johanna D. and Martha E. Pollack. <lb/>1992. A problem for RST: The need for <lb/>multi-level discourse analysis. <lb/>Computational Linguistics, 18(4):537-544. <lb/>Morris, Jane. 1988. Lexical cohesion, the <lb/>thesaurus, and the structure of text. <lb/>Technical Report CSRI-219, Computer <lb/>Systems Research Institute, University of <lb/>Toronto. <lb/>Morris, Jane and Graeme Hirst. 1991. <lb/>Lexical cohesion computed by thesaural <lb/>relations as an indicator of the structure of <lb/>text. Computational Linguistics, 17(1):21-48. <lb/>Nakatani, Christine H., Barbara J. Grosz, <lb/>David D. Ahn, and Julia Hirschberg. 1995. <lb/>Instructions for annotating discourses. <lb/>Technical Report TR-25-95, Harvard <lb/>University Center for Research in <lb/>Computing Technolog~ Cambridge, MA. <lb/>Nomoto, Tadashi and Yoshihiko Nitta. 1994. <lb/>A grammatico-statistical approach to <lb/>discourse partitioning. In Proceedings of the <lb/>Fifteenth International Conference on <lb/>Computational Linguistics (COLING), pages <lb/>1145-1150, Kyoto, Japan, August. <lb/>Paice, Chris D. 1990. Constructing literature <lb/>abstracts by computer: Techniques and <lb/>prospects. Information Processing and <lb/>Management, 26(1):171-186. <lb/>Passonneau, Rebecca J. and Diane J. Litman. <lb/>1993. Intention-based segmentation: <lb/>Human reliability and correlation with <lb/>linguistic cues. In Proceedings of the 31st <lb/>Annual Meeting, pages 148-155. <lb/>Association for Computational <lb/>Linguistics. <lb/>Phillips, Martin. 1985. Aspects of Text <lb/>Structure: An Investigation of the Lexical <lb/>Organisation of Text. North-Holland, <lb/>Amsterdam. <lb/>Resnik, Philip. 1995. Using information <lb/>content to evaluate semantic similarity in <lb/>a taxonomy. In Proceedings of the <lb/>International Joint Conference on Artificial <lb/>Intelligence (IJCAI-95), volume 1, pages <lb/>448-453, Montreal, Canada. <lb/>Reynar, Jeffrey C. 1994. An automatic <lb/>method of finding topic boundaries. In <lb/>Proceedings of the 32nd Annual Meeting <lb/>(Student Session), pages 331-333, Las <lb/>Cruces, NM. Association for <lb/>Computational Linguistics. <lb/>RosG Carolyn Penstein. 1995. Conversation <lb/>acts, interactional structure, and <lb/>conversational outcomes. In Johanna <lb/>Moore and Marilyn Walker, editors, <lb/>Empirical Methods in Discourse: <lb/>Interpretation &amp; Generation, AAAI Technical <lb/>Report SS-95-06. AAAI Press, Menlo Park, <lb/>CA. <lb/>Rumelhart, David. 1975. Notes on a schema <lb/>for stories. In Daniel G. Bobrow and <lb/>Allan Collins, editors, Representation and <lb/>Understanding. Academic Press, New <lb/>York, pages 211-236. <lb/>Salton, Gerard. 1989. Automatic Text <lb/>Processing: The Transformation, Analysis, and <lb/>Retrieval of Information by Computer. <lb/>Addison-Wesley, Reading, MA. <lb/>Salton, Gerard and James Allan. 1993. <lb/>Selective text utilization and text <lb/>traversal. In Proceedings of ACM Hypertext <lb/>&quot;93. <lb/>Salton, Gerard, James Allan, and Chris <lb/>Buckley. 1993. Approaches to passage <lb/>retrieval in full text information systems. <lb/>In Proceedings of the 16th Annual <lb/>International ACM/SIGIR Conference, pages <lb/>49-58, Pittsburgh, PA. <lb/>Salton, Gerard, Amit Singhal, Chris Buckley, <lb/>and Mandar Mitra. 1996. Automatic text <lb/>decomposition using text segmentation <lb/>and text themes. In Proceedings of <lb/>Hypertext &apos;96, Seventh ACM Conference on <lb/>Hypertext, pages 53-65, Washington, D.C. <lb/>Schiffrin, Deborah. 1987. Discourse Markers. <lb/>Cambridge University Press, Cambridge. <lb/>SchCitze, Hinrich. 1993. Word space. In <lb/>Stephen J. Hanson, Jack D. Cowan, and <lb/>C. Lee Giles, editors, Advances in Neural <lb/>Information Processing Systems 5. Morgan <lb/>Kaufmann, San Mateo, CA. <lb/>Skorochod&apos;ko, E.E 1972. Adaptive method <lb/>of automatic abstracting and indexing. In <lb/>C.V. Freiman, editor, Information Processing <lb/></listBibl>

			<page>63 <lb/></page>

			<note place="headnote">Computational Linguistics <lb/>Volume 23, Number 1 <lb/></note>

			<listBibl>71: Proceedings of the IFIP Congress 71, <lb/>pages 1179-1182. North-Holland <lb/>Publishing Company. <lb/>Stark, Heather. 1988. What do paragraph <lb/>markers do? Discourse Processes, <lb/>11(3):275-304. <lb/>Stoddard, Sally. 1991. Text and Texture: <lb/>Patterns of Cohesion. Advances in <lb/>Discourse Processes, volume XL. Ablex <lb/>Publishing Corporation, Norwood, NJ. <lb/>Tannen, Deborah. 1989. Talking Voices: <lb/>Repetition, Dialogue, and Imagery in <lb/>Conversational Discourse. Studies in <lb/>Interactional Sociolinguistics 6. <lb/>Cambridge University Press. <lb/>Tombaugh, J., A. Lickorish, and P. Wright. <lb/>1987. Multi-window displays for readers <lb/>of lengthy texts. International Journal of <lb/>Man [sic] -Machine Studies, 26:597-615. <lb/>van der Eijk, Pim. 1994. Comparative <lb/>discourse analysis of parallel texts. <lb/>Technical Report cmp-lg/9407022, Digital <lb/>Equipment Corporation. <lb/>van Dijk, Teun A. 1980. Macrostructures. <lb/>Lawrence Erlbaum Associates, Hillsdale, <lb/>N.J. <lb/>van Dijk, Teun A. 1981. Studies in the <lb/>Pragmatics of Discourse. Mouton, The <lb/>Hague. <lb/>Walker, Marilyn A. 1992. Redundancy in <lb/>collaborative dialogue. In Proceedings of the <lb/>Fourteenth International Conference on <lb/>Computational Linguistics (COLING), pages <lb/>345-351, Nantes, France, July. <lb/>Walker, Marilyn and Steve Whittaker. 1990. <lb/>Mixed initiative dialogue: An <lb/>investigation into discourse segmentation. <lb/>In Proceedings of the 28th Annual Meeting, <lb/>pages 70-78. Association for <lb/>Computational Linguistics. <lb/>Webber, Bonnie Lynn. 1987. The <lb/>interpretation of tense in discourse. In <lb/>Proceedings of the 25th Annual Meeting, <lb/>pages 147-154, Stanford, CA. Association <lb/>for Computational Linguistics. <lb/>Webber, Bonnie Lynn. 1988. Discourse <lb/>deixis: Reference to discourse segments. <lb/>In Proceedings of the 26th Annual Meeting, <lb/>pages 113-122, Buffalo, NY. Association <lb/>for Computational Linguistics. <lb/>Yarowsky, David. 1992. Word sense <lb/>disambiguation using statistical models of <lb/>Roget&apos;s categories trained on large <lb/>corpora. In Proceedings of the Fourteenth <lb/>International Conference on Computational <lb/>Linguistics, pages 454-460, Nantes, France, <lb/>July. <lb/>Youmans, Gilbert. 1991. A new tool for <lb/>discourse analysis: The <lb/>vocabulary-management profile. <lb/>Language, 67(4):763-789. <lb/></listBibl>

			<page>64 </page>


	</text>
</tei>
