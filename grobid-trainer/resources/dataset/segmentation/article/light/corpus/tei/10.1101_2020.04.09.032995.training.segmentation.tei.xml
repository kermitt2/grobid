<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Acoustic regularities in infant-directed vocalizations across cultures <lb/>Cody J. Moser* 1 , Harry Lee-Rubin 1 , Constance M. Bainbridge 1 , S. Atwood 1,2 , Jan Simson 1 , Dean Knox 3 , Luke <lb/>Glowacki 4 , Andrzej Galbarczyk 5 , Grazyna Jasienska 5 , Cody T. Ross 6 , Mary Beth Neff 7 , Alia Martin 7 , Laura <lb/>K. Cirelli 8,9 , Sandra E. Trehub 9 , Jinqi Song 10 , Minju Kim 11 , Adena Schachner 11 , Tom A. Vardy 12 , Quentin <lb/>D. Atkinson 12,13 , Jan Antfolk 14 , Purnima Madhivanan 15,16,17,18 , Anand Siddaiah 19,20 , Caitlyn D. Placek 21 , Gul <lb/>Deniz Salali 22 , Sarai Keestra 22 , Manvir Singh 1,23 , Scott A. Collins 24 , John Q. Patton 25 , Camila Scaff 26 , Jonathan <lb/>Stieglitz 27,28 , Cristina Moya 29 , Rohan R. Sagar 30 , Brian M. Wood 31 , Max M. Krasnow 1 , and Samuel A. Mehr* 1,7,32 <lb/>1 Department of Psychology, Harvard University, Cambridge, MA 02138, USA <lb/>2 Department of Psychology, University of Washington, Seattle, WA 98105, USA <lb/>3 Department of Politics, Princeton University, Princeton, NJ 08544, USA <lb/>4 Department of Anthropology, Pennsylvania State University, University Park, PA 16802, USA <lb/>5 Department of Environmental Health, Faculty of Health Sciences, Jagiellonian University Medical College, 31-531 Krakow, Poland <lb/>6 Department of Human Behavior, Ecology and Culture, Max Planck Institute for Evolutionary Anthropology, 04103 Leipzig, Germany <lb/>7 School of Psychology, Victoria University of Wellington, Wellington 6012, New Zealand <lb/>8 Department of Psychology, University of Toronto Scarborough, Toronto, Ontario M1C 1A4, Canada <lb/>9 Department of Psychology, University of Toronto Mississauga, Mississauga, Ontario L5L 1C6, Canada <lb/>10 Department of Mathematics, Univesity of California Los Angeles, Los Angeles, CA 90095, USA <lb/>11 Department of Psychology, University of California, San Diego, La Jolla, CA 92093-0109, USA <lb/>12 School of Psychology, University of Auckland, Auckland 1010, New Zealand <lb/>13 Department of Linguistic and Cultural Evolution, Max Planck Institute for the Science of Human History, D-07745 Jena, Germany <lb/>14 Department of Psychology, Åbo Akademi, 20500 Turku, Finland <lb/>15 Department of Health Promotion Sciences, Mel &amp; Enid Zuckerman College of Public Health, University of Arizona, Tucson, AZ <lb/>85724, USA <lb/>16 Division of Infectious Diseases, College of Medicine, University of Arizona, Tucson, AZ 85724, USA <lb/>17 Department of Family &amp; Community Medicine, College of Medicine, University of Arizona, Tucson, AZ 85724, USA <lb/>18 Public Health Research Institute of India, Yadavgiri, Mysore 560020, India <lb/>19 Department Of Epidemiology, Stempel School Of Public Health, Florida International University, Miami, FL 33157, USA <lb/>20 Public Health Research Institute of India, Mysuru 570020, India <lb/>21 Department of Anthropology, Ball State University, Muncie, IN 47306, USA <lb/>22 Department of Anthropology, University College London, WC1H 0BW London, UK <lb/>23 Department of Human Evolutionary Biology, Harvard University, Cambridge, MA 02138, USA <lb/>24 School of Human Evolution and Social Change, Arizona State University, Tempe, AZ 85281, USA <lb/>25 Division of Anthropology, California State University, Fullerton, CA 92831, USA <lb/>26 Institute of Evolutionary Medicine, University of Zurich, 8006 Zürich, Switzerland <lb/>27 Université Toulouse 1 Capitole, 31080 Toulouse Cedex 6, France <lb/>28 Institute for Advanced Study in Toulouse, 31080 Toulouse Cedex 6, France <lb/>29 Department of Anthropology, University of California, Davis, Davis, CA 95616, USA <lb/>30 Future Generations University, Circle Ville, WV 26807, USA <lb/>31 Department of Anthropology, University of California, Los Angeles, Los Angeles, CA 90095, USA <lb/>32 Data Science Initiative, Harvard University, Cambridge, MA 02138, USA <lb/>*Corresponding author. Emails: cmoser@g.harvard.edu (C.J.M.); sam@wjh.harvard.edu (S.A.M.) <lb/>Abstract <lb/>Humans often produce vocalizations for infants that differ from vocalizations for adults. Is this property common <lb/>across societies? The forms of infant-directed vocalizations may be shaped by their function in parent-infant com-<lb/>munication. If so, infant-directed song and speech should be differentiable from adult-directed song and speech on <lb/>the basis of their acoustic features, and this property should be relatively invariant across cultures. To test this <lb/>hypothesis, we built a corpus of 1,614 recordings of infant-and adult-directed singing and speech produced by 411 <lb/>people living in 21 urban, rural, and small-scale societies. We studied the corpus in a massive online experiment <lb/>and in a series of acoustic analyses. Naïve listeners (N = 13,218) reliably identified infant-directed vocalizations as <lb/>infant-directed, and adult-directed speech (but not songs) as adult-directed, at rates far higher than chance. Ratings <lb/>of infant-directed song were the most accurate and the most consistent across all societies; infant-directed speech was <lb/>accurately identified on average, but inconsistently across societies. To determine the mechanisms underlying these <lb/>results, we extracted many acoustic features from each recording and identified those that most reliably characterize <lb/>infant-directed song and speech across cultures, via preregistered exploratory-confirmatory analyses and machine <lb/>classification. The features distinguishing infant-and adult-directed song and speech concerned pitch, rhythmic, <lb/>phonetic, and timbral attributes; a hypothesis-free classifier with cross-validation across societies reliably identified <lb/>all vocalization types, with highest accuracy for infant-directed song. Last, we isolated 12 acoustic features that <lb/>were predictive of perceived infant-directedness; of these, two pitch attributes (median F0 and its variability) were by <lb/>far the most explanatory. These findings demonstrate cross-cultural regularities in infant-directed vocalizations that <lb/>are suggestive of universality; moreover, infant-directed song appears to be more cross-culturally stereotyped than <lb/>infant-directed speech, informing hypotheses of the functions and evolution of both. <lb/>Keywords: vocalization, human infants, human parents, music, speech, form and function, cross-cultural <lb/></front>

			<page>1 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>1 Background <lb/>The forms of many animal signals are shaped by their functions, a link arising from production-and reception-<lb/>related rules that help to maintain reliable signal detection within and across species 1-6 . This is especially <lb/>true of vocal signals, where form-function links have been demonstrated across many species, including <lb/>nonhuman primates 3 , meerkats 7 , grackles 8 , frogs 9 , and fish 10 . <lb/>The link between form and function in vocalizations is also evident from listeners&apos; behavior. For example, <lb/>humans 11 , red deer 12 , and canines 13 reliably detect the intentions of heterospecific signalers on the basis of <lb/>the sounds of their signals. A classic demonstration of this fact is the ability of some species to eavesdrop <lb/>on the alarm signals of other species, whether or not their own species has an extended vocal repertoire 14,15 . <lb/>In humans, an area of particular importance for effectively transmitting vocal signals is between parents <lb/>and infants. This is because human infants are especially helpless to manage their own nutrition, safety, <lb/>and comfort. Infants use a distinctive alarm signal to elicit care from those around them -they cry 16 . In <lb/>response, adults and children produce infant-directed vocalizations, which are known to differ reliably from <lb/>adult-directed vocalizations in at least some societies, in the form of speech 17,18 or song 19-21 . <lb/>Are the forms and functions of infant-directed vocalizations linked, like the vocal signals of many other <lb/>species? Fernald 22 noted that a number of features of infant-directed vocalizations observed in Western <lb/>societies follow Wiley&apos;s criteria for signal detection in biological systems 5 . Many others have proposed <lb/>ways in which infant-directed and adult-directed speech might differ; for example, when compared to adult-<lb/>directed speech, infant-directed speech may have longer voice-onset times 23 ; higher pitch 24,25 ; more formant <lb/>variability 26 ; longer and more carefully articulated vowels 27,28 , with an upwards-shifted vowel space 29 ; more <lb/>repetition, with longer pitch curves 30 ; and more temporal amplitude variability 31 . Many of these features <lb/>are predicted by functional accounts of stereotyped infant-directed speech, which propose that it facilitates <lb/>word segmentation 32 , distinction of sound categories 33 , the elicitation of infant attention 34 , or parent-infant <lb/>communication at a distance 35 . <lb/>Infants appear to be receptive to at least some of these features, across at least some languages. For <lb/>example, the ManyBabies Consortium study of 2,329 monolingual infants found reliable preferences for <lb/>North American English infant-directed speech (relative to North American English adult-directed speech), <lb/>even when, for more than half of the infants, North American English was not their native language 36 . <lb/>Infants also have expectations about the infant-directed speech they hear: they look longer at videos of <lb/>infant-directed speech being directed to an adult-like character, relative to videos of infant-directed speech <lb/>being directed to an infant-like character, across several languages 37 . <lb/>Whether or not infant-directedness is characterized by universal acoustic features is unknown, however. <lb/>Infant-directed speech has rarely been studied outside of Western, Educated, Industrialized, Rich, or <lb/>Democratic (WEIRD) societies 38 , despite a longstanding interest in cross-cultural regularities in infant <lb/>development 39,40 . No corpora have systematically measured the acoustics of infant-directed speech across a <lb/>variety of societies, and the pattern of results in smaller studies is unclear. <lb/>The prosody of infant-directed speech is similar across tonal and non-tonal languages 41,42 ; across French, <lb/>Italian, German, Japanese, and British and American English 43 ; and across Fijian, Kenyan, and North <lb/>American adults 44 . Across North American English, Swedish, and Russian, infant-directed speech includes <lb/>vowel accentuation to a more extreme extent than does adult-directed speech 28 . Adults from the Shuar, a <lb/>South American hunter-horticulturalist group, accurately distinguish infant-from adult-directed speech in <lb/>recordings of North American English mothers 17 ; they do so, in part, on the basis of pitch. This finding <lb/>echoes reports of raised pitch in Lebanese infant-directed speech 45 . In contrast, the infant-directed speech <lb/>of fathers in a small-scale Vanuatuan society is rather different in pitch and speech rate than that of North <lb/>American fathers 46 . And the timbre of infant-directed speech differs from adult-directed speech in ten <lb/>languages, though with very small samples of speakers 18 . (Note that several studies of the frequency of <lb/>occurrence of infant-directed speech have been conducted in non-WEIRD and small-scale societies 47,48 , but <lb/>these address a separate question from what acoustic features characterize infant-directed vocalizations when <lb/>they do occur). <lb/></body>

			<page>2 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>In the domain of music, Mehr and Krasnow proposed that infant-directed song emerged through arms-race co-<lb/>evolution as an honest signal of parental attention, with acoustic forms elaborated from other vocalizations, <lb/>such as non-human primate contact calls, so as to provide infants with reliable information that they were <lb/>being kept safe 49 . This idea is supported by at least three forms of evidence. First, infant-directed song <lb/>modulates infant arousal, whether the songs are familiar 50 or not 51 , and delays the onset of infant distress <lb/>longer than does infant-directed speech 52 . Second, people with genomic imprinting disorders, which are <lb/>characterized by altered parental investment behaviors, such as those related to food consumption 53,54 , also <lb/>have altered music perception ability and responses to music 55,56 . Last, consistent with classic ideas in <lb/>the psychology of music 57-59 substantial evidence demonstrates that lullabies, one typical form of infant-<lb/>directed song, are a human universal: singing is associated with infant care across the ethnographies of a <lb/>representative sample of human small-scale societies, even after correcting for reporting biases 21 , and parents <lb/>use singing to calm infants in several of the most genetically distant human societies, the Hadza, Mbuti, and <lb/>!Kung San hunter-gatherers of East, Central, and South Africa 60-62 . Other forms of infant-directed song, <lb/>like excitatory play songs and singing games for children, also appear to be widespread 21,63 , and parents <lb/>produce them often 64 . <lb/>The universality of infant-directed song is also supported by evidence showing that its acoustics differ from <lb/>those of other forms of music. For example, naïve listeners reliably identify lullabies as infant-directed in a <lb/>cross-culturally representative sample of vocal music, both when rating multiple functions (e.g., rating the <lb/>songs more highly as &quot;used to soothe a baby&quot; than &quot;used for dancing&quot; 20 ) and in a forced-choice classification <lb/>task 21 . This finding echoes earlier work, wherein adult listeners were able to distinguish lullabies from love <lb/>songs recorded in some foreign societies 19 . And machine classifiers reliably distinguish lullabies from healing, <lb/>dance, and love songs based only on pitches and rhythms of the vocalizations, as opposed to acoustic features <lb/>merely associated with the vocalization, such as the sound of an infant crying 21 . <lb/>In sum, while infant-directed song and speech seem to appear universally, the ways in which they are <lb/>acoustically distinct from other vocalizations are not fully understood, nor is it clear whether those acoustic <lb/>distinctions are themselves universal. This makes it difficult to evaluate the theories of the functions of infant-<lb/>directed vocalizations mentioned above 32-35,49,57-59 , all of which imply the presence of universal acoustic <lb/>structure in infant-directed speech or song. <lb/>To explore these questions, we built a corpus of infant-directed song, infant-directed speech, adult-directed <lb/>song, and adult-directed speech from a diverse set of 21 human societies. Each participant provided all four <lb/>recordings, enabling within-person analyses of the differences between the vocalization types. The corpus is <lb/>open-access at https://osf.io/m5yn2. Here, we report tests of the cross-cultural regularity of the acoustics <lb/>of infant-directed song and speech, studied via (1) a large-scale listener experiment, where naïve adults <lb/>recruited online from many countries were asked to discriminate between infant-directed and adult-directed <lb/>vocalizations in the corpus; and (2) a series of acoustic analyses, to determine reliably-occurring differences <lb/>in the production and perception of infant-directed vocalizations worldwide. <lb/>2 Vocalization corpus <lb/>We built a corpus of recordings of infant-directed song, infant-directed speech, adult-directed song, and <lb/>adult-directed speech. Participants (N = 411) living in 21 societies (Figure 1 and Table 1) produced each <lb/>of these vocalizations, respectively, with a median of 15 participants per society (range: 6-57). From those <lb/>participants for whom information was available, most were female (86%) and nearly all were parents and/or <lb/>grandparents (95%). Recordings were collected by principal investigators and/or staff at their field sites, <lb/>all using the same data collection protocol. They translated instructions to the native language of the <lb/>participants, following the standard research practices at each site. <lb/>For infant-directed song and infant-directed speech, participants sang or spoke to their infant as if they were <lb/>fussy, where &quot;fussy&quot; could refer to anything from frowning or mild whimpering to a full tantrum (note that <lb/>each language had its own word for &quot;fussy&quot;, suggesting that participants had an intuitive understanding of <lb/>it). For most participants (90%) an infant was physically present during the recording (the infants were 48% <lb/>female; mean age 11.4 mo; SD = 0.6 mo; range: 0.5-48). When an infant was not present, participants were <lb/></body>

			<page>3 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>Afrocolombians <lb/>Arawak <lb/>Hadza <lb/>Jenu Kurubas <lb/>Mbendjele <lb/>Mentawai Islanders <lb/>Colombian Mestizos <lb/>Wellington <lb/>Nyangatom <lb/>Enga <lb/>Quechua <lb/>Rural Polish <lb/>Sápara &amp; Achuar <lb/>Turku <lb/>Toposa <lb/>Tsimane <lb/>Beijing <lb/>Krakow <lb/>San Diego <lb/>Toronto <lb/>Tannese Vanuatuans <lb/>Figure 1. Societies from which vocalizations were recorded. Diamonds denote urban societies; circles denote <lb/>rural or small-scale societies. <lb/>asked to imagine that they were vocalizing to their own infant or grandchild, and simulated their infant-<lb/>directed vocalizations. For adult-directed song, participants sang a song that was not intended for infants; <lb/>they also stated what that song was for (e.g., &quot;a celebration song&quot;). For adult-directed speech, participants <lb/>spoke to the researcher about a topic of their choice (e.g., they described their daily routine). <lb/>In all cases, participants were free to determine the content of their vocalizations. This was intentional: <lb/>imposing a specific content category on their vocalizations (e.g., &quot;sing a lullaby&quot;) would likely alter the <lb/>acoustic features of their vocalizations, which are known to be influenced by experimental contexts 65 . <lb/>All recordings were made with Zoom H2n digital field recorders, using foam windscreens (where available). To <lb/>ensure that participants were audible along with researchers (who stated information about the participant <lb/>and environment before and after the vocalizations), recordings were made with a 360-degree dual-X/Y <lb/>microphone pattern. This produced two uncompressed stereo audio files (WAV) per participant at 44.1 kHz; <lb/>we only analyzed audio from the two-channel file on which the participant was loudest. <lb/>We manually extracted the longest continuous and uninterrupted section of audio from each of the four <lb/>samples per participant (i.e., isolating vocalizations by the participant from interruptions from other speakers, <lb/>the infant, and so on), using Adobe Audition. We then used the silence detection tool in Praat 66 , with <lb/>minimum sounding intervals at 0.1 seconds and minimum silent intervals at 0.3 seconds, to remove all <lb/>portions of the audio where the participant was not speaking (i.e., the silence between vocalization phrases). <lb/>These were manually concatenated in Python, producing denoised recordings, which were subsequently <lb/>checked manually to ensure minimal loss of content. Further details of the acoustic analyses are in the <lb/>Supplementary Information. <lb/>3 Naïve listener experiment <lb/>We used the citizen science platform https://themusiclab.org to play excerpts of each item in the corpus to <lb/>listeners who were unaware of the type of vocalization they heard and who were presumably unfamiliar with <lb/>many of the societies in which the vocalizations were recorded. This experiment is similar in style to other <lb/>studies of form and function in vocalization 11,19-21 . <lb/></body>

			<page>4 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>Region <lb/>Sub-Region <lb/>Society <lb/>Language <lb/>Language Family <lb/>Subsistence Type <lb/>N <lb/>Africa <lb/>Central Africa <lb/>Mbendjele <lb/>BaYaka <lb/>Mbendjele <lb/>Niger-Congo <lb/>Hunter-Gatherer <lb/>Eastern Africa <lb/>Hadza <lb/>Hadza <lb/>Hadza <lb/>Hunter-Gatherer <lb/>Nyangatom <lb/>Nyangatom <lb/>Nilotic <lb/>Pastoralist <lb/>Toposa <lb/>Toposa <lb/>Nilotic <lb/>Pastoralist <lb/>Asia <lb/>East Asia <lb/>Beijing <lb/>Mandarin <lb/>Sino-Tibetan <lb/>Urban <lb/>124 <lb/>South Asia <lb/>Jenu Kurubas Kannada <lb/>Dravidian <lb/>Other <lb/>Southeast Asia <lb/>Mentawai <lb/>Islanders <lb/>Mentawai <lb/>Austronesian <lb/>Horticulturalist <lb/>Europe <lb/>Eastern Europe <lb/>Krakow <lb/>Polish <lb/>Indo-European <lb/>Urban <lb/>Rural Poland <lb/>Polish <lb/>Indo-European <lb/>Intensive <lb/>Agriculturalists <lb/>Scandinavia <lb/>Turku <lb/>Finnish &amp; <lb/>Swedish <lb/>Uralic and <lb/>Indo-European <lb/>Urban <lb/>North <lb/>America <lb/>North America <lb/>San Diego <lb/>English <lb/>(USA) <lb/>Indo-European <lb/>Urban <lb/>116 <lb/>Toronto <lb/>English <lb/>(Canadian) <lb/>Indo-European <lb/>Urban <lb/>198 <lb/>Oceania <lb/>Melanesia <lb/>Tannese <lb/>Vanuatuans <lb/>Bislama <lb/>Indo-European <lb/>Creole <lb/>Horticulturalist <lb/>Enga <lb/>Enga <lb/>Trans-New <lb/>Guinea <lb/>Horticulturalist <lb/>Polynesia <lb/>Wellington <lb/>English (New <lb/>Zealand) <lb/>Indo-European <lb/>Urban <lb/>228 <lb/>South <lb/>America <lb/>Amazonia <lb/>Arawak <lb/>English <lb/>Creole <lb/>Indo-European <lb/>Other <lb/>Tsimane <lb/>Tsimane <lb/>Moseten-Tsimane <lb/>Horticulturalist <lb/>Sápara &amp; <lb/>Achuar <lb/>Quechua &amp; <lb/>Achuar <lb/>Quechuan &amp; <lb/>Jivaroan <lb/>Horticulturalist <lb/>Central Andes <lb/>Quechua <lb/>Quechua <lb/>Quechuan <lb/>Agro-Pastoralist <lb/>Northwestern <lb/>South America <lb/>Afrocolombians Spanish <lb/>Indo-European <lb/>Horticulturalist <lb/>Colombian <lb/>Mestizos <lb/>Spanish <lb/>Indo-European <lb/>Commercial <lb/>Economy <lb/>Table 1. <lb/>Societies from which recordings were gathered. N refers to the total number of recordings from each <lb/>site, not the number of participants. <lb/>3.1 Methods <lb/>124 <lb/>We analyzed all data available at the time of writing this manuscript from the &quot;Who&apos;s Listening?&quot; game at <lb/>125 <lb/>https://themusiclab.org/quizzes/ids, a jsPsych 67 experiment distributed via Pushkin 68 to both desktop and <lb/>126 <lb/>mobile web browsers. Participants (N = 13,218; gender: 4,405 female, 7,043 male, 176 other, 1,594 did not <lb/>127 <lb/>disclose; age: median 31 years, interquartile range 23-43) listened to at least 1 and at most 16 vocalizations <lb/>128 <lb/>drawn at random from the corpus, for a total of 164,759 ratings (infant-directed song: n = 47,798; infant-<lb/>129 <lb/>directed speech: n = 38,913; adult-directed song: n = 41,277; adult-directed speech: n = 37,071). This <lb/>130 <lb/>yielded over 100 ratings per vocalization (median = 117; interquartile range 107-154) and thousands of <lb/>131 <lb/>ratings for each society (median = 6,394; interquartile range: 4,664-9,569). Most participants (n = 7,241) <lb/>132 <lb/></body>

			<page>5 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>played the full game, listening to all 16 songs. Participants self-reported living in 109 different countries and <lb/>speaking 96 different native languages; roughly half the participants were native English speakers from the <lb/>United States. We excluded excerpts less than 10 seconds in duration from the online experiment, studying <lb/>1405 excerpts in total (with representation from all societies). <lb/>Participants were asked to classify each vocalization as either infant-or adult-directed (Figure S1), as quickly <lb/>as possible, either by pressing a key corresponding to a drawing of an infant or adult face (when the <lb/>participant used a desktop computer) or by tapping one of the faces (when the participant used a tablet or <lb/>smartphone). As soon as they made a choice, playback stopped. They were given corrective feedback along <lb/>with a score at the end of the experiment. Because each instance of the experiment included a new random <lb/>draw of recordings, we did not exclude participants who disclosed that they had played it more than once <lb/>(n = 279); note, however, that given a random draw of 16 vocalizations from the truncated corpus of 1405 <lb/>in each instance of the experiment, repeat plays for the 279 participants who played more than once are <lb/>expected to be rare. <lb/>We analyzed the patterns of successful identification of vocalization target across the full corpus and within <lb/>each society, using both the raw identification accuracy and d-prime scores. We also analyzed response time <lb/>from the onset of each recording, for the subset of responses that were accurate, to explore the speed with <lb/>which participants made accurate inferences about vocalization types. <lb/>3.2 Results <lb/>We computed an average score for each vocalization, by averaging across all listeners, and used them as <lb/>the raw data for the following analyses. Corpus-wide, scores were above chance level, at 65.3% correct (SD <lb/>= 14.8%, 95% CI: [63.9%, 66.8%]; t = 20.9, p &lt; .0001, one-sample t-test relative to 50.0%). Accuracy <lb/>varied substantially, however, as a function of the vocalization type (Figure 2A): infant-directed song was <lb/>identified most accurately (79.7% correct), followed by adult-directed speech (75.4%), and infant-directed <lb/>speech (68.0%); all these were well above chance (ps &lt; .0001). In contrast, adult-directed song was reliably <lb/>classified incorrectly, with only 38.4% accuracy (below chance at p &lt; .0001). Here there was also substantial <lb/>consistency across societies, with all but 2 showing an identical ordering of identification accuracy (in these <lb/>remaining 2 societies, Wellington and San Diego, infant-directed speech was the highest-accuracy vocalization <lb/>type). <lb/>To examine the degree to which these results held worldwide, we collapsed scores for the vocalizations from <lb/>each society, in isolation, and analyzed each vocalization type independently (Figure S2; n.b., this analysis <lb/>substantially reduces the sample size, as some societies had very few recordings available in the naïve listener <lb/>experiment). <lb/>For infant-directed song, the result replicated robustly across societies: infant-directed songs were identified <lb/>as infant-directed at a significantly higher rate than chance in 19 of 21 sites. In the remaining two societies, <lb/>perceived infant-directedness trended above chance (Papua New Guinea: M = .603; Quechua: M = .689) <lb/>but these sites had only 6 and 5 infant-directed songs, respectively, making it difficult to interpret their <lb/>non-significant test statistics. Similarly, adult-directed speech was reliably identified as adult-directed in 19 <lb/>of 21 sites, with trending results in the remaining two sites (Arawak: M = .552, N = 2; Sápara/Achuar: M <lb/>= .605, N = 13). <lb/>These results contrast, however, with the identification of infant-directed speech: here, accuracy replicated <lb/>in only 9 societies, fewer than half of those represented in the corpus. The societies where the naïve listeners <lb/>failed to identify infant-directed speech accurately tended to be small-scale, including the Hadza, Tsimane, <lb/>Mbendjele, Toposa, Nyangatom, and Mentawai Islanders (see Figure S2). <lb/>To ensure that the above findings were not attributable to response biases, we repeated the overall result <lb/>using a d-prime analysis, which measures accuracy after adjusting for the base rates of response, which <lb/>were skewed somewhat toward infant-directedness (approximately 60% of items were classified as infant-<lb/>directed, despite only half actually being infant-directed). This analysis confirmed the main finding reported <lb/>above (infant-directed song: d&apos; = 1.11; adult-directed speech: d&apos; = 1.30; infant-directed speech: d&apos; = 0.93; <lb/></body>

			<page>6 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>ID Song <lb/>ID Speech <lb/>AD Song <lb/>AD Speech <lb/>Perceived infant−directedness <lb/>(proportion of &quot;baby&quot; responses) <lb/>(A) <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>ID Song <lb/>ID Speech <lb/>AD Song <lb/>AD Speech <lb/>Response time (in seconds) when answering correctly <lb/>(B) <lb/>Figure 2. Results of the naïve listener experiment. (A) Listeners accurately identify infant-directed song <lb/>and infant-directed speech as directed towards infants, and adult-directed speech as directed towards adults; <lb/>however, they do not identify adult-directed song as directed toward adults. The horizontal dotted line <lb/>represents chance level of 0.50. (B) When responding correctly, listeners are fastest to identify infant-<lb/>directed song, followed by infant-directed speech, adult-directed song, and adult-directed speech. In both <lb/>panels, the points indicate averages for each recording; the gray lines connecting the points indicate the <lb/>groups of vocalizations produced by the same participant; the half-violins are kernel density estimations; <lb/>and the boxplots represent the medians, interquartile ranges, and 95% confidence intervals (indicated by the <lb/>notches). Abbreviations: infant-directed (ID); adult-directed (AD). <lb/>adult-directed song: d&apos; = -0.07; d&apos; scores greater than 0 represent significant results after adjusting for false <lb/>positives). <lb/>Given theoretically-derived predictions that specifically concern the function of infant-directed singing 49 , and <lb/>following our preregistered analysis plan (at https://osf.io/5r72u) for acoustic feature comparisons across <lb/>vocalization types, we tested for differences in perceived infant-directedness across three comparisons of the <lb/>vocalizations: (1) infant-directed vs. adult-directed vocalizations, overall; (2) infant-directed song vs. adult-<lb/>directed song; and (3) infant-directed song vs. infant-directed speech. <lb/>In all cases, we analyze within-voice differences in perceived infant-directedness (e.g., for all voices, comparing <lb/>the proportion of &quot;baby&quot; responses for infant-directed songs to infant-directed speech produced by the same <lb/>voice). This procedure ensures that participant-wise differences in voice characteristics cannot account for <lb/>differences in the perceived infant-directedness of each vocalization. <lb/>We found substantial support for all three predictions (Figure 2A). Perceived infant-directedness was higher <lb/>in infant-directed vocalizations (proportion of &quot;baby&quot; responses; M = .743, SD = .187, 95% CI [.724, .762]) <lb/>than adult-directed vocalizations, overall (M = .448, SD = .182, 95% CI [.430, .467]; t(372) = 20.8, p &lt; <lb/>.0001, d = 2.07, paired t-test); higher in infant-directed song (M = .799, SD = .152, 95% CI [.783, .815]) <lb/>than adult-directed song (M = .615, SD = .208, 95% CI [.593, .637]; t(348) = 13.4, p &lt; .0001, d = 1.29); <lb/>and higher in infant-directed song (M = .806, SD = .152, 95% CI [.789, .824]) than infant-directed speech <lb/>(M = .688, SD = .263, 95% CI [.659, .718]; t(301) = 8.92, p &lt; .0001, d = 0.83). <lb/>Response time analyses paralleled these findings (Figure 2B). When restricting the sample to correct re-<lb/>sponses, participants answered more quickly for infant-directed vocalizations (in seconds, M = 3.34, SD = <lb/>0.61, 95% CI [3.28, 3.40]) than adult-directed vocalizations (M = 3.58, SD = 0.46, 95% CI [3.53, 3.62]; <lb/></body>

			<page>7 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>t(372) = 6.27, p &lt; .0001, d = 0.54, paired t-test); more quickly for infant-directed song (M = 3.24, SD = <lb/>0.65, 95% CI [3.17, 3.31]) than adult-directed song (M = 3.54, SD = 0.59, 95% CI [3.47, 3.60]; t(348) = <lb/>6.99, p &lt; .0001, d = 0.70); and more quickly for infant-directed song (M = 3.19, SD = 0.64, 95% CI [3.12, <lb/>3.27]) than infant-directed speech (M = 3.50, SD = 0.76, 95% CI [3.41, 3.58]; t(301) = 6.89, p &lt; .0001, d = <lb/>0.70). Because web-based participants may halt their participation during a trial (producing extremely long <lb/>response times) or answer quickly at random (producing extremely short response times), in these analyses <lb/>we removed observations below the 1st and above the 99th percentiles. Also note that in these and the <lb/>previous paragraph&apos;s analyses, summary statistics vary across the comparisons, because a small number of <lb/>participants did not provide all four of the vocalization types, and because recordings with a duration of <lb/>less than 10 seconds were excluded from the online experiment. Effect sizes (ds) were computed using the <lb/>overall standard deviation of accuracy, for consistency across tests. <lb/>3.3 Interim discussion <lb/>The naïve listener experiment provides evidence that infant-directed vocalizations from around the world <lb/>are discriminable from adult-directed vocalizations. This effect was most consistent for infant-directed song, <lb/>which was reliably identified within each society represented in the corpus; while infant-directed speech was <lb/>reliably identified on average, its society-wise results were less consistent. <lb/>Why are listeners so good at identifying infant-directed song? Cross-cultural identification of infant-<lb/>directedness in music might be due to universal acoustic cues, as predicted from functional accounts of <lb/>infant-directed vocalizations. In the rest of this paper, we analyze the acoustic features that most reliably <lb/>characterize infant-directed song, using both confirmatory and hypothesis-free methods, and test the degree <lb/>to which these features explain overall ratings in the naïve listener experiment. <lb/>4 Analysis of acoustic features <lb/>We studied a broad range of acoustic features in each vocalization, using Praat 66 , MIRtoolbox 69 , discrete <lb/>Fourier transforms for rhythmic variability 70 , and normalized pairwise variability indices 71 . The acoustic <lb/>features consisted of measurements of pitch (e.g., F 0 , the fundamental frequency), timbre (e.g., roughness), <lb/>and rhythm (e.g., tempo); all summarized over time. We extracted a variety of summary variables for each <lb/>feature, producing 94 variables in total. For example, in the domain of pitch, we included 9 summaries of <lb/>the feature F 0 (mean, median, minimum, maximum, range, standard deviation, first quartile, third quartile, <lb/>and interquartile range), and similar summaries for F 1 and F 2 , change in F 0 , and so on. A codebook for all <lb/>features is in Table S1. <lb/>We ran three sets of analyses. First, we randomly selected half the recordings in the corpus for exploratory <lb/>analyses, confirming the results on the other half of the corpus, so as to reduce the risk of Type I error. Of <lb/>particular interest in these analyses were the set of confirmatory hypotheses that we preregistered, following <lb/>the exploratory analysis, based on functional theories of infant-directed vocalization 32-35,49,57-59 and general <lb/>principles of signal detection 5 . <lb/>Second, we used an hypothesis-free machine learning tool, least absolute shrinkage and selection operator <lb/>(LASSO) classification 72 . To assess how distinct each vocalization type was, in terms of its acoustic features, <lb/>we evaluated classification accuracy with a cross-validation procedure in which each society&apos;s recordings were <lb/>classified using statistical models trained on the 20 other societies. This design allows us to gauge whether <lb/>acoustic patterns are consistent cross-culturally (following prior research using a similar classification task 21 ). <lb/>The algorithm also includes a variable selection step to identify the specific acoustic features that most reliably <lb/>characterize each vocalization type across the 21 societies. <lb/>Third, we explored the degree to which the convergent results of the first two analyses -namely, the acoustic <lb/>features that most reliably characterized infant-directed song and infant-directed speech -can explain the <lb/>results of the naïve listener experiment. We regressed an infant-directedness score for each recording on <lb/>the acoustic features that predicted infant-directedness in both analyses, using a strict inclusion criterion <lb/></body>

			<page>8 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>and a conservative correction for multiple tests, to determine the core set of acoustic features characterizing <lb/>infant-directedness worldwide. <lb/>4.1 Exploratory-confirmatory analyses <lb/>In exploratory analyses, we fitted a multi-level mixed-effects model for each acoustic feature, adjusting for <lb/>subject and society and using three predictors: (1) target (infant-directed or adult-directed); (2) utterance <lb/>type (song or speech); and (3) their interaction. For each model, we tested three linear combinations, to <lb/>examine differences between (1) infant-and adult-directed vocalizations, overall; (2) infant-directed song <lb/>and adult-directed song; and (3) infant-directed song and infant-directed speech. This procedure, which <lb/>was preregistered, mirrors the pairwise comparison analyses in the naïve listener experiment. The linear <lb/>combinations were evaluated with one-tailed z-tests, using an alpha level of .05. We did not correct for <lb/>multiple tests in these analyses because the exploratory-confirmatory design restricts the number of tests to <lb/>those with a strong directional prediction. We did all this with half the corpus, weighted by participant. <lb/>In the course of the exploratory analyses, we noted a small number of extreme outliers, typically attributable <lb/>to anomalies in the recording environment (e.g., loud wind). As such, before running confirmatory analyses, <lb/>we Winsorized all features at the lowest and highest 5 percentile ranks, and also restricted the set of features <lb/>analyzed to those less sensitive to extreme observations (e.g., using the median as a measure of central <lb/>tendency rather than the mean). These data were used for all subsequent analyses. This decision had no <lb/>impact on the interpretation of results, but is preferable to trimming extreme values 73 ; an alternate method, <lb/>imputing extreme values with the mean observation for each feature, yielded comparable results. <lb/>We ran confirmatory models on the subset of acoustic features that were found to distinguish vocalization <lb/>types in exploratory findings (Table S2), using the other half of the corpus. We were particularly inter-<lb/>ested in those features for which we had a preregistered directional prediction. These included predictions <lb/>derived from Mehr and Krasnow 49 , suggesting that infant-directed song may universally have longer attack <lb/>envelopes and pitch contours than infant-directed speech, as well as slower amplitude decay, lower F 0 , clearer <lb/>signal-to-noise parameters, and greater vowel prolongation and stability; slower tempo 22 , differential rhyth-<lb/>mic variance 70,74 , less roughness 75 , and shifted vowel spaces 29,76 . The full list of theoretically-motivated <lb/>hypotheses is at the preregistration (https://osf.io/5r72u) and is summarized in Table S3. <lb/>The exploratory-confirmatory procedure yielded 46 significant differences across the three comparison types, <lb/>confirming some of the preregistered predictions, in terms of pitch, formant, timbre, and temporal features <lb/>(Figure 3 and Table S4). For example, relative to adult-directed vocalizations, infant-directed vocalizations <lb/>had a higher pitch and wider pitch variability, faster rates of pitch change and more variability in those <lb/>rates, and a wider pitch space; a faster rate of vowel space change and more variability in that space; more <lb/>intensity changes and more variability in intensity; a lower energy profile; and lower inharmonicity. We <lb/>found similar differences in the other two comparison types, including a few additional acoustic features, <lb/>such as the normalized pairwise variability index (nPVI, a measure of durational contrast) and attack slopes <lb/>(a measure of the amplitude change in the onset of acoustic events). The full results are in Table S4. <lb/>4.2 Hypothesis-free classification <lb/>To validate the results of the exploratory-confirmatory models, we used a hypothesis-free LASSO-regularized <lb/>categorical classifier 72 to identify the four different vocalization types on the basis of their acoustic features <lb/>alone. Cross-cultural accuracy was assessed using society-wise leave-one-out cross-validation, as in previous <lb/>research 21 . We then rotated the held-out society 20 more times, to analyze accuracy across all 21 societies. <lb/>The classifier used acoustic features standardized within-voices, eliminating between-voice variability in the <lb/>acoustic features. <lb/>The classifier accurately identified 70.5% of held-out recordings from unseen societies ([62.9%, 78.0%]; 95% <lb/>CIs from corrected and resampled t-tests 77 ), far above chance level of 25%. This finding justifies a strong <lb/>claim of corpus-wide consistency: to predict vocalization types in a given society, the classifier only used <lb/>information available from other societies, and did so with a high degree of accuracy (Figure 4A). <lb/></body>

			<page>9 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>−2 <lb/>0 <lb/>2 <lb/>ID Song vs. ID Speech <lb/>−2 <lb/>0 <lb/>2 <lb/>ID Song vs. AD Song <lb/>−2 <lb/>0 <lb/>2 <lb/>ID vs. AD (overall) <lb/>Vowel Space (Median) <lb/>Vowel Space (IQR) <lb/>Vowel Rate (Whole) <lb/>Vowel Rate (Median) <lb/>Vowel Rate (IQR) <lb/>Roughness (Median) <lb/>Roughness (IQR) <lb/>Pitch Space (Median) <lb/>Pitch Space (IQR) <lb/>Pitch Rate (Whole) <lb/>Pitch Rate (Median) <lb/>Pitch Rate (IQR) <lb/>Pitch (Median) <lb/>Pitch (IQR) <lb/>nPVI (per recording) <lb/>nPVI (per phrase) <lb/>Intensity Space (Median) <lb/>Intensity Space (IQR) <lb/>Intensity Rate (Whole) <lb/>Intensity Rate (Median) <lb/>Intensity Rate (IQR) <lb/>Intensity (Median) <lb/>Intensity (IQR) <lb/>Inharmonicity <lb/>First Formant (Median) <lb/>Energy Roll−off (85th %ile) <lb/>Attack Curve Slope (Median) <lb/>−2 0 2 <lb/>ID vs. AD (overall) <lb/>z-score <lb/>ID vs. AD (overall) <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>* <lb/>** <lb/>* <lb/>*** <lb/>*** <lb/>*** <lb/>ID Song vs. AD Song <lb/>* <lb/>* <lb/>*** <lb/>* <lb/>*** <lb/>* <lb/>* <lb/>* <lb/>ID Song vs. ID Speech <lb/>** <lb/>* <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>** <lb/>* <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>*** <lb/>Figure 3. Confirmatory results. The boxplots represent the 25 acoustic features with a significant difference <lb/>in at least one main comparison (e.g., infant-directed song vs. infant-directed speech, in the right panel), <lb/>in both the exploratory and confirmatory analyses. All variables are normalized across participants. The <lb/>boxplots represent the median and interquartile range; the whiskers indicate 1.5 × IQR; and the notches <lb/>represent the 95% confidence intervals of the medians. Faded comparisons did not reach significance in <lb/>exploratory analyses. Abbreviations: infant-directed (ID); adult-directed (AD). Significance values are com-<lb/>puted via linear combinations, following multi-level mixed-effects models. ***p &lt; .001; **p &lt; .01; *p &lt; <lb/>.05 <lb/></body>

			<page>10 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>131 <lb/>306 <lb/>238 <lb/>328 <lb/>137 <lb/>8 <lb/>63 <lb/>236 <lb/>16 <lb/>6 <lb/>10 <lb/>52 <lb/>6 <lb/>11 <lb/>17 <lb/>7 <lb/>ID Song <lb/>ID Speech <lb/>AD Song <lb/>AD Speech <lb/>ID Song <lb/>ID Speech <lb/>AD Song <lb/>AD Speech <lb/>Vocalization Type <lb/>LASSO Prediction <lb/>(A) <lb/>10% <lb/>20% <lb/>30% <lb/>40% <lb/>50% <lb/>60% <lb/>70% <lb/>80% <lb/>90% <lb/>100% <lb/>ID vs. AD <lb/>(overall) <lb/>ID Song vs. <lb/>AD Song <lb/>ID Speech vs. <lb/>AD Speech <lb/>ID Song vs. <lb/>ID Speech <lb/>Comparison <lb/>Accuracy <lb/>(B) <lb/>Figure 4. Accuracy of hypothesis-free classifiers. (A) The confusion matrix for the four-way categorical <lb/>LASSO classifier shows successful classification in all four vocalization types. When misclassifying, the model <lb/>is more likely to confuse the target (infant or adult) than the vocalization type (song or speech). (B) The <lb/>bar graph displays the accuracy of each of the pairwise classifiers; all pairwise classifications were above <lb/>chance level of 50% (denoted by the horizontal dotted line). Error bars denote 95% confidence intervals from <lb/>corrected and re-sampled t-tests. Abbreviations: infant-directed (ID); adult-directed (AD). <lb/>The confusion matrix also reveals patterns of misidentification: in the 29.5% of recordings that are misiden-<lb/>tified, the model rarely classifies songs as speech (or vice versa), but sometimes confuses the utterance target <lb/>within the correct vocalization type. For example, infant-directed songs are more than 10 times more likely <lb/>to be classified inaccurately as adult-directed songs than to be classified inaccurately as adult-directed speech <lb/>-but nevertheless, the model accurately identifies them as infant-directed songs most of the time (60.0% <lb/>relative to chance level of 25%). <lb/>To identify the acoustic features that most reliably differentiate pairs of vocalization types, we continued with <lb/>a logistic LASSO classifier to test the same three pairwise comparisons as in the exploratory-confirmatory <lb/>analyses and the analysis of the naïve listener experiment: (1) infant-directed vs. adult-directed vocalizations, <lb/>overall; (2) infant-directed song vs. adult-directed song; and (3) infant-directed song vs. infant-directed <lb/>speech. We also ran a fourth pairwise comparison, between infant-directed speech and adult-directed speech, <lb/>as an exploratory analysis. <lb/>The classifiers performed strikingly well (Figure 4B; infant-directed vs. adult-directed vocalizations, overall: <lb/>70.7% [61.6%, 79.8%]; infant-directed song vs. adult-directed song: 64.2% [55.4%, 73.0%]; infant-directed <lb/>song vs. infant-directed speech: 93.9% [89.8%, 98.0%]). Infant-directed speech was also reliably distinguished <lb/>from adult-directed speech (83.4% [74.4%, 92.3%]). <lb/>Last, we examined the acoustic features identified by the variable selection step of the LASSO procedure, <lb/>which most reliably predict vocalization type across all 21 societies. These are reported in Table 2. <lb/>There was substantial overlap between the results of the two approaches (Table 2): out of 31 features selected <lb/>by the LASSO classifier, 22 were supported by at least one exploratory-confirmatory result, and of those, 6 <lb/>were preregistered. Consistent with the exploratory-confirmatory analyses, the acoustic features that reliably <lb/>distinguished between each vocalization form concerned pitch, formant, timbre, and temporal features; in <lb/>some cases, these included additional variables, such as pulse clarity (the strength of the beats, detected <lb/>via music information retrieval) and temporal modulation (the frequency decomposition of the amplitude <lb/>envelope, or how quickly loudness changes). <lb/></body>

			<page>11 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>Feature <lb/>Statistic <lb/>ID [+] vs. AD <lb/>[−] (overall) <lb/>ID Song [+] vs. <lb/>AD Song [−] <lb/>ID Song [+] vs. <lb/>ID Speech [−] <lb/>ID Speech [+] vs. <lb/>AD Speech [−] <lb/>Attack Curve Slope <lb/>IQR <lb/>0.155 <lb/>0.182 <lb/>. <lb/>0.108 <lb/>Median pre <lb/>−0.139 <lb/>−0.373 <lb/>−0.352 <lb/>0.176 <lb/>Inharmonicity <lb/>Whole pre <lb/>−0.125 <lb/>−0.204 <lb/>−0.029 <lb/>−0.04 <lb/>Pulse Clarity <lb/>Whole pre <lb/>0.161 <lb/>0.069 <lb/>0.336 <lb/>0.19 <lb/>85th Energy Percentile <lb/>Whole pre <lb/>−0.243 <lb/>−0.216 <lb/>. <lb/>−0.152 <lb/>Roughness <lb/>IQR <lb/>−0.162 <lb/>−0.159 <lb/>−0.151 <lb/>. <lb/>Median <lb/>0.178 <lb/>. <lb/>−0.520 <lb/>0.002 <lb/>Tempo <lb/>Whole pre <lb/>. <lb/>0.047 <lb/>0.12 <lb/>−0.007 <lb/>nPVI per Phrase <lb/>Whole pre <lb/>−0.053 <lb/>−0.061 <lb/>−0.021 <lb/>. <lb/>Pitch <lb/>IQR <lb/>0.093 <lb/>−0.16 <lb/>. <lb/>0.386 <lb/>Median <lb/>0.738 <lb/>0.097 <lb/>0.259 <lb/>1.276 <lb/>Pitch Space <lb/>IQR <lb/>−0.112 <lb/>−0.105 <lb/>−0.782 <lb/>. <lb/>Median <lb/>0.108 <lb/>−0.216 <lb/>−0.909 <lb/>0.128 <lb/>Pitch Rate <lb/>IQR <lb/>0.146 <lb/>−0.052 <lb/>−0.735 <lb/>0.123 <lb/>Median <lb/>0.178 <lb/>0.306 <lb/>. <lb/>. <lb/>First Formant <lb/>IQR <lb/>0.032 <lb/>0.024 <lb/>. <lb/>. <lb/>Median <lb/>−0.115 <lb/>−0.114 <lb/>−0.369 <lb/>. <lb/>Range <lb/>−0.23 <lb/>−0.328 <lb/>−0.121 <lb/>−0.009 <lb/>Second Formant <lb/>Median <lb/>0.042 <lb/>−0.149 <lb/>0.082 <lb/>0.176 <lb/>Intensity <lb/>IQR <lb/>0.471 <lb/>0.295 <lb/>−0.225 <lb/>0.456 <lb/>Median <lb/>−0.406 <lb/>−0.511 <lb/>0.595 <lb/>. <lb/>Intensity Space <lb/>IQR <lb/>−0.72 <lb/>−0.543 <lb/>. <lb/>−0.523 <lb/>Median <lb/>−0.436 <lb/>−0.154 <lb/>−0.368 <lb/>−0.295 <lb/>Intensity Rate <lb/>IQR <lb/>0.466 <lb/>. <lb/>. <lb/>0.08 <lb/>Median <lb/>. <lb/>0.6 <lb/>. <lb/>. <lb/>Vowel Space <lb/>IQR <lb/>0.51 <lb/>0.911 <lb/>. <lb/>. <lb/>Median <lb/>0.032 <lb/>0.062 <lb/>. <lb/>. <lb/>Vowel Travel Rate <lb/>IQR <lb/>0.234 <lb/>0.567 <lb/>. <lb/>. <lb/>Median <lb/>. <lb/>−1.033 <lb/>−1.256 <lb/>0.984 <lb/>Temporal Modulation <lb/>Peak pre <lb/>0.166 <lb/>0.138 <lb/>. <lb/>. <lb/>SD pre <lb/>0.069 <lb/>0.005 <lb/>0.045 <lb/>0.03 <lb/>Table 2. <lb/>Acoustic features that reliably differentiate the four vocalization types, selected via LASSO clas-<lb/>sification with cross-validation across societies. The table reports coefficients from penalized logistic regressions <lb/>using acoustic features (standardized within-voices). Changes in the values of the coefficients produce changes in <lb/>the predicted log-odds ratio, so the values in the table can be interpreted as in a logistic regression. The fea-<lb/>tures supported by convergent evidence from the exploratory-confirmatory analyses are in bold; those that were <lb/>preregistered are marked with a superscript pre. Abbreviations: infant-directed (ID); adult-directed (AD). <lb/></body>

			<page>12 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>4.3 Convergent analysis: Predicting listener intuitions from acoustic features <lb/>Last, we examined the degree to which the naïve listener&apos;s perceptions of infant-directedness were explicable <lb/>from the primary acoustic features identified by the exploratory-confirmatory and hypothesis-free analy-<lb/>ses of the corpus. To reduce the risk of introducing false-positives in a large dataset, we only analyzed <lb/>acoustic features that had convergent evidence from at least one summary statistic in both the exploratory-<lb/>confirmatory and LASSO analyses, in at least one comparison type. In these analyses, we collapsed across <lb/>all vocalization types and attempted to predict only whether naïve listeners rated a given vocalization as <lb/>infant-or adult-directed (regardless of society or vocalization type). This yielded 21 features. To justify a <lb/>strong interpretation of potential relations between these 21 features and infant-directedness in the corpus, <lb/>we regressed each vocalization&apos;s average infant-directedness score on each of the 21 features individually, <lb/>using a strict Bonferroni-adjusted alpha level of .0024. <lb/>This procedure yielded 12 features that were significantly predictive of listeners&apos; perceptions of infant-<lb/>directedness after this selection procedure (Figure 5 and Table S5). The most reliably associated feature, by <lb/>far, was pitch: median F 0 (Figure 5A) and its variability (Figure 5B) each accounted for about 30% of the <lb/>variability in perceived infant-directedness; other features related to infant-directedness included intensity <lb/>space (Figure 5C), temporal modulation (Figure 5D), roughness (Figure 5E), and inharmonicity (Figure 5F). <lb/>Last, we entered all 12 features into a multiple linear regression. These features explained 45.0% of the <lb/>variability in perceived infant-directedness (F (12, 1081) = 73.7, p &lt; .0001). When entered into the regression <lb/>together, 5 of the 12 features had significant partial effects (median F 0 : β = 0.30; F 0 IQR: β = 0.33; median <lb/>intensity travel rate: β = −0.17; roughness IQR: β = −0.12; median F 1 : β = −0.09). Thus, while 12 <lb/>core acoustic features are reliably associated with infant-directedness across the corpus, there is nonetheless <lb/>substantial additional variability in the infant-directedness of vocalizations that is left unexplained. <lb/>5 Discussion <lb/>We provide convergent evidence for widespread regularities in the acoustic design of infant-directed vocal-<lb/>izations, in both the domains of language and music. Naïve listeners reliably identified infant-directed vocal-<lb/>izations as infant-directed, despite the fact that the vocalizations were largely of unfamiliar geographic and <lb/>linguistic origin, and more consistently in song than in speech. A series of hypothesis-and data-driven anal-<lb/>yses showed consistent acoustic distinctions between infant-directed and adult-directed vocalizations over-<lb/>all, between infant-directed and adult-directed song, and between infant-directed song and infant-directed <lb/>speech. These acoustic distinctions together explained nearly half the variability in listeners&apos; perceptions of <lb/>infant-directedness. <lb/>The most consistent ways in which infant-directed vocalizations differ from adult-directed vocalizations, <lb/>worldwide, concern pitch: nearly every comparison revealed differences in pitch, pitch space, and pitch <lb/>rate (Figure 3), and, moreover, F 0 median and interquartile range explained by far the largest proportion <lb/>of variability in listeners&apos; perceived infant-directedness (Figure 5). But other acoustic features also reliably <lb/>distinguished infant-directed vocalizations from adult-directed vocalizations, infant-directed song from adult-<lb/>directed song, and infant-directed song from infant-directed speech -albeit in subtler ways that the LASSO <lb/>classifier detected more reliably than did naïve listeners. These features included rhythmic, phonetic, and <lb/>timbral characteristics of the vocalizations, such as temporal modulation, durational contrast, roughness, <lb/>inharmonicity, and intensity space (Figure 4, Table 2, and Table S4). <lb/>Simply put: across many voices from many cultures producing many speech and song utterances, infant-<lb/>directed vocalizations tend to sound different than adult-directed vocalizations. The differences are salient <lb/>enough for naïve listeners to detect, because they are characterized by a core set of acoustic dimensions <lb/>-more consistently in infant-directed song than in infant-directed speech. Taken together, these findings <lb/>suggest a link between form and function in the design of infant-directed vocalizations. <lb/>Surprisingly, however, naïve listeners&apos; intuitions about infant-directed speech were far less consistent across <lb/>societies than their intuitions about infant-directed song. Corpus-wide, both vocalization types were iden-<lb/>13 <lb/></body>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>R 2 = 0.31 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−1 <lb/>0 <lb/>1 <lb/>Pitch (Median) <lb/>Infant−directedness <lb/>(A) <lb/>R 2 = 0.27 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−2 <lb/>−1 <lb/>0 <lb/>1 <lb/>Pitch (IQR) <lb/>(B) <lb/>R 2 = 0.12 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>Intensity Space (Median) <lb/>(C) <lb/>R 2 = 0.06 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>Temporal Modulation (SD) <lb/>Infant−directedness <lb/>(D) <lb/>R 2 = 0.06 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−0.50 <lb/>−0.25 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>Roughness (IQR) <lb/>(E) <lb/>R 2 = 0.04 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−2 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>Inharmonicity <lb/>(F) <lb/>R 2 = 0.04 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−0.50 <lb/>−0.25 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>Roughness (Median) <lb/>Infant−directedness <lb/>(G) <lb/>R 2 = 0.03 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−2 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>Pitch Space (IQR) <lb/>(H) <lb/>R 2 = 0.03 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>Attack Curve Slope (Median) <lb/>(I) <lb/>R 2 = 0.02 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−1 <lb/>0 <lb/>1 <lb/>Energy Roll−off (85th %ile) <lb/>Infant−directedness <lb/>(J) <lb/>R 2 = 0.02 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>First Formant (Median) <lb/>(K) <lb/>R 2 = 0.01 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>−2 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>nPVI (per phrase) <lb/>(L) <lb/>q <lb/>q <lb/>q <lb/>q <lb/>ID Song <lb/>ID Speech <lb/>AD Song <lb/>AD Speech <lb/>Figure 5. Twelve acoustic features reliably predict infant-directedness across societies. (A-L) The scatter-<lb/>plots each correspond to a single acoustic feature (indicated on the x-axis). They represent the average naïve <lb/>listener ratings of infant-directedness for each recording in the corpus (measured by the average proportion <lb/>of &quot;baby&quot; responses in the online experiment), as a function of each acoustic feature (normalized across par-<lb/>ticipants and centered within participants). The features plotted here survived a Bonferroni correction for 21 <lb/>tests and, further, were included only if they were supported by convergent evidence from both LASSO and <lb/>exploratory-confirmatory analyses. The black line represents the linear model corresponding to the reported <lb/>R 2 , which is significant at p &lt; .0024; the gray shaded area a 95% confidence interval; and the blue line a <lb/>LOESS regression. The x-axes of some panels are truncated to facilitate visualization. <lb/></body>

			<page>14 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>tified well above chance level, but this analysis masked some cross-cultural variability: when analyzing per-<lb/>formance within each society independently, infant-directed song was always identified reliably, but infant-<lb/>directed speech was identified reliably less than half the time. Moreover, those societies where the naïve <lb/>listeners failed to identify infant-directed speech tended to be small-scale, contrasting with typical &quot;citizen <lb/>scientist&quot; participants, who are recruited mostly from industrialized populations. This suggests that the <lb/>corpus-wide identification rate for infant-directed speech is inflated by the listeners&apos; familiarity with the <lb/>style of infant-directed speech found in societies similar to their own -and raises the intruiging possibility <lb/>that infant-directed speech is more variable, worldwide, than is infant-directed song. <lb/>This research leaves open at least four questions. First, while the results point in the direction of universality, <lb/>because the corpus covers a swath of geographic locations (21 societies on 6 continents), languages (12 <lb/>language families), and different subsistence regimes (8 types), the participants whose vocalizations we <lb/>studied do not constitute a representative sample of humans. As such, no strong claims of universality <lb/>are justified concerning the acoustic structure of infant-directedness. This issue could be addressed by <lb/>(a) studying larger, representative samples of infant-directed vocalizations; (b) using phylogenetic methods <lb/>to examine whether people in societies that are very distantly related nonetheless produce similar infant-<lb/>directed vocalizations; (c) testing perceived infant-directedness in a more diverse sample of listeners, to more <lb/>accurately characterize cross-cultural variability in the perception of infant-directedness; and (d) testing <lb/>listener intuitions among groups with reduced exposure to a given set of infant-directed vocalizations, such <lb/>as very young infants or people from distantly related small-scale societies. <lb/>Second, despite a large body of work in bioacoustics examining the structure of vocal signals 1-3,3-15 , it is <lb/>not yet clear the extent to which the variability in acoustic features identified here is unique to humans, or <lb/>whether it reflects more general principles underlying cross-species regularities in vocal signals. It is notable, <lb/>for example, that many of the acoustic features that are reduced in infant-directed vocalization (Table 2) are <lb/>associated with harsh, nonlinear sounds commonly accentuated in alarm calls across species 4,78 . Comparative <lb/>studies may help to disentangle the ways in which human vocal signals are shaped in ways that are different <lb/>from other animals, or not. <lb/>Third, our findings say little about the content of infant-directed vocalizations, which are known to vary <lb/>widely: song and speech are used in a wide variety of contexts with infants, of which soothing (the type <lb/>of vocalization we elicited from participants) is just one. One curious finding reported here, where naïve <lb/>listeners reliably characterize adult-directed song inaccurately as infant-directed, may bear on this question <lb/>-perhaps this simply reflects a predisposition in our listeners to finding solo, mostly female voices, as <lb/>soothing -given a wider variety of contexts for the solo singing, perhaps the naïve listeners would have <lb/>responded differently. Similarly, the sounds of arousing or alerting infant-directed speech and soothing <lb/>infant-directed speech are likely to differ consistently from one another across cultures 22 , just as different <lb/>forms of infant-directed song differ from one another (e.g., lullabies vs. play songs 63 ). Future studies should <lb/>determine the degree of generality of the present findings across a wider variety of contexts. <lb/>Last, the corpus-building approach used here may help to empirically test theories on the origins and functions <lb/>of music and speech in infancy. For example, if infant-directed song communicates the costly investment of <lb/>parental attention 55 , then infant-directed song should feature increased flashiness and variability in salient <lb/>acoustic characteristics for infants -consistent with the present findings of higher energy in second formants <lb/>(important for vowel recognition 79 ) and faster travel over a vowel space. Moreover, the relation between <lb/>infant-directedness and the sounds of vowels is consistent with classic experimental evidence demonstrating <lb/>infants&apos; robust perceptual sensitivity to vowels 79-81 . In contrast, cross-cultural variability in infant-directed <lb/>speech found in the naïve listener experiment weighs against any universality prediction from functional <lb/>accounts of infant-directed speech 32-35 ; however, given the relatively high accuracy of the LASSO classifiers <lb/>in distinguishing infant-from adult-directed speech across the societies studied, more research is needed to <lb/>clarify those aspects of infant-directed speech that are culturally invariant. <lb/>Whatever the answers to these questions, the results presented here demonstrate that infant-directed vo-<lb/>calizations -and especially infant-directed song -are a fundamental aspect of human communication, <lb/>characterized by acoustic regularities across many cultures. <lb/></body>

			<page>15 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<div type="availability">Data, code, and materials availability <lb/>Data and code are available at https://github.com/themusiclab/infant-vocal; the corpus is available at <lb/>https://osf.io/m5yn2; the preregistration is at https://osf.io/5r72u; and readers may participate in the <lb/>naïve listener experiment at https://themusiclab.org/quizzes/ids. <lb/></div>

			<div type="annex">Author contributions <lb/>S.A.M. and M.M.K. conceived of the research, provided funding, and coordinated the recruitment of collab-<lb/>orators and creation of the corpus. L.G., A.G., G.J., C.T.R., M.B.N., A.M., L.K.C., S.E.T., J. Song, M.K., <lb/>A.S., T.A.V., Q.D.A., J.A., P.M., A.S., C.D.P., G.D.S., S.K., M.S., S.A.C., J.Q.P., C.S., J. Stieglitz, C.M., <lb/>R.R.S., and B.M.W. collected the field recordings. C.M.B. and S.A. provided essential research assistance. <lb/>S.A.M., C.M.B., and J. Simson designed and implemented the online experiment. C.J.M. and H.L-R. pro-<lb/>cessed all recordings and designed the acoustic feature extraction in collaboration with S.A.M. and M.M.K. <lb/>S.A.M. led analyses, with contributions from C.J.M., D.K., and M.M.K. S.A.M. made the figures. C.J.M., <lb/>H.L-R., M.M.K., and S.A.M. wrote the manuscript and all authors approved it. <lb/></div>

			<div type="annex">Ethics <lb/>Informed consent was obtained from all participants. Ethics approval for the naïve listener experiment was <lb/>provided by the Committee on the Use of Human Subjects, Harvard University&apos;s Insitutional Review Board <lb/>(protocol #IRB17-1206). Ethics approval for the collection of recordings and their use in research was <lb/>decentralized; each collaborating research arranged ethics approval with their local institution. <lb/></div>

			<div type="annex">Competing interests <lb/>We declare we have no competing interests. <lb/></div>

			<div type="funding">Funding <lb/>This research was supported by the Harvard University Department of Psychology (M.M.K. and S.A.M.); <lb/>the Harvard College Research Program (H.L-R.); the Harvard Data Science Initiative (S.A.M.); the National <lb/>Institutes of Health Director&apos;s Early Independence Award DP5OD024566 (S.A.M.); the Academy of Finland <lb/>Grant 298513 (J.A.); the Royal Society of New Zealand Te Apārangi Rutherford Discovery Fellowship RDF-<lb/>UOA1101 (Q.D.A., T.A.V.); the Social Sciences and Humanities Research Council of Canada (L.K.C.); the <lb/>Polish Ministry of Science and Higher Education grant N43/DBS/000068 (G.J.); the Fogarty International <lb/>Center and National Heart, Lung, and Blood Institute, and the National Institute of Neurological Disorders <lb/>and Stroke Award D43 TW010540 (P.M., C.D.P.); the National Institute of Allergy and Infectious Dis-<lb/>eases Award R15-AI128714-01 (P.M.); the Max Planck Institute for Evolutionary Anthropology (C.T.R.); a <lb/>British Academy Research Fellowship and Grant SRG-171409 (G.D.S.); the Institute for Advanced Study in <lb/>Toulouse, under an Agence nationale de la recherche grant, Investissements d&apos;Avenir ANR-17-EURE-0010 <lb/>(J. Stieglitz); and the Natural Sciences and Engineering Research Council of Canada (S.E.T.). <lb/></div>

			<div type="acknowledgement">Acknowledgments <lb/>We thank the participants and their families for providing recordings; D. Amir, who sparked the idea for this <lb/>research in conversation with S.A.M. at the 2016 Annual Conference of the Human Behavior &amp; Evolution <lb/>Society; J. Du, E. Pillsworth, L. Sugiyama, P. Wiessner, and J. Ziker, who collected or attempted to collect <lb/>additional recordings; A. Bergson, Z. Jurewicz, D. Li, L. Lopez, and E. Radytė for research assistance; and <lb/>M. Bertolo, J. Kominsky, and L. Yurdum for feedback on the manuscript. <lb/></div>

			<page>16 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<div type="annex">Supplementary Information <lb/>Details of acoustic feature extraction <lb/>Praat <lb/>We extracted intensity, pitch, and first and second formant values from the denoised recordings every 0.03125 <lb/>seconds. For male participants, the pitch floor was set at 75 Hz, with a pitch ceiling at 300 Hz, and a maximum <lb/>formant of 5000 Hz. For females these values were 100 Hz, 600 Hz, and 5500 Hz, respectively. From these <lb/>data, several summary values were calculated per recording: mean and maximum first and second formants, <lb/>mean pitch, and minimum intensity. In addition to these summary statistics, we measured the intensity and <lb/>pitch rates as change in these values over time. For vowel measures, the first and second formants were used <lb/>to calculate both the average vowel space used, as well as the vowel change rate (measured as change in <lb/>Euclidean formant space) over time. <lb/>MIRtoolbox <lb/>All MIRtoolbox (v. 1.7.2) features were extracted with default parameters 69 . mirattackslope returns a list of <lb/>all attack slopes detected, so final analyses were done on summary features (e.g., mean, median, etc.). Final <lb/>analyses were also done on summary features for mirroughness, which returns time series data of roughness <lb/>measures in 50ms windows. We RMS-normalized the mean of mirroughness following 82 . MIRtoolbox features <lb/>were computed on the denoised recordings, with the exception of mirtempo and mirpulseclarity, where <lb/>removing the silences between vocalizations would have altered the tempo. <lb/>Rhythmic variability <lb/>For temporal modulation spectra we followed Ding&apos;s 83 method, which combines discrete Fourier transforms <lb/>applied to contiguous six-second excerpts. To analyze the entirety of each recording, we appended all <lb/>recordings with silence to be exact multiples of six-seconds. The location of the peak (Hz) and variance of <lb/>the temporal modulation spectra were extracted from their RMS values. <lb/>Normalized pairwise variability index <lb/>The nPVI represents the temporal variance of data with discrete events, which makes it especially useful for <lb/>comparing speech and music 70 . We used an automated syllable-and phrase-detection algorithm to extract <lb/>events 71 . We computed nPVI in two ways: by averaging the nPVI of each phrase within a recording, as <lb/>well as by treating the entire recording as a single phrase. Because intervening silence would influence both <lb/>temporal modulation and nPVI measures, we used recordings before they had been denoised. <lb/></div>

			<page>17 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<div type="annex">Figure S1. Screenshot from the naïve listener experiment (desktop computer version). On each trial, <lb/>participants heard a randomly selected vocalization from the corpus and were asked to quickly guess to <lb/>whom the vocalization was directed: an adult or an infant. <lb/></div>

			<page>18 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>Q <lb/>u e c h u a <lb/>A r a w a k <lb/>E n g a <lb/>C <lb/>o lo <lb/>m <lb/>b ia <lb/>n <lb/>M <lb/>e s t iz o s <lb/>A f r o − C <lb/>o lo <lb/>m <lb/>b ia <lb/>n s <lb/>H <lb/>a d z a <lb/>K r a k o w <lb/>T s im <lb/>a n e <lb/>M <lb/>b e n d je <lb/>le <lb/>S á p a r a <lb/>&amp; <lb/>A c h u a r <lb/>T o p o s a <lb/>R <lb/>u r a l P o la <lb/>n d <lb/>N <lb/>y a n g a t o m <lb/>M <lb/>e n t a w a i I s la <lb/>n d e r s <lb/>J e n u <lb/>K u r u b a s <lb/>T u r k u <lb/>T a n n e s e <lb/>V a n u a t u a n s <lb/>B e ij in <lb/>g <lb/>S a n <lb/>D <lb/>ie <lb/>g o <lb/>T o r o n t o W <lb/>e ll in <lb/>g t o n <lb/>Infant−directedness <lb/>Vocalization Type <lb/>ID Song <lb/>ID Speech <lb/>AD Song <lb/>AD Speech <lb/>Figure S2. Perceived infant-directedness, analyzed separately for each society. For each vocalization type, <lb/>the boxplots indicate the within-society median (horizontal black line), interquartile range (box), 1.5 × IQR <lb/>(whiskers), and outliers (gray points). The societies are ordered from the smallest to largest number of <lb/>recordings (from left to right). Abbreviations: infant-directed (ID); adult-directed (AD). <lb/></body>

			<page>19 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<div type="annex">Variable <lb/>Label <lb/>Description <lb/>id <lb/>filename <lb/>mir_attack <lb/>Attack Curve Slope <lb/>MIRtoolbox detects events in the audio; for a subset of those it can compute an <lb/>attack slope, which is the slope of the line from the beginning of the event to its <lb/>peak. <lb/>mir_roughness <lb/>Roughness <lb/>A roughness measure based on the dissonant beating patterns produced by <lb/>interference frequencies in the spectrum of the sound. MIRtoolbox produces a <lb/>roughness curve; following Buyens et al. (2017), we reduce this to a single measure <lb/>by taking the RMS-normalized mean. <lb/>mir_rolloff85 <lb/>85th Energy <lb/>Percentile <lb/>One way to estimate the amount of high frequency in the signal consists in finding <lb/>the frequency such that a certain fraction of the total energy is contained below <lb/>that frequency. This ratio is fixed by default to .85 (following Tzanetakis and <lb/>Cook, 2002), other have proposed .95 (Pohle, Pampalk and Widmer, 2005). <lb/>mir_inharmonicity <lb/>Inharmonicity <lb/>mirinharmonicity &quot;estimates the inharmonicity, i.e., the amount of partials that <lb/>are not multiples of the [automatically detected] fundamental frequency, as a value <lb/>between 0 and 1. More precisely, the inharmonicity considered here takes into <lb/>account the amount of energy outside the ideal harmonic series.&quot; (MIRtoolbox <lb/>manual) <lb/>mir_tempo <lb/>Tempo <lb/>MIRtoolbox tempo detection with default parameters. Based on MIRtoolbox&apos;s <lb/>event detection. Outputs a single number. <lb/>mir_pulseclarity <lb/>Pule Clarity <lb/>Estimates the rhythmic clarity, indicating the strength of the beats estimated by <lb/>the mirtempo function. <lb/>npvi_total <lb/>nPVI Recording <lb/>The nPVI equation measures the &quot;average degree of durational contrast between <lb/>adjacent events in a sequence&quot; (Daniele &amp; Patel, 2015). This makes it especially <lb/>useful for comparing rhythmic units across language and music (i.e., syllables vs. <lb/>notes). To automatically detect events, we used Mertens&apos; (2004) syllable detection <lb/>algorithm. <lb/>npvi_phrase <lb/>nPVI Phrase <lb/>In addition to detecting syllables, Mertens&apos; algorithm detects phrases. Whereas <lb/>npvi_total computes nPVI based on the whole file as a continuous phrase, this <lb/>measure computes the nPVI for each detected phrase and reports the mean. In <lb/>other words, it excludes the distances between the ends and beginnings of phrases. <lb/>tm_std_hz <lb/>Temporal <lb/>Modulation <lb/>The temporal modulations spectrum is the frequency decomposition of the <lb/>amplitude envelope of a signal. This measures how loud something is at any given <lb/>moment, and then we measure how fast the loudness changes. Trivial example: if <lb/>the song is someone singing a note every second, the spectrum will have a peak at <lb/>1Hz. If the song is someone singing a note three times a second, but with an <lb/>emphasis every three seconds, there will be a large peak at 1Hz, and a smaller <lb/>peak at 3Hz. We&apos;re interested in the standard deviation of the spectrum, which <lb/>we&apos;re construing as how exaggerated the peak is. <lb/>praat_f0 <lb/>Pitch <lb/>The pitch (f0) in Hertz for each song <lb/>praat_pitch_rate <lb/>Pitch Rate <lb/>The pitch rate is a measure of pitch change over unit time. In essence, the pitch <lb/>rate gives us a measure of pitch curve smoothness (a lower value corresponds to a <lb/>smoother curve). <lb/>praat_vowtrav <lb/>Vowel Space <lb/>The euclidian distance travelled in vowel space. This is equivalent to distance <lb/>between two formants. <lb/>praat_vowtrav_rate <lb/>Vowel Space Travel <lb/>Rate <lb/>The euclidian distance travelled in vowel space over a rate of time. This is <lb/>equivalent to distance between two formants divided by rate of travel. <lb/>praat_intensity <lb/>Amplitude <lb/>A measure of amplitude (loudness) in decibels <lb/>praat_intensity_rate Amplitude Rate <lb/>A measure of decay in intensity curves in each song measured as change in <lb/>intensity over rate in time. <lb/>praat_f1 <lb/>First Formant <lb/>The frequency in Herz of the first formant at each (.03125/sec) point <lb/>praat_f2 <lb/>Second Formant <lb/>The frequency in Herz of the second formant at each (.03125/sec) point <lb/>meta_length <lb/>File duration <lb/>The length of the unedited sound files <lb/>meta_edit_length <lb/>Concatenated file <lb/>duration <lb/>The length of the concatenated versions of the sound files <lb/>Table S1. <lb/>Codebook for acoustic features. Variable names are stubs, i.e., in the datasets, suffixes are added to denote summary <lb/>statistics. Abbreviations: infant-directed (ID); adult-directed (AD). <lb/></div>

			<page>20 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/>Comparison <lb/></note>

			<body>Feature <lb/>Statistic <lb/>Est. <lb/>SE <lb/>z <lb/>p <lb/>ID vs. AD <lb/>(overall) <lb/>85th Energy <lb/>Percentile <lb/>Whole <lb/>-744.65 <lb/>155.62 <lb/>-4.79 <lb/>&lt; .001 <lb/>Attack Curve Slope Median <lb/>0.41 <lb/>0.20 <lb/>2.03 <lb/>0.043 <lb/>First Formant <lb/>Maximum <lb/>-172.06 <lb/>35.97 <lb/>-4.78 <lb/>&lt; .001 <lb/>Range <lb/>-186.41 <lb/>38.91 <lb/>-4.79 <lb/>&lt; .001 <lb/>Inharmonicity <lb/>Whole <lb/>-0.01 <lb/>0.00 <lb/>-4.28 <lb/>&lt; .001 <lb/>Intensity <lb/>IQR <lb/>0.68 <lb/>0.30 <lb/>2.22 <lb/>0.026 <lb/>Minimum <lb/>0.86 <lb/>0.38 <lb/>2.27 <lb/>0.023 <lb/>Intensity Rate <lb/>Whole <lb/>-4.42 <lb/>0.48 <lb/>-9.25 <lb/>&lt; .001 <lb/>Whole <lb/>2.99 <lb/>0.43 <lb/>6.92 <lb/>&lt; .001 <lb/>Intensity Space <lb/>Mean <lb/>0.62 <lb/>0.11 <lb/>5.79 <lb/>&lt; .001 <lb/>St. Dev. <lb/>1.76 <lb/>0.26 <lb/>6.65 <lb/>&lt; .001 <lb/>Pitch <lb/>First Quartile <lb/>27.88 <lb/>4.04 <lb/>6.91 <lb/>&lt; .001 <lb/>Third Quartile <lb/>59.44 <lb/>11.28 <lb/>5.27 <lb/>&lt; .001 <lb/>IQR <lb/>31.52 <lb/>8.55 <lb/>3.69 <lb/>&lt; .001 <lb/>Mean <lb/>42.19 <lb/>6.91 <lb/>6.11 <lb/>&lt; .001 <lb/>Median <lb/>45.47 <lb/>7.34 <lb/>6.19 <lb/>&lt; .001 <lb/>Minimum <lb/>8.13 <lb/>2.72 <lb/>2.99 <lb/>0.003 <lb/>St. Dev. <lb/>13.00 <lb/>3.64 <lb/>3.57 <lb/>&lt; .001 <lb/>Pitch Rate <lb/>Whole <lb/>-37.30 <lb/>4.34 <lb/>-8.59 <lb/>&lt; .001 <lb/>Whole <lb/>23.36 <lb/>4.62 <lb/>5.05 <lb/>&lt; .001 <lb/>Pitch Space <lb/>First Quartile <lb/>0.51 <lb/>0.10 <lb/>5.18 <lb/>&lt; .001 <lb/>Mean <lb/>3.24 <lb/>1.34 <lb/>2.42 <lb/>0.015 <lb/>Median <lb/>1.61 <lb/>0.33 <lb/>4.87 <lb/>&lt; .001 <lb/>St. Dev. <lb/>6.99 <lb/>2.35 <lb/>2.98 <lb/>0.003 <lb/>Second Formant <lb/>Maximum <lb/>-114.81 <lb/>25.77 <lb/>-4.46 <lb/>&lt; .001 <lb/>Median <lb/>35.63 <lb/>12.51 <lb/>2.85 <lb/>0.004 <lb/>Range <lb/>-115.51 <lb/>33.30 <lb/>-3.47 <lb/>0.001 <lb/>Vowel Space <lb/>Third Quartile <lb/>46.81 <lb/>15.12 <lb/>3.10 <lb/>0.002 <lb/>IQR <lb/>45.23 <lb/>12.97 <lb/>3.49 <lb/>&lt; .001 <lb/>Mean <lb/>38.13 <lb/>10.68 <lb/>3.57 <lb/>&lt; .001 <lb/>St. Dev. <lb/>51.71 <lb/>10.80 <lb/>4.79 <lb/>&lt; .001 <lb/>Vowel Space Travel <lb/>Rate <lb/>Whole <lb/>212.31 <lb/>37.85 <lb/>5.61 <lb/>&lt; .001 <lb/>ID Song vs. AD <lb/>Song <lb/>Attack Curve Slope First Quartile <lb/>-0.45 <lb/>0.21 <lb/>-2.12 <lb/>0.034 <lb/>Median <lb/>-0.80 <lb/>0.41 <lb/>-1.97 <lb/>0.049 <lb/>First Formant <lb/>Median <lb/>-19.66 <lb/>9.70 <lb/>-2.03 <lb/>0.043 <lb/>Inharmonicity <lb/>Whole <lb/>-0.01 <lb/>0.00 <lb/>-2.15 <lb/>0.032 <lb/>Intensity <lb/>First Quartile <lb/>-1.95 <lb/>0.55 <lb/>-3.57 <lb/>&lt; .001 <lb/>Third Quartile <lb/>-1.45 <lb/>0.50 <lb/>-2.88 <lb/>0.004 <lb/>Maximum <lb/>-1.13 <lb/>0.51 <lb/>-2.22 <lb/>0.027 <lb/>Mean <lb/>-1.60 <lb/>0.48 <lb/>-3.35 <lb/>0.001 <lb/>Median <lb/>-1.63 <lb/>0.51 <lb/>-3.18 <lb/>0.001 <lb/>Minimum <lb/>-0.80 <lb/>0.31 <lb/>-2.59 <lb/>0.01 <lb/>nPVI Recording <lb/>Whole <lb/>-2.14 <lb/>0.86 <lb/>-2.50 <lb/>0.012 <lb/>Pitch <lb/>Minimum <lb/>-9.00 <lb/>3.00 <lb/>-3.00 <lb/>0.003 <lb/>Tempo <lb/>Whole <lb/>5.80 <lb/>2.75 <lb/>2.11 <lb/>0.035 <lb/>Temporal <lb/>Modulation <lb/>Peak <lb/>0.65 <lb/>0.32 <lb/>2.03 <lb/>0.042 <lb/>Vowel Space <lb/>Third Quartile <lb/>27.90 <lb/>11.00 <lb/>2.54 <lb/>0.011 <lb/>IQR <lb/>24.94 <lb/>9.58 <lb/>2.60 <lb/>0.009 <lb/>Mean <lb/>20.29 <lb/>6.74 <lb/>3.01 <lb/>0.003 <lb/>St. Dev. <lb/>18.94 <lb/>6.31 <lb/>3.00 <lb/>0.003 <lb/>Vowel Space Travel <lb/>Rate <lb/>Whole <lb/>23.44 <lb/>11.22 <lb/>2.09 <lb/>0.037 <lb/>ID Song vs. ID <lb/>Speech <lb/>Attack Curve Slope First Quartile <lb/>-0.67 <lb/>0.26 <lb/>-2.59 <lb/>0.01 <lb/></body>

			<page>21 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<div type="annex">(continued) <lb/>Comparison <lb/>Feature <lb/>Statistic <lb/>Est. <lb/>SE <lb/>z <lb/>p <lb/>Third Quartile <lb/>-1.85 <lb/>0.37 <lb/>-5.05 <lb/>&lt; .001 <lb/>IQR <lb/>-1.19 <lb/>0.27 <lb/>-4.39 <lb/>&lt; .001 <lb/>Mean <lb/>-1.11 <lb/>0.27 <lb/>-4.13 <lb/>&lt; .001 <lb/>Median <lb/>-1.17 <lb/>0.32 <lb/>-3.65 <lb/>&lt; .001 <lb/>First Formant <lb/>First Quartile <lb/>-24.19 <lb/>7.30 <lb/>-3.31 <lb/>0.001 <lb/>Third Quartile <lb/>-57.27 <lb/>19.77 <lb/>-2.90 <lb/>0.004 <lb/>Maximum <lb/>112.08 <lb/>30.98 <lb/>3.62 <lb/>&lt; .001 <lb/>Mean <lb/>-39.98 <lb/>10.94 <lb/>-3.66 <lb/>&lt; .001 <lb/>Median <lb/>-41.69 <lb/>11.88 <lb/>-3.51 <lb/>&lt; .001 <lb/>Minimum <lb/>-26.11 <lb/>5.25 <lb/>-4.97 <lb/>&lt; .001 <lb/>Range <lb/>138.18 <lb/>34.18 <lb/>4.04 <lb/>&lt; .001 <lb/>Inharmonicity <lb/>Whole <lb/>-0.01 <lb/>0.00 <lb/>-3.10 <lb/>0.002 <lb/>Intensity <lb/>First Quartile <lb/>1.29 <lb/>0.48 <lb/>2.68 <lb/>0.007 <lb/>IQR <lb/>-1.44 <lb/>0.35 <lb/>-4.15 <lb/>&lt; .001 <lb/>Minimum <lb/>-0.94 <lb/>0.35 <lb/>-2.64 <lb/>0.008 <lb/>St. Dev. <lb/>-0.60 <lb/>0.20 <lb/>-2.92 <lb/>0.003 <lb/>Intensity Space <lb/>First Quartile <lb/>-0.29 <lb/>0.03 <lb/>-9.56 <lb/>&lt; .001 <lb/>Third Quartile <lb/>-1.68 <lb/>0.23 <lb/>-7.30 <lb/>&lt; .001 <lb/>IQR <lb/>-1.39 <lb/>0.20 <lb/>-6.80 <lb/>&lt; .001 <lb/>Mean <lb/>-1.73 <lb/>0.15 <lb/>-11.74 <lb/>&lt; .001 <lb/>Median <lb/>-0.76 <lb/>0.08 <lb/>-9.48 <lb/>&lt; .001 <lb/>St. Dev. <lb/>-2.66 <lb/>0.29 <lb/>-9.08 <lb/>&lt; .001 <lb/>nPVI Phrase <lb/>Whole <lb/>7.21 <lb/>1.27 <lb/>5.67 <lb/>&lt; .001 <lb/>nPVI Recording <lb/>Whole <lb/>5.68 <lb/>1.34 <lb/>4.25 <lb/>&lt; .001 <lb/>Pitch <lb/>Maximum <lb/>-23.98 <lb/>11.46 <lb/>-2.09 <lb/>0.036 <lb/>St. Dev. <lb/>-11.25 <lb/>5.10 <lb/>-2.21 <lb/>0.027 <lb/>Pitch Space <lb/>First Quartile <lb/>-0.54 <lb/>0.16 <lb/>-3.38 <lb/>0.001 <lb/>Third Quartile <lb/>-14.25 <lb/>1.78 <lb/>-8.02 <lb/>&lt; .001 <lb/>IQR <lb/>-13.71 <lb/>1.81 <lb/>-7.57 <lb/>&lt; .001 <lb/>Maximum <lb/>-23.15 <lb/>11.48 <lb/>-2.02 <lb/>0.044 <lb/>Mean <lb/>-16.16 <lb/>1.50 <lb/>-10.76 <lb/>&lt; .001 <lb/>Median <lb/>-2.97 <lb/>0.31 <lb/>-9.70 <lb/>&lt; .001 <lb/>Range <lb/>-23.15 <lb/>11.48 <lb/>-2.02 <lb/>0.044 <lb/>St. Dev. <lb/>-18.79 <lb/>2.56 <lb/>-7.35 <lb/>&lt; .001 <lb/>Pulse Clarity <lb/>Whole <lb/>0.02 <lb/>0.01 <lb/>3.44 <lb/>0.001 <lb/>Roughness <lb/>Third Quartile <lb/>-13.00 <lb/>3.99 <lb/>-3.26 <lb/>0.001 <lb/>Distance <lb/>-746.17 <lb/>224.00 <lb/>-3.33 <lb/>0.001 <lb/>IQR <lb/>-12.96 <lb/>3.91 <lb/>-3.32 <lb/>0.001 <lb/>Mean <lb/>-177.13 <lb/>41.50 <lb/>-4.27 <lb/>&lt; .001 <lb/>Median <lb/>-2.55 <lb/>0.96 <lb/>-2.66 <lb/>0.008 <lb/>St. Dev. <lb/>-54.89 <lb/>18.84 <lb/>-2.91 <lb/>0.004 <lb/>Second Formant <lb/>Maximum <lb/>83.42 <lb/>27.09 <lb/>3.08 <lb/>0.002 <lb/>Median <lb/>-49.14 <lb/>21.99 <lb/>-2.23 <lb/>0.025 <lb/>Minimum <lb/>-69.20 <lb/>23.20 <lb/>-2.98 <lb/>0.003 <lb/>Range <lb/>152.58 <lb/>41.31 <lb/>3.69 <lb/>&lt; .001 <lb/>Temporal <lb/>Modulation <lb/>St. Dev. <lb/>0.53 <lb/>0.06 <lb/>8.23 <lb/>&lt; .001 <lb/>Vowel Space <lb/>First Quartile <lb/>-24.33 <lb/>3.59 <lb/>-6.77 <lb/>&lt; .001 <lb/>Third Quartile <lb/>-97.33 <lb/>14.50 <lb/>-6.71 <lb/>&lt; .001 <lb/>IQR <lb/>-73.02 <lb/>11.76 <lb/>-6.21 <lb/>&lt; .001 <lb/>Mean <lb/>-82.31 <lb/>9.28 <lb/>-8.87 <lb/>&lt; .001 <lb/>Median <lb/>-47.59 <lb/>6.97 <lb/>-6.83 <lb/>&lt; .001 <lb/>St. Dev. <lb/>-83.54 <lb/>10.56 <lb/>-7.91 <lb/>&lt; .001 <lb/>Vowel Space Travel <lb/>Rate <lb/>Whole <lb/>-298.47 <lb/>32.34 <lb/>-9.23 <lb/>&lt; .001 <lb/>Table S2. <lb/>Significant results from exploratory analyses, using post-hoc linear combinations following multi-level mixed-<lb/>effects models. Abbreviations: infant-directed (ID); adult-directed (AD). <lb/></div>

			<page>22 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<div type="annex">Feature <lb/>Variable <lb/>ID vs. AD <lb/>ID Song vs. AD <lb/>Song <lb/>ID Song vs. ID <lb/>Speech <lb/>85th Energy <lb/>Percentile <lb/>Whole <lb/>− 1 <lb/>− <lb/>− <lb/>Attack Curve <lb/>Slopes <lb/>Median <lb/>− <lb/>− 1 <lb/>− 1 <lb/>Attack Curve <lb/>Slopes <lb/>Mean <lb/>− <lb/>− <lb/>− 1 <lb/>First Formant <lb/>Mean <lb/>− <lb/>− <lb/>− 1 <lb/>First Formant <lb/>Max <lb/>− 1 <lb/>− <lb/>− 0 <lb/>Inharmonicity <lb/>Whole <lb/>− 1 <lb/>− <lb/>− 1 <lb/>Intensity <lb/>Minimum <lb/>− 1 <lb/>− <lb/>− 1 <lb/>Intensity Rate <lb/>Whole <lb/>− 0 <lb/>− <lb/>− 1 <lb/>nPVI per Phrase <lb/>Whole <lb/>+ <lb/>+ <lb/>+ 1 <lb/>nPVI per <lb/>Recording <lb/>Whole <lb/>+ <lb/>+ <lb/>+ 1 <lb/>Pitch <lb/>Mean <lb/>+ 1 <lb/>− <lb/>− <lb/>Pitch Space <lb/>Mean <lb/>− <lb/>− <lb/>− 1 <lb/>Pitch Rate <lb/>Whole <lb/>− 0 <lb/>− <lb/>− 1 <lb/>Pulse Clarity <lb/>Whole <lb/>+ 1 <lb/>+ <lb/>+ <lb/>Roughness <lb/>Mean <lb/>− <lb/>− <lb/>− 1 <lb/>Second Formant <lb/>Mean <lb/>− <lb/>− <lb/>− <lb/>Second Formant <lb/>Max <lb/>− 1 <lb/>− 0 <lb/>− 0 <lb/>Tempo <lb/>Whole <lb/>− <lb/>− <lb/>− <lb/>Temporal <lb/>Modulation <lb/>St. Dev. <lb/>− <lb/>− <lb/>− 0 <lb/>Temporal <lb/>Modulation <lb/>Peak <lb/>− 0 <lb/>− <lb/>− 1 <lb/>Vowel Space <lb/>Mean <lb/>+ 1 <lb/>+ 1 <lb/>+ <lb/>Vowel Space <lb/>Travel Rate <lb/>Whole <lb/>+ 1 <lb/>+ 1 <lb/>+ <lb/>Table S3. Preregistered predictions. Predictions that were supported by the exploratory-confirmatory <lb/>analyses are marked 1 while predictions which were significantly falsified in the opposite direction are <lb/>marked 0 . Abbreviations: infant-directed (ID); adult-directed (AD). <lb/></div>

			<page>23 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/>Comparison <lb/></note>

			<div type="annex">Feature <lb/>Statistic <lb/>Est. <lb/>SE <lb/>z <lb/>p <lb/>ID vs. AD <lb/>(overall) <lb/>85th Energy <lb/>Percentile <lb/>Whole <lb/>-665.11 <lb/>182.20 <lb/>-3.65 <lb/>&lt; .001 <lb/>Inharmonicity <lb/>Whole <lb/>-0.01 <lb/>0.00 <lb/>-3.03 <lb/>0.002 <lb/>Intensity <lb/>IQR <lb/>0.46 <lb/>0.18 <lb/>2.51 <lb/>0.012 <lb/>Intensity Rate <lb/>Whole <lb/>2.07 <lb/>0.43 <lb/>4.81 <lb/>&lt; .001 <lb/>IQR <lb/>2.08 <lb/>0.52 <lb/>4.04 <lb/>&lt; .001 <lb/>Median <lb/>0.85 <lb/>0.21 <lb/>4.05 <lb/>&lt; .001 <lb/>Pitch <lb/>IQR <lb/>26.27 <lb/>5.89 <lb/>4.46 <lb/>&lt; .001 <lb/>Median <lb/>41.55 <lb/>7.64 <lb/>5.44 <lb/>&lt; .001 <lb/>Pitch Rate <lb/>Whole <lb/>13.20 <lb/>3.30 <lb/>4.00 <lb/>&lt; .001 <lb/>IQR <lb/>12.61 <lb/>3.29 <lb/>3.84 <lb/>&lt; .001 <lb/>Median <lb/>3.12 <lb/>0.66 <lb/>4.70 <lb/>&lt; .001 <lb/>Pitch Space <lb/>Median <lb/>1.19 <lb/>0.25 <lb/>4.73 <lb/>&lt; .001 <lb/>Vowel Space <lb/>IQR <lb/>30.83 <lb/>12.52 <lb/>2.46 <lb/>0.014 <lb/>Vowel Space Travel <lb/>Rate <lb/>Whole <lb/>144.97 <lb/>29.12 <lb/>4.98 <lb/>&lt; .001 <lb/>IQR <lb/>179.47 <lb/>41.49 <lb/>4.33 <lb/>&lt; .001 <lb/>Median <lb/>71.18 <lb/>15.08 <lb/>4.72 <lb/>&lt; .001 <lb/>ID Song vs. AD <lb/>Song <lb/>Attack Curve Slope Median <lb/>-0.44 <lb/>0.19 <lb/>-2.31 <lb/>0.021 <lb/>First Formant <lb/>Median <lb/>-12.58 <lb/>6.02 <lb/>-2.09 <lb/>0.037 <lb/>Intensity <lb/>IQR <lb/>-1.73 <lb/>0.24 <lb/>-7.17 <lb/>&lt; .001 <lb/>Median <lb/>-1.20 <lb/>0.54 <lb/>-2.22 <lb/>0.026 <lb/>Vowel Space <lb/>IQR <lb/>26.84 <lb/>7.02 <lb/>3.82 <lb/>&lt; .001 <lb/>Vowel Space Travel <lb/>Rate <lb/>Whole <lb/>24.82 <lb/>11.50 <lb/>2.16 <lb/>0.031 <lb/>IQR <lb/>39.07 <lb/>16.13 <lb/>2.42 <lb/>0.015 <lb/>Median <lb/>11.72 <lb/>5.80 <lb/>2.02 <lb/>0.043 <lb/>ID Song vs. ID <lb/>Speech <lb/>Attack Curve Slope Median <lb/>-0.81 <lb/>0.25 <lb/>-3.29 <lb/>0.001 <lb/>First Formant <lb/>Median <lb/>-33.81 <lb/>6.47 <lb/>-5.23 <lb/>&lt; .001 <lb/>Inharmonicity <lb/>Whole <lb/>-0.01 <lb/>0.00 <lb/>-2.02 <lb/>0.043 <lb/>Intensity Rate <lb/>Whole <lb/>-3.92 <lb/>0.36 <lb/>-10.89 <lb/>&lt; .001 <lb/>IQR <lb/>-5.03 <lb/>0.43 <lb/>-11.61 <lb/>&lt; .001 <lb/>Median <lb/>-2.11 <lb/>0.17 <lb/>-12.25 <lb/>&lt; .001 <lb/>Intensity Space <lb/>IQR <lb/>-1.33 <lb/>0.16 <lb/>-8.46 <lb/>&lt; .001 <lb/>Median <lb/>-0.83 <lb/>0.07 <lb/>-11.32 <lb/>&lt; .001 <lb/>nPVI per Phrase <lb/>Whole <lb/>4.39 <lb/>1.14 <lb/>3.87 <lb/>&lt; .001 <lb/>nPVI per <lb/>Recording <lb/>Whole <lb/>4.81 <lb/>0.88 <lb/>5.45 <lb/>&lt; .001 <lb/>Pitch Rate <lb/>Whole <lb/>-28.11 <lb/>2.54 <lb/>-11.05 <lb/>&lt; .001 <lb/>IQR <lb/>-31.97 <lb/>2.56 <lb/>-12.51 <lb/>&lt; .001 <lb/>Median <lb/>-5.78 <lb/>0.56 <lb/>-10.25 <lb/>&lt; .001 <lb/>Pitch Space <lb/>IQR <lb/>-9.99 <lb/>0.77 <lb/>-12.94 <lb/>&lt; .001 <lb/>Median <lb/>-2.70 <lb/>0.23 <lb/>-11.63 <lb/>&lt; .001 <lb/>Roughness <lb/>IQR <lb/>-6.63 <lb/>2.04 <lb/>-3.25 <lb/>0.001 <lb/>Median <lb/>-1.52 <lb/>0.73 <lb/>-2.07 <lb/>0.038 <lb/>Vowel Space <lb/>IQR <lb/>-55.11 <lb/>7.82 <lb/>-7.05 <lb/>&lt; .001 <lb/>Median <lb/>-31.27 <lb/>3.98 <lb/>-7.87 <lb/>&lt; .001 <lb/>Vowel Space Travel <lb/>Rate <lb/>Whole <lb/>-227.44 <lb/>21.26 <lb/>-10.70 <lb/>&lt; .001 <lb/>IQR <lb/>-310.78 <lb/>27.95 <lb/>-11.12 <lb/>&lt; .001 <lb/>Median <lb/>-124.53 <lb/>11.05 <lb/>-11.27 <lb/>&lt; .001 <lb/>Table S4. Significant results from confirmatory analyses, after Winsorization and excluding variables with extreme obser-<lb/>vations (e.g., using median and IQR instead of mean and standard deviation), using post-hoc linear combinations following <lb/>multi-level mixed-effects models. <lb/></div>

			<page>24 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<body>Feature <lb/>F (1, 1094) <lb/>p <lb/>R 2 <lb/>Pitch (Median) <lb/>489.9 5.27 × 10 −90 <lb/>0.309 <lb/>Pitch (IQR) <lb/>411.6 6.30 × 10 −78 <lb/>0.273 <lb/>Intensity Space (Median) <lb/>149.5 2.61 × 10 −32 <lb/>0.120 <lb/>Temporal Modulation <lb/>74.5 2.16 × 10 −17 <lb/>0.064 <lb/>Roughness (IQR) <lb/>69.9 1.86 × 10 −16 <lb/>0.060 <lb/>Inharmonicity <lb/>51.1 1.59 × 10 −12 <lb/>0.045 <lb/>Roughness (Median) <lb/>49.2 4.05 × 10 −12 <lb/>0.043 <lb/>Pitch Space (IQR) <lb/>29.2 7.89 × 10 −8 <lb/>0.026 <lb/>Attack Curve Slope (Median) <lb/>29.0 8.74 × 10 −8 <lb/>0.026 <lb/>Energy Roll-off (85th %ile) <lb/>26.6 2.92 × 10 −7 <lb/>0.024 <lb/>First Formant (Median) <lb/>19.4 1.14 × 10 −5 <lb/>0.017 <lb/>nPVI (per phrase) <lb/>13.1 3.01 × 10 −4 <lb/>0.012 <lb/>Table S5. <lb/>Omnibus tests from simple linear regressions of perceived infant-directedness (from the <lb/>naive listener experiment) on each of 12 acoustic features validated by exploratory-<lb/>confirmatory and LASSO analyses. All tests are significant at the Bonferroni-corrected <lb/>alpha level of .0024. <lb/></body>

			<page>25 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<listBibl>References <lb/>1. Morton, E. S. On the occurrence and significance of motivation-structural rules in some bird and mammal <lb/>sounds. The American Naturalist 111, 855-869 (1977). <lb/>2. Endler, J. A. Some general comments on the evolution and design of animal communication systems. <lb/>Philosophical Transactions of the Royal Society B: Biological Sciences 340, 215-225 (1993). <lb/>3. Owren, M. J. &amp; Rendall, D. Sound on the rebound: Bringing form and function back to the forefront in <lb/>understanding nonhuman primate vocal signaling. Evolutionary Anthropology 10, 58-71 (2001). <lb/>4. Fitch, W. T., Neubauer, J. &amp; Herzel, H. Calls out of chaos: The adaptive significance of nonlinear <lb/>phenomena in mammalian vocal production. Animal Behaviour 63, 407-418 (2002). <lb/>5. Wiley, R. H. The evolution of communication: Information and manipulation. Animal Behaviour 2, <lb/>156-189 (1983). <lb/>6. Krebs, J. &amp; Dawkins, R. Animal signals: Mind-reading and manipulation. in Behavioural Ecology: An <lb/>Evolutionary Approach (eds. Krebs, J. &amp; Davies, N.) 380-402 (Blackwell, 1984). <lb/>7. Karp, D., Manser, M. B., Wiley, E. M. &amp; Townsend, S. W. Nonlinearities in meerkat alarm calls prevent <lb/>receivers from habituating. Ethology 120, 189-196 (2014). <lb/>8. Slaughter, E. I., Berlin, E. R., Bower, J. T. &amp; Blumstein, D. T. A test of the nonlinearity hypothesis in <lb/>great-tailed grackles (Quiscalus mexicanus). Ethology 119, 309-315 (2013). <lb/>9. Wagner, W. E. Fighting, assessment, and frequency alteration in Blanchard&apos;s cricket frog. Behavioral <lb/>Ecology and Sociobiology 25, 429-436 (1989). <lb/>10. Ladich, F. Sound production by the river bullhead, Cottus gobio L. (Cottidae, Teleostei). Journal of <lb/>Fish Biology 35, 531-538 (1989). <lb/>11. Filippi, P. et al. Humans recognize emotional arousal in vocalizations across all classes of terrestrial <lb/>vertebrates: Evidence for acoustic universals. Proceedings of the Royal Society B: Biological Sciences 284, <lb/>(2017). <lb/>12. Lingle, S. &amp; Riede, T. Deer mothers are sensitive to infant distress vocalizations of diverse mammalian <lb/>species. The American Naturalist 184, 510-522 (2014). <lb/>13. Custance, D. &amp; Mayer, J. Empathic-like responding by domestic dogs (Canis familiaris) to distress in <lb/>humans: An exploratory study. Animal Cognition 15, 851-859 (2012). <lb/>14. Magrath, R. D., Haff, T. M., McLachlan, J. R. &amp; Igic, B. Wild birds learn to eavesdrop on heterospecific <lb/>alarm calls. Current Biology 25, 2047-2050 (2015). <lb/>15. Lea, A. J., Barrera, J. P., Tom, L. M. &amp; Blumstein, D. T. Heterospecific eavesdropping in a nonsocial <lb/>species. Behavioral Ecology 19, 1041-1046 (2008). <lb/>16. Soltis, J. The signal functions of early infant crying. Behavioral and Brain Sciences 27, 443-458 (2004). <lb/>17. Bryant, G. A. &amp; Barrett, H. C. Recognizing intentions in infant-directed speech: Evidence for universals. <lb/>Psychological Science 18, 746-751 (2007). <lb/>18. Piazza, E. A., Iordan, M. C. &amp; Lew-Williams, C. Mothers consistently alter their unique vocal fingerprints <lb/>when communicating with infants. Current Biology 27, 3162-3167 (2017). <lb/>19. Trehub, S. E., Unyk, A. M. &amp; Trainor, L. J. Adults identify infant-directed music across cultures. Infant <lb/>Behavior and Development 16, 193-211 (1993). <lb/>20. Mehr, S. A., Singh, M., York, H., Glowacki, L. &amp; Krasnow, M. M. Form and function in human song. <lb/>Current Biology 28, 356-368 (2018). <lb/>21. Mehr, S. A. et al. Universality and diversity in human song. Science 366, 957-970 (2019). <lb/></listBibl>

			<page>26 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<listBibl>22. Fernald, A. Human maternal vocalizations to infants as biologically relevant signals: An evolutionary <lb/>perspective. in The adapted mind: Evolutionary psychology and the generation of culture (eds. Barkow, J. <lb/>H., Cosmides, L. &amp; Tooby, J.) 391-428 (Oxford University Press, 1992). <lb/>23. Burnham, E., Gamache, J. L., Bergeson, T. &amp; Dilley, L. Voice-onset time in infant-directed speech over <lb/>the first year and a half. in Proceedings of Meetings on Acoustics ICA2013 19, 060094 (ASA, 2013). <lb/>24. Fernald, A. Prosody in speech to children: Prelinguistic and linguistic functions. Annals of Child <lb/>Development 8, 43-80 (1991). <lb/>25. Ferguson, C. A. Baby talk in six languages. American Anthropologist 66, 103-114 (1964). <lb/>26. Audibert, N. &amp; Falk, S. Vowel space and f0 characteristics of infant-directed singing and speech. in <lb/>Proceedings of the 19th international conference on speech prosody 153-157 (2018). <lb/>27. Ratner, N. B. Phonological rule usage in mother-child speech. Journal of Phonetics 12, 245-254 (1984). <lb/>28. Kuhl, P. K. et al. Cross-language analysis of phonetic units in language addressed to infants. Science <lb/>277, 684-686 (1997). <lb/>29. Englund, K. T. &amp; Behne, D. M. Infant directed speech in natural interaction: Norwegian vowel quantity <lb/>and quality. Journal of Psycholinguistic Research 34, 259-280 (2005). <lb/>30. Fernald, A. &amp; Simon, T. Expanded intonation contours in mothers&apos; speech to newborns. Developmental <lb/>Psychology 20, 104-113 (1984). <lb/>31. Falk, S. &amp; Kello, C. T. Hierarchical organization in the temporal structure of infant-direct speech and <lb/>song. Cognition 163, 80-86 (2017). <lb/>32. Thiessen, E. D., Hill, E. A. &amp; Saffran, J. R. Infant-directed speech facilitates word segmentation. Infancy <lb/>7, 53-71 (2005). <lb/>33. Trainor, L. J. &amp; Desjardins, R. N. Pitch characteristics of infant-directed speech affect infants&apos; ability to <lb/>discriminate vowels. Psychonomic Bulletin &amp; Review 9, 335-340 (2002). <lb/>34. Werker, J. F. &amp; McLeod, P. J. Infant preference for both male and female infant-directed talk: A <lb/>developmental study of attentional and affective responsiveness. Canadian Journal of Psychology/Revue <lb/>Canadienne de Psychologie 43, 230-246 (1989). <lb/>35. Falk, D. Prelinguistic evolution in early hominins: Whence motherese? Behavioral and Brain Sciences <lb/>27, 491-502 (2004). <lb/>36. ManyBabies Consortium. Quantifying sources of variability in infancy research using the infant-directed-<lb/>speech preference. Advances in Methods and Practices in Psychological Science 3, 24-52 (2020). <lb/>37. Soley, G. &amp; Sebastian-Galles, N. Infants&apos; expectations about the recipients of infant-directed and adult-<lb/>directed speech. Cognition 198, 104214 (2020). <lb/>38. Henrich, J., Heine, S. J. &amp; Norenzayan, A. The weirdest people in the world? Behavioral and Brain <lb/>Sciences 33, 61-83 (2010). <lb/>39. Bowlby, J. Attachment and Loss (Vol. I: Attachment). (Basic Books, 1969). <lb/>40. Konner, M. The evolution of childhood: Relationships, emotion, mind. (Belknap Press of Harvard <lb/>University Press, 2010). <lb/>41. Grieser, D. L. &amp; Kuhl, P. K. Maternal speech to infants in a tonal language: Support for universal <lb/>prosodic features in motherese. Developmental Psychology 24, 14 (1988). <lb/>42. Fisher, C. &amp; Tokura, H. Acoustic cues to grammatical structure in infant-directed speech: Cross-linguistic <lb/>evidence. Child Development 67, 3192-3218 (1996). <lb/>43. Fernald, A. et al. A cross-language study of prosodic modifications in mothers&apos; and fathers&apos; speech to <lb/>preverbal infants. Journal of Child Language 16, 477-501 (1989). <lb/></listBibl>

			<page>27 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<listBibl>44. Broesch, T. L. &amp; Bryant, G. A. Prosody in infant-directed speech is similar across western and traditional <lb/>cultures. Journal of Cognition and Development 16, 31-43 (2015). <lb/>45. Farran, L. K., Lee, C.-C., Yoo, H. &amp; Oller, D. K. Cross-cultural register differences in infant-directed <lb/>speech: An initial study. PLoS ONE 11, (2016). <lb/>46. Broesch, T. &amp; Bryant, G. A. Fathers&apos; infant-directed speech in a small-scale society. Child Development <lb/>89, e29-e41 (2018). <lb/>47. Cristia, A., Dupoux, E., Gurven, M. &amp; Stieglitz, J. Child-directed speech is infrequent in a forager-farmer <lb/>population: A time allocation study. Child Development 90, 759-773 (2019). <lb/>48. Konner, M. Infancy among the Kalahari desert San. in Culture and Infancy: Variations in the Human <lb/>Experience (eds. Leiderman, H., Tulkin, S. R. &amp; Rosenfeld, A. H.) 287-328 (Academic Press, 1977). <lb/>49. Mehr, S. A. &amp; Krasnow, M. M. Parent-offspring conflict and the evolution of infant-directed song. <lb/>Evolution and Human Behavior 38, 674-684 (2017). <lb/>50. Cirelli, L. K. &amp; Trehub, S. E. Familiar songs reduce infant distress. Developmental Psychology (2020). <lb/>doi:10.1037/dev0000917 <lb/>51. Bainbridge, C. et al. Infants relax in response to unfamiliar foreign lullabies. PsyArXiv (2020). <lb/>doi:10.31234/osf.io/xcj52 <lb/>52. Corbeil, M., Trehub, S. E. &amp; Peretz, I. Singing delays the onset of infant distress. Infancy 21, 373-391 <lb/>(2016). <lb/>53. Cassidy, S. B. &amp; Driscoll, D. J. Prader-Willi syndrome. European Journal of Human Genetics 17, 3-13 <lb/>(2008). <lb/>54. Williams, C. A. et al. Angelman syndrome 2005: Updated consensus for diagnostic criteria. American <lb/>Journal of Medical Genetics 140, 413-418 (2006). <lb/>55. Mehr, S. A., Kotler, J., Howard, R. M., Haig, D. &amp; Krasnow, M. M. Genomic imprinting is implicated <lb/>in the psychology of music. Psychological Science 28, 1455-1467 (2017). <lb/>56. Kotler, J., Mehr, S. A., Egner, A., Haig, D. &amp; Krasnow, M. M. Response to vocal music in Angelman <lb/>syndrome contrasts with Prader-Willi syndrome. Evolution and Human Behavior 40, 420-426 (2019). <lb/>57. Trehub, S. E. Musical predispositions in infancy. Annals of the New York Academy of Sciences 930, <lb/>1-16 (2001). <lb/>58. Peretz, I. The nature of music from a biological perspective. Cognition 100, 1-32 (2006). <lb/>59. McDermott, J. &amp; Hauser, M. The origins of music: Innateness, uniqueness, and evolution. Music <lb/>Perception 23, 29-59 (2005). <lb/>60. Fan, S. et al. African evolutionary history inferred from whole genome sequence data of 44 indigenous <lb/>African populations. Genome Biology 20, 82 (2019). <lb/>61. Konner, M. Aspects of the developmental ethology of a foraging people. in Ethological Studies of Child <lb/>Behaviour (ed. Blurton Jones, N. G.) 285-304 (Cambridge University Press, 1972). <lb/>62. Marlowe, F. The Hadza hunter-gatherers of Tanzania. (University of California Press, 2010). <lb/>63. Trehub, S. E. &amp; Trainor, L. Singing to infants: Lullabies and play songs. Advances in Infancy Research <lb/>12, 43-78 (1998). <lb/>64. Trehub, S. E. et al. Mothers&apos; and fathers&apos; singing to infants. Developmental Psychology 33, 500-507 <lb/>(1997). <lb/>65. Trehub, S. E., Hill, D. S. &amp; Kamenetsky, S. B. Parents&apos; sung performances for infants. Canadian Journal <lb/>of Experimental Psychology 51, 385-396 (1997). <lb/>66. Boersma, P. W. Praat: Doing phonetics by computer. (2019). <lb/></listBibl>

			<page>28 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint <lb/></note>

			<listBibl>67. de Leeuw, J. R. jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. <lb/>Behavior Research Methods 47, 1-12 (2015). <lb/>68. Hartshorne, J. K., de Leeuw, J., Goodman, N., Jennings, M. &amp; O&apos;Donnell, T. J. A thousand studies for <lb/>the price of one: Accelerating psychological science with Pushkin. Behavior Research Methods 51, 1782-1803 <lb/>(2019). <lb/>69. Lartillot, O., Toiviainen, P. &amp; Eerola, T. A Matlab toolbox for music information retrieval. in Data <lb/>analysis, machine learning and applications (eds. Preisach, C., Burkhardt, H., Schmidt-Thieme, L. &amp; Decker, <lb/>R.) 261-268 (Springer Berlin Heidelberg, 2008). <lb/>70. Patel, A. D., Iversen, J. R. &amp; Rosenberg, J. C. Comparing the rhythm and melody of speech and music: <lb/>The case of British English and French. The Journal of the Acoustical Society of America 119, 3034 (2006). <lb/>71. Mertens, P. The prosogram: Semi-automatic transcription of prosody based on a tonal perception model. <lb/>in Speech Prosody 2004, International Conference (2004). <lb/>72. Friedman, J., Hastie, T. &amp; Tibshirani, R. Lasso and elastic-net regularized generalized linear models. <lb/>Rpackage version 2.0-5. (2016). <lb/>73. Yale, C. &amp; Forsythe, A. B. Winsorized regression. Technometrics 18, 291-300 (1976). <lb/>74. Salselas, I. &amp; Herrera, P. Development of perception and representation of rhythmic information: Towards <lb/>a computational model. in 2010 9th International Conference on Development and Learning (IEEE, 2010). <lb/>75. Arnal, L. H., Flinker, A., Kleinschmidt, A., Giraud, A.-L. &amp; Poeppel, D. Human screams occupy a <lb/>privileged niche in the communication soundscape. Current Biology 25, 2051-2056 (2015). <lb/>76. Diehl, R. L. Acoustic and auditory phonetics: The adaptive design of speech sound systems. Philosophical <lb/>Transactions of the Royal Society B: Biological Sciences 363, 965-978 (2007). <lb/>77. Nadeau, C. &amp; Bengio, Y. Inference for the generalization error. Machine Learning 52, 239-281 (2003). <lb/>78. Blumstein, D. T., Bryant, G. A. &amp; Kaye, P. The sound of arousal in music is context-dependent. Biology <lb/>Letters 8, 744-747 (2012). <lb/>79. Polka, L. &amp; Werker, J. F. Developmental changes in perception of nonnative vowel contrasts. Journal <lb/>of Experimental Psychology: Human Perception and Performance 20, 421-435 (1994). <lb/>80. Bertoncini, J., Bijeljac-Babic, R., Jusczyk, P. W., Kennedy, L. J. &amp; Mehler, J. An investigation of young <lb/>infants&apos; perceptual representations of speech sounds. Journal of Experimental Psychology: General 117, <lb/>21-33 (1988). <lb/>81. Werker, J. F. &amp; Lalonde, C. E. Cross-language speech perception: Initial capabilities and developmental <lb/>change. Developmental Psychology 24, 672 (1988). <lb/>82. Buyens, W., Moonen, M., Wouters, J. &amp; van Dijk, B. A model for music complexity applied to music <lb/>preprocessing for cochlear implants. in 2017 25th European Signal Processing Conference (EUSIPCO) 971-<lb/>975 (IEEE, 2017). <lb/>83. Ding, N. et al. Temporal modulations in speech and music. Neuroscience &amp; Biobehavioral Reviews 81, <lb/>(2017). <lb/></listBibl>

			<page>29 <lb/></page>

			<note place="headnote">. <lb/>CC-BY-NC-ND 4.0 International license <lb/>available under a <lb/>was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made <lb/>The copyright holder for this preprint (which <lb/>this version posted April 11, 2020. <lb/>; <lb/>https://doi.org/10.1101/2020.04.09.032995 <lb/>doi: <lb/>bioRxiv preprint </note>


	</text>
</tei>
