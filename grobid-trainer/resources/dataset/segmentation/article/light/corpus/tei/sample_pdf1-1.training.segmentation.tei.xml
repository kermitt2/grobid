<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>EXPLORATION BY RANDOM NETWORK DISTILLATION <lb/>Yuri Burda * <lb/>OpenAI <lb/>Harrison Edwards * <lb/>OpenAI <lb/>Amos Storkey <lb/>Univ. of Edinburgh <lb/>Oleg Klimov <lb/>OpenAI <lb/>ABSTRACT <lb/>We introduce an exploration bonus for deep reinforcement learning methods that <lb/>is easy to implement and adds minimal overhead to the computation performed. <lb/>The bonus is the error of a neural network predicting features of the observations <lb/>given by a fixed randomly initialized neural network. We also introduce a method <lb/>to flexibly combine intrinsic and extrinsic rewards. We find that the random <lb/>network distillation (RND) bonus combined with this increased flexibility enables <lb/>significant progress on several hard exploration Atari games. In particular we <lb/>establish state of the art performance on Montezuma&apos;s Revenge, a game famously <lb/>difficult for deep reinforcement learning methods. To the best of our knowledge, <lb/>this is the first method that achieves better than average human performance on this <lb/>game without using demonstrations or having access to the underlying state of the <lb/>game, and occasionally completes the first level. <lb/></front>

			<body>1 INTRODUCTION <lb/>Reinforcement learning (RL) methods work by maximizing the expected return of a policy. This <lb/>works well when the environment has dense rewards that are easy to find by taking random sequences <lb/>of actions, but tends to fail when the rewards are sparse and hard to find. In reality it is often <lb/>impractical to engineer dense reward functions for every task one wants an RL agent to solve. In <lb/>these situations methods that explore the environment in a directed way are necessary. <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>11 12 13 <lb/>14 <lb/>15 <lb/>16 <lb/>17 <lb/>18 <lb/>19-21 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>11 <lb/>12 <lb/>13 <lb/>14 <lb/>15 <lb/>16 <lb/>17 <lb/>18 <lb/>19 <lb/>20 <lb/>21 <lb/>Steps <lb/>Intrinsic reward <lb/>Figure 1: RND exploration bonus over the course of the first episode where the agent picks up the <lb/>torch (19-21). To do so the agent passes 17 rooms and collects gems, keys, a sword, an amulet, and <lb/>opens two doors. Many of the spikes in the exploration bonus correspond to meaningful events: losing <lb/>a life (2,8,10,21), narrowly escaping an enemy (3,5,6,11,12,13,14,15), passing a difficult obstacle <lb/>(7,9,18), or picking up an object (20,21). The large spike at the end corresponds to a novel experience <lb/>of interacting with the torch, while the smaller spikes correspond to relatively rare events that the <lb/>agent has nevertheless experienced multiple times. See here for videos. <lb/></body>

			<front> * Alphabetical ordering; the first two authors contributed equally. <lb/></front>

			<page>1 <lb/></page>

			<front>arXiv:1810.12894v1 [cs.LG] 30 Oct 2018 <lb/></front>

			<body>Recent developments in RL seem to suggest that solving the most challenging tasks (Silver et al., <lb/>2016; Zoph &amp; Le, 2016; Horgan et al., 2018; Espeholt et al., 2018; OpenAI, 2018; OpenAI et al., <lb/>2018) requires processing large numbers of samples obtained from running many copies of the <lb/>environment in parallel. In light of this it is desirable to have exploration methods that scale well <lb/>with large amounts of experience. However many of the recently introduced exploration methods <lb/>based on counts, pseudo-counts, information gain or prediction gain are difficult to scale up to large <lb/>numbers of parallel environments. <lb/>
			This paper introduces an exploration bonus that is particularly simple to implement, works well with <lb/>high-dimensional observations, can be used with any policy optimization algorithm, and is efficient <lb/>to compute as it requires only a single forward pass of a neural network on a batch of experience. <lb/>Our exploration bonus is based on the observation that neural networks tend to have significantly <lb/>lower prediction errors on examples similar to those on which they have been trained. This motivates <lb/>the use of prediction errors of networks trained on the agent&apos;s past experience to quantify the novelty <lb/>of new experience. <lb/>As pointed out by many authors, agents that maximize such prediction errors tend to get attracted <lb/>to transitions where the answer to the prediction problem is a stochastic function of the inputs. <lb/>For example if the prediction problem is that of predicting the next observation given the current <lb/>observation and agent&apos;s action (forward dynamics), an agent trying to maximize this prediction error <lb/>will tend to seek out stochastic transitions, like those involving randomly changing static noise on a <lb/>TV, or outcomes of random events such as coin tosses. This observation motivated the use of methods <lb/>that quantify the relative improvement of the prediction, rather than its absolute error. Unfortunately, <lb/>as previously mentioned, such methods are hard to implement efficiently. <lb/>We propose an alternative solution to this undesirable stochasticity by defining an exploration bonus <lb/>using a prediction problem where the answer is a deterministic function of its inputs. Namely we <lb/>predict the output of a fixed randomly initialized neural network on the current observation. <lb/>Atari games have been a standard benchmark for deep reinforcement learning algorithms since the <lb/>pioneering work by Mnih et al. (2013). Bellemare et al. (2016) identified among these games the hard <lb/>exploration games with sparse rewards: Freeway, Gravitar, Montezuma&apos;s Revenge, Pitfall!, Private <lb/>Eye, Solaris, and Venture. RL algorithms tend to struggle on these games, often not finding even a <lb/>single positive reward. <lb/>In particular, Montezuma&apos;s Revenge is considered to be a difficult problem for RL agents, requiring a <lb/>combination of mastery of multiple in-game skills to avoid deadly obstacles, and finding rewards that <lb/>are hundreds of steps apart from each other even under optimal play. Significant progress has been <lb/>achieved by methods with access to either expert demonstrations (Pohlen et al., 2018; Aytar et al., <lb/>2018; Garmulewicz et al., 2018), special access to the underlying emulator state (Tang et al., 2017; <lb/>Stanton &amp; Clune, 2018), or both (Salimans &amp; Chen, 2018). However without such aids, progress <lb/>on the exploration problem in Montezuma&apos;s Revenge has been slow, with the best methods finding <lb/>about half the rooms (Bellemare et al., 2016). For these reasons we provide extensive ablations of <lb/>our method on this environment. <lb/>We find that even when disregarding the extrinsic reward altogether, an agent maximizing the RND <lb/>exploration bonus consistently finds more than half of the rooms in Montezuma&apos;s Revenge. To <lb/>combine the exploration bonus with the extrinsic rewards we introduce a modification of Proximal <lb/>Policy Optimization (PPO, Schulman et al. (2017)) that uses two value heads for the two reward <lb/>streams. This allows the use of different discount rates for the different rewards, and combining <lb/>episodic and non-episodic returns. With this additional flexibility, our best agent often finds 22 out of <lb/>the 24 rooms on the first level in Montezuma&apos;s Revenge, and occasionally (though not frequently) <lb/>passes the first level. The same method gets state of the art performance on Venture and Gravitar. <lb/>2 METHOD <lb/>2.1 EXPLORATION BONUSES <lb/>Exploration bonuses are a class of methods that encourage an agent to explore even when the <lb/>environment&apos;s reward e t is sparse. They do so by replacing e t with a new reward r t = e t + i t , where <lb/>i t is the exploration bonus associated with the transition at time t. <lb/></body>

			<page>2 <lb/></page>

			<body>To encourage the agent to visit novel states, it is desirable for i t to be higher in novel states than <lb/>in frequently visited ones. Count-based exploration methods provide an example of such bonuses. <lb/>In a tabular setting with a finite number of states one can define i t to be a decreasing function <lb/>of the visitation count n t (s) of the state s. In particular i t = 1/n t (s) and i t = 1/ n t (s) have <lb/>been used in prior work (Bellemare et al., 2016; Ostrovski et al., 2018). In non-tabular cases it is <lb/>not straightforward to produce counts, as most states will be visited at most once. One possible <lb/>generalization of counts to non-tabular settings is pseudo-counts (Bellemare et al., 2016) which uses <lb/>changes in state density estimates as an exploration bonus. In this way the counts derived from the <lb/>density model can be positive even for states that have not been visited in the past, provided they are <lb/>similar to previously visited states. <lb/>An alternative is to define i t as the prediction error for a problem related to the agent&apos;s transitions. <lb/>Generic examples of such problems include forward dynamics (Schmidhuber, 1991b; Stadie et al., <lb/>2015; Achiam &amp; Sastry, 2017; Pathak et al., 2017; Burda et al., 2018) and inverse dynamics (Haber <lb/>et al., 2018). Non-generic prediction problems can also be used if specialized information about the <lb/>environment is available, like predicting physical properties of objects the agent interacts with (Denil <lb/>et al., 2016). Such prediction errors tend to decrease as the agent collects more experience similar <lb/>to the current one. For this reason even trivial prediction problems like predicting a constant zero <lb/>function can work as exploration bonuses (Fox et al., 2018). <lb/>2.2 RANDOM NETWORK DISTILLATION <lb/>This paper introduces a different approach where the prediction problem is randomly generated. <lb/>This involves two neural networks: a fixed and randomly initialized target network which sets the <lb/>prediction problem, and a predictor network trained on data collected by the agent. The target network <lb/>takes an observation to an embedding f : O → R k and the predictor neural network f : O → R k <lb/>is trained by gradient descent to minimize the expected MSE f (x; θ) -f (x) 2 with respect to its <lb/>parameters θ f . This process distills a randomly initialized neural network into a trained one. The <lb/>prediction error is expected to be higher for novel states dissimilar to the ones the predictor has been <lb/>trained on. <lb/>To build intuition we consider a toy model of this process on MNIST. We train a predictor neural <lb/>network to mimic a randomly initialized target network on training data consisting of a mixture of <lb/>images with the label 0 and of a target class, varying the proportion of the classes, but not the total <lb/>number of training examples. We then test the predictor network on the unseen test examples of <lb/>the target class and report the MSE. In this model the zeros are playing the role of states that have <lb/>been seen many times before, and the target class is playing the role of states that have been visited <lb/>infrequently. The results are shown in Figure 2. The figure shows that test error decreases as a <lb/>function of the number of training examples in the target class, suggesting that this method can be <lb/>used to detect novelty. Figure 1 shows that the intrinsic reward is high in novel states in an episode of <lb/>Montezuma&apos;s Revenge. <lb/>One objection to this method is that a sufficiently powerful optimization algorithm might find a <lb/>predictor that mimics the target random network perfectly on any input (for example the target <lb/>network itself would be such a predictor). However the above experiment on MNIST shows that <lb/>standard gradient-based methods don&apos;t overgeneralize in this undesirable way. <lb/>2.2.1 SOURCES OF PREDICTION ERRORS <lb/>In general, prediction errors can be attributed to a number of factors: <lb/>1. Amount of training data. Prediction error is high where few similar examples were seen by <lb/>the predictor (epistemic uncertainty). <lb/>2. Stochasticity. Prediction error is high because the target function is stochastic (aleatoric un-<lb/>certainty). Stochastic transitions are a source of such error for forward dynamics prediction. <lb/>3. Model misspecification. Prediction error is high because necessary information is missing, <lb/>or the model class is too limited to fit the complexity of the target function. <lb/>4. Learning dynamics. Prediction error is high because the optimization process fails to find a <lb/>predictor in the model class that best approximates the target function. <lb/></body>

			<page>3 <lb/></page>

			<body>Factor 1 is what allows one to use prediction error as an exploration bonus. In practice the prediction <lb/>error is caused by a combination of all of these factors, not all of them desirable. <lb/>For instance if the prediction problem is forward dynamics, then factor 2 results in the &apos;noisy-TV&apos; <lb/>problem. This is the thought experiment where an agent that is rewarded for errors in the prediction <lb/>of its forward dynamics model gets attracted to local sources of entropy in the environment. A TV <lb/>showing white noise would be such an attractor, as would a coin flip. <lb/>To avoid the undesirable factors 2 and 3, methods such as those by Schmidhuber (1991a); Oudeyer <lb/>et al. (2007); Lopes et al. (2012); Achiam &amp; Sastry (2017) instead use a measurement of how much <lb/>the prediction model improves upon seeing a new datapoint. However these approaches tend to be <lb/>computationally expensive and hence difficult to scale. <lb/>RND obviates factors 2 and 3 since the target network can be chosen to be deterministic and inside <lb/>the model-class of the predictor network. <lb/>2.2.2 RELATION TO UNCERTAINTY QUANTIFICATION <lb/>RND prediction error is related to an uncertainty quantification method introduced by Osband et al. <lb/>(2018). Namely, consider a regression problem with data distribution D = {x i , y i } i . In the Bayesian <lb/>setting we would consider a prior p(θ * ) over the parameters of a mapping f θ * and calculate the <lb/>posterior after updating on the evidence. <lb/>Let F be the distribution over functions g θ = f θ + f θ * , where θ * is drawn from p(θ * ) and θ is given <lb/>by minimizing the expected prediction error <lb/>θ = arg min <lb/>θ <lb/>E (xi,yi)∼D f θ (x i ) + f θ * (x i ) -y i <lb/>2 + R(θ), <lb/>(1) <lb/>where R(θ) is a regularization term coming from the prior (see Lemma 3, Osband et al. (2018)). <lb/>Osband et al. (2018) argue (by analogy to the case of Bayesian linear regression) that the ensemble F <lb/>is an approximation of the posterior. <lb/>If we specialize the regression targets y i to be zero, then the optimization problem <lb/>arg min θ E (xi,yi)∼D f θ (x i ) + f θ * (x i ) 2 is equivalent to distilling a randomly drawn function from <lb/>the prior. Seen from this perspective, each coordinate of the output of the predictor and target net-<lb/>works would correspond to a member of an ensemble (with parameter sharing amongst the ensemble), <lb/>and the MSE would be an estimate of the predictive variance of the ensemble (assuming the ensemble <lb/>is unbiased). In other words the distillation error could be seen as a quantification of uncertainty in <lb/>predicting the constant zero function. <lb/>2.3 COMBINING INTRINSIC AND EXTRINSIC RETURNS <lb/>In preliminary experiments that used only intrinsic rewards, treating the problem as non-episodic <lb/>resulted in better exploration. In that setting the return is not truncated at &quot;game over&quot;. We argue that <lb/>this is a natural way to do exploration in simulated environments, since the agent&apos;s intrinsic return <lb/>should be related to all the novel states that it could find in the future, regardless of whether they all <lb/>occur in one episode or are spread over several. It is also argued in (Burda et al., 2018) that using <lb/>episodic intrinsic rewards can leak information about the task to the agent. <lb/>We also argue that this is closer to how humans explore games. For example let&apos;s say Alice is playing <lb/>a videogame and is attempting a tricky maneuver to reach a suspected secret room. Because the <lb/>maneuver is tricky the chance of a game over is high, but the payoff to Alice&apos;s curiosity will be high <lb/>if she succeeds. If Alice is modelled as an episodic reinforcement learning agent, then her future <lb/>return will be exactly zero if she gets a game over, which might make her overly risk averse. The real <lb/>cost of a game over to Alice is the opportunity cost incurred by having to play through the game from <lb/>the beginning (which is presumably less interesting to Alice having played the game for some time). <lb/>However using non-episodic returns for extrinsic rewards could be exploited by a strategy that finds a <lb/>reward close to the beginning of the game, deliberately restarts the game by getting a game over, and <lb/>repeats this in an endless cycle. <lb/>It is not obvious how to estimate the combined value of the non-episodic stream of intrinsic rewards <lb/>i t and the episodic stream of extrinsic rewards e t . Our solution is to observe that the return is linear in <lb/></body>

			<page>4 <lb/></page>

			<body>Figure 2: Novelty detection on MNIST: a predic-<lb/>tor network mimics a randomly initialized target <lb/>network. The training data consists of varying <lb/>proportions of images from class &quot;0&quot; and a target <lb/>class. Each curve shows the test MSE on held out <lb/>target class examples plotted against the number <lb/>of training examples of the target class (log scale). <lb/>Figure 3: Mean episodic return and number <lb/>of rooms found by pure exploration agents on <lb/>Montezuma&apos;s Revenge trained without access <lb/>to the extrinsic reward. The agents explores <lb/>more in the non-episodic setting (see also Sec-<lb/>tion 2.3) <lb/>the rewards and so can be decomposed as a sum R = R E + R I of the extrinsic and intrinsic returns <lb/>respectively. Hence we can fit two value heads V E and V I separately using their respective returns, <lb/>and combine them to give the value function V = V E + V I . This same idea can also be used to <lb/>combine reward streams with different discount factors. <lb/>Note that even where one is not trying to combine episodic and non-episodic reward streams, or <lb/>reward streams with different discount factors, there may still be a benefit to having separate value <lb/>functions since there is an additional supervisory signal to the value function. This may be especially <lb/>important for exploration bonuses since the extrinsic reward function is stationary whereas the <lb/>intrinsic reward function is non-stationary. <lb/>2.4 REWARD AND OBSERVATION NORMALIZATION <lb/>One issue with using prediction error as an exploration bonus is that the scale of the reward can <lb/>vary greatly between different environments and at different points in time, making it difficult to <lb/>choose hyperparameters that work in all settings. In order to keep the rewards on a consistent scale <lb/>we normalized the intrinsic reward by dividing it by a running estimate of the standard deviations of <lb/>the intrinsic returns. <lb/>Observation normalization is often important in deep learning but it is crucial when using a random <lb/>neural network as a target, since the parameters are frozen and hence cannot adjust to the scale of <lb/>different datasets. Lack of normalization can result in the variance of the embedding being extremely <lb/>low and carrying little information about the inputs. To address this issue we use an observation <lb/>normalization scheme often used in continuous control problems whereby we whiten each dimension <lb/>by subtracting the running mean and then dividing by the running standard deviation. We then clip <lb/>the normalized observations to be between -5 and 5. We initialize the normalization parameters by <lb/>stepping a random agent in the environment for a small number of steps before beginning optimization. <lb/>We use the same observation normalization for both predictor and target networks but not the policy <lb/>network. <lb/>3 EXPERIMENTS <lb/>We begin with an intrinsic reward only experiment on Montezuma&apos;s Revenge in Section 3.1 to isolate <lb/>the inductive bias of the RND bonus, follow by extensive ablations of RND on Montezuma&apos;s Revenge <lb/>in Sections 3.2-3.4 to understand the factors that contribute to RND&apos;s performance, and conclude with <lb/>a comparison to baseline methods on 6 hard exploration Atari games in Section 3.6. For details of <lb/>hyperparameters and architectures we refer the reader to Appendices A.3 and A.4. Most experiments <lb/>are run for 30K rollouts of length 128 per environment with 128 parallel environments, for a total of <lb/>1.97 billion frames of experience. <lb/></body>

			<page>5 <lb/></page>

			<body>3.1 PURE EXPLORATION <lb/>In this section we explore the performance of RND in the absence of any extrinsic reward. In Section <lb/>2.3 we argued that exploration with RND might be more natural in the non-episodic setting. By <lb/>comparing the performance of the pure exploration agent in episodic and non-episodic settings we <lb/>can see if this observation translates to improved exploration performance. <lb/>We report two measures of exploration performance in Figure 3: mean episodic return, and the <lb/>number of rooms the agent finds over the training run. Since the pure exploration agent is not aware <lb/>of the extrinsic rewards or number of rooms, it is not directly optimizing for any of these measures. <lb/>However obtaining some rewards in Montezuma&apos;s Revenge (like getting the key to open a door) <lb/>is required for accessing more interesting states in new rooms, and hence we observe the extrinsic <lb/>reward increasing over time up to some point. The best return is achieved when the agent interacts <lb/>with some of the objects, but the agent has no incentive to keep doing the same once such interactions <lb/>become repetitive, hence returns are not consistently high. <lb/>We clearly see in Figure 3 that on both measures of exploration the non-episodic agent performs best, <lb/>consistent with the discussion in Section 2.3. The non-episodic setting with γ I = 0.999 explores <lb/>more rooms than γ I = 0.99, with one of the runs exploring 21 rooms. The best return achieved by 4 <lb/>out 5 runs of this setting was 6,700. <lb/>3.2 COMBINING EPISODIC AND NON-EPISODIC RETURNS <lb/>In Section 3.1 we saw that the non-episodic setting resulted in more exploration than the episodic <lb/>setting when exploring without any extrinsic rewards. Next we consider whether this holds in the case <lb/>where we combine intrinsic and extrinsic rewards. As discussed in Section 2.3 in order to combine <lb/>episodic and non-episodic reward streams we require two value heads. This also raises the question <lb/>of whether it is better to have two value heads even when both reward streams are episodic. In Figure <lb/>4 we compare episodic intrinsic rewards to non-episodic intrinsic rewards combined with episodic <lb/>extrinsic rewards, and additionally two value heads versus one for the episodic case. The discount <lb/>factors are γ I = γ E = 0.99. <lb/>(a) RNN policies <lb/>(b) CNN policies <lb/>Figure 4: Different ways of combining intrinsic and extrinsic rewards. Combining non-episodic <lb/>stream of intrinsic rewards with the episodic stream of extrinsic rewards outperforms combining <lb/>episodic versions of both steams in terms of number of explored rooms, but performs similarly in <lb/>terms of mean return. Single value estimate of the combined stream of episodic returns performs a <lb/>little better than the dual value estimate. The differences are more pronounced with RNN policies. <lb/>CNN runs are more stable than the RNN counterparts. <lb/>In Figure 4 we see that using a non-episodic intrinsic reward stream increases the number of rooms <lb/>explored for both CNN and RNN policies, consistent with the experiments in Section 3.1, but that the <lb/>difference is less dramatic, likely because the extrinsic reward is able to preserve useful behaviors. <lb/>We also see that the difference is less pronounced for the CNN experiments, and that the RNN results <lb/>tend to be less stable and perform worse for γ E = 0.99. <lb/>Contrary to our expectations (Section 2.3) using two value heads did not show any benefit over a <lb/>single head in the episodic setting. Nevertheless having two value heads is necessary for combining <lb/>reward streams with different characteristics, and so all further experiments use two value heads. <lb/></body>

			<page>6 <lb/></page>

			<body>Figure 5: Performance of different discount fac-<lb/>tors for intrinsic and extrinsic reward streams. A <lb/>higher discount factor for the extrinsic rewards <lb/>leads to better performance, while for intrinsic <lb/>rewards it hurts exploration. <lb/>Figure 6: Mean episodic return improves as the <lb/>number of parallel environments used for collect-<lb/>ing the experience increases for both the CNN <lb/>policy (left) and the RNN policy (right). The <lb/>runs have processed 0.5,2, and 16B frames. <lb/>3.3 DISCOUNT FACTORS <lb/>Previous experiments (Salimans &amp; Chen, 2018; Pohlen et al., 2018; Garmulewicz et al., 2018) <lb/>solving Montezuma&apos;s Revenge using expert demonstrations used a high discount factor to achieve <lb/>the best performance, enabling the agent to anticipate rewards far into the future. We compare the <lb/>performance of the RND agent with γ E ∈ {0.99, 0.999} and γ I = 0.99. We also investigate the <lb/>effect of increasing γ I to 0.999. The results are shown in Figure 5. <lb/>In Figure 5 we see that increasing γ E to 0.999 while holding γ I at 0.99 greatly improves performance. <lb/>We also see that further increasing γ I to 0.999 hurts performance. This is at odds with the results in <lb/>Figure 3 where increasing γ I did not significantly impact performance. <lb/>3.4 SCALING UP TRAINING <lb/>In this section we report experiments showing the effect of increased scale on training. The intrinsic <lb/>rewards are non-episodic with γ I = 0.99, and γ E = 0.999. <lb/>To hold the rate at which the intrinsic reward decreases over time constant across experiments with <lb/>different numbers of parallel environments, we downsample the batch size when training the predictor <lb/>to match the batch size with 32 parallel environments (for full details see Appendix A.4). Larger <lb/>numbers of environments results in larger batch sizes per update for training the policy, whereas <lb/>the predictor network batch size remains constant. Since the intrinsic reward disappears over time <lb/>it is important for the policy to learn to find and exploit these transitory rewards, since they act as <lb/>stepping-stones to nearby novel states. <lb/>Figure 6 shows that agents trained with larger batches of experience collected from more parallel <lb/>environments obtain higher mean returns after similar numbers of updates. They also achieve better <lb/>final performance. This effect seems to saturate earlier for the CNN policy than for the RNN policy. <lb/>We allowed the RNN experiment with 32 parallel environments to run for more time, eventually <lb/>reaching a mean return of 7,570 after processing 1.6 billion frames over 1.6 million parameter updates. <lb/>One of these runs visited all 24 rooms, and passed the first level once, achieving a best return of <lb/>17,500. The RNN experiment with 1024 parallel environments had mean return of 10,070 at the end <lb/>of training, and yielded one run with mean return of 14,415. <lb/>3.5 RECURRENCE <lb/>Montezuma&apos;s Revenge is a partially observable environment even though large parts of the game state <lb/>can be inferred from the screen. For example the number of keys the agent has appears on the screen, <lb/>but not where they come from, how many keys have been used in the past, or what doors have been <lb/>opened. To deal with this partial observability, an agent should maintain a state summarizing the past, <lb/>for example the state of a recurrent policy. Hence it would be natural to hope for better performance <lb/>from agents with recurrent policies. Contrary to expectations in Figure 4 recurrent policies performed <lb/>worse than non-recurrent counterparts with γ E = 0.99. However in Figure 6 the RNN policy with <lb/></body>

			<page>7 <lb/></page>

			<body>γ E = 0.999 outperformed the CNN counterpart at each scale 1 . Comparison of Figures 7 and 9 shows <lb/>that across multiple games the RNN policy outperforms the CNN more frequently than the other way <lb/>around. <lb/>3.6 COMPARISON TO BASELINES <lb/>In this section we compare RND to two baselines: PPO without an exploration bonus and an <lb/>alternative exploration bonus based on forward dynamics error. We evaluate RND&apos;s performance on <lb/>six hard exploration Atari games: Gravitar, Montezuma&apos;s Revenge, Pitfall!, Private Eye, Solaris, and <lb/>Venture. We first compare to the performance of a baseline PPO implementation without intrinsic <lb/>reward. For RND the intrinsic rewards are non-episodic with γ I = 0.99, while γ E = 0.999 for both <lb/>PPO and RND. The results are shown in Figure 7 for the RNN policy and summarized in Table 1 (see <lb/>also Figure 9 for the CNN policy). <lb/>Figure 7: Mean episodic return of RNN-based policies: RND, dynamics-based exploration method, <lb/>and PPO with extrinsic reward only on 6 hard exploration Atari games. RND achieves state of the art <lb/>performance on Gravitar, Montezuma&apos;s Revenge, and Venture, significantly outperforming PPO on <lb/>the latter two. <lb/>In Gravitar we see that RND does not consistently exceed the performance of PPO. However both <lb/>exceed average human performance with an RNN policy, as well as the previous state of the art. On <lb/>Montezuma&apos;s Revenge and Venture RND significantly outperforms PPO, and exceeds state of the art <lb/>performance and average human performance. On Pitfall! both algorithms fail to find any positive <lb/>rewards. This is a typical result for this game, as the extrinsic positive reward is very sparse. On <lb/>Private Eye RND&apos;s performance exceeds that of PPO. On Solaris RND&apos;s performance is comparable <lb/>to that of PPO. <lb/>Next we consider an alternative exploration bonus based on forward dynamics error. There are <lb/>numerous previous works using such a bonus (Schmidhuber, 1991b; Stadie et al., 2015; Achiam &amp; <lb/>Sastry, 2017; Pathak et al., 2017; Burda et al., 2018). Fortuitously Burda et al. (2018) show that <lb/>training a forward dynamics model in a random feature space typically works as well as any other <lb/>feature space when used to create an exploration bonus. This means that we can easily implement <lb/>an apples to apples comparison and change the loss in RND so the predictor network predicts the <lb/>random features of the next observation given the current observation and action, while holding fixed <lb/>all other parts of our method such as dual value heads, non-episodic intrinsic returns, normalization <lb/></body>

			<note place="footnote">1 The results in Figure 5 for the CNN policy were obtained as an average of 5 random seeds. When we ran <lb/>10 different seeds for the best performing setting for Figure 6 we found a large discrepancy in performance. This <lb/>discrepancy is likely explained by the fact that the distribution of results on Montezuma&apos;s Revenge dominated by <lb/>effects of discrete choices (such as going left or right from the first room), and hence contains a preponderance <lb/>of outliers. In addition, the results in Figure 5 were run with an earlier version of our code base and it is possible <lb/>that subtle differences between that version and the publicly released one have contributed to the discrepancy. <lb/>The results in Figure 6 were reproduced with the publicly released code and so we suggest that future work <lb/>compares against these results. <lb/></note>

			<page>8 <lb/></page>

			<body>schemes etc. This provides an ablation of the prediction problem defining the exploration bonus, <lb/>while also being representative of a class of prior work using forward dynamics error. Our expectation <lb/>was that these methods should be fairly similar except where the dynamics-based agent is able to <lb/>exploit non-determinism in the environment to get intrinsic reward. <lb/>Figure 7 shows that dynamics-based exploration performs significantly worse than RND with the <lb/>same CNN policy on Montezuma&apos;s Revenge, PrivateEye, and Solaris, and performs similarly on <lb/>Venture, Pitfall, and Gravitar. By analyzing agent&apos;s behavior at convergence we notice that in <lb/>Montezuma&apos;s Revenge the agent oscillates between two rooms. This leads to an irreducibly high <lb/>prediction error, as the non-determinism of sticky actions makes it impossible to know whether, once <lb/>the agent is close to crossing a room boundary, making one extra step will result in it staying in <lb/>the same room, or crossing to the next one. This is a manifestation of the &apos;noisy TV&apos; problem, or <lb/>aleatoric uncertainty discussed in Section 2.2.1. Similar behavior emerges in PrivateEye and Pitfall!. <lb/>In Table 1 the final training performance for each algorithm is listed, alongside the state of the art <lb/>from previous work and average human performance. <lb/>Gravitar Montezuma&apos;s Revenge Pitfall! PrivateEye Solaris Venture <lb/>RND <lb/>3,906 <lb/>8,152 <lb/>-3 <lb/>8,666 <lb/>3,282 <lb/>1,859 <lb/>PPO <lb/>3,426 <lb/>2,497 <lb/>0 <lb/>105 <lb/>3,387 <lb/>0 <lb/>Dynamics <lb/>3,371 <lb/>400 <lb/>0 <lb/>33 <lb/>3,246 <lb/>1,712 <lb/>SOTA <lb/>2,209 1 <lb/>3,700 2 <lb/>0 <lb/>15,806 2 12,380 1 1,813 3 <lb/>Avg. Human 3,351 <lb/>4,753 <lb/>6,464 <lb/>69,571 <lb/>12,327 1,188 <lb/>Table 1: Comparison to baselines results. Final mean performance for various methods. State of <lb/>the art results taken from: [1] (Fortunato et al., 2017) [2] (Bellemare et al., 2016) [3] (Horgan et al., <lb/>2018) <lb/>3.7 QUALITATIVE ANALYSIS: DANCING WITH SKULLS <lb/>By observing the RND agent, we notice that frequently once it obtains all the extrinsic rewards that <lb/>it knows how to obtain reliably (as judged by the extrinsic value function), the agent settles into a <lb/>pattern of behavior where it keeps interacting with potentially dangerous objects. For instance in <lb/>Montezuma&apos;s Revenge the agent jumps back and forth over a moving skull, moves in between laser <lb/>gates, and gets on and off disappearing bridges. We also observe similar behavior in Pitfall!. It might <lb/>be related to the very fact that such dangerous states are difficult to achieve, and hence are rarely <lb/>represented in agent&apos;s past experience compared to safer states. <lb/>4 RELATED WORK <lb/>Exploration. Count-based exploration bonuses are a natural and effective way to do exploration <lb/>(Strehl &amp; Littman, 2008) and a lot of work has studied how to tractably generalize count bonuses to <lb/>large state spaces (Bellemare et al., 2016; Fu et al., 2017; Ostrovski et al., 2017; Tang et al., 2017; <lb/>Machado et al., 2018; Fox et al., 2018). <lb/>Another class of exploration methods rely on errors in predicting dynamics (Schmidhuber, 1991b; <lb/>Stadie et al., 2015; Achiam &amp; Sastry, 2017; Pathak et al., 2017; Burda et al., 2018). As discussed in <lb/>Section 2.2, these methods are subject to the &apos;noisy TV&apos; problem in stochastic or partially-observable <lb/>environments. This has motivated work on exploration via quantification of uncertainty (Still &amp; <lb/>Precup, 2012; Houthooft et al., 2016) or prediction improvement measures (Schmidhuber, 1991a; <lb/>Oudeyer et al., 2007; Lopes et al., 2012; Achiam &amp; Sastry, 2017). <lb/>Other methods of exploration include adversarial self-play (Sukhbaatar et al., 2018), maximizing <lb/>empowerment (Gregor et al., 2017), parameter noise (Plappert et al., 2017; Fortunato et al., 2017), <lb/>identifying diverse policies (Eysenbach et al., 2018; Achiam et al., 2018), and using ensembles of <lb/>value functions (Osband et al., 2018; 2016; Chen et al., 2017). <lb/>Montezuma&apos;s Revenge. Early neural-network based reinforcement learning algorithms that were <lb/>successful on a significant portion of Atari games (Mnih et al., 2015; 2016; Hessel et al., 2017) failed <lb/></body>

			<page>9 <lb/></page>

			<body>to make meaningful progress on Montezuma&apos;s Revenge, not finding a way out of the first room <lb/>reliably. This is not necessarily a failure of exploration, as even a random agent finds the key in the <lb/>first room once every few hundred thousand steps, and escapes the first room every few million steps. <lb/>Indeed, a mean return of about 2,500 can be reliably achieved without special exploration methods <lb/>(Horgan et al., 2018; Espeholt et al., 2018; Oh et al., 2018). <lb/>Combining DQN with a pseudo-count exploration bonus Bellemare et al. (2016) set a new state of <lb/>the art performance, exploring 15 rooms and getting best return of 6,600. Since then a number of <lb/>other works have achieved similar performance (O&apos;Donoghue et al., 2017; Ostrovski et al., 2018; <lb/>Machado et al., 2018; Osband et al., 2018), without exceeding it. <lb/>Special access to the underlying RAM state can also be used to improve exploration by using it to <lb/>hand-craft exploration bonuses (Kulkarni et al., 2016; Tang et al., 2017; Stanton &amp; Clune, 2018). <lb/>Even with such access previous work achieves performance inferior to average human performance. <lb/>Expert demonstrations can be used effectively to simplify the exploration problem in Montezuma&apos;s <lb/>Revenge, and a number of works (Salimans &amp; Chen, 2018; Pohlen et al., 2018; Aytar et al., 2018; <lb/>Garmulewicz et al., 2018) have achieved performance comparable to or better than that of human <lb/>experts. Learning from expert demonstrations benefits from the game&apos;s determinism. The suggested <lb/>training method (Machado et al., 2017) to prevent an agent from simply memorizing the correct <lb/>sequence of actions is to use sticky actions (i.e. randomly repeating previous action) has not been <lb/>used in these works. In this work we use sticky actions and thus don&apos;t rely on determinism. <lb/>Random features. Features of randomly initialized neural networks have been extensively studied <lb/>in the context of supervised learning (Rahimi &amp; Recht, 2008; Saxe et al., 2011; Jarrett et al., 2009; <lb/>Yang et al., 2015). More recently they have been used in the context of exploration (Osband et al., <lb/>2018; Burda et al., 2018). The work Osband et al. (2018) provides motivation for random network <lb/>distillation as discussed in Section 2.2. <lb/>Vectorized value functions. Pong et al. (2018) find that a vectorized value function (with coordinates <lb/>corresponding to additive factors of the reward) improves their method. Bellemare et al. (2017) <lb/>parametrize the value as a linear combination of value heads that estimate probabilities of discretized <lb/>returns. However the Bellman backup equation used there is not itself vectorized. <lb/>5 DISCUSSION <lb/>This paper introduced an exploration method based on random network distillation and experimentally <lb/>showed that the method is capable of performing directed exploration on several Atari games with <lb/>very sparse rewards. These experiments suggest that progress on hard exploration games is possible <lb/>with relatively simple generic methods, especially when applied at scale. They also suggest that <lb/>methods that are able to treat the stream of intrinsic rewards separately from the stream of extrinsic <lb/>rewards (for instance by having separate value heads) can benefit from such flexibility. <lb/>We find that the RND exploration bonus is sufficient to deal with local exploration, i.e. exploring the <lb/>consequences of short-term decisions, like whether to interact with a particular object, or avoid it. <lb/>However global exploration that involves coordinated decisions over long time horizons is beyond <lb/>the reach of our method. <lb/>To solve the first level of Montezuma&apos;s Revenge, the agent must enter a room locked behind two <lb/>doors. There are four keys and six doors spread throughout the level. Any of the four keys can open <lb/>any of the six doors, but are consumed in the process. To open the final two doors the agent must <lb/>therefore forego opening two of the doors that are easier to find and that would immediately reward it <lb/>for opening them. <lb/>To incentivize this behavior the agent should receive enough intrinsic reward for saving the keys to <lb/>balance the loss of extrinsic reward from using them early on. From our analysis of the RND agent&apos;s <lb/>behavior, it does not get a large enough incentive to try this strategy, and only stumbles upon it rarely. <lb/>Solving this and similar problems that require high level exploration is an important direction for <lb/>future work. <lb/></body>

			<page>10 <lb/></page>

			<listBibl>REFERENCES <lb/>Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement <lb/>learning. arXiv:1703.01732, 2017. <lb/>Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery <lb/>algorithms. arXiv preprint arXiv:1807.10299, 2018. <lb/>Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, and Nando de Freitas. Playing <lb/>hard exploration games by watching YouTube. arXiv preprint arXiv:1805.11592, 2018. <lb/>Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. <lb/>Unifying count-based exploration and intrinsic motivation. In NIPS, 2016. <lb/>Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement <lb/>learning. arXiv preprint arXiv:1707.06887, 2017. <lb/>Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros. <lb/>Large-scale study of curiosity-driven learning. In arXiv:1808.04355, 2018. <lb/>Richard Y Chen, John Schulman, Pieter Abbeel, and Szymon Sidor. UCB and infogain exploration <lb/>via q-ensembles. arXiv:1706.01502, 2017. <lb/>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger <lb/>Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for <lb/>statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. <lb/>Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, and Nando de Fre-<lb/>itas. Learning to perform physics experiments via deep reinforcement learning. arXiv preprint <lb/>arXiv:1611.01843, 2016. <lb/>Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam <lb/>Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. IMPALA: Scalable distributed Deep-RL with <lb/>importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018. <lb/>Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: <lb/>Learning skills without a reward function. arXiv preprint, 2018. <lb/>Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, <lb/>Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. <lb/>Noisy networks for exploration. arXiv:1706.10295, 2017. <lb/>Lior Fox, Leshem Choshen, and Yonatan Loewenstein. Dora the explorer: Directed outreaching <lb/>reinforcement action-selection. International Conference on Learning Representations, 2018. <lb/>Justin Fu, John D Co-Reyes, and Sergey Levine. EX2: Exploration with exemplar models for deep <lb/>reinforcement learning. NIPS, 2017. <lb/>Michał Garmulewicz, Henryk Michalewski, and Piotr Miłoś. Expert-augmented actor-critic for <lb/>vizdoom and montezumas revenge. arXiv preprint arXiv:1809.03447, 2018. <lb/>Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. ICLR <lb/>Workshop, 2017. <lb/>Nick Haber, Damian Mrowca, Li Fei-Fei, and Daniel LK Yamins. Learning to play with intrinsically-<lb/>motivated self-aware agents. arXiv preprint arXiv:1802.07442, 2018. <lb/>Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan <lb/>Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in <lb/>deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017. <lb/>Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, <lb/>and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, <lb/>2018. <lb/></listBibl>

			<page>11 <lb/></page>

			<listBibl>Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME: <lb/>Variational information maximizing exploration. In NIPS, 2016. <lb/>Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture <lb/>for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pp. <lb/>2146-2153. IEEE, 2009. <lb/>Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015. <lb/>Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep <lb/>reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in <lb/>neural information processing systems, pp. 3675-3683, 2016. <lb/>Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-based <lb/>reinforcement learning by empirically estimating learning progress. In NIPS, 2012. <lb/>Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael <lb/>Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for <lb/>general agents. arXiv preprint arXiv:1709.06009, 2017. <lb/>Marlos C Machado, Marc G Bellemare, and Michael Bowling. Count-based exploration with the <lb/>successor representation. arXiv preprint arXiv:1807.11622, 2018. <lb/>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan <lb/>Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint <lb/>arXiv:1312.5602, 2013. <lb/>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-<lb/>mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, <lb/>Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, <lb/>Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. <lb/>Nature, 518(7540):529-533, February 2015. <lb/>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim <lb/>Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement <lb/>learning. In ICML, 2016. <lb/>Brendan O&apos;Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty Bellman <lb/>equation and exploration. arXiv preprint arXiv:1709.05380, 2017. <lb/>Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. arXiv preprint <lb/>arXiv:1806.05635, 2018. <lb/>OpenAI. OpenAI Five. https://blog.openai.com/openai-five/, 2018. <lb/>OpenAI, :, M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, <lb/>A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, <lb/>and W. Zaremba. Learning Dexterous In-Hand Manipulation. ArXiv e-prints, August 2018. <lb/>Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via <lb/>bootstrapped DQN. In NIPS, 2016. <lb/>Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement <lb/>learning. arXiv preprint arXiv:1806.03335, 2018. <lb/>Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Rémi Munos. Count-based exploration <lb/>with neural density models. arXiv:1703.01310, 2017. <lb/>Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Rémi Munos. Count-based exploration <lb/>with neural density models. International Conference for Machine Learning, 2018. <lb/>Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-<lb/>tonomous mental development. Evolutionary Computation, 2007. <lb/></listBibl>

			<page>12 <lb/></page>

			<listBibl>Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by <lb/>self-supervised prediction. In ICML, 2017. <lb/>Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, <lb/>Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. <lb/>arXiv:1706.01905, 2017. <lb/>Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, <lb/>Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Večerík, et al. Observe and look further: <lb/>Achieving consistent performance on Atari. arXiv preprint arXiv:1805.11593, 2018. <lb/>Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-<lb/>free deep RL for model-based control. arXiv preprint arXiv:1802.09081, 2018. <lb/>Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in <lb/>neural information processing systems, pp. 1177-1184, 2008. <lb/>Tim Salimans and Richard Chen. Learning Montezuma&apos;s Revenge from a single demonstration. <lb/>https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/, 2018. <lb/>Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. <lb/>On random weights and unsupervised feature learning. In ICML, pp. 1089-1096, 2011. <lb/>Jürgen Schmidhuber. Curious model-building control systems. In Neural Networks, 1991. 1991 IEEE <lb/>International Joint Conference on, pp. 1458-1463. IEEE, 1991a. <lb/>Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building <lb/>neural controllers. In Proceedings of the First International Conference on Simulation of Adaptive <lb/>Behavior, 1991b. <lb/>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy <lb/>optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. <lb/>David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, <lb/>Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, <lb/>Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine <lb/>Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with <lb/>deep neural networks and tree search. Nature, 529(7587):484-489, Jan 2016. ISSN 0028-0836. <lb/>doi: 10.1038/nature16961. <lb/>Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement <lb/>learning with deep predictive models. NIPS Workshop, 2015. <lb/>Christopher Stanton and Jeff Clune. Deep curiosity search: Intra-life exploration improves perfor-<lb/>mance on challenging deep reinforcement learning problems. arXiv preprint arXiv:1806.00553, <lb/>2018. <lb/>Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement <lb/>learning. Theory in Biosciences, 2012. <lb/>Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for <lb/>markov decision processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008. <lb/>Sainbayar Sukhbaatar, Ilya Kostrikov, Arthur Szlam, and Rob Fergus. Intrinsic motivation and <lb/>automatic curricula via asymmetric self-play. In ICLR, 2018. <lb/>Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, <lb/>Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep <lb/>reinforcement learning. In NIPS, 2017. <lb/>Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu <lb/>Wang. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer <lb/>Vision, pp. 1476-1483, 2015. <lb/>Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint <lb/>arXiv:1611.01578, 2016. <lb/></listBibl>

			<page>13 <lb/></page>

			<div type="annex">A APPENDIX <lb/>A.1 REINFORCEMENT LEARNING ALGORITHM <lb/>An exploration bonus can be used with any RL algorithm by modifying the rewards used to train the <lb/>model (i.e., r t = i t + e t ). We combine our proposed exploration bonus with a baseline reinforcement <lb/>learning algorithm PPO (Schulman et al., 2017). PPO is a policy gradient method that we have found <lb/>to require little tuning for good performance. For algorithmic details see Algorithm 1. <lb/>A.2 RND PSEUDO-CODE <lb/>Algorithm 1 gives an overall picture of the RND method. Exact details of the method can be found in <lb/>the code accompanying this paper. <lb/>Algorithm 1 RND pseudo-code <lb/>N ← number of rollouts <lb/>N opt ← number of optimization steps <lb/>K ← length of rollout <lb/>M ← number of initial steps for initializing observation normalization <lb/>t = 0 <lb/>Sample state s 0 ∼ p 0 (s 0 ) <lb/>for m = 1 to M do <lb/>sample a t ∼ Uniform(a t ) <lb/>sample s t+1 ∼ p(s t+1 |s t , a t ) <lb/>Update observation normalization parameters using s t+1 <lb/>t += 1 <lb/>end for <lb/>for i = 1 to N do <lb/>for j = 1 to K do <lb/>sample a t ∼ π(a t |s t ) <lb/>sample s t+1 , e t ∼ p(s t+1 , e t |s t , a t ) <lb/>calculate intrinsic reward i t = f (s t+1 ) -f (s t+1 ) 2 <lb/>add s t , s t+1 , a t , e t , i t to optimization batch B i <lb/>Update reward normalization parameters using i t <lb/>t += 1 <lb/>end for <lb/>Normalize the intrinsic rewards contained in B i <lb/>Calculate returns R I,i and advantages A I,i for intrinsic reward <lb/>Calculate returns R E,i and advantages A E,i for extrinsic reward <lb/>Calculate combined advantages A i = A I,i + A E,i <lb/>Update observation normalization parameters using B i <lb/>for j = 1 to N opt do <lb/>optimize θ π wrt PPO loss on batch B i , R i , A i using Adam <lb/>optimize θ f wrt distillation loss on B i using Adam <lb/>end for <lb/>end for <lb/>A.3 PREPROCESSING DETAILS <lb/>Table 2 contains details of how we preprocessed the environment for our experiments. We followed the <lb/>recommendations in Machado et al. (2017) in using sticky actions in order to make the environments <lb/>non-deterministic so that memorization of action sequences is not possible. In Table 3 we show <lb/>additional preprocessing details for the policy and value networks. In Table 4 we show additional <lb/>preprocessing details for the predictor and target networks. <lb/></div>

			<page>14 <lb/></page>

			<div type="annex">Hyperparameter <lb/>Value <lb/>Grey-scaling <lb/>True <lb/>Observation downsampling (84,84) <lb/>Extrinsic reward clipping <lb/>[-1, 1] <lb/>Intrinsic reward clipping <lb/>False <lb/>Max frames per episode <lb/>18K <lb/>Terminal on loss of life <lb/>False <lb/>Max and skip frames <lb/>4 <lb/>Random starts <lb/>False <lb/>Sticky action probability <lb/>0.25 <lb/>Table 2: Preprocessing details for the environments for all experiments. <lb/>Hyperparameter <lb/>Value <lb/>Frames stacked <lb/>4 <lb/>Observation <lb/>x → x/255 <lb/>normalization <lb/>Table 3: Preprocessing details for policy and <lb/>value network for all experiments. <lb/>Hyperparameter <lb/>Value <lb/>Frames stacked <lb/>1 <lb/>Observation <lb/>x → CLIP ((x -µ)/σ, [-5, 5]) <lb/>normalization <lb/>Table 4: Preprocessing details for target and pre-<lb/>dictor networks for all experiments. <lb/>A.4 PPO AND RND HYPERPARAMETERS <lb/>In Table 5 the hyperparameters for the PPO RL algorithm along with any additional hyperparameters <lb/>used for RND are shown. Complete details for how these hyperparameters are used can be found in <lb/>the code accompanying this paper. <lb/>Hyperparameter <lb/>Value <lb/>Rollout length <lb/>128 <lb/>Total number of rollouts per environment <lb/>30K <lb/>Number of minibatches <lb/>4 <lb/>Number of optimization epochs <lb/>4 <lb/>Coefficient of extrinsic reward <lb/>2 <lb/>Coefficient of intrinsic reward <lb/>1 <lb/>Number of parallel environments <lb/>128 <lb/>Learning rate <lb/>0.0001 <lb/>Optimization algorithm <lb/>Adam (Kingma &amp; Ba (2015)) <lb/>λ <lb/>0.95 <lb/>Entropy coefficient <lb/>0.001 <lb/>Proportion of experience used for training predictor <lb/>0.25 <lb/>γ E <lb/>0.999 <lb/>γ I <lb/>0.99 <lb/>Clip range <lb/>[0.9, 1.1] <lb/>Policy architecture <lb/>CNN <lb/>Table 5: Default hyperparameters for PPO and RND algorithms for experiments where applicable. <lb/>Any differences to these defaults are detailed in the main text. <lb/>Initial preliminary experiments with RND were run with only 32 parallel environments. We expected <lb/>that increasing the number of parallel environments would improve performance by allowing the <lb/>policy to adapt more quickly to transient intrinsic rewards. This effect could have been mitigated <lb/>however if the predictor network also learned more quickly. To avoid this situation when scaling <lb/>up from 32 to 128 environments we kept the effective batch size for the predictor network the <lb/>same by randomly dropping out elements of the batch with keep probability 0.25. Similarly in <lb/>our experiments with 256 and 1,024 environments we dropped experience for the predictor with <lb/>respective probabilities 0.125 and 0.03125. <lb/></div>

			<page>15 <lb/></page>

			<div type="annex">A.5 ARCHITECTURES <lb/>In this paper we use two policy architectures: an RNN and a CNN. Both contain convolutional <lb/>encoders identical of those in the standard architecture from (Mnih et al., 2015). The RNN architecture <lb/>additionally contains GRU (Cho et al., 2014) cells to capture longer contexts. The layer sizes of the <lb/>policies were chosen so that the number of parameters matches closely. The architectures of the <lb/>target and predictor networks also have convolutional encoders identical to the ones in (Mnih et al., <lb/>2015) followed by dense layers. Exact details are given in the code accompanying this paper. <lb/>A.6 ADDITIONAL EXPERIMENTAL RESULTS <lb/>Figure 8: Comparison of RND with a CNN policy with γ I = 0.99 and γ E = 0.999 with an <lb/>exploration defined by the reconstruction error of an autoencoder, holding all other choices constant <lb/>(e.g. using dual value, treating intrinsic return as non-episodic etc). The performance of the <lb/>autoencoder-based agent is worse than that of RND, but exceeds that of baseline PPO. <lb/>Figure 8 compares the performance of RND with an identical algorithm, but with the exploration <lb/>bonus defined as the reconstruction error of an autoencoder. The autoencoding task is similar in <lb/>nature to the random network distillation, as it also obviates the second (though not necessarily the <lb/>third) sources of prediction error from section 2.2.1. The experiment shows that the autoencoding <lb/>task can also be successfully used for exploration. <lb/>Figure 9 compares the performance of RND to PPO and dynamics prediction-based baselines for <lb/>CNN policies. <lb/>A.7 ADDITIONAL EXPERIMENTAL DETAILS <lb/>In Table 6 we show the number of seeds used for each experiment, indexed by figure. <lb/></div>

			<page>16 <lb/></page>

			<div type="annex">Figure 9: Mean episodic return of CNN-based policies: RND, dynamics-based exploration method, <lb/>and PPO with extrinsic reward only on 6 hard exploration Atari games. RND significantly outperforms <lb/>PPO on Montezuma&apos;s Revenge, Private Eye, and Venture. <lb/>Figure number Number of seeds <lb/>1 <lb/>NA <lb/>2 <lb/>10 <lb/>3 <lb/>5 <lb/>4 <lb/>5 <lb/>5 <lb/>10 <lb/>6 <lb/>5 <lb/>7 <lb/>3 <lb/>8 <lb/>5 <lb/>9 <lb/>5 <lb/>Table 6: The numbers of seeds run for each experiment is shown in the table. The results of each <lb/>seed are then averaged to provide a mean curve in each figure, and the standard error is used make <lb/>the shaded region surrounding each curve. <lb/></div>

			<page>17 </page>


	</text>
</tei>
