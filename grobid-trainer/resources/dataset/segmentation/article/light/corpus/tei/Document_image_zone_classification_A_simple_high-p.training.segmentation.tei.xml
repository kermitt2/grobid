<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<titlePage>See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221415986 <lb/>Document image zone classification : A simple high-performance approach <lb/>Conference Paper • January 2007 <lb/>Source: DBLP <lb/>CITATIONS <lb/>57 <lb/>READS <lb/>446 <lb/>3 authors: <lb/>Some of the authors of this publication are also working on these related projects: <lb/>Religion in Native American literature View project <lb/>Dictionary Learning and Sparse Coding View project <lb/>Daniel Keysers <lb/>Google Inc. <lb/>139 PUBLICATIONS 5,893 CITATIONS <lb/>SEE PROFILE <lb/>Faisal Shafait <lb/>University of Western Australia <lb/>195 PUBLICATIONS 6,153 CITATIONS <lb/>SEE PROFILE <lb/>Thomas Breuel <lb/>Google Inc. <lb/>260 PUBLICATIONS 6,183 CITATIONS <lb/>SEE PROFILE <lb/>All content following this page was uploaded by Daniel Keysers on 12 July 2014. <lb/>The user has requested enhancement of the downloaded file. <lb/></titlePage>

            <front>DOCUMENT IMAGE ZONE CLASSIFICATION <lb/> A Simple High-Performance Approach <lb/>Daniel Keysers, Faisal Shafait <lb/>German Research Center for Artificial Intelligence (DFKI) GmbH, Kaiserslautern, Germany <lb/>{daniel.keysers, faisal.shafait}@dfki.de <lb/>Thomas M. Breuel <lb/>Technical University of Kaiserslautern, Germany <lb/>tmb@informatik.uni-kl.de <lb/>Keywords: <lb/>Document Image Analysis, Zone Classification <lb/>Abstract: <lb/>We describe a simple, fast, and accurate system for document image zone classification -an important sub-<lb/>problem of document image analysis -that results from a detailed analysis of different features. Using <lb/>a novel combination of known algorithms, we achieve a very competitive error rate of 1.46% (n = 13811) <lb/>in comparison to (Wang et al., 2006) who report an error rate of 1.55% (n = 24177) using more complicated <lb/>techniques. The experiments were performed on zones extracted from the widely used UW-III database, which <lb/>is representative of images of scanned journal pages and contains ground-truthed real-world data. <lb/></front>

			<body>1 INTRODUCTION <lb/>One important subtask of document image processing <lb/>is the classification of blocks detected by the physical <lb/>layout analysis system into one of a set of predefined <lb/>classes. For example, we may want to distinguish be-<lb/>tween text blocks and drawings to pass the former to <lb/>an OCR system and the latter to an image enhancer. <lb/>For a detailed discussion of the task and its relevance <lb/>please see e.g. (Wang et al., 2006). <lb/>During the design of our block classification sys-<lb/>tem we noticed that among the approaches we found <lb/>in the literature a detailed comparison of different fea-<lb/>tures was usually not performed, and in particular we <lb/>did not find a comparison that included features as <lb/>they are typically used in other image classification or <lb/>retrieval tasks. In this paper we address this shortcom-<lb/>ing by comparing a large set of commonly used fea-<lb/>tures for block classification and include in the com-<lb/>parison three features that are known to yield good <lb/>performance in content-based image retrieval (CBIR) <lb/>and are applicable to binary images (Deselaers et al., <lb/>2004). Interestingly, we found that the single feature <lb/>with the best performance is the Tamura texture his-<lb/>togram, which belongs to this latter class. Another re-<lb/>sult we transfer from experience in the area of CBIR <lb/>is that often a histogram is a more powerful feature <lb/>than using statistics of a distribution like mean and <lb/>variance only. We show that the use of histograms im-<lb/>proves the performance for block classification signif-<lb/>icantly in our experiments. By combining a number <lb/>of different features, we achieve a very competitive <lb/>error rate of less than 1.5% on a data set of blocks ex-<lb/>tracted from the well-known University of Washing-<lb/>ton III (UW-III) database. In addition to the data used <lb/>in prior work we include a class of &apos;speckles&apos; blocks <lb/>that often occur during photocopying and for which a <lb/>correct classification can facilitate further processing <lb/>of a document image. Figure 1 shows example block <lb/>images for each of the eight types distinguished in our <lb/>approach. We also present a very fast (but at 2.1% er-<lb/>ror slightly less accurate) classifier, using simple fea-<lb/>tures and only a fraction of a second to classify one <lb/>block on average on a standard PC. <lb/>
            2 RELATED WORK AND <lb/>CONTRIBUTION <lb/>We briefly discuss some related work in this section, <lb/>for a more detailed overview of related work in the <lb/>field of document zone classification please refer to <lb/>(Okun et al., 1999; Wang et al., 2006). Table 1 shows <lb/>an overview of related results in zone classification. <lb/>Inglis and Witten (Inglis and Witten, 1995) <lb/>math <lb/>logo <lb/>text <lb/>table <lb/>drawing <lb/>halftone <lb/>ruling <lb/>speckles <lb/>Figure 1: Examples of document image block types distinguished in our approach. <lb/>Table 1: Summary of UW zone classification error rates from the literature along with the number of pages, zones and block <lb/>types used. Note that an exact comparison between all error rates is not possible. <lb/>reference <lb/># pages # zones # types error [%] <lb/>(Inglis and Witten, 1995) <lb/>1001 <lb/>13831 <lb/>3 <lb/>6.7 <lb/>(Liang et al., 1996) <lb/>979 <lb/>13726 <lb/>8 <lb/>5.4 <lb/>(Sivaramakrishnan et al., 1995) <lb/>979 <lb/>13726 <lb/>9 <lb/>3.3 <lb/>(Wang et al., 2000) <lb/>1600 <lb/>24177 <lb/>9 <lb/>2.5 <lb/>(Wang et al., 2006) <lb/>1600 <lb/>24177 <lb/>9 <lb/>1.5 <lb/>this work <lb/>713 <lb/>13811 <lb/>8 <lb/>1.5 <lb/>present a study of the zone classification problem <lb/>as a machine learning problem. They use 13831 <lb/>zones from the UW database and distinguish the three <lb/>classes text, halftone, and drawing. Using seven <lb/>features based on connected components and run <lb/>lengths, the authors apply various machine learning <lb/>techniques to the problem, of which the C4.5 decision <lb/>tree performs best at 6.7% error rate. <lb/>The review paper by Okun et al. (Okun et al., <lb/>1999) succinctly summarizes the main approaches <lb/>used for document zone classification in the 1990s. <lb/>The predominant feature type is based on connected <lb/>components (see also for example (Liang et al., <lb/>1996)) and run-length statistics. Other features used <lb/>include the cross-correlation between scan-lines, ver-<lb/>tical projection profiles, wavelet coefficients, learned <lb/>masks, and the black pixel distribution. The most <lb/>common classifier used is a neural network. <lb/>The widespread use of features based on con-<lb/>nected components run-length statistics, combined <lb/>with the simplicity of implementation of such fea-<lb/>tures, led us to use these feature types in our exper-<lb/>iments as well, comparing them to the use of features <lb/>used in content-based image retrieval. Our CBIR fea-<lb/>tures are based on the open source image retrieval <lb/>system FIRE (Deselaers et al., 2004). We restrict <lb/>our analysis for zone classification to those features <lb/>that are promising for the analysis of binary images <lb/>as described in the following section. (The overall <lb/>most successful features in CBIR are usually based <lb/>on color information.) <lb/>The most recent and detailed overview of the <lb/>progress in document zone classification and a very <lb/>accurate system is presented in (Wang et al., 2006). <lb/>The authors use a decision tree classifier and model <lb/>contextual dependencies for some zones. In our work <lb/>we do not model zone context, although it is likely <lb/>that a context model (which can be integrated in a <lb/>similar way as presented by Wang et al.) would help <lb/>the overall classification performance. Wang et al. <lb/>use 24177 zones extracted from the UW-III database <lb/>to evaluate their approach. In our experiments we <lb/>use only 11804 labeled zones (plus 2007 additional <lb/>zones of type &apos;speckles&apos;) extracted from the UW-III <lb/>database because many zones occur in different ver-<lb/>sions in the database. In Section 5 we further illus-<lb/>trate this shortcoming and our approach to overcome <lb/>it. As the authors use 9-fold cross-validation to obtain <lb/>their results, it might be possible that the error rates <lb/>they present (the best result is an overall error rate of <lb/>1.5%) may be influenced positively by this fact, be-<lb/>cause it is likely that instances of blocks of the same <lb/>document occur in training and test set. In a simi-<lb/>lar direction, Wang et al. use one feature that &quot;uses <lb/>a statistical method to classify glyphs and was exten-<lb/>sively trained on the UWCDROM-III document im-<lb/>age database.&quot; It is not clear to us if this implies that <lb/>the glyphs that occur in testing have also been used in <lb/>the training of the glyph classifier. <lb/>
            We expand on the work presented in (Wang et al., <lb/>2006) in the following ways: <lb/>• We include a detailed feature comparison includ-<lb/>ing a comparison with commonly used CBIR fea-<lb/>tures. It turns out that the single best feature is <lb/>the Tamura texture histogram which was not pre-<lb/>viously used for zone classification. <lb/>• We present results both for a simple nearest neigh-<lb/>bor classifier and for a very fast linear classifier <lb/>based on logistic regression and the maximum en-<lb/>tropy criterion. <lb/>• We introduce a new class of blocks containing <lb/>speckles that has not been labeled in the UW-III <lb/>database. This typical class of noise is important <lb/>to detect during the layout analysis especially for <lb/>images of photocopied documents. <lb/>• We present results for the part of the UW-III <lb/>database without using duplicates and achieve a <lb/>similar error rate of 1.5%. <lb/>• We introduce the use of histograms for the <lb/>measurements of connected components and run <lb/>lengths and show that this leads to a performance <lb/>increase. <lb/>3 FEATURE EXTRACTION <lb/>We extract the following features from each block, <lb/>where features 1-3 are chosen based on their perfor-<lb/>mance in CBIR (Deselaers et al., 2004) feature 4 was <lb/>expected to help distinguish between the types &apos;draw-<lb/>ing&apos; and &apos;text&apos; and features 5-9 were chosen based on <lb/>their common use in block classification (Okun et al., <lb/>1999; Wang et al., 2006). Due to space limitations we <lb/>refer the interested reader to the references for imple-<lb/>mentation details. <lb/>1. Tamura texture features histogram (TTFH) <lb/>2. Relational invariant feature histograms (RIFH) <lb/>3. Down-scaled images of size 32 × 32 (DSI) <lb/>4. The fill ratio, i.e. the ratio of the number of black <lb/>pixels in a horizontally smeared (Wong et al., <lb/>1982) image to the area of the image (FR) <lb/>5. Run-length histograms of black and white pixels <lb/>along horizontal, vertical, main diagonal, and side <lb/>diagonal directions; each histogram uses eight <lb/>bins, spaced apart as powers of 2, i.e. counting <lb/>runs of length ≤ 1, 3, 7, 15, 31, 63, 127 and ≥ 128 <lb/>(RL{B,W}{X,Y,M,S}H) <lb/>6. The vector formed by the total number, mean, <lb/>and variance of the runs of black and white pixels <lb/>along the horizontal, vertical, main diagonal, and <lb/>side diagonal directions as used in (Wang et al., <lb/>2006) (RL{B,W}{X,Y,M,S}V) <lb/>7. Histograms (as in 5) of the widths and heights of <lb/>connected components (CCXH, CCYH) <lb/>8. The joint distribution of the widths and heights of <lb/>connected components as a 2-dimensional 64-bin <lb/>histogram (CCXYH) <lb/>9. The histogram of the distances between a con-<lb/>nected component and its nearest neighbor com-<lb/>ponent (CCNNH) <lb/>4 CLASSIFICATION <lb/>To evaluate the various features, we use a simple near-<lb/>est neighbor classifier, that is, a test sample is clas-<lb/>sified into the class the closest training sample be-<lb/>longs to. The distance measures used are the Jensen-<lb/>Shannon divergence for histograms and the Euclidean <lb/>distance for all other features (Deselaers et al., 2004). <lb/>If different feature sets are combined, the overall dis-<lb/>tance is calculated as the weighted sum of the indi-<lb/>vidual normalized distances. The weights are pro-<lb/>portional to the inverse of the error rate of a particu-<lb/>lar feature. No tuning with respect to these weights <lb/>or with respect to the distance measures has been <lb/>performed. Although a k-nearest-neighbor approach <lb/>gives better results in many cases we only evaluated <lb/>the 1-nearest-neighbor classifier. The nearest neigh-<lb/>bor error rates are determined using leave-one-out <lb/>cross-validation. <lb/>The nearest neighbor classifier serves as a good <lb/>baseline classifier, although in many cases we can find <lb/>a more suitable classifier for a given task. As we con-<lb/>centrate on features in this paper, we did not test any <lb/>other classifiers. However, an important shortcoming <lb/>of the nearest neighbor classifier is its requirement on <lb/>computational resources. Both memory and run-time <lb/>can be prohibitive for some applications. To explore <lb/>a very fast approach with minimum requirements on <lb/>computational resources, we also trained a log-linear <lb/>classifier using the maximum entropy criterion (Key-<lb/>sers et al., 2002). The classification using this clas-<lb/>sifier can be obtained by computing a dot product of <lb/>the feature vector with a weight vector for each class <lb/>and choosing the maximum, and is thus very fast. <lb/>As only these weight vectors need to be stored, the <lb/>memory requirement is also minimal. Furthermore, <lb/>the maximum entropy approach yields a probabilistic <lb/>model, such that we obtain an estimate of the poste-<lb/>rior probability for each class. The maximum entropy <lb/>approach was evaluated on a regular 50/50 split of the <lb/>data into training and test set and thus only uses half <lb/>the amount of training data. The histograms were not <lb/>normalized for the maximum-entropy approach, but <lb/>the absolute numbers were used instead to allow the <lb/>classifier to utilize this additional information. <lb/>5 DATA SET <lb/>To evaluate our approach for document zone classifi-<lb/>cation, we use the University of Washington III (UW-<lb/>III) database (Guyon et al., 1997). The database con-<lb/>sists of 1600 English document images with bound-<lb/>ing boxes of 24177 homogeneous page segments or <lb/>blocks, which are manually labeled into different <lb/>classes depending on their contents, making the data <lb/>very suitable for evaluating a block classification sys-<lb/>tem, e.g. (Inglis and Witten, 1995; Wang et al., 2006). <lb/>The documents in the UW-III dataset are catego-<lb/>rized based on their degradation type as follows: <lb/>1. Direct scans of original English journals <lb/>2. Scans of first generation English journal photo-<lb/>copies <lb/>3. Scans of second or later generation English jour-<lb/>nal photocopies <lb/>Many of the documents in the dataset are dupli-<lb/>cated and differ sometimes only by the degradation <lb/>(a) E00D <lb/>(b) C000 <lb/>(c) E04A (photocopy) <lb/>(d) W033 (direct scan) <lb/>Figure 2: Example document pages from the UW-III database. Note that some documents, as shown on the right, occur in <lb/>different versions. For our experiments, we made sure that no such duplicates were used. <lb/>applied to them. This type of collection is useful <lb/>when one is evaluating a page segmentation algorithm <lb/>to see how well the algorithm performs when pho-<lb/>tocopy effect degradation is applied to a document. <lb/>However, the degradation introduced by photocopy-<lb/>ing a document does not affect the contents of a doc-<lb/>ument to a large extent. One such example can be <lb/>seen in Figure 2, where the same document is present <lb/>in the dataset four times (E04A, W033, S04A, W133, <lb/>two of them shown here). Although the photocopied <lb/>documents are darker than the corresponding direct <lb/>scans, the difference is not substantial. This dupli-<lb/>cation of documents tends to bias the evaluation re-<lb/>sults towards lower error rates when some of these <lb/>documents are used in training, while others are used <lb/>in testing. This effect seems to have been unnoticed <lb/>previously by some researchers who use the complete <lb/>dataset for the evaluation of their algorithms. <lb/>We decided to use a subset of the UW-III dataset <lb/>to avoid using duplicate documents. We chose doc-<lb/>uments in the scans from the first generation photo-<lb/>copies category because they were largest in number. <lb/>We use all the documents with prefixes A0, C0, D0, <lb/>IG, H0, J0, K0, E0, V0, I03, and I04. There are 713 <lb/>documents of this type. We extracted the ground-<lb/>truth zones and their labels from each of these 713 <lb/>documents. We observed that there were very few <lb/>examples of some of the zone types like &apos;seal&apos;, &apos;an-<lb/>nouncement&apos;, &apos;advertisement&apos;, etc. Therefore we se-<lb/>lected only those classes for evaluation that contained <lb/>at least ten example images. <lb/>One limitation of the UW-III ground-truth zones is <lb/>that they do not contain any example of noise regions, <lb/>i.e. regions that emerge from noise introduced dur-<lb/>ing the scanning or photocopy process. These regions <lb/>mostly consist of speckles and dots present along the <lb/>border of the document. Since such regions often ap-<lb/>pear in practice, it is important to detect such regions <lb/>as noise so that these can be removed from further <lb/>processing. The UW-III dataset images contain many <lb/>such regions but these are not labeled. In order to ex-<lb/>tract examples of such regions we used the page seg-<lb/>mentation algorithm from (Kise et al., 1998) to ex-<lb/>tract page segments. Then all the segments that did <lb/>not overlap with any of the ground-truth zones were <lb/>filtered out as examples of the noise zones. However <lb/>these contained both textual and non-textual noise. <lb/>Textual noise appears only along the left or the right <lb/>side of a document when the facing pages of a book <lb/>are scanned. Since these extraneous symbols cannot <lb/>be distinguished from the actual contents of the doc-<lb/>ument based on their appearance alone, we do not <lb/>consider examples of textual noise. Therefore we <lb/>only take examples of non-textual noise, i.e. speck-<lb/>les as noise class. The speckles heavily depend on the <lb/>degradation of the document and vary considerably <lb/>from the direct scan of a document to its first genera-<lb/>tion photocopy as can be seen in Figure 2. Therefore, <lb/>for the speckles class, we extracted examples from all <lb/>1600 documents of the UW-III database. The corre-<lb/>sponding number of examples used for each zone type <lb/>is included in Table 3. <lb/>6 EXPERIMENTAL RESULTS <lb/>Table 2 shows the error rates that the nearest neighbor <lb/>classifier achieves for each single feature along with <lb/>the dimensionality of the feature vectors and the av-<lb/>erage time used to compute the feature vector. (All <lb/>timing information is given for a standard PC with <lb/>1.8GHz AMD Athlon processor without special per-<lb/>formance tuning of the algorithms.) The last rows <lb/>show results for combined feature sets. <lb/>Table 2: Leave-one-out nearest neighbor error rates and ex-<lb/>traction run-times for each feature and for combinations. <lb/>feature <lb/># features extr.-time [s] error [%] <lb/>TTFH <lb/>512 <lb/>5.51 <lb/>3.4 <lb/>RIFH <lb/>512 <lb/>12.59 <lb/>7.8 <lb/>DSI <lb/>1024 <lb/>0.01 <lb/>8.1 <lb/>FR <lb/>1 <lb/>0.02 <lb/>27.3 <lb/>RLBXH <lb/>8 <lb/>0.01 <lb/>7.9 <lb/>RLWXH <lb/>8 <lb/>0.01 <lb/>5.1 <lb/>RLBYH <lb/>8 <lb/>0.01 <lb/>8.2 <lb/>RLWYH <lb/>8 <lb/>0.01 <lb/>5.6 <lb/>RLBMH <lb/>8 <lb/>0.01 <lb/>11.8 <lb/>RLWMH <lb/>8 <lb/>0.01 <lb/>6.6 <lb/>RLBSH <lb/>8 <lb/>0.01 <lb/>10.5 <lb/>RLWSH <lb/>8 <lb/>0.01 <lb/>6.2 <lb/>RLBXV <lb/>3 <lb/>0.01 <lb/>12.9 <lb/>RLWXV <lb/>3 <lb/>0.01 <lb/>9.7 <lb/>RLBYV <lb/>3 <lb/>0.01 <lb/>14.6 <lb/>RLWYV <lb/>3 <lb/>0.01 <lb/>12.1 <lb/>RLBMV <lb/>3 <lb/>0.01 <lb/>17.2 <lb/>RLWMV <lb/>3 <lb/>0.01 <lb/>12.6 <lb/>RLBSV <lb/>3 <lb/>0.01 <lb/>16.7 <lb/>RLWSV <lb/>3 <lb/>0.01 <lb/>12.2 <lb/>CCXH <lb/>8 <lb/>0.04 <lb/>14.5 <lb/>CCYH <lb/>8 <lb/>0.04 <lb/>14.9 <lb/>CCXYH <lb/>64 <lb/>0.04 <lb/>6.2 <lb/>CCNNH <lb/>8 <lb/>0.05 <lb/>19.0 <lb/>RL**V, constant weight <lb/>4.1 <lb/>RL**H, constant weight <lb/>1.8 <lb/>RL*, CC*, 1/error weight <lb/>1.5 <lb/>FR, RL*, CC*, 1/error weight <lb/>1.5 <lb/>TTFH, FR, RL*, CC*, 1/error weight <lb/>1.5 <lb/>RL*, CC*, logistic, 50/50 data split <lb/>2.1 <lb/>We can observe the following results: <lb/>• The Tamura texture feature is the single best fea-<lb/>ture but is more than 100 times slower to compute <lb/>than most other features. <lb/>• The use of as descriptors of the run-<lb/>lengths distribution leads to much lower error <lb/>rates than the use of number, mean, and variance. <lb/>The combination of these histograms alone leads <lb/>to a very good error rate of 1.8%. <lb/>• Interestingly, the use of the white (background) <lb/>runs for the computation of features consistently <lb/>leads to better results than the use of black (fore-<lb/>ground) runs. <lb/>• Among the run-lengths based features, those <lb/>based on the horizontal runs lead to the best er-<lb/>ror rates. <lb/>• The fill ratio as a single feature does not lead to <lb/>good results, which is not surprising as it consists <lb/>only of a single number. However, it is very use-<lb/>ful to distinguish drawings from text. This is how-<lb/>ever also achieved by using the distribution of the <lb/>white run lengths, such that the FR feature is not <lb/>part of the best observed feature set. <lb/>• By using a logistic classifier trained with the max-<lb/>imum entropy criterion (training time a few min-<lb/>utes, time for one classification in the order of a <lb/>few microseconds) on a feature set that is very <lb/>fast to extract, we can construct a zone type clas-<lb/>sifier that can classify more than five zones per <lb/>second even without performance tuning. At the <lb/>same time, the error rate is at 2.1% only slightly <lb/>higher than that of the best observed classifier. <lb/>Table 3 shows the frequency of misclassifications <lb/>between different classes of the best classifier. We can <lb/>observe that high recognition accuracy was achieved <lb/>for the text, ruling, speckles, math, halftone, and <lb/>drawing classes. However, our system failed to rec-<lb/>ognize logos correctly, and most of the logos were <lb/>misclassified as either text, or halftone/drawing. Note <lb/>that the accuracy rate for type &apos;logo&apos; in (Wang et al., <lb/>2006) is even lower at 0.0%. The reason for this ef-<lb/>fect is the very small number of samples for this class, <lb/>which on the other hand implies that it has only a very <lb/>small influence on the overall system error rate. Sim-<lb/>ilarly, the table detection accuracy was not high, and <lb/>about 21% of the tables were misclassified as text. <lb/>To visualize the errors made, we looked at the <lb/>nearest-neighbor images for each misclassified block. <lb/>Figure 3 shows some typical examples. It can be seen <lb/>that some of these images cannot be simply classified <lb/>correctly by using the block content alone, and even <lb/>humans are likely to make errors if they are asked to <lb/>classify these images. <lb/>7 CONCLUSION <lb/>From the analysis of the obtained results we can con-<lb/>clude that we can construct a very accurate classi-<lb/>fier based on run-lengths histograms alone. These <lb/>features are very easy to implement and fast to ex-<lb/>tract and thus should be part of any practical baseline <lb/>system. Interestingly, the distribution of the back-<lb/>ground runs is more important for document zone <lb/>classification than the distribution of the foreground <lb/>runs. Including a few more features based on run-<lb/>length and connected component measurements we <lb/>achieved a very competitive 1 error rate of below 1.5% <lb/>on zones extracted form the UW-III database without <lb/></body>

            <note place="footnote">1 For a comparison to our results also note that at most <lb/>0.2% (53/24177) of the error rate Wang et al. present is <lb/>caused by their distinction between the text classes of dif-<lb/></note>

            <body>Table 3: Contingency table showing the distribution of the classification of zones of a particular type in percent. (The total <lb/>number of errors equals 201 within 13811 tests.) The labels M, L, T, A, D, H, R, S correspond to the types math, logo, text, <lb/>table, drawing, halftone, ruling, and speckles, respectively. <lb/>M <lb/>L <lb/>T <lb/>A <lb/>D <lb/>H <lb/>R <lb/>S error [%] # samples <lb/>M 90.8 <lb/>0.0 <lb/>8.6 <lb/>0.0 <lb/>0.0 <lb/>0.6 <lb/>0.0 <lb/>0.0 <lb/>9.2 <lb/>476 <lb/>L <lb/>9.1 27.3 36.4 <lb/>0.0 <lb/>9.1 <lb/>9.1 <lb/>0.0 <lb/>9.1 <lb/>72.7 <lb/>11 <lb/>T <lb/>0.1 <lb/>0.0 99.8 <lb/>0.0 <lb/>0.0 <lb/>0.0 <lb/>0.0 <lb/>0.0 <lb/>0.2 <lb/>10450 <lb/>A <lb/>0.8 <lb/>0.0 20.7 68.6 <lb/>9.9 <lb/>0.8 <lb/>0.0 <lb/>0.0 <lb/>31.4 <lb/>121 <lb/>1.5 <lb/>0.3 <lb/>3.0 <lb/>5.5 86.0 <lb/>3.5 <lb/>0.0 <lb/>0.3 <lb/>14.0 <lb/>401 <lb/>H <lb/>0.0 <lb/>0.9 <lb/>0.0 <lb/>0.0 <lb/>9.7 86.7 <lb/>0.9 <lb/>1.8 <lb/>13.3 <lb/>113 <lb/>R <lb/>0.4 <lb/>0.0 <lb/>1.3 <lb/>0.0 <lb/>0.4 <lb/>0.0 96.1 <lb/>2.2 <lb/>3.9 <lb/>232 <lb/>S <lb/>0.1 <lb/>0.0 <lb/>0.5 <lb/>0.0 <lb/>0.1 <lb/>0.1 <lb/>0.0 99.4 <lb/>0.6 <lb/>2007 <lb/>the need for features based on glyphs or the Fourier <lb/>transform. By employing a fast logistic (log-linear) <lb/>classifier trained using the maximum entropy crite-<lb/>rion on these features, we arrived at a fast and ac-<lb/>curate, yet easy to implement overall classifier with <lb/>a slightly higher error rate of 2.1%. In our experi-<lb/>ments we did not use context information as done in <lb/>(Wang et al., 2006) and thus could keep the decision <lb/>rule very simple. However, context models are likely <lb/>to help in the overall classification and an inclusion <lb/>of our approach into Wang et al.&apos;s context model is <lb/>possible. Examining the errors made by the system <lb/>makes it seem likely that further improvements sig-<lb/>nificantly below the reached error rate may be difficult <lb/>to achieve without a significantly increased effort, for <lb/>example by using a dedicated sub-classifier to distin-<lb/>guish between text and table zones. <lb/></body>

			<div type="acknowledgement">ACKNOWLEDGEMENTS <lb/>We wish to thank Oleg Nagaitsev for help with the im-<lb/>plementation and Thomas Deselaers for making avail-<lb/>able the open source image retrieval system FIRE, <lb/>which provided us with the implementation of some <lb/>of the features used. This work was partially funded <lb/>by the BMBF (German Federal Ministry of Education <lb/>and Research), project IPeT (01 IW D03). <lb/></div>

			<listBibl>REFERENCES <lb/>Deselaers, T., Keysers, D., and Ney, H. (2004). Features for <lb/>image retrieval: A quantitative comparison. In DAGM <lb/>2004, Pattern Recognition, 26th DAGM Symposium, <lb/>volume 3175 of Lecture Notes in Computer Science, <lb/>pages 228-236, Tübingen, Germany. <lb/>Guyon, I., Haralick, R. M., Hull, J. J., and Phillips, I. T. <lb/>(1997). Data sets for OCR and document image un-<lb/></listBibl>

            <note place="footnote">ferent font-sizes and the class &apos;other&apos; with the remaining <lb/>classes. On the other hand, we add a new class &apos;speckles&apos;, <lb/>which is related to 0.15% (21/13811) error. <lb/></note>

            <listBibl>derstanding research. In Bunke, H. and Wang, P., <lb/>editors, Handbook of character recognition and doc-<lb/>ument image analysis, pages 779-799. World Scien-<lb/>tific, Singapore. <lb/>Inglis, S. and Witten, I. (1995). Document zone classifica-<lb/>tion using machine learning. In Proc Digital Image <lb/>Computing: Techniques and Applications, pages 631-<lb/>636, Brisbane, Australia. <lb/>Keysers, D., Och, F.-J., and Ney, H. (2002). Maximum en-<lb/>tropy and Gaussian models for image object recogni-<lb/>tion. In Pattern Recognition, 24th DAGM Symposium, <lb/>volume 2449 of Lecture Notes in Computer Science, <lb/>pages 498-506, Zürich, Switzerland. Springer. <lb/>Kise, K., Sato, A., and Iwata, M. (1998). Segmentation of <lb/>page images using the area Voronoi diagram. Com-<lb/>puter Vision and Image Understanding, 70(3):370-<lb/>382. <lb/>Liang, J., Phillips, I., Ha, J., and Haralick, R. (1996). Doc-<lb/>ument zone classification using the sizes of connected <lb/>components. In Proc. SPIE, volume 2660, Document <lb/>Recognition III, pages 150-157, San Jose, CA. <lb/>Okun, O., Doermann, D., and Pietikainen, M. (1999). Page <lb/>Segmentation and Zone Classification: The State of <lb/>the Art. Technical Report LAMP-TR-036, CAR-TR-<lb/>927, CS-TR-4079, University of Maryland, College <lb/>Park. <lb/>Sivaramakrishnan, R., Phillips, I. T., Ha, J., Subramanium, <lb/>S., and Haralick, R. M. (1995). Zone classification in <lb/>a document using the method of feature vector genera-<lb/>tion. In ICDAR &apos;95: Proceedings of the Third Interna-<lb/>tional Conference on Document Analysis and Recog-<lb/>nition (Volume 2), page 541ff. <lb/>Wang, Y., Haralick, R., and Phillips, I. (2000). Improve-<lb/>ment of zone content classification by using back-<lb/>ground analysis. In Fourth IAPR International Work-<lb/>shop on Document Analysis Systems (DAS2000). <lb/>Wang, Y., Phillips, I. T., and Haralick, R. M. (2006). Doc-<lb/>ument zone content classification and its performance <lb/>evaluation. Pattern Recognition, 39:57-73. <lb/>Wong, K. Y., Casey, R. G., and Wahl, F. M. (1982). Doc-<lb/>ument analysis system. IBM Journal of Research and <lb/>Development, 26(6):647-656. <lb/></listBibl>

			<body>Missclassified <lb/>Nearest <lb/>Missclassified <lb/>Nearest <lb/>image <lb/>neighbor <lb/>image <lb/>neighbor <lb/>math <lb/>text <lb/>ruling <lb/>speckles <lb/>math <lb/>text <lb/>logo <lb/>speckles <lb/>logo <lb/>halftone <lb/>table <lb/>text <lb/>text <lb/>math <lb/>text <lb/>speckles <lb/>table <lb/>drawing <lb/>drawing <lb/>table <lb/>drawing <lb/>halftone <lb/>drawing <lb/>halftone <lb/>drawing <lb/>speckles <lb/>halftone <lb/>drawing <lb/>table <lb/>text <lb/>speckles <lb/>text <lb/>ruling <lb/>speckles <lb/>speckles <lb/>text <lb/>Figure 3: Examples of misclassifications showing the misclassified image and its nearest neighbor from a different class. <lb/></body>

			View publication stats <lb/>View publication stats


	</text>
</tei>
