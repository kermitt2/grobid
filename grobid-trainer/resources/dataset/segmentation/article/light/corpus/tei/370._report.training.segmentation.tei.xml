<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<page>1 <lb/></page>

			<front>______________________________________________________________________________ <lb/>Network of Workstations Active Messages Target for <lb/>Ptolemy C Code Generation <lb/>by Patrick Warner <lb/>______________________________________________________________________________ <lb/>Memorandum No. UCB/ERL M97/8 <lb/>January 24, 1997 <lb/>Submitted to the Department of Electrical Engineering and Computer Science, University of Cal-<lb/>ifornia at Berkeley, in partial satisfaction of the requirements for the degree of Master of Science, <lb/>Plan II. <lb/>2 <lb/>Abstract: <lb/>Ptolemy is a software environment for simulating, prototyping, and synthesizing heterogeneous <lb/>systems. For studying network, parallel, and distributed systems, a Ptolemy Code Generation in <lb/>C (CGC) domain target using Active Messages (AM) on a Network of Workstations (NOW) <lb/>is a valuable tool. Active messages provide a low-overhead, portable communications protocol <lb/>that can take advantage of performance improvements in the underlying network. A NOW pro-<lb/>vides a parallel platform to study Ptolemy simulations on. This project implements a NOW AM <lb/>target for Ptolemy&apos;s CGC domain and explores the issues involved in creating appropriate NOW <lb/>Ptolemy systems. <lb/></front>

			<body>1. Introduction <lb/>Ptolemy is a software environment for simulating, prototyping, and synthesizing heterogeneous <lb/>systems[1]. Examples of such systems include an ATM network for transmitting video signals, a <lb/>Unix workstation and Digital Signal Processing (DSP) target board used for music tone genera-<lb/>tion, and a VHDL (VHSIC Hardware Description Language) model of a computer Central Pro-<lb/>cessing Unit interacting with serial data. Ptolemy has existing support for network, parallel, and <lb/>distributed systems through its use of dataflow modeling and scheduling. Ptolemy can model a <lb/>system with dataflow paradigms, then generate a schedule that will execute the system in a paral-<lb/>lel or concurrent manner. What is lacking in Ptolemy is a parallel platform on which to run the <lb/>systems. The ability to run Ptolemy systems on a parallel platform, such as Berkeley&apos;s Network <lb/>of Workstations (NOW) or any group of well networked workstations, would be valuable because <lb/>the systems would be observed on an actual parallel system (as opposed to running a parallel sim-<lb/>ulation on a single workstation), and simulations would take less time to run because of the <lb/>improved processing speed. <lb/>A NOW is a collection of stand-alone computers connected by a high-speed, low-latency, reliable <lb/>network. A NOW can serve as a parallel platform for computing massive programs. The idea <lb/>behind a NOW is to eventually create a distributed supercomputer on a building-wide scale. By <lb/>using a low-latency, reliable, fully-connected network model, a NOW interconnect mirrors that of <lb/>a massively parallel processor (MPP) computer. As Ptolemy schedulers can already map systems <lb/>on to multiple processors, using a NOW with Ptolemy becomes a matter of defining how the pro-<lb/>cessors will communicate with each other. A communication protocol matching that of a MPP <lb/>seems reasonable for a NOW, and AM is a proven MPP communication method[2]. <lb/>AM was originally developed strictly for MPP platforms as the Generic Active Messages (GAM) <lb/>specification [3]. The GAM library was restricted to SPMD (Single Process Multiple Data) pro-<lb/>grams [4] that required the same code image to execute on each computing node. As the uses for <lb/>GAM extended into more distributed, client-server computing (operating systems, file systems, <lb/>network RAM), the SPMD model became too restrictive. The resulting AM specification elimi-<lb/>nated the SPMD requirement and added an error model that had not existed in GAM. The <lb/>Ptolemy multiprocessor code generation facilities automatically produce a different code image <lb/>for each processor in a distributed system, making AM a more useful communication protocol on <lb/>NOW than GAM. The AM implementation on Berkeley&apos;s NOW seeks to provide low-level <lb/>access to the network hardware, an essential feature for effective parallel simulations. <lb/></body>

			<page>3 <lb/></page>

			<body>With all the components necessary for NOW interaction with Ptolemy in place, the issue becomes <lb/>how best to use these systems together. As mentioned briefly above, Ptolemy has existing support <lb/>for parallel and distributed platforms in its schedulers and code generation facilities. Ptolemy is <lb/>divided into models of computation called domains. Ptolemy&apos;s Code Generation (CG) domain <lb/>takes modeled systems and translates them into a program. The Ptolemy Code Generation in C <lb/>(CGC) domain generates C programs. The AM library is coded in C, so the CGC domain is an <lb/>appropriate place to integrate the NOW platform. A Target within the CGC domain defines how <lb/>the C code will be generated for a specific hardware platform. The solution for using Ptolemy and <lb/>a NOW together is a NOW AM (NOWam) target in the CGC domain. <lb/>The main motivation in creating a CGC NOWam target is to expand the uses of Ptolemy into <lb/>broader parallel and networking areas of research. With a parallel and distributed platform such <lb/>as a NOW to run on, parallel and distributed Ptolemy simulations can be observed executing on an <lb/>distributed system. The primary issue in implementing a CGC NOWam target is interprocess <lb/>communication (IPC). IPC is accomplished using the AM library, so the AM library&apos;s implemen-<lb/>tation itself becomes an important issue in CGC NOWam target development. <lb/>A reference implementation of AM over the User Datagram Protocol/Internet Protocol (UDP/IP) <lb/>(UDPAM) was created to test the AM specification. Although using UDP/IP doesn&apos;t conform to <lb/>AM&apos;s goal of direct network hardware access, it does provide a useful and portable reference for <lb/>testing purposes. Because of UDP/IPs computation overhead, bulk data transfers are more effi-<lb/>cient than a fine grain message passing approach common to MPP platforms. At the time of this <lb/>project, UDPAM is the only AM implementation available, so a bulk transfer technique for pass-<lb/>ing data is chosen. <lb/>The second part of this report discusses AM in more detail. Section 3 gives an overview of <lb/>Ptolemy, focusing on code generation details. The following part of the report details the imple-<lb/>mentation of the NOWam target. Section 5 discusses parallel simulations for the NOWam target, <lb/>and the results of running simulations on a NOW are discussed in Section 6. The final section <lb/>summarizes the findings of the project and proposes future work. <lb/>2. Active Messages <lb/>AM seek to minimize the latency associated with communication protocol overhead and maxi-<lb/>mize user-level application bandwidth. Research at the University of California at Berkeley <lb/>observed &quot;communication protocols such as TCP/IP add unnecessary overhead to the base hard-<lb/>ware cost&quot;[5]. As network hardware continues to advance into the area of multi-gigabit band-<lb/>widths and sub-microsecond switch latencies, the existing protocol software overheads will <lb/>dominate communication costs. AM provide a simple, portable, and general-purpose communi-<lb/>cations interface with direct application-network interactions that bypass the operating system on <lb/>high-performance implementations [2]. <lb/>AM have been shown to be a useful means for building high-performance communication proto-<lb/>cols, run-time environments, and message passing libraries on MPPs. In the original implementa-<lb/>tion of AM, GAM, all communication occurred within individual parallel processes with one <lb/>network port per process. The parallel program itself had to be SPMD, a restriction making cli-<lb/></body>

			<page>4 <lb/></page>

			<body>ent/server applications and more general communication impossible. The current AM specifica-<lb/>tion is no longer restricted to parallel SPMD applications, and is general enough to support such <lb/>applications as file systems, operating systems, client/server programs, peer/peer programs, and <lb/>parallel programs as well. <lb/>High-performance network hardware often uses an embedded processor or controller on the inter-<lb/>face device, allowing communication and computation to overlap. This overlap feature is <lb/>achieved by the embedded processor performing loads and stores from the communication host&apos;s <lb/>memory. The operating system is bypassed in this case, and the user-level application has a direct <lb/>connection to the network device using load and store operations. AM implementations seek to <lb/>take advantage of the embedded processor to provide the direct application to network device <lb/>channel. <lb/>The AM interface allows arbitrary serial and parallel processes to create multiple communication <lb/>endpoints [2]. The communication endpoints are completely independent and secure network <lb/>ports that resemble conventional Berkeley Unix sockets. Messages can be sent from any endpoint <lb/>to any other endpoint, with the restriction that they use a common tag for protection purposes. <lb/>Each endpoint has a message send pool, a message receive pool, a bulk transfer virtual memory <lb/>segment, an authentication tag for receiving messages, a handler table that translates indices into <lb/>handler functions, and a translation table that translates indices into global endpoint names. <lb/>A set of AM endpoints grouped together as a single unit for communication, synchronization, and <lb/>event management is referred to as a bundle. The components of a bundle is shown in Figure 1. <lb/>A bundle has (1) a collection of endpoints, (2) a thread synchronization variable, (3) an event <lb/>mask that selects which state or state transitions generate events to an AM application, and (4) an <lb/>access mode flag to indicate if concurrent access to the bundle or its endpoints is expected. <lb/>Figure 1: AM Bundle <lb/>The AM library is accessed through an ANSI C application programming interface (API), which <lb/>is detailed in [2]. The features of primary interest to the NOWam target are: <lb/>ea_t <lb/>This type designates an endpoint address (see above for endpoint description). <lb/>eb_t <lb/>Endpoint <lb/>Semaphore <lb/>Event Mask <lb/>Access Mode Flag <lb/>(1) <lb/>(4) <lb/>(3) <lb/>(2) <lb/>Bundle <lb/>Network <lb/></body>

			<page>5 <lb/></page>

			<body>This type designates an endpoint bundle address (see above for bundle description). <lb/>handler_t <lb/>This type designates a handler-table index (described above with endpoints). <lb/>AM_Init() <lb/>This function initializes the AM layer and must be called before any interface function is used. <lb/>AM_Terminate() <lb/>This function cleans up and releases system resources used by AM and is called when finished <lb/>using AM. <lb/>AM_Request4( <lb/>ea_t request_endpoint, /* endpoint sending request */ <lb/>int reply_endpoint, <lb/>/* index of endpoint sending reply */ <lb/>handler_t handler, /* index into destination endpoint&apos;s handler table */ <lb/>int arg0, int arg1, int arg2, int arg3) <lb/>This function sends 4 integers to the destination. The receiver invokes the message handler when <lb/>the request is received. The handler prototype is: void handler(void *token, int arg0, int arg1, int <lb/>arg2, int arg3) where token represents the sending endpoint and arg0-3 are the 4 integer argu-<lb/>ments. <lb/>AM_Reply4( <lb/>void *token, <lb/>handler_t handler, /* index into destination endpoint&apos;s handler table */ <lb/>int arg0, int arg1, int arg2, int arg3) <lb/>This function sends the reply message to the requesting endpoint responsible for invoking the <lb/>request handler making the reply. The receiver of the reply invokes a handler with the same pro-<lb/>totype as AM_Request4 above. <lb/>AM_RequestXferAsync4( <lb/>ea_t request_endpoint, /* endpoint sending request */ <lb/>int reply_endpoint, <lb/>/* index of endpoint sending reply */ <lb/>int dest_offset, <lb/>handler_t handler, /* index into destination endpoint&apos;s handler table */ <lb/>void *source_addr, <lb/>int nbytes, <lb/>int arg0, int arg1, int arg2, int arg3) <lb/>This function sends nbytes of contiguous data and then 4 integers to the destination. The receiver <lb/>invokes the message handler with a pointer to the virtual-memory segment offset by dest_offset <lb/>bytes, the number of data bytes transferred, and the 4 integers when the request is received. The <lb/>handler prototype is: void handler(void *token, void *buf, int nbytes, int arg0, int arg1, int arg2, <lb/>int arg3) where token represents the sending endpoint, buf is the virtual memory segment pointer <lb/>with offset, nbytes is the number of bytes sent, and arg0-3 are the 4 integer arguments. <lb/>AM_Poll(eb_t bundle) <lb/>This function services incoming active messages from all endpoints in the specified bundle. <lb/></body>

			<page>6 <lb/></page>

			<body>Communication between two endpoints is based on a request-reply model. An Active Message is <lb/>sent from an endpoint send pool with an AM_Request or AM_Reply, and received into an endpoint <lb/>receive pool. The message has an index that selects the handler function for the message from the <lb/>receiving endpoint&apos;s handler table. After AM_Poll is called, the request or reply handler is <lb/>invoked with the sent arguments. A bundle will handle multiple requests and replies on each <lb/>AM_Poll. After messages are sent or received, they are removed from their respective message <lb/>pools. <lb/>The above features will be used as the IPC mechanism in the NOWam target. With the IPC mech-<lb/>anism defined, the Ptolemy system itself, emphasizing the code generation facilities, must be <lb/>examined to produce the NOWam target. <lb/>3. Ptolemy Overview <lb/>As stated in Section 1, Ptolemy is a software environment for simulating, prototyping, and synthe-<lb/>sizing code for heterogeneous systems. Ptolemy uses an object-oriented framework (C++) to <lb/>build its systems. A collection of various C++ classes are used to create Ptolemy applications. <lb/>Using the base classes and paradigms of dataflow, higher-level constructs define Ptolemy&apos;s sched-<lb/>uling and code generation behavior. This section begins with a brief description of Ptolemy&apos;s <lb/>base classes, and concludes with an introduction to its scheduling and code generation functional-<lb/>ity. <lb/>The basic element of Ptolemy&apos;s modularity is the Block. The Block contains the go() method, <lb/>which defines the Block&apos;s behavior at run-time. Derived from Block, the Star class is the elemen-<lb/>tal module in Ptolemy implemented by user code. A Star performs some computation, such as a <lb/>single add or a complicated Fast Fourier Transform (FFT), or generates code to do so. A Galaxy, <lb/>also derived from Block, contains a collection of Stars and/or other Galaxies internally. Another <lb/>Block derivative, a Target, controls the execution of a Ptolemy application, or Universe. A Uni-<lb/>verse is a type of Galaxy that also inherits from class Runnable, which defines the execution and <lb/>simulation or code generation behavior of the Universe. <lb/>The model of computation for which Ptolemy code synthesis has been best defined is synchro-<lb/>nous dataflow (SDF), a special case of the dataflow model of computation developed by Dennis <lb/>[6]. SDF Stars produce and consume a constant number of data tokens at each invocation. <lb/>Because of this, the execution order and resource requirements of SDF Stars can be determined at <lb/>compile time. The SDF paradigm is used extensively in the definitions of Ptolemy&apos;s scheduling <lb/>and code generation facilities. <lb/>With both a Universe of functional blocks to be scheduled and a Target describing the layout and <lb/>behavior of the system for which code is to be generated, the Scheduler will: (a) determine which <lb/>Block invocation will execute on which processor (for multi-processor systems); (b) determine <lb/>the execution order of Stars on a processor; and (c) arrange the Stars&apos; execution order into stan-<lb/>dard control structures, such as nested loops. After the scheduling phase, each processing ele-<lb/>ment (single or multiple) is assigned scheduler-determined order of Blocks to execute. The Target <lb/>class supplies the Scheduler with IPC information, enabling scheduling and code synthesis. The <lb/>Scheduler uses IPC communication cost data to determine whether the cost is low enough to merit <lb/></body>

			<page>7 <lb/></page>

			<body>parallel execution. <lb/>The SDF model produces a graph that exposes the functional parallelism available in an algo-<lb/>rithm. The next step in multiprocessor scheduling is to construct an Acyclic Precedence Expan-<lb/>sion Graph (APEG) from the original SDF graph[7]. The APEG adds additional information such <lb/>as communication costs between graph nodes and node computation costs. With the application <lb/>APEG, a Ptolemy multiprocessor scheduler will map the graph nodes onto processors, taking into <lb/>account IPC costs and communication overlap capabilities of the Target <lb/>A Ptolemy Domain is made up of Blocks, Targets, and Schedulers that follow a common compu-<lb/>tational model. A &quot;computational model&quot; in this context is the operational semantics that govern <lb/>Block interaction. In code generation, a Domain&apos;s Blocks and Targets synthesize the same lan-<lb/>guage. A Scheduler uses a Domain&apos;s definition to help order Block execution. Domains with the <lb/>same computational model are subdivided further according to the language used. The CGC <lb/>domain, under which the NOWam target is created, is derived from the CG domain. The CG <lb/>domain obeys SDF semantics, allowing it to be scheduled at compile time. The relationship <lb/>between the CG Domain, CGC Subdomain and their Targets, Schedulers, and Stars is pictured in <lb/>Figure 2. <lb/>Figure 2: Ptolemy Domain and Subdomain Relationships <lb/>Code generation consists of two phases, scheduling and synthesis. The scheduling phase, <lb/>described briefly above, partitions functional operators for possible parallel execution and orders <lb/>Block execution on each target processor (single or multiple). Send and Receive Stars are spliced <lb/>into the partition graph for IPC. In the synthesis phase, the scheduler-ordered code segments <lb/>associated with each Block are stitched together. Most code generation work related to memory <lb/>allocation and symbol generation is processor-independent, allowing these facilities to be con-<lb/>tained in generic CG classes. <lb/>For a single processor CG Target, two scheduling choices are available, non-looping and looping. <lb/>The first option, non-looping, is a quick and efficient scheduling method for applications that <lb/>don&apos;t make abundant rate changes. The graph nodes are ordered according to Target resource <lb/>costs. If Blocks are invoked many times, the generated code can become massive, as each invoca-<lb/>tion of a Block results in its own piece of code being added to the overall program. Using looping <lb/>can result in dramatic code size reduction when the application makes large sample rate changes. <lb/>CGMultiTarget <lb/>Star <lb/>CGCNOWamTarget <lb/>Star <lb/>Star <lb/>Star <lb/>Star <lb/>CGCDomain <lb/>CGDomain <lb/>Scheduler <lb/>Scheduler <lb/>Scheduler <lb/></body>

			<page>8 <lb/></page>

			<body>Multiple invocations of a Block can be placed inside a loop, with appropriate rate changes taking <lb/>place between iterations. <lb/>Multiprocessor targets use the APEG described earlier to map graph nodes onto multiple proces-<lb/>sors. If IPC is ignored Hu&apos;s level-based list scheduling is used. This method is known a Highest <lb/>Levels First with Estimated Times list scheduling, and assigns nodes to processors when they are <lb/>ready to be executed and a processor is available, not taking communication cost into account. <lb/>Using IPC communication costs and allowing overlapping communication, Sih&apos;s dynamic level <lb/>(DL) scheduling[7] results in less communication because a node must have a high enough execu-<lb/>tion time to warrant the IPC overhead costs of transferring data between processors. This tech-<lb/>nique allows communication and computation to overlap, assuming dedicated communication <lb/>hardware is available. The final Scheduler, Sih&apos;s declustering scheduling[7], makes multiple iter-<lb/>ations over the graph, whereas Sih&apos;s DL scheduling makes a single pass, grouping nodes into <lb/>&quot;clusters&quot; and analyzing trade-offs between parallel execution and IPC overhead. <lb/>The base CG star class (CGStar) contains the methods shared by all code generation Stars. <lb/>CGStar consists of portholes, states, code blocks, a start() method, an initCode() method, a go() <lb/>method, a wrapup() method and an execTime() method. Portholes designate the inputs and out-<lb/>puts of the Star. States represent parameters that can be set by the user or internal memory states <lb/>needed in the generated code. Code blocks contain the target language and Star macro functions. <lb/>Macro functions include parameter value substitution, unique symbol generation with multiple <lb/>scopes, and state reference substitution [8]. The start() method is invoked prior to any scheduling <lb/>or memory allocation because it initializes any information that will affect these actions. The init-<lb/>Code() method is called before scheduling but after memory allocation. The go() method is called <lb/>by the Scheduler, synthesizing the code of the main loop. The wrapup() method places code out-<lb/>side of the main loop, after scheduling is done. The execTime() method returns a number that <lb/>estimates the time to complete one Star invocation. This information is used by the Scheduler for <lb/>determining the parallel execution of Stars within a Universe. <lb/>In review, Ptolemy applications, Universes, are made up of: <lb/>(a) elemental functions, Stars; <lb/>(b) collections of these functions and other collections, Galaxies; <lb/>(c) a scheduling procedure, Scheduler; <lb/>and (d) an execution host definition, Target. <lb/>A Universe is developed within a Domain, which describes how all the Universe&apos;s pieces interact <lb/>with one another. For a CG Domain, the Stars eventually produce the Domain&apos;s target language, <lb/>using their go() method. The Target definition designates how the code will be compiled and exe-<lb/>cuted, and how IPC will take place. The Scheduler communicates with the Stars (execTime) and <lb/>the Target (IPC mechanism) to determine the order of execution of all modules and any parallel <lb/>execution possible. After the scheduling is completed, the code is synthesized by the CGStar <lb/>methods, start(), initCode(), go(), and wrapup(). With the AM library described in section 2, and <lb/>Ptolemy&apos;s code generation facilities outlined here, a CGC NOWam Target can be implemented. <lb/>4. NOW AM CGC Target <lb/>In order to create a new CGC target in Ptolemy two major components are needed: the target def-<lb/></body>

			<page>9 <lb/></page>

			<body>inition and the IPC mechanism. The target definition itself describes how the code will be col-<lb/>lected, specifies and allocates resources, defines any needed platform initialization code, and <lb/>finally dictates how to compile and run the generated code. The IPC component consists of send <lb/>and receive actors for implementation. <lb/>The first step in creating a new target is to create a new instantiation of the Ptolemy Target class. <lb/>In Ptolemy development, new Stars are usually created using a ptlang file. Ptlang is a high-level <lb/>language which is preprocessed into C++ .h and .cc files for integration into the Ptolemy kernel. <lb/>Ptlang cannot be used for defining CG targets, so C++ must be coded directly. The existing CGC <lb/>target, CGCMultiTarget, was used as a model to code the CGCNOWamTarget from. A major dif-<lb/>ference in the resulting code is that CGCNOWamTarget inherits directly from the CGMultiTarget, <lb/>which models a fully-connected multiprocessor architecture that allows overlapping communica-<lb/>tion. CGCMultiTarget inherits from CGSharedBus, which being based on a shared bus architec-<lb/>ture, does not allow communication to overlap. The Berkeley NOW behaves as a fully-connected <lb/>architecture, so the CGMultiTarget design is the best suited for it. The .h and .cc source files for <lb/>the CGCNOWamTarget can be found in Appendix A. Because CGCNOWamTarget is a multipro-<lb/>cessor target, several additional design issues in Ptolemy are raised. <lb/>To support multiprocessor targets, a concept of parent-child target relationships is used [9]. The <lb/>parent target defines the IPC mechanism and resources to be shared by the children. A hierarchy <lb/>of child targets, which may themselves be complex heterogeneous multiprocessors or a single <lb/>processor, completes the multiprocessor target definition. The child targets manage resources <lb/>local to themselves. For the NOWam target, CGCNOWamTarget is the parent, and each proces-<lb/>sor(child) is represented by a default-CGC or Makefile-C target. <lb/>Sih&apos;s DL scheduling is a natural match for the CGCNOWamTarget. IPC costs are considered in <lb/>assigning APEG nodes to processors and DL scheduling allows communication and computation <lb/>to overlap. This technique fits in well with the AM practice of using dedicated network hardware. <lb/>Sih&apos;s DL scheduling makes a single-pass over the graph, taking IPC overheads and resource con-<lb/>straints into account to schedule the communications and computations. IPC communication <lb/>costs can be adjusted to have higher values for bulk transfers, and lowered when fast, dedicated <lb/>network hardware is present. <lb/>As discussed in Section 3, the Scheduler splices Send and Receive Stars into the code where IPC <lb/>takes place. It it up to the parent target in a multiprocessor target to define the Send and Receive <lb/>stars. The Send and Receive Stars dictate the underlying communication mechanism used in the <lb/>Target. For the CGCNOWamTarget, AM is used for the Send and Receive Stars. <lb/>The initial implementation of the NOWam target used a fine grain transfer approach, passing sin-<lb/>gle floating point values between processors. This is a strait-forward approach that integrates well <lb/>with the existing Ptolemy CG IPC architecture. At the time of this research UDPAM is the only <lb/>AM implementation available. With UDPAM, a bulk transfer method is more efficient because <lb/>the UDP/IP overhead is balanced out by sending large amounts of data with each UDP invocation. <lb/>Ptolemy does not currently support a matrix data type in the CGC domain, so a conditional go() <lb/>method was used. The code to send the data is inserted only once for each Send/Receive pair. <lb/></body>

			<page>10 <lb/></page>

			<body>The Send star uses the AM_RequestXferAsync4() function described in Section 2, to send its <lb/>data. AM_RequestXferAsync4() was used in lieu of AM_RequestXfer4 so that computation <lb/>could overlap the communication. AM_RequestXferAsync4() returns control to the calling pro-<lb/>cess immediately. The Receive Star includes the definition of the request handler function, and a <lb/>call to AM_Poll(). The request handler moves the received data from the endpoint virtual mem-<lb/>ory segment into a local data structure that the rest of the application can access. The AM_Wait() <lb/>function was initially used to block the Receive Star until new data arrived. Because each <lb/>AM_Poll() serves multiple endpoints within an AM bundle, the Send and Receive Stars in an <lb/>application would become unsynchronized when multiple Sends occurred before a Receive. To <lb/>correct this problem, a synchronizing while loop was used in the Receive star. This causes the <lb/>Receive to wait until its new data has arrived. Both the Send and Receive Stars were coded in <lb/>ptlang, Ptolemy&apos;s high-level description language. After being preprocessed, .h and .cc files were <lb/>produced for integration into the Ptolemy kernel. The .pl files for the CGCNOWamSend and <lb/>CGCNOWamReceive can be examined in Appendix B. <lb/>The data being passed in the CGCNOWamTarget applications is double floating point numbers. <lb/>For the original CGCNOWam target, a method was devised to pass single floating point values <lb/>efficiently. This technique will be important for future CGCNOWam implementations where fine <lb/>grain communication is acceptable, such as on the Myrinet LANai cards. There is no <lb/>AM_Request message that directly sends double floating point data. For this reason, an initial IPC <lb/>implementation used the AM_RequestI4() function, which sends a buffer of data. The double <lb/>floating point number was sent as a stream of bytes and retrieved using the ANSI C memcpy() <lb/>function. An improvement in performance can be achieved in AM by using the AM_Request4 <lb/>function, which sends the least amount of data (4 integers) in AM. A C union type was created to <lb/>transport a double floating point number as two integers. The type definition is: <lb/>typedef union ints_or_double { <lb/>int asInt[2]; <lb/>double asDouble; <lb/>} convert; <lb/>The data is sent as follows: <lb/>convert myData; <lb/>myData.asDouble = double_float; <lb/>AM_Request4(endpoint, reply_index, handler_index,myData.asInt[0], <lb/>myData.asInt[1], 0, 0); <lb/>The data is retrieved as follows: <lb/>void handler(void *token, int arg0, int arg1, int arg2, int arg3) <lb/>{ <lb/>convert temp; <lb/>temp.asInt[0] = arg0; <lb/>temp.asInt[1] = arg1; <lb/>Receive_double = temp.asDouble; <lb/>} <lb/>As UDPAM is the only AM implementation available at the time of this research, a bulk transfer <lb/></body>

			<page>11 <lb/></page>

			<body>method for sending data proves to be more efficient. The UPD/IP overhead results in simulations <lb/>over 10 times slower running in parallel than on a single workstation when passing single floating <lb/>point values. For the bulk transfer method, the data to be sent is first copied into the sending end-<lb/>point&apos;s virtual memory segment. AM_RequestXferAsync() is then invoked. On the receiving <lb/>side, the data is copied out of the virtual memory segment. A possible performance enhancement <lb/>would be to map the input and output data directly into the endpoint&apos;s virtual memory segments <lb/>using AM_SetSeg(), eliminating two potentially large data copies. The disadvantage to this <lb/>approach is that the local data segments might be corrupted by the application while the transfer is <lb/>taking place asynchronously. For this reason, the copies have been left in place. <lb/>UDPAM uses IP address/port pairs to create the unique AM global endpoint names. The CGC-<lb/>NOWamTarget takes a list of host names corresponding to the number of processors, or network <lb/>nodes, to be used in the Ptolemy application. The IP addresses of all the hosts are passed to the <lb/>Send and Receive Stars to be used by the AM endpoint and bundle initialization code. Each Send/ <lb/>Receive pair in the application is represented by two endpoints with the same port number. Each <lb/>child target in the CGCNOWamTarget has one bundle to handle all endpoint communication, syn-<lb/>chronization, and event management. Multiple bundles were tested, but found to cause too much <lb/>application overhead (each bundle creates a new thread to handle endpoint management). <lb/>The AM specification does not supply any mechanism for sharing endpoint names across applica-<lb/>tions. This capability is not included because global endpoint names will differ from implementa-<lb/>tion to implementation (an IP address/port pair for UDPAM, a processor number for an MPP). It <lb/>recommends the use of an external name server that stores endpoint names and supplies them to <lb/>applications requesting them. In order to eliminate the need for an external name server in the <lb/>CGCNOWamTarget, the UDPAM library was modified to use predefined ports in creating end-<lb/>points. This allowed the CGCNOWamTarget definition of global endpoint names as IP address/ <lb/>port pairs to be implemented without a name server. <lb/>With the CGCNOWamTarget implementation complete, appropriate Ptolemy simulations must be <lb/>developed to test its correctness and performance. <lb/>5. Parallel Simulations <lb/>Appropriate Ptolemy applications for a NOW are those where computation dominates communi-<lb/>cation. In order to ensure that the CGCNOWamTarget produced correct results, an existing <lb/>Ptolemy application with known results is desirable for testing. There are several existing <lb/>Ptolemy simulations developed to model DSP systems with large amounts of computation. FFTs <lb/>and Finite Impulse Response(FIR) filters involve performing complex math computations on <lb/>potentially large amounts of data. One such simulation, an up-sample system, accomplishes the <lb/>difficult task of converting signal sampling rates from 44.1 kHz to 48 kHz. The rate conversion is <lb/>performed in multiple stages, each stage of which can be mapped on to multiple processors. See <lb/>Appendix C for a detailed description of what the system does. The schematic is pictured in Fig-<lb/>ure 3. <lb/></body>

			<page>12 <lb/></page>

			<body>Figure 3: Up Sample Ptolemy Schematic <lb/>Manual scheduling was used for this simulation in order to create well-defined divisions for bulk <lb/>transfers. The simulation was tested with only one bulk transfer taking place between processor 1 <lb/>and processor 2, one between processor 2 and processor 3, and one between processor 3 and pro-<lb/>cessor 4. Sending all the data at once is anticipated to make the UDP/IP overhead insignificant. <lb/>Alternate schedules were also produced in order to compare the different scheduling algorithms. <lb/>Although Ptolemy&apos;s built-in schedulers were able to produce schedules in most cases, the com-<lb/>puter&apos;s memory was exhausted before code could be generated. Because of the lack of source <lb/>code to compile and test, the respective scheduler&apos;s executions could not be compared, but their <lb/>schedules could (see the next section, Results). <lb/>When manual scheduling is used, Ptolemy follows a one Star on one processor rule. This limits <lb/>parallelism that may be available by spreading Stars across multiple processors. Each of four pro-<lb/>cessors was assigned one rate Galaxy to execute, and all Stars leading up to one of the respective <lb/>graph Stars, labeled with frequency values on the schematic pictured in Figure 3. The internals of <lb/>the rate Galaxy are pictured in Figure 4. <lb/>Figure 4: Rate Ptolemy Schematic <lb/>DB <lb/>FFTCx <lb/>DB <lb/>FFTCx <lb/>DB <lb/>FFTCx <lb/>Xgraph <lb/>Xgraph <lb/>Xgraph <lb/>Gain <lb/>Gain <lb/>Gain <lb/>Impulse <lb/>rateChange <lb/>rateChange <lb/>rateChange <lb/>rateChange <lb/>DB <lb/>FFTCx <lb/>Xgraph <lb/>Gain <lb/>Chop <lb/>Chop <lb/>Chop <lb/>Chop <lb/>4:3 <lb/>5:7 <lb/>4:7 <lb/>48 <lb/>117.6 <lb/>84 <lb/>44.1 <lb/>Sample Rate Conversion <lb/>44.1 kHz -48 kHz (160:147) <lb/>impulse <lb/>2:1 <lb/>88.2 <lb/>DFT of the <lb/>impulse response <lb/>use edit-comment <lb/>for documentation <lb/>FIR <lb/>UpSample <lb/>XMgraph <lb/>UpSample <lb/>Gain <lb/>Fork <lb/>DB <lb/>DB <lb/>DB <lb/>FFTCx <lb/>WaveForm <lb/>Fork <lb/>Chop <lb/>Chop <lb/>blockFFT <lb/>blockFFT <lb/>Rational Sample Rate Change <lb/>plot signals on common <lb/>frequency scale <lb/></body>

			<page>13 <lb/></page>

			<body>The work in each of the rate Galaxies is not exactly equal due to the different sampling rates. <lb/>Each processor does execute one FIR filter and four FFTs, but these computations differ in the <lb/>number of data samples operated on and in the order of the FFTs. In looking at the schedule, an <lb/>equal number of FFTs with equal execution times appear on each processor. The difference in <lb/>workload is most obvious in counting the number of FIR filters executed by each processor. Pro-<lb/>cessor 1 executes 147 FIR filters, processor 2 executes 98 FIR filters, processor 3 executes 56 FIR <lb/>filters, and processor 4 executes 40 FIR filters. These differences are due to the different sampling <lb/>rates of each rate Galaxy. Refer to Appendix C for more information on this simulation. Examin-<lb/>ing the information from the schedule, processor 1 is observed to do the most amount of work. A <lb/>benefit of the up-sample system is that all of the processors can overlap computation immediately. <lb/>Within each of the rate Galaxies, a wave signal is passed into an FFT. Before the last 3 processors <lb/>wait to receive data, they can work on this part of the simulation while the first processor prepares <lb/>to send the data. Unlike the other processors, processor 1 does not have to wait for data to arrive <lb/>to start computing. This fact may help balance out processor 1&apos;s heavier workload. <lb/>For a two processor test, the first two stages of the up-sample simulation were used. The simula-<lb/>tion was also scheduled manually, in order to provide a better comparison with the four processor <lb/>simulation. Because of the two processor test&apos;s smaller size, the built-in Ptolemy schedulers may <lb/>have been able to generate code. But the manual schedule makes the overall experiment more <lb/>uniform, so it is used. The same division for processor 1 and 2 is used for the two processor test, <lb/>where one bulk transfer is made between processor 1 and 2. An attempt was made to schedule all <lb/>four stages of the up-sample simulation on just two processors, but the generated code was too <lb/>large and exhausted the computer&apos;s memory during compilation. <lb/>A side-effect of using manual scheduling was non-looping code being generated for each proces-<lb/>sor. As discussed in Section 3, non-looping code leads to simulations with changing sampling <lb/>rates to generating code for every invocation of a Star, resulting in massive code explosion. As an <lb/>example, the WaveForm Star inside the rate Galaxy is invoked 256 times. Rather than a loop with <lb/>256 iterations, the generated code contains 256 instances of the WaveForm code. This code <lb/>explosion is what caused the compiler to run out of memory when compiling all four stages of the <lb/>up-sample simulation on two processors. A solution to this problem is to use Ptolemy&apos;s hierarchi-<lb/>cal scheduling[10]. This scheduler allows one of the three multiprocessor schedulers discussed in <lb/>Section 3 to be used as a top-level scheduler. For each child of the multiprocessor target, a single <lb/>processor looping scheduler is used to reduce code explosion. The effects of using this schedul-<lb/>ing technique is left to future research. <lb/>In order to determine the maximum speedup possible for the simulation, information from the <lb/>Ptolemy schedule can be used. Speedup is defined as the time it takes the program to execute on a <lb/>single processor divided by the total elapsed time to execute the program in parallel[4]. A graph <lb/>with node computation costs and communication costs is pictured in Figure 5. Each node in the <lb/>graph is assigned to a processor, taking into account communication delays and delays resulting <lb/>from scheduling. The scheduling delays result when a node must wait for another node to com-<lb/>plete before it can compute. The schedule, including communication delay, is shown in Figure 6. <lb/>The time to execute the program in serial is calculated by adding up the node computation times, <lb/>2 + 1 + 2 + 4 + 2 + 2 + 2 + 2 + 4 = 21. The parallel time is the total time over the breadth of the <lb/>graph, or makespan, 10. The speedup is calculated by taking the time to execute the program in <lb/></body>

			<page>14 <lb/></page>

			<body>serial divided by the makespan, 21 / 10 = 2.1. <lb/>Figure 5: Example APEG <lb/>Figure 6: Example Multiprocessor Schedule <lb/>Another way to calculate the maximum speedup is to determine how much of the parallel time all <lb/>processors are being used, reported the Ptolemy schedule as the utilization. For the schedule in <lb/>Figure 6, processor 1 has a 8 / 10 = 80% utilization, processor 2 has a 5 / 10 = 50% utilization, and <lb/>processor 3 has a 8 / 10 = 80% utilization. The total for all 3 processors is (80% + 50% + 80%) / <lb/>3 = 70%. Without any communication or scheduling delays, the maximum speedup would be <lb/>equal to the number of processors, 3. With communication and scheduling delays accounted for, <lb/>the maximum speedup is .7 x 3 = 2.1. <lb/>6. Results <lb/>The use of manual scheduling for the multiprocessor simulations resulted in non-looping code <lb/>being generated. For a fair comparison, the single workstation code was initially generated using <lb/>a non-looping schedule, which resulted in a huge C source file (191736 lines). This code was not <lb/>able to be compiled because the computer&apos;s memory was exhausted. Instead, a looping schedule <lb/>was used for the single processor code, which resulted in a 65 times reduction in code size (2967 <lb/>lines). Timing measurements were made, and the results for the two and four processor simula-<lb/>tions are presented below. <lb/>The resulting schedule for four processors reports that 74% utilization is achieved, showing that <lb/>the processors are computing 74% of the total execution time, and idle for 26% of the time. Tak-<lb/>ing this into account, a maximum speedup of .74 x 4 = 2.96 is theoretically possible for the four <lb/>processor simulation. <lb/>The two processor simulation schedule shows 84% utilization, signifying a 1.68 maximum <lb/>speedup should be possible. <lb/>For the two processor simulation, two SparcStation 20s with 128 MB of RAM connected by 100 <lb/>Mbps switched Ethernet network were used. The results are summarized in Table 1. As can be <lb/>seen, the simulation attains approximately a 1.6 times speedup running on two processors over <lb/>one. The speedup is calculated using the time of processor 2 in this case, as both processors begin <lb/>computing at the same time. Processor 2 spends 17% of its time, on average, waiting for the data <lb/>from processor 1 to arrive. The time is takes processor 1 to send data, about 1.3 ms on average, is <lb/>A <lb/>B <lb/>C <lb/>D <lb/>E <lb/>F <lb/>G <lb/>H <lb/>I <lb/>Proc 1 <lb/>Proc 2 <lb/>Proc 3 <lb/>1 2 3 4 5 6 7 8 9 10 <lb/>A <lb/>B <lb/>C <lb/>D <lb/>E <lb/>F <lb/>G <lb/>H <lb/>I <lb/>2 <lb/>1 <lb/>2 <lb/>4 <lb/>2 <lb/>2 <lb/>2 <lb/>2 <lb/>4 <lb/>3 <lb/>2 <lb/>1 <lb/>2 <lb/>3 <lb/>1 <lb/>2 <lb/>1 <lb/>3 <lb/></body>

			<page>15 <lb/></page>

			<body>negligible. If the time waiting for the receive data could be eliminated, the maximum speedup of <lb/>1.68 would be approached. Efficiency in parallel computing is the speedup divided by the number <lb/>of processors[4]. For the two processor simulation, the efficiency is 80%. <lb/>The four processor simulation was carried out using three SparcStation 20s with 128 MB of <lb/>RAM, and one SparcStation 10 with 80 MB of RAM, all connected by a 100 Mbps switched <lb/>Ethernet network. The results are summarized in Table 2. The timing for the stand-alone system <lb/>differed from the SparcStation 20s to the SparcStation 10. The SparcStation 20s were about 1.7 <lb/>times faster than the SparcStation 10 in executing the program on a single processor. The average <lb/>time between all four of these systems was used to calculate the one processor value. The fourth <lb/>processor in this simulation spends almost 50% of its time waiting for its data to arrive. The third <lb/>processor spends about 23% of its total time waiting for data, the second processor almost 10%. <lb/>As in the two processor system, removing these times would allow the maximum speedup to be <lb/>approached. This waiting time is due to scheduling. The latter processors finish the work that is <lb/>not dependent on the data they receive before the receive data is ready to be sent. This results in <lb/>the processors wasting compute cycles waiting for that data to arrive. This wait time increases <lb/>with later processors, as in this simulation all the processors do the same amount of computation <lb/>before waiting for receive data to arrive. More efficient scheduling could undoubtedly improve <lb/>performance, but better scheduling in this specific simulation may not be applicable as there is <lb/>only a limited amount of work that can be carried out without the received data (essentially one <lb/>FFT). The speedup for the four processor simulation is 1.8, the efficiency is 45%. <lb/>An alternate schedule that divides the rate Galaxy work more evenly among the processors may <lb/>result in better performance, but this would also incur more communication costs. A schedule <lb/>was generated using DL scheduling and assuming fine grain communication. This schedule <lb/>resulted in 72% utilization with communication sends and receives were spread throughout the <lb/>generated code. With the bulk transfer method, the DL scheduler arranged sends and receives into <lb/>larger groupings, and a 75% utilization was reported. <lb/>Table 1: Two Processor Results <lb/>Processor 1 <lb/>Processor 2 <lb/>Single Processor <lb/>Send Time <lb/>1.310 ms <lb/>N/A <lb/>N/A <lb/>Receive Time <lb/>N/A <lb/>138.8 ms <lb/>N/A <lb/>Execution Time <lb/>696.0 ms <lb/>827.2 ms <lb/>1316.0 ms <lb/>Table 2: Four Processor Results <lb/>Processor 1 Processor 2 Processor 3 Processor 4 Single Processor <lb/>Send Time <lb/>1.140 ms <lb/>1.230 ms <lb/>0.712 ms <lb/>N/A <lb/>N/A <lb/>Receive Time <lb/>N/A <lb/>98.10 ms <lb/>287.9 ms <lb/>782.8 ms <lb/>N/A <lb/>Execution Time <lb/>804.8 ms <lb/>976.3 ms <lb/>1234.1 ms <lb/>1647.9 ms <lb/>2884.3 ms <lb/></body>

			<page>16 <lb/></page>

			<body>These results show that a definite benefit is gained in using multiple processors (workstations) for <lb/>large Ptolemy simulations. With better scheduling, higher efficiencies can be achieved. When the <lb/>times are measured in milliseconds, the benefits may not appear as significant, but milliseconds <lb/>are very significant to a CPU. The speedup is graphed in Figure 7. <lb/>Figure 7: Up-Sample Simulation Speedup <lb/>7. Summary <lb/>An initial implementation of a CGCNOWamTarget was built using GAM. The target was not <lb/>functional due to GAM&apos;s requirement of a SPMD parallel program, i.e. each node in the process <lb/>had to be running the same code image. Test data was collected by stitching each file produced by <lb/>the CGCNOWamTarget into a single file that was then compiled and run on each node. The GAM <lb/>implementation tested was built on TCP/IP, so the TCP/IP overhead canceled any benefit gained <lb/>using Berkeley&apos;s NOW. <lb/>The UDPAM implementation of the CGCNOWamTarget is fully integrated into Ptolemy, as the <lb/>SPMD requirement of GAM is no longer part of the AM specification. Because UDPAM is built <lb/>on top of UDP/IP, a CGCNOWamTarget application can be created, compiled, and run any net-<lb/>work supporting UDP/IP. The use of UDP/IP is a benefit in the sense CGCNOWamTarget can be <lb/>run on systems other than Berkeley&apos;s NOW, but it has the same detriment that the original CGC-<lb/>NOWamTarget using TCP/IP did, costly UDP/IP overhead. Even with protocol overhead, appro-<lb/>priate Ptolemy applications can be designed and run on Berkeley&apos;s NOW or other networks <lb/>supporting UDP/IP. These applications would simply have to have a higher degree of computa-<lb/>tion than communication. <lb/>The up-sample simulations show that Ptolemy systems do exist that benefit from running on mul-<lb/>tiple processors. The 80% efficiency and 1.6 times speedup achieved with two processors is <lb/>excellent and certainly shows an added benefit from running the simulation on two processors <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>1 <lb/>2 <lb/>3 <lb/>Number of Processors <lb/>Speedup <lb/>Measured <lb/>Maximum <lb/></body>

			<page>17 <lb/></page>

			<body>rather than one. These values were close the maximum possible speedup of 1.68 and efficiency of <lb/>84%. Although the simulation appears not to scale well to four processors, the 1.8 times speedup <lb/>is amplified in importance if overall execution time is critical, and is 60% of the possible maxi-<lb/>mum of 3. If a massive simulation took 10 days to run on one processor, only taking 5 1/2 days <lb/>would definitely prove beneficial, even at only 45% efficiency. If nothing else, these results show <lb/>the potential for creating Ptolemy simulations that will execute in less time on multiple proces-<lb/>sors, and the CGCNOWam target is a means of using multiple processors. <lb/>The current CGCNOWamTarget is not portable across AM implementations because of its use of <lb/>known ports in creating endpoints. Although a name server is suggested in the AM documenta-<lb/>tion, no interface is provided. If a name server was developed for the CGCNOWamTarget, there <lb/>is still no guarantee the name server interface would be the same for other AM implementations. <lb/>Although not restricting global names of endpoints is an essential feature of the AM specification <lb/>for portability, it is a potential weakness unless a common name server interface is defined. <lb/>Although the building-wide supercomputer has not yet been realized, the CGCNOWam target <lb/>running over 100 Mbps Ethernet shows the goal is being approached. It is already the case where <lb/>accessing data over the network from the memory of another computer is faster than accessing a <lb/>local hard disk. This fast network communication lends itself naturally to using networked com-<lb/>puters as a parallel platform. <lb/>Future potential work on the CGCNOWamTarget includes a port to the new LAM II library, <lb/>which is an AM implementation running on Myrinet LANai network cards. This implementation <lb/>is true to the AM goals in that it provides a direct link between an application and the network <lb/>hardware, bypassing the operating system. This new CGCNOWamTarget would perform closely <lb/>to a MPP system, and would be able to run a wider range of Ptolemy applications more efficiently <lb/>than the UDPAM CGCNOWamTarget. Other future work could include experimenting with addi-<lb/>tional Ptolemy scheduling techniques and developing more NOW simulations. <lb/></body>

			<listBibl>8. References <lb/>[1] Joseph Buck, Soonhoi Ha, Edward A. Lee, David G. Messerschmitt, &quot;Ptolemy: A Framework <lb/>for Simulating and Prototyping Heterogeneous Systems,&quot; International Journal of Computer Sim-<lb/>ulation, 1992. <lb/>[2] Alan M. Mainwaring, &quot;Active Message Applications Programming Interface and Communica-<lb/>tion Subsystem Organization,&quot; Draft Technical Report, University of California at Berkeley, Com-<lb/>puter Science Department, 1996. <lb/>[3] David Culler, Kim Keeton, Cedric Krumbein, Lok Tin Liu, Alan Mainwaring, Rich Martin, <lb/>Steve Rodrigues, Kristin Wright, Chad Yoshikawa, &quot;Generic Active Message Interface Specifica-<lb/>tion,&quot; Version 1.1, University of California at Berkeley, Computer Science Department, 1995. <lb/>[4] Vipin Kumar, Ananth Grama, Anshul Gupta, George Karypis, Introduction to Parallel Com-<lb/>puting: Design and Analysis of Algorithms, The Benjamin/Cummings Publishing Company, Inc., <lb/>Redwood City, California, 1994. <lb/></listBibl>

			<page>18 <lb/></page>

			<listBibl>[5] Remzi H. Arpaci, Andrea Dusseau, Amin M. Vahdat, Lok T. Liu, Thomas E. Anderson, and <lb/>David A. Patterson, &quot;The Interaction of Parallel and Sequential Workloads on a Network of Work-<lb/>stations,&quot; Technical Report CS-94-838, University of California at Berkeley, Computer Science <lb/>Department, 1994. <lb/>[6] Edward A. Lee and David G. Messerschmitt, &quot;Synchronous Data Flow,&quot; Proceedings of the <lb/>IEEE, September, 1987. <lb/>[7] Gilbert C. Sih, &quot;Multiprocessor Scheduling To Account For Interprocessor Communication&quot;, <lb/>Ph.D. Thesis, University of California at Berkeley, Department of Electrical Engineering and <lb/>Computer Science, 1991. <lb/>[8] Jose L. Pino, &quot;Software Synthesis for Single-Processor DSP Systems Using Ptolemy,&quot; Mas-<lb/>ter&apos;s Report, University of California at Berkeley, Department of Electrical Engineering and Com-<lb/>puter Science, 1994. <lb/>[9] Jose L. Pino, Soonhoi Ha, Edward A. Lee, Joseph T. Buck, &quot;Software Synthesis for DSP <lb/>Using Ptolemy,&quot; Journal of VLSI Signal Processing, 9, 7-21, 1995. <lb/>[10] Jose L. Pino and Edward A. Lee, &quot;Hierarchical Static Scheduling of Dataflow Graphs onto <lb/>Multiple Processors,&quot; Proceedings of the IEEE International Conference on Acoustics, Speech, <lb/>and Signal Processing, May 1995, pp. 2643-2646. <lb/></listBibl>

			<page>19 <lb/></page>

			<div type="annex">Appendix A <lb/>CGCNOWamTarget.h <lb/>/****************************************************************** <lb/>Version identification: <lb/>@(#)CGCNOWamTarget.h1.6 3/4/96 <lb/>Copyright (c) 1995-1996 The Regents of the University of California. <lb/>All Rights Reserved. <lb/>Permission is hereby granted, without written agreement and without <lb/>license or royalty fees, to use, copy, modify, and distribute this <lb/>software and its documentation for any purpose, provided that the above <lb/>copyright notice and the following two paragraphs appear in all copies <lb/>of this software. <lb/>IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY <lb/>FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES <lb/>ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF <lb/>THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF <lb/>SUCH DAMAGE. <lb/>THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, <lb/>INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF <lb/>MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE <lb/>PROVIDED HEREUNDER IS ON AN &quot;AS IS&quot; BASIS, AND THE UNIVERSITY OF <lb/>CALIFORNIA HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, <lb/>ENHANCEMENTS, OR MODIFICATIONS. <lb/>COPYRIGHTENDKEY <lb/>Programmer: Patrick Warner <lb/>*******************************************************************/ <lb/>#ifndef _CGCNOWamTarget_h <lb/>#define _CGCNOWamTarget_h 1 <lb/>#ifdef __GNUG__ <lb/>#pragma interface <lb/>#endif <lb/>#include &quot;CGMultiTarget.h&quot; <lb/>#include &quot;StringState.h&quot; <lb/>#include &quot;IntArrayState.h&quot; <lb/>#include &quot;IntState.h&quot; <lb/>class EventHorizon; <lb/>class CGCTarget; <lb/>class VirtualInfo { <lb/>friend class CGCNOWamTarget; <lb/>unsigned long inetAddr; // internet address <lb/>int virtNode; <lb/>// active message virtual node <lb/></div>

			<page>20 <lb/></page>

			<div type="annex">const char* nm; <lb/>// machine name <lb/>public: <lb/>VirtualInfo(): virtNode(0), nm(0) {} <lb/>}; <lb/>class CGCNOWamTarget : public CGMultiTarget { <lb/>public: <lb/>CGCNOWamTarget(const char* name, const char* starclass, const char* <lb/>desc); <lb/>~CGCNOWamTarget(); <lb/>Block* makeNew() const; <lb/>int isA(const char*) const; <lb/>// redefine IPC funcs <lb/>DataFlowStar* createSend(int from, int to, int num); <lb/>DataFlowStar* createReceive(int from, int to, int num); <lb/>// spread and collect <lb/>DataFlowStar* createSpread(); <lb/>DataFlowStar* createCollect(); <lb/>// redefine <lb/>void pairSendReceive(DataFlowStar* s, DataFlowStar* r); <lb/>// get VirtualInfo <lb/>VirtualInfo* getVirtualInfo() { return machineInfo; } <lb/>void setMachineAddr(CGStar*, CGStar*); <lb/>// signal TRUE when replication begins, or FALSE when ends <lb/>void signalCopy(int flag) { replicateFlag = flag; } <lb/>protected: <lb/>void setup(); <lb/>// redefine <lb/>int sendWormData(PortHole&amp;); <lb/>int receiveWormData(PortHole&amp;); <lb/>private: <lb/>// states indicate which machines to use. <lb/>StringState machineNames; <lb/>StringState nameSuffix; <lb/>// In case, the cody body is replicated as in &quot;For&quot; and &quot;Recur&quot; <lb/>// construct, save this information to be used in getMachineAddr(). <lb/>IntArray* mapArray; <lb/>int baseNum; <lb/>int replicateFlag; <lb/>// information on the machines <lb/>VirtualInfo* machineInfo; <lb/>// number of send/receive pairs <lb/></div>

			<page>21 <lb/></page>

			<div type="annex">int pairs; <lb/>// identify machines <lb/>int identifyMachines(); <lb/>// return the machine_id of the given target. <lb/>int machineId(Target*); <lb/>}; <lb/>#endif <lb/>CGCNOWamTarget.cc <lb/>static const char file_id[] = &quot;CGCNOWamTarget.cc&quot;; <lb/>/****************************************************************** <lb/>Version identification: <lb/>@(#)CGCNOWamTarget.cc1.10 3/8/96 <lb/>Copyright (c) 1995-1996 The Regents of the University of California. <lb/>All Rights Reserved. <lb/>Permission is hereby granted, without written agreement and without <lb/>license or royalty fees, to use, copy, modify, and distribute this <lb/>software and its documentation for any purpose, provided that the above <lb/>copyright notice and the following two paragraphs appear in all copies <lb/>of this software. <lb/>IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY <lb/>FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES <lb/>ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF <lb/>THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF <lb/>SUCH DAMAGE. <lb/>THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, <lb/>INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF <lb/>MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE <lb/>PROVIDED HEREUNDER IS ON AN &quot;AS IS&quot; BASIS, AND THE UNIVERSITY OF <lb/>CALIFORNIA HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, <lb/>ENHANCEMENTS, OR MODIFICATIONS. <lb/>COPYRIGHTENDKEY <lb/>Programmer: Patrick Warner <lb/>*******************************************************************/ <lb/>#ifdef __GNUG__ <lb/>#pragma implementation <lb/>#endif <lb/>#include &quot;pt_fstream.h&quot; <lb/>#include &quot;Error.h&quot; <lb/>#include &quot;CGUtilities.h&quot; <lb/>#include &quot;CGCStar.h&quot; <lb/></div>

			<page>22 <lb/></page>

			<div type="annex">#include &quot;KnownTarget.h&quot; <lb/>#include &quot;CGCNOWamTarget.h&quot; <lb/>#include &quot;CGCTarget.h&quot; <lb/>#include &quot;CGCSpread.h&quot; <lb/>#include &quot;CGCCollect.h&quot; <lb/>#include &quot;CGCNOWamSend.h&quot; <lb/>#include &quot;CGCNOWamRecv.h&quot; <lb/>#include &lt;ctype.h&gt; <lb/>#include &lt;stdio.h&gt; <lb/>#include &lt;sys/types.h&gt; <lb/>#include &lt;netdb.h&gt; <lb/>#include &lt;netinet/in.h&gt; <lb/>#include &lt;arpa/inet.h&gt; // Sol2 needs this for inet_addr() <lb/>// stream for logging information. It is opened by the setup method. <lb/>static pt_ofstream feedback; <lb/>// ---------------------------------------------------------------------------<lb/>CGCNOWamTarget::CGCNOWamTarget(const char* name,const char* starclass, <lb/>const char* desc) : CGMultiTarget (name,starclass,desc) { <lb/>// specify machine names <lb/>addState(machineNames.setState(&quot;machineNames&quot;,this,&quot;lucky, babbage&quot;, <lb/>&quot;machine names (separated by a comma)&quot;)); <lb/>addState(nameSuffix.setState(&quot;nameSuffix&quot;,this,&quot;&quot;, <lb/>&quot;common suffix of machine names(e.g. .berkeley.edu)&quot;)); <lb/>// make some states invisible <lb/>childType.setInitValue(&quot;default-CGC&quot;); <lb/>compileFlag.setInitValue(&quot;NO&quot;); <lb/>runFlag.setInitValue(&quot;NO&quot;); <lb/>displayFlag.setInitValue(&quot;YES&quot;); <lb/>resources.setInitValue(&quot;&quot;); <lb/>machineInfo = 0; <lb/>pairs = 0; <lb/>baseNum = 0; <lb/>mapArray = 0; <lb/>replicateFlag = 0; <lb/>} <lb/>CGCNOWamTarget :: ~CGCNOWamTarget() { <lb/>// <lb/>if (inherited() == 0) { <lb/>LOG_DEL; delete [] machineInfo; <lb/>// <lb/>} <lb/>} <lb/>// ---------------------------------------------------------------------------<lb/>DataFlowStar* CGCNOWamTarget :: createSend(int, int, int) { <lb/>LOG_NEW; CGCNOWamSend* s = new CGCNOWamSend; <lb/>return s; <lb/>} <lb/></div>

			<page>23 <lb/></page>

			<div type="annex">DataFlowStar* CGCNOWamTarget :: createReceive(int, int, int) { <lb/>LOG_NEW; CGCNOWamRecv* r = new CGCNOWamRecv; <lb/>return r; <lb/>} <lb/>DataFlowStar* CGCNOWamTarget :: createSpread() { <lb/>LOG_NEW; return (new CGCSpread); <lb/>} <lb/>DataFlowStar* CGCNOWamTarget :: createCollect() { <lb/>LOG_NEW; return (new CGCCollect); <lb/>} <lb/>void CGCNOWamTarget :: pairSendReceive(DataFlowStar* s, DataFlowStar* r) { <lb/>feedback &lt;&lt; &quot;\tpairing &quot; &lt;&lt; s-&gt;fullName() &lt;&lt; &quot; --&gt; &quot; &lt;&lt; r-&gt;fullName() <lb/>&lt;&lt; &quot;\n&quot;; feedback.flush(); <lb/>CGCNOWamSend* cs = (CGCNOWamSend*) s; <lb/>CGCNOWamRecv* cr = (CGCNOWamRecv*) r; <lb/>int pnum = (int)nprocs; <lb/>cs-&gt;numNodes.setInitValue(pnum); <lb/>cr-&gt;numNodes.setInitValue(pnum); <lb/>cs-&gt;pairNumber.setInitValue(pairs); <lb/>cr-&gt;pairNumber.setInitValue(pairs); <lb/>pairs++; <lb/>StringList nodeAddrs = &quot; &quot;; <lb/>for (int i = 0; i &lt; pnum; i++) { <lb/>nodeAddrs &lt;&lt; (int)(machineInfo[i].inetAddr) &lt;&lt; &quot; &quot;; <lb/>} <lb/>cs-&gt;nodeIPs.setInitValue(hashstring(nodeAddrs)); <lb/>cr-&gt;nodeIPs.setInitValue(hashstring(nodeAddrs)); <lb/>cs-&gt;partner = cr; <lb/>} <lb/>void CGCNOWamTarget :: setMachineAddr(CGStar* s, CGStar* r) { <lb/>CGCNOWamSend* cs = (CGCNOWamSend*) s; <lb/>CGCNOWamRecv* cr = (CGCNOWamRecv*) r; <lb/>CGTarget* tg = cr-&gt;cgTarget(); <lb/>if (replicateFlag &amp;&amp; mapArray) { <lb/>int six = -1; <lb/>int rix = -1; <lb/>CGTarget* sg = cs-&gt;cgTarget(); <lb/>int numMatch = 0; <lb/>for (int i = 0; i &lt; mapArray-&gt;size(); i++) { <lb/>CGTarget* temp = (CGTarget*) child(mapArray-&gt;elem(i)); <lb/>if (temp == sg) { <lb/>six = i; numMatch++; <lb/>} else if (temp == tg) { <lb/>rix = i; numMatch++; <lb/>} <lb/></div>

			<page>24 <lb/></page>

			<div type="annex">if (numMatch &gt;= 2) break; <lb/>} <lb/>if ((six &lt; 0) || (rix &lt; 0) || (numMatch != 2)) { <lb/>Error :: abortRun(&quot;setMachineAddr failed.&quot;); <lb/>return; <lb/>} <lb/>int zz = six /baseNum; <lb/>if ((rix / baseNum) != zz) { <lb/>int newIx = zz * baseNum + (rix % baseNum); <lb/>tg = (CGTarget*) child(mapArray-&gt;elem(newIx)); <lb/>} <lb/>} <lb/>// machine address <lb/>int dix = machineId(tg); <lb/>if (dix &lt; 0) { <lb/>Error :: abortRun(*cr, &quot;no child target for this star.&quot;); <lb/>return; <lb/>} <lb/>cs-&gt;hostAddr.setInitValue(machineInfo[dix].virtNode); <lb/>} <lb/>int CGCNOWamTarget :: machineId(Target* t) { <lb/>for (int i = 0; i &lt; nChildrenAlloc; i++) { <lb/>if (child(i) == t) return i; <lb/>} <lb/>return -1; <lb/>} <lb/>// ---------------------------------------------------------------------------<lb/>/////////////////// <lb/>// setup <lb/>/////////////////// <lb/>void CGCNOWamTarget :: setup() { <lb/>// <lb/>if (inherited()) { <lb/>// <lb/>CGCNOWamTarget* orgT = (CGCNOWamTarget*) child(0)-&gt;parent(); <lb/>// <lb/>machineInfo = orgT-&gt;getVirtualInfo(); <lb/>// <lb/>CGMultiTarget :: setup(); <lb/>// <lb/>return; <lb/>// <lb/>} <lb/>// all runs will append to the same file. <lb/>// FIXME: should not be done this way. <lb/>if (!feedback) feedback.open(&quot;CGCNOWam_log&quot;); <lb/>if (!feedback) return; <lb/>// machine idetifications <lb/>if (identifyMachines() == FALSE) return; <lb/>CGMultiTarget :: setup(); <lb/>// machine name setup <lb/>for (int i = 0; i &lt; nChildrenAlloc; i++) { <lb/></div>

			<page>25 <lb/></page>

			<div type="annex">CGCTarget* t = (CGCTarget*) child(i); <lb/>t-&gt;setHostName(machineInfo[i].nm); <lb/>} <lb/>feedback.flush(); <lb/>} <lb/>int CGCNOWamTarget :: identifyMachines() { <lb/>// construct machine information table <lb/>int pnum = int(nprocs); <lb/>LOG_NEW; machineInfo = new VirtualInfo[pnum]; <lb/>feedback &lt;&lt; &quot; ** machine identification ** \n&quot;; <lb/>const char* p = machineNames; <lb/>int i = 0; <lb/>while (*p) { <lb/>char buf[80], *b = buf; <lb/>while (isspace(*p)) p++; <lb/>while ((*p != &apos;,&apos;) &amp;&amp; (*p != 0)) { <lb/>if (isspace(*p)) p++; <lb/>else *b++ = *p++; <lb/>} <lb/>if (*p == &apos;,&apos;) p++; <lb/>*b = 0; <lb/>// end of string. <lb/>// record names <lb/>StringList mname = (const char*) buf; <lb/>mname &lt;&lt; (const char*) nameSuffix; <lb/>machineInfo[i].nm = hashstring((const char*) mname); <lb/>// internet address calculation <lb/>struct hostent* hp; <lb/>if ((hp = gethostbyname((const char*) mname)) == NULL) { <lb/>StringList errMsg; <lb/>errMsg &lt;&lt; &quot;host name error: &quot; &lt;&lt; mname; <lb/>Error :: abortRun(errMsg); <lb/>return FALSE; <lb/>} <lb/>struct in_addr* ptr = (struct in_addr*) hp-&gt;h_addr_list[0]; <lb/>machineInfo[i].inetAddr = inet_addr(hashstring(inet_ntoa(*ptr))); <lb/>machineInfo[i].virtNode = i; <lb/>// monitoring. <lb/>feedback &lt;&lt; &quot;machine(&quot; &lt;&lt; i &lt;&lt; &quot;) = &quot;; <lb/>feedback &lt;&lt; mname &lt;&lt; &quot;: &quot;; <lb/>feedback &lt;&lt; machineInfo[i].virtNode &lt;&lt; &quot;\n&quot;; <lb/>feedback.flush(); <lb/>i++; <lb/>} <lb/>// check if the number of processors and the machine names are matched. <lb/>if (i != pnum) { <lb/>Error :: abortRun(*this, &quot;The number of processors and&quot;, <lb/>&quot; the number of machine names are not equal.&quot;); <lb/>return FALSE; <lb/>} <lb/></div>

			<page>26 <lb/></page>

			<div type="annex">return TRUE; <lb/>} <lb/>// ---------------------------------------------------------------------------<lb/>Block* CGCNOWamTarget :: makeNew() const { <lb/>LOG_NEW; return new CGCNOWamTarget(name(),starType(),descriptor()); <lb/>} <lb/>// ---------------------------------------------------------------------------<lb/>///////////////////////////// <lb/>// wormhole interface method <lb/>///////////////////////////// <lb/>int CGCNOWamTarget :: receiveWormData(PortHole&amp; p) { <lb/>CGPortHole&amp; cp = *(CGPortHole*)&amp;p; <lb/>cp.forceSendData(); <lb/>return TRUE; <lb/>} <lb/>// ---------------------------------------------------------------------------<lb/>int CGCNOWamTarget :: sendWormData(PortHole&amp; p) { <lb/>CGPortHole&amp; cp = *(CGPortHole*)&amp;p; <lb/>cp.forceGrabData(); <lb/>return TRUE; <lb/>} <lb/>// ---------------------------------------------------------------------------<lb/>ISA_FUNC(CGCNOWamTarget,CGMultiTarget); <lb/>static CGCNOWamTarget targ(&quot;CGCNOWam&quot;,&quot;CGCStar&quot;, <lb/>&quot;A NOW target for parallel C code generation&quot;); <lb/>static KnownTarget entry(targ,&quot;CGCNOWam&quot;); <lb/></div>

			<page>27 <lb/></page>

			<div type="annex">Appendix B <lb/>CGCNOWamRecv.pl <lb/>defstar { <lb/>name { NOWamRecv } <lb/>domain { CGC } <lb/>desc { <lb/>Receive star between NOW processors. <lb/>} <lb/>version { @(#)CGCNOWamRecv.pl1.25 8/22/96 } <lb/>author { Patrick Warner } <lb/>copyright { <lb/>Copyright(c) 1995-1996 The Regents of the University of California <lb/>All rights reserved. <lb/>See the file $PTOLEMY/copyright for copyright notice, <lb/>limitation of liability, and disclaimer of warranty provisions. <lb/>} <lb/>location { CGC NOW Active Message target library } <lb/>explanation { <lb/>Produce code for inter-process communication (receive-side). <lb/>} <lb/>private { <lb/>friend class CGCNOWamTarget; <lb/>} <lb/>output { <lb/>name {output} <lb/>type {FLOAT} <lb/>} <lb/>state { <lb/>name { numData } <lb/>type { int } <lb/>default { 1 } <lb/>desc { number of tokens to be transferred } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { nodeIPs } <lb/>type { intarray } <lb/>default { &quot;0 1 2 3&quot; } <lb/>desc { IP addresses of nodes in program. } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { numNodes } <lb/>type { int } <lb/>default { 0 } <lb/>desc { Number of nodes in program. } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { pairNumber } <lb/>type { int } <lb/></div>

			<page>28 <lb/></page>

			<div type="annex">default { 0 } <lb/>desc { Send Receive pair number for unique IP port. } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { runCount } <lb/>type { int } <lb/>default { 0 } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>defstate { <lb/>name { localData } <lb/>type { floatarray } <lb/>default { &quot;0&quot; } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>setup { <lb/>numData = 400; <lb/>localData.resize(numData); <lb/>output.setSDFParams(int(numData), int(numData)-1); <lb/>} <lb/>codeblock (outData) { <lb/>for (i = $val(numData) -1; i &gt;= 0; i--) { <lb/>$ref(output,i) = $ref(localData)[j++]; <lb/>} <lb/>} <lb/>codeblock (timeincludes) { <lb/>#ifdef TIME_INFO1 <lb/>#include &lt;sys/time.h&gt; <lb/>#endif <lb/>} <lb/>codeblock (ipcHandler) { <lb/>void $starSymbol(ipc_handler)(void *source_token, void *buf, int nbytes, <lb/>int d1, int d2, int d3, int d4) <lb/>{ <lb/>$starSymbol(RecvData) = 100.0; <lb/>} <lb/>} <lb/>codeblock (errorHandler) { <lb/>void error_handler(int status, op_t opcode, void *argblock) <lb/>{ <lb/>switch (opcode) { <lb/>case EBADARGS: <lb/>fprintf(stderr,&quot;Bad Args:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADENTRY: <lb/>fprintf(stderr,&quot;Bad Entry:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADTAG: <lb/>fprintf(stderr,&quot;Bad Tag:&quot;); <lb/></div>

			<page>29 <lb/></page>

			<div type="annex">fflush(stderr); <lb/>break; <lb/>case EBADHANDLER: <lb/>fprintf(stderr,&quot;Bad Handler:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADSEGOFF: <lb/>fprintf(stderr,&quot;Bad Seg offset:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADLENGTH: <lb/>fprintf(stderr,&quot;Bad Length:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADENDPOINT: <lb/>fprintf(stderr,&quot;Bad Endpoint:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case ECONGESTION: <lb/>fprintf(stderr,&quot;Congestion:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EUNREACHABLE: <lb/>fprintf(stderr,&quot;Unreachable:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>} <lb/>} <lb/>} <lb/>codeblock (amdecls) { <lb/>en_t global; <lb/>eb_t bundle; <lb/>} <lb/>codeblock (timedecls) { <lb/>#ifdef TIME_INFO1 <lb/>hrtime_t timeRun; <lb/>hrtime_t beginRun; <lb/>hrtime_t endRun; <lb/>#endif <lb/>} <lb/>codeblock (stardecls) { <lb/>#ifdef TIME_INFO3 <lb/>hrtime_t $starSymbol(timeRecv); <lb/>hrtime_t $starSymbol(beginRecv); <lb/>hrtime_t $starSymbol(endRecv); <lb/>#endif <lb/>int $starSymbol(i); <lb/>en_t *$starSymbol(endname); <lb/>ea_t $starSymbol(endpoint); <lb/>} <lb/>codeblock (aminit) { <lb/>AM_Init(); <lb/>if (AM_AllocateBundle(AM_PAR, &amp;bundle) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_AllocateBundle failed\n&quot;); <lb/></div>

			<page>30 <lb/></page>

			<div type="annex">exit(1); <lb/>} <lb/>if (AM_SetEventMask(bundle, AM_NOTEMPTY) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetEventMask error\n&quot;); <lb/>exit(1); <lb/>} <lb/>} <lb/>codeblock (timeinit) { <lb/>#ifdef TIME_INFO3 <lb/>$starSymbol(timeRecv) = 0.0; <lb/>#endif <lb/>#ifdef TIME_INFO1 <lb/>beginRun = gethrtime(); <lb/>#endif <lb/>} <lb/>codeblock (starinit) { <lb/>$starSymbol(RecvData) = -0.001; <lb/>if (AM_AllocateKnownEndpoint(bundle, &amp;$starSymbol(endpoint), <lb/>&amp;$starSymbol(endname), HARDPORT + $val(pairNumber)) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_AllocateKnownEndpoint failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetTag($starSymbol(endpoint), 1234) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetTag failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetHandler($starSymbol(endpoint), 0, error_handler) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetHandler failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetHandler($starSymbol(endpoint), 2, $starSymbol(ipc_handler)) != <lb/>AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetHandler failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetSeg($starSymbol(endpoint), (void *)$ref(localData), $val(numData) * <lb/>sizeof(double)) != AM_OK) { <lb/>fprintf(stderr, &quot;AM_SetSeg error\n&quot;); <lb/>exit(-1); <lb/>} <lb/>for ($starSymbol(i) = 0; $starSymbol(i) &lt; $val(numNodes); $starSymbol(i)++) { <lb/>global.ip_addr = $ref(nodeIPs, $starSymbol(i)); <lb/>global.port = HARDPORT + $val(pairNumber); <lb/>if (AM_Map($starSymbol(endpoint), $starSymbol(i), global, 1234) != <lb/>AM_OK) { <lb/>fprintf(stderr, &quot;AM_Map error\n&quot;); <lb/>fflush(stderr); <lb/>exit(-1); <lb/>} <lb/>} <lb/>} <lb/></div>

			<page>31 <lb/></page>

			<div type="annex">initCode { <lb/>addGlobal(&quot;#define HARDPORT 61114\n&quot;, &quot;hardPort&quot;); <lb/>addGlobal(&quot;double $starSymbol(RecvData);\n&quot;); <lb/>addInclude(&quot;&lt;stdio.h&gt;&quot;); <lb/>addInclude(&quot;&lt;stdlib.h&gt;&quot;); <lb/>addInclude(&quot;&lt;string.h&gt;&quot;); <lb/>addInclude(&quot;&lt;thread.h&gt;&quot;); <lb/>addInclude(&quot;&lt;udpam.h&gt;&quot;); <lb/>addInclude(&quot;&lt;am.h&gt;&quot;); <lb/>addCompileOption( <lb/>&quot;-I$PTOLEMY/src/domains/cgc/targets/NOWam/libudpam&quot;); <lb/>addLinkOption( <lb/>&quot;-L$PTOLEMY/lib.$PTARCH -ludpam -lnsl -lsocket -lthread&quot;); <lb/>addCode(timeincludes, &quot;include&quot;, &quot;timeIncludes&quot;); <lb/>addProcedure(ipcHandler); <lb/>addProcedure(errorHandler, &quot;CGCNOWam_ErrorHandler&quot;); <lb/>addCode(amdecls, &quot;mainDecls&quot;, &quot;amDecls&quot;); <lb/>addCode(timedecls, &quot;mainDecls&quot;, &quot;timeDecls&quot;); <lb/>addCode(stardecls, &quot;mainDecls&quot;); <lb/>addCode(timeinit, &quot;mainInit&quot;, &quot;timeInit&quot;); <lb/>addCode(aminit, &quot;mainInit&quot;, &quot;amInit&quot;); <lb/>addCode(starinit, &quot;mainInit&quot;); <lb/>} <lb/>codeblock (block2) { <lb/>/* run receive once */ <lb/>} <lb/>codeblock (block) { <lb/>int i, j = 0; <lb/>#ifdef TIME_INFO3 <lb/>$starSymbol(beginRecv) = gethrtime(); <lb/>#endif <lb/>while ($starSymbol(RecvData) == -0.001) { <lb/>if (AM_Poll(bundle) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_Poll failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>} <lb/>$starSymbol(RecvData) = -0.001; <lb/>#ifdef TIME_INFO3 <lb/>$starSymbol(endRecv) = gethrtime(); <lb/>$starSymbol(timeRecv) += $starSymbol(endRecv) -$starSymbol(beginRecv); <lb/>printf(&quot;Cumulative time to receive %lld usec\n&quot;, $starSymbol(timeRecv) / <lb/>1000); <lb/>#endif <lb/>} <lb/>go { <lb/>if (runCount == 0) { <lb/></div>

			<page>32 <lb/></page>

			<div type="annex">addCode(block); <lb/>addCode(outData); <lb/>runCount = 1; <lb/>} else { <lb/>addCode(block2); <lb/>} <lb/>} <lb/>codeblock (runtime) { <lb/>#ifdef TIME_INFO1 <lb/>endRun = gethrtime(); <lb/>timeRun = endRun -beginRun; <lb/>printf(&quot;Time to run %lld usec\n&quot;, timeRun / 1000); <lb/>#endif <lb/>} <lb/>wrapup { <lb/>addCode(runtime, &quot;mainClose&quot;, &quot;runTime&quot;); <lb/>addCode(&quot;AM_Terminate();\n&quot;, &quot;mainClose&quot;, &quot;amTerminate&quot;); <lb/>} <lb/>} <lb/>CGCNOWamSend.pl <lb/>defstar { <lb/>name { NOWamSend } <lb/>domain { CGC } <lb/>desc { <lb/>Send star between NOWam processors. <lb/>} <lb/>version { @(#)CGCNOWamSend.pl1.20 8/22/96 } <lb/>author { Patrick O. Warner } <lb/>copyright { <lb/>Copyright(c) 1995-1996 The Regents of the University of California <lb/>} <lb/>location { CGC NOW Active Message target library } <lb/>explanation { <lb/>Produce code for inter-process communication (send-side) <lb/>} <lb/>private { <lb/>friend class CGCNOWamTarget; <lb/>CGStar* partner; <lb/>} <lb/>input { <lb/>name {input} <lb/>type {FLOAT} <lb/>} <lb/>state { <lb/>name { numData } <lb/>type { int } <lb/>default { 1 } <lb/>desc { number of tokens to be transferred } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { nodeIPs } <lb/></div>

			<page>33 <lb/></page>

			<div type="annex">type { intarray } <lb/>default { &quot;0 1 2 3&quot; } <lb/>desc { IP addresses of nodes in program. } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { hostAddr } <lb/>type { int } <lb/>default { 0 } <lb/>desc { Host virtual node for server } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { numNodes } <lb/>type { int } <lb/>default { 0 } <lb/>desc { Number of nodes in program. } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { pairNumber } <lb/>type { int } <lb/>default { 0 } <lb/>desc { Send Receive pair number for unique IP port. } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>state { <lb/>name { runCount } <lb/>type { int } <lb/>default { 0 } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>defstate { <lb/>name { localData } <lb/>type { floatarray } <lb/>default { &quot;0&quot; } <lb/>attributes { A_NONSETTABLE } <lb/>} <lb/>hinclude { &quot;CGCNOWamTarget.h&quot; } <lb/>setup { <lb/>numData = 400; <lb/>localData.resize(numData); <lb/>input.setSDFParams (int(numData), int(numData)-1); <lb/>} <lb/>codeblock(loadCode) { <lb/>int i, check, j = 0; <lb/>for (i = $val(numData) -1; i &gt;= 0; i--) { <lb/>$ref(localData)[j++] = $ref(input,i); <lb/>} <lb/>} <lb/>codeblock (timeincludes) { <lb/></div>

			<page>34 <lb/></page>

			<div type="annex">#ifdef TIME_INFO1 <lb/>#include &lt;sys/time.h&gt; <lb/>#endif <lb/>} <lb/>codeblock (replyHandler) { <lb/>void reply_handler(void *source_token, int d1, int d2, int d3, int d4) <lb/>{ <lb/>} <lb/>} <lb/>codeblock (errorHandler) { <lb/>void error_handler(int status, op_t opcode, void *argblock) <lb/>{ <lb/>switch (opcode) { <lb/>case EBADARGS: <lb/>fprintf(stderr,&quot;Bad Args:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADENTRY: <lb/>fprintf(stderr,&quot;Bad Entry:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADTAG: <lb/>fprintf(stderr,&quot;Bad Tag:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADHANDLER: <lb/>fprintf(stderr,&quot;Bad Handler:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADSEGOFF: <lb/>fprintf(stderr,&quot;Bad Seg offset:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADLENGTH: <lb/>fprintf(stderr,&quot;Bad Length:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EBADENDPOINT: <lb/>fprintf(stderr,&quot;Bad Endpoint:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case ECONGESTION: <lb/>fprintf(stderr,&quot;Congestion:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>case EUNREACHABLE: <lb/>fprintf(stderr,&quot;Unreachable:&quot;); <lb/>fflush(stderr); <lb/>break; <lb/>} <lb/>} <lb/>} <lb/>codeblock (amdecls) { <lb/>en_t global; <lb/></div>

			<page>35 <lb/></page>

			<div type="annex">eb_t bundle; <lb/>} <lb/>codeblock (timedecls) { <lb/>#ifdef TIME_INFO1 <lb/>hrtime_t timeRun; <lb/>hrtime_t beginRun; <lb/>hrtime_t endRun; <lb/>#endif <lb/>} <lb/>codeblock (stardecls) { <lb/>#ifdef TIME_INFO2 <lb/>hrtime_t $starSymbol(timeSend); <lb/>hrtime_t $starSymbol(beginSend); <lb/>hrtime_t $starSymbol(endSend); <lb/>#endif <lb/>en_t *$starSymbol(endname); <lb/>ea_t $starSymbol(endpoint); <lb/>int $starSymbol(i); <lb/>} <lb/>codeblock (timeinit) { <lb/>#ifdef TIME_INFO2 <lb/>$starSymbol(timeSend) = 0.0; <lb/>#endif <lb/>#ifdef TIME_INFO1 <lb/>beginRun = gethrtime(); <lb/>#endif <lb/>} <lb/>codeblock (aminit) { <lb/>AM_Init(); <lb/>if (AM_AllocateBundle(AM_PAR, &amp;bundle) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_AllocateBundle failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetEventMask(bundle, AM_NOTEMPTY) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetEventMask error\n&quot;); <lb/>exit(1); <lb/>} <lb/>} <lb/>codeblock (starinit) { <lb/>if (AM_AllocateKnownEndpoint(bundle, &amp;$starSymbol(endpoint), <lb/>&amp;$starSymbol(endname), HARDPORT + $val(pairNumber)) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_AllocateKnownEndpoint failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetTag($starSymbol(endpoint), 1234) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetTag failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetHandler($starSymbol(endpoint), 0, error_handler) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetHandler failed\n&quot;); <lb/>exit(1); <lb/></div>

			<page>36 <lb/></page>

			<div type="annex">} <lb/>if (AM_SetHandler($starSymbol(endpoint), 1, reply_handler) != AM_OK) { <lb/>fprintf(stderr, &quot;error: AM_SetHandler failed\n&quot;); <lb/>exit(1); <lb/>} <lb/>if (AM_SetSeg($starSymbol(endpoint), (void *)$ref(localData), $val(numData) * <lb/>sizeof(double)) != AM_OK) { <lb/>fprintf(stderr, &quot;AM_SetSeg error\n&quot;); <lb/>exit(-1); <lb/>} <lb/>for ($starSymbol(i) = 0; $starSymbol(i) &lt; $val(numNodes); $starSymbol(i)++) { <lb/>global.ip_addr = $ref(nodeIPs, $starSymbol(i)); <lb/>global.port = HARDPORT + $val(pairNumber); <lb/>if (AM_Map($starSymbol(endpoint), $starSymbol(i), global, 1234) != <lb/>AM_OK) { <lb/>fprintf(stderr, &quot;AM_Map error\n&quot;); <lb/>fflush(stderr); <lb/>exit(-1); <lb/>} <lb/>} <lb/>} <lb/>initCode { <lb/>// obtain the hostAddr state from parent MultiTarget. <lb/>// Note that this routine should be placed here. <lb/>CGCNOWamTarget* t = (CGCNOWamTarget*) cgTarget()-&gt;parent(); <lb/>t-&gt;setMachineAddr(this, partner); <lb/>hostAddr.initialize(); <lb/>// code generation. <lb/>addGlobal(&quot;#define HARDPORT 61114\n&quot;, &quot;hardPort&quot;); <lb/>addInclude(&quot;&lt;stdio.h&gt;&quot;); <lb/>addInclude(&quot;&lt;stdlib.h&gt;&quot;); <lb/>addInclude(&quot;&lt;thread.h&gt;&quot;); <lb/>addInclude(&quot;&lt;udpam.h&gt;&quot;); <lb/>addInclude(&quot;&lt;am.h&gt;&quot;); <lb/>addCompileOption( <lb/>&quot;-I$PTOLEMY/src/domains/cgc/targets/NOWam/libudpam&quot;); <lb/>addLinkOption( <lb/>&quot;-L$PTOLEMY/lib.$PTARCH -ludpam -lnsl -lsocket -lthread&quot;); <lb/>addCode(timeincludes, &quot;include&quot;, &quot;timeIncludes&quot;); <lb/>addProcedure(replyHandler, &quot;CGCNOWam_ReplyHandler&quot;); <lb/>addProcedure(errorHandler, &quot;CGCNOWam_ErrorHandler&quot;); <lb/>addCode(amdecls, &quot;mainDecls&quot;, &quot;amDecls&quot;); <lb/>addCode(timedecls, &quot;mainDecls&quot;, &quot;timeDecls&quot;); <lb/>addCode(stardecls, &quot;mainDecls&quot;); <lb/>addCode(timeinit, &quot;mainInit&quot;, &quot;timeInit&quot;); <lb/>addCode(aminit, &quot;mainInit&quot;, &quot;amInit&quot;); <lb/>addCode(starinit, &quot;mainInit&quot;); <lb/>} <lb/>codeblock (block2) { <lb/>/* run send once */ <lb/></div>

			<page>37 <lb/></page>

			<div type="annex">} <lb/>codeblock (block) { <lb/>#ifdef TIME_INFO2 <lb/>$starSymbol(beginSend) = gethrtime(); <lb/>#endif <lb/>check = AM_RequestXferAsync4($starSymbol(endpoint), $val(hostAddr), 0, <lb/>2, (void *)$ref(localData), $val(numData)*sizeof(double), 0, 0, 0, 0); <lb/>if (check == -1) { <lb/>fprintf(stderr, &quot;Error in sending data\n&quot;); <lb/>fflush(stderr); <lb/>} <lb/>#ifdef TIME_INFO2 <lb/>$starSymbol(endSend) = gethrtime(); <lb/>$starSymbol(timeSend) += $starSymbol(endSend) -$starSymbol(beginSend); <lb/>printf(&quot;Cumulative time to send %lld usec\n&quot;, $starSymbol(timeSend) / <lb/>1000); <lb/>#endif <lb/>} <lb/>go { <lb/>if (runCount == 0) { <lb/>addCode(loadCode); <lb/>addCode(block); <lb/>runCount = 1; <lb/>} else { <lb/>addCode(block2); <lb/>} <lb/>} <lb/>codeblock (runtime) { <lb/>#ifdef TIME_INFO1 <lb/>endRun = gethrtime(); <lb/>timeRun = endRun -beginRun; <lb/>printf(&quot;Time to run %lld usec\n&quot;, timeRun / 1000); <lb/>#endif <lb/>} <lb/>wrapup { <lb/>addCode(runtime, &quot;mainClose&quot;, &quot;runTime&quot;); <lb/>addCode(&quot;AM_Terminate();\n&quot;, &quot;mainClose&quot;, &quot;amTerminate&quot;); <lb/>} <lb/>} <lb/></div>

			<page>38 <lb/></page>

			<div type="annex">Appendix C <lb/>Up-Sample Simulation (from Ptolemy Galaxy comments) <lb/>Converting sampling rates from 44.1 kHz to 48 kHz is a difficult problem. A naive approach <lb/>would be to interpolate (upsample) to a sampling frequency which is the least common multiple <lb/>of these two frequencies, filter to prevent aliasing, then decimate (downsample) to the desired out-<lb/>put rate. Unfortunately the sampling rate ratio in this case is 160:147. This would require inter-<lb/>polating to an intermediate frequency of 7.056 MHz. Designing a lowpass filter with a pass band <lb/>of 0-20 kHz and a stop band of 22.05-3528 kHz would be very challenging. Such a high-Q filter <lb/>would require many, many coefficients to obtain reasonable performance. <lb/>A better approach is to perform the rate conversion in multiple stages. Rate conversion ratios <lb/>are chosen by examining the prime factorization of the two sampling rates. The prime factoriza-<lb/>tions of 48000 and 44100 are 2^7 * 3 * 5^3 and 2^2 * 3^2 * 5^2 * 7^2, respectively. Thus the <lb/>ratio 48000:44100 is 2^5 * 5 : 3 * 7^2 or 160:147. In this example the conversion is performed in <lb/>four stages -2:1, 4:3, 5:7, and 4:7. <lb/>The first stage requires a filter with a relatively sharp cut-off with a transition band from 20-<lb/>22.05 kHz. Because of this, the ratio for this stage was chosen to be 2:1. With the smallest possi-<lb/>ble interpolation factor of 2, the cut-off frequency of 20 kHz is as high as possible with respect to <lb/>the intermediate sampling rate (88.2 kHz in this case). This means that the filter for this stage will <lb/>require fewer coefficients than if a higher interpolation factor had been chosen. Unfortunately, no <lb/>decimation can take place in this stage since the smallest decimation factor is 3. The first filter, <lb/>which has 173 taps, interpolates by a factor of 2 and does not decimate. The pass band is 0-20 <lb/>kHz and the stop band is 22.05-44.1 kHz. Note that the filter operates at a sampling rate of 2x44.1 <lb/>= 88.2 kHz. The output of this filter is a signal at a 88.2 kHz sampling rate with no energy above <lb/>22.05 kHz. <lb/>The second filter, which has 31 taps, interpolates by a factor of 4 and decimates by a factor of <lb/>3. The pass band is 0-20 kHz and the stop bands are 44.1 kHz wide and are centered at multiples <lb/>of 88.2 kHz (the sampling rate of the input to this stage). More specifically, the stop bands are <lb/>66.15-110.25 kHz and 154.35-176.4 kHz. Note that this filter operates at a sampling rate of <lb/>4x88.2 = 352.8 kHz. The output of this filter is a signal at a 117.6 kHz sampling rate. <lb/>The third filter, which has 33 taps, interpolates by a factor of 5 and decimates by a factor of 7. <lb/>The pass band is 0-20 kHz and the stop bands are 44.1 kHz wide and are centered at multiples of <lb/>117.6 kHz (the sampling rate of the input to this stage). More specifically, the stop bands are <lb/>95.55-139.65 kHz and 213.15-257.25 kHz. Note that this filter operates at a sampling rate of 5 x <lb/>117.6 = 588 kHz. The output of this filter is a signal at a 84 kHz sampling rate. <lb/>The fourth filter, which has 33 taps, interpolates by a factor of 4 and decimates by a factor of <lb/>7. The pass band is 0-20 kHz and the stop bands are 44.1 kHz wide and are centered at multiples <lb/>of 84 kHz (the sampling rate of the input to this stage). More specifically, the stop bands are <lb/>61.95-106.05 kHz and 145.95-168 kHz. Note that this filter operates at a sampling rate of 4 x 84 <lb/>= 336 kHz. The output of this filter is a signal at a 48 kHz sampling rate. Because the second filter <lb/>has the same interpolation factor as the fourth and operates at a higher rate, it can actually use the <lb/>same filter coefficients. </div>


	</text>
</tei>
