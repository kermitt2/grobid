<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>PARALLEL CONSTRAINT DISTRIBUTION <lb/>M. C. FERRIS y AND O. L. MANGASARIAN y <lb/>Abstract. Constraints of a mathematical program are distributed among parallel processors to-<lb/>gether with an appropriately constructed augmented Lagrangian for each processor, which contains <lb/>Lagrangian information on the constraints handled by the other processors. Lagrange multiplier in-<lb/>formation is then exchanged between processors. Convergence is established under suitable conditions <lb/>for strongly convex quadratic programs and for general convex programs. <lb/>Key words. Parallel Optimization, Augmented Lagrangians, Quadratic Programs, Convex Pro-<lb/>grams <lb/></front>

			<body>1. Introduction. We are concerned with the problem <lb/>minimize <lb/>f(x) <lb/>subject to g 1 (x) 0; . . . ; g k (x) 0 <lb/>(1.1) <lb/>where f, g 1 ; . . .; g k are di erentiable convex functions from the n{dimensional real space <lb/>IR n to IR, IR m 1 ; . . .; IR m k respectively, with f being strongly convex on IR n . Our prin-<lb/>cipal aim is to distribute the k constraint blocks among k parallel processors together <lb/>with an appropriately modi ed objective function. We then solve each of these k sub-<lb/>problems independently, share Lagrange multiplier information among the processors <lb/>and repeat. Other recently proposed decomposition methods and applications thereof <lb/>can be found in 22, 8, 5, 21]. The key to our approach lies in the precise form of <lb/>the modi ed objective function to be optimized by each processor. Considerable ex-<lb/>perimentation with various Lagrangian terms 3] has highlighted the di erence between <lb/>theoretical convergence and computational e ciency. We believe that we now have ef-<lb/>fective modi ed objectives for each processor that can best be described as augmented <lb/>Lagrangian functions 19, 20, 1]. The modi ed objectives are made up of the original <lb/>objective function plus augmented Lagrangian terms involving the constraints handled <lb/>by the other processors. Computational experience on the Sequent Symmetry S{81 <lb/>shared memory multiprocessor with constraint distribution for quadratic programs de-<lb/>rived from a least{norm solution of linear programs, has been encouraging. This is <lb/>described in Section 4 of the paper. Section 2 is devoted to the quadratic programming <lb/>case for which we obtain the strongest convergence results in Theorem 2.1. Under the <lb/>assumption of a strongly convex quadratic objective and linear independence of each of <lb/>the distributed constraint blocks, the parallel constraint distribution (PCD) algorithm <lb/>converges from any starting point for a solvable problem. The key to the convergence <lb/>proof is to show that in the dual space, the proposed parallel constraint distribution <lb/>algorithm is equivalent to a subsequentially{convergent iterative method with step{size <lb/></body>

			<front>This material is based on research supported by the Air Force O ce of Scienti c Research Grant <lb/>AFOSR{89{0410 and National Science Foundation Grants DCR{8521228 and CCR{8723091 <lb/>y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, <lb/>Wisconsin 53706 <lb/></front>

			<page>1 <lb/></page>

			<body>proposed in 11, Algorithm 2.1] for which full sequential convergence has just recently <lb/>been established 9, 4, 17]. In Section 3 we establish a weaker convergence of the PCD <lb/>algorithm (Theorem 3.2) for the general convex program (1.1) with a strongly convex <lb/>objective function. The method of proof in this section is entirely di erent from that <lb/>of Section 2, and relies on the Lipschitz continuity of the solution variables of each <lb/>subproblem in the xed Lagrangian multipliers obtained from the other subproblems <lb/>(Lemma 3.1). Unfortunately, to establish convergence, we need to assume that the <lb/>distance between successive values of the multipliers approaches zero. We believe this <lb/>assumption may be considerably relaxed and probably eliminated if one uses ideas of <lb/>nonlinear Jacobi relaxation 18] for solving nonlinear complementarity problems. <lb/>A word about our notation now. For a vector x in the n{dimensional real space IR n , <lb/>x + will denote the vector in IR n with components (x + ) i : = maxfx i ; 0g, i = 1; . . . ; n. The <lb/>standard inner product of IR n will be denoted either by hx; yi or x T y. The Euclidean <lb/>or 2{norm (x T x) 1 <lb/>2 , will be denoted by k k. For an m n real matrix A, signi ed by <lb/>A 2 IR m n , A T will denote the transpose. The identity matrix of any order will be <lb/>given by I. The nonnegative orthant in IR n will be denoted by IR n + . We will use the <lb/>convention that s = (s 1 ; . . . ; s k ), with each s i representing either a component of the <lb/>vector s or a block of components of the vector s. The meaning should be clear from <lb/>the context. <lb/>2. Parallel constraint distribution for quadratic programs. For simplicity <lb/>we consider a quadratic program with 3 blocks of inequality constraints. Routine exten-<lb/>sion to k blocks can be achieved by appropriate extension and permutation of subscripts. <lb/>Equality constraints can also be incorporated in an straightforward manner. Consider <lb/>then the problem <lb/>minimize <lb/>c T x + 1 <lb/>2 x T Qx <lb/>subject to A l x a l ; l = 1; 2; 3 <lb/>(2.1) <lb/>where c 2 IR n , Q 2 IR n n , A l 2 IR m l n , a l 2 IR m l and Q is symmetric and positive <lb/>de nite. Furthermore, let <lb/>A: = <lb/>2 <lb/>6 6 6 6 4 <lb/>A 1 <lb/>A 2 <lb/>A 3 <lb/>3 <lb/>7 7 7 7 5 <lb/>and a: = <lb/>2 <lb/>6 6 6 6 4 <lb/>a 1 <lb/>a 2 <lb/>a 3 <lb/>3 <lb/>7 7 7 7 5 <lb/>At iteration i of the algorithm we distribute the constraints of this problem among <lb/>3 parallel processors (l = 1; 2; 3) as follows <lb/>minimize x l <lb/>c T x l + 1 <lb/>2 x T <lb/>l Qx l + 1 <lb/>2 <lb/>&quot; P 3 <lb/>j=1 <lb/>j6 =l <lb/>(A j x l ? a j ) + p i <lb/>jl + <lb/>2 # <lb/>+ x T <lb/>l r i <lb/>l <lb/>subject to <lb/>A l x l a l <lb/>(2.2) <lb/></body>

			<page>2 <lb/></page>

			<body>where is a positive number and p i <lb/>jl and r i <lb/>l , j; l = 1; 2; 3 are de ned below in (2.20) <lb/>and (2.21). We note that the p i <lb/>jl play the roles of multipliers and in fact converge <lb/>to the optimal multipliers eventually, while r i <lb/>l replaces estimates of the multipliers <lb/>by their most recent values obtained from each of the other subproblems (see (2.20)). <lb/>Note that the objectives of the subproblems (2.2) are quadratic augmented Lagrangians <lb/>19, 20, 1] perturbed by the linear terms x T <lb/>l r i <lb/>l . The motivation of this reformulation is <lb/>that in each subproblem some constraints are treated explicitly as constraints while the <lb/>remaining ones are treated as augmented Lagrangian terms in the objective function. <lb/>The updating of the multipliers is done by solving the subproblems explicitly rather <lb/>than the traditional, and often slow, gradient updating scheme in the dual space of <lb/>the augmented Lagrangian approach 1]. Hence our method does not use a gradient <lb/>or a proximal point multiplier updating scheme. The key to the convergence of our <lb/>algorithm, for the quadratic case, is the choice of the parameters p i <lb/>jl and r i <lb/>l in such a <lb/>way that the PCD algorithm is equivalent to a convergent iterative matrix{splitting <lb/>method 11, 9, 14, 17] for a symmetric linear complementarity problem in the dual <lb/>variables of the problem. This choice is by no means unique and we have experimented <lb/>computationally with a number of choices for the p i <lb/>jl and r i <lb/>l which we report on in <lb/>Section 4. We shall establish convergence of only one of our choices in this section of the <lb/>paper, which may not necessarily be the best computationally. Further experimentation <lb/>is needed to determine the best splitting. We now proceed to show how the parameters <lb/>p i <lb/>jl and r i <lb/>l are chosen and to justify these choices from the point of view of convergent <lb/>matrix splitting. A simpler splitting approach for constraint distribution for quadratic <lb/>programs is given in 6]. <lb/>Firstly, note that it is easy to verify algebraically the following equivalence for any <lb/>two vectors b and d in IR m l <lb/>b = d + () b ? d 0; b T (b ? d) = 0; b 0 <lb/>(2.3) <lb/>Also, the linear complementarity problem (LCP) in the variable z <lb/>Mz + q 0; hz; Mz + qi = 0; z 0 <lb/>can be written as <lb/>z = (z ? Mz ? q) + <lb/>by using (2.3). <lb/>Now, let ( x i+1 <lb/>l ; s i+1 <lb/>l ) 2 IR n+m l , l = 1; 2; 3, i = 1; . . . satisfy the Karush{Kuhn{Tucker <lb/>conditions 10] for subproblems (2.2). We shall signify this by <lb/>( x i+1 <lb/>l ; s i+1 <lb/>l ) 2 arg KKT(2:2) <lb/>Using (2.3), we see that ( x i+1 <lb/>l ; s i+1 <lb/>l ) satisfy the following Karush{Kuhn{Tucker condi-<lb/>tions <lb/>c + Q x i+1 <lb/>l <lb/>+ P 3 <lb/>j=1 <lb/>j6 =l A T <lb/>j <lb/>(A j x i+1 <lb/>l <lb/>? a j ) + p i <lb/>jl + + r i <lb/>l + A T <lb/>l s i+1 <lb/>l <lb/>= 0 <lb/>s i+1 <lb/>l <lb/>= s i+1 <lb/>l <lb/>+ (A l x i+1 <lb/>l <lb/>? a l ) + <lb/>l = 1; 2; 3 <lb/>(2.4) <lb/></body>

			<page>3 <lb/></page>

			<body>or equivalently <lb/>x i+1 <lb/>l <lb/>= ?Q ?1 (c + P 3 <lb/>j=1 <lb/>j6 =l A T <lb/>j t i+1 <lb/>jl + r i <lb/>l + A T <lb/>l s i+1 <lb/>l ) <lb/>s i+1 <lb/>l <lb/>= s i+1 <lb/>l <lb/>+ (A l x i+1 <lb/>l <lb/>? a l ) + <lb/>t i+1 <lb/>jl = (A j x i+1 <lb/>l <lb/>? a j ) + p i <lb/>jl + <lb/>l = 1; 2; 3; j = 1; 2; 3; j 6 = l <lb/>(2.5) <lb/>Eliminating x i+1 <lb/>l <lb/>by using the rst equation of (2.5) leads to <lb/>s i+1 <lb/>l <lb/>= s i+1 <lb/>l <lb/>? A l Q ?1 (A T <lb/>l s i+1 <lb/>l <lb/>+ P 3 <lb/>k=1 <lb/>k6 =l A T <lb/>k t i+1 <lb/>kl + r i <lb/>l + c) + a l + <lb/>t i+1 <lb/>jl = ? A j Q ?1 (A T <lb/>l s i+1 <lb/>l <lb/>+ P 3 <lb/>k=1 <lb/>k6 =l A T <lb/>k t i+1 <lb/>kl + r i <lb/>l + c) + a j + p i <lb/>jl + <lb/>(2.6) <lb/>for l = 1; 2; 3;, j = 1; 2; 3, j 6 = l. Note that by (2.3), this is an LCP in the variable z i+1 <lb/>de ned by <lb/>z i+1 : = ( s i+1 1 ; s i+1 2 ; s i+1 3 ; t i+1 12 ; t i+1 23 ; t i+1 31 ; t i+1 13 ; t i+1 21 ; t i+1 32 ) <lb/>(2.7) <lb/>In order to express the above LCP succinctly, we introduce the following notation. <lb/>De ne the permutations <lb/>1 = (1; 2; 3); 2 = (2; 3; 1); 3 = (3; 1; 2); <lb/>with j (k) denoting the kth component of j , j; k = 1; 2; 3. We use the following <lb/>conventions to group p i <lb/>jl , t i <lb/>jl and r i <lb/>l : <lb/>r i j : = r i <lb/>j (k) <lb/>and <lb/>p i j : = p i <lb/>k; j (k) ; t i j : = t i <lb/>k; j (k) ; j = 2; 3; k = 1; 2; 3 <lb/>(For example, r i 2 = (r i <lb/>2 ; r i <lb/>3 ; r i <lb/>1 ) and t i 2 = (t i 12 ; t i 23 ; t i 31 )). Using this notation, (2.6) <lb/>corresponds to the following symmetric LCP in the variable z i+1 <lb/>B z i+1 + h i + q 0; <lb/>D z i+1 ; B z i+1 + h i + q <lb/>E = 0; z i+1 0 <lb/>(2.8) <lb/>where <lb/>B: = <lb/>2 <lb/>6 6 6 6 4 <lb/>R 1 R T 2 <lb/>R T 3 <lb/>R 2 I + R 1 R T 2 <lb/>R 3 R 2 I + R 1 <lb/>3 <lb/>7 7 7 7 5 <lb/>(2.9) <lb/></body>

			<page>4 <lb/></page>

			<body>with <lb/>R : = <lb/>2 <lb/>6 6 6 6 4 <lb/>A 1 Q ?1 A T 1 <lb/>0 <lb/>0 <lb/>0 <lb/>A 2 Q ?1 A T 2 <lb/>0 <lb/>0 <lb/>0 <lb/>A 3 Q ?1 A T 3 <lb/>3 <lb/>7 7 7 7 5 <lb/>R : = <lb/>2 <lb/>6 6 6 6 4 <lb/>0 <lb/>A 1 Q ?1 A T 2 <lb/>0 <lb/>0 <lb/>0 <lb/>A 2 Q ?1 A T 3 <lb/>A 3 Q ?1 A T 1 <lb/>0 <lb/>0 <lb/>3 <lb/>7 7 7 7 5 <lb/>R : = <lb/>2 <lb/>6 6 6 6 4 <lb/>0 <lb/>0 <lb/>A 1 Q ?1 A T 3 <lb/>A 2 Q ?1 A T 1 <lb/>0 <lb/>0 <lb/>0 <lb/>A 3 Q ?1 A T 2 <lb/>0 <lb/>3 <lb/>7 7 7 7 5 <lb/>(2.10) <lb/>and <lb/>h i : = <lb/>2 <lb/>6 6 6 6 4 <lb/>AQ ?1 r i 1 <lb/>AQ ?1 r i 2 ? p i 2 <lb/>AQ ?1 r i 3 ? p i 3 <lb/>3 <lb/>7 7 7 7 5 <lb/>; q: = <lb/>2 <lb/>6 6 6 6 4 <lb/>AQ ?1 c + a <lb/>AQ ?1 c + a <lb/>AQ ?1 c + a <lb/>3 <lb/>7 7 7 7 5 <lb/>(2.11) <lb/>(For the general case, the analog of these equations can be constructed easily by using <lb/>the appropriate permutations of 1; . . . ; l and noting that the nonzero entries of the <lb/>R i correspond precisely to the ith permutation). Note that this algorithm can be <lb/>implemented in parallel in the x space as outlined at the start of this section for any <lb/>choice of p i <lb/>jl and r i <lb/>l . In the remainder of this section, we show how to choose p i <lb/>jl and r i <lb/>l <lb/>in order to guarantee the convergence of the algorithm. Our convergence analysis will <lb/>be based on results for matrix{splitting methods for complementarity problems, and <lb/>we give a very brief review of the pertinent results in the following paragraph. <lb/>A matrix{splitting method for solving the LCP <lb/>Mz + q 0; hz; Mz + qi = 0; z 0 <lb/>(2.12) <lb/>uses a \regular splitting&quot; M = B + C, with M, B, and C satisfying certain properties <lb/>such as: <lb/>B + C symmetric, <lb/>(1 ? =2)B ? ( =2)C positive de nite for some 2 (0; 1] <lb/>(2.13) <lb/>or <lb/>B + C symmetric positive semide nite, <lb/>(1 ? =2)B ? ( =2)C positive de nite for some 2 (0; 1] <lb/>(2.14) <lb/></body>

			<page>5 <lb/></page>

			<body>Under (2.13), a solution of (2.12) is obtained 11] from each accumulation point of the <lb/>sequence fz i g generated by iteratively solving the following LCP for z i+1 <lb/>B z i+1 + Cz i + q 0; <lb/>D z i+1 ; B z i+1 + Cz i + q <lb/>E = 0; z i+1 0 <lb/>(2.15) <lb/>and then determining z i+1 by using a step{size , that is <lb/>z i+1 = (1 ? )z i + z i+1 ; 2 (0; 1] <lb/>(2.16) <lb/>Under assumption (2.14) the whole sequence fz i g generated by (2.15) and (2.16) con-<lb/>verges to a solution of (2.12) provided the latter is solvable 9, 4, 17]. <lb/>To apply these results to our algorithm, we have to choose p i <lb/>jl and r i <lb/>l as particular <lb/>functions of <lb/>z i : = (s i 1 ; s i 2 ; s i 3 ; t i 12 ; t i 23 ; t i 31 ; t i 13 ; t i 21 ; t i 32 ) <lb/>(2.17) <lb/>so that <lb/>h i = Cz i , for some matrix C <lb/>(2.18) <lb/>and <lb/>B + C constitutes a \regular splitting&quot; of some symmetric M <lb/>The matrix C is determined by the choice of p i <lb/>jl and r i <lb/>l in (2.2) or equivalently in <lb/>(2.11), and this is precisely where the power (and at the same time the di culty) of <lb/>the proposed method lies. <lb/>The simplest choice for p i <lb/>jl and r i <lb/>l we propose for the nonlinear (not necessarily <lb/>quadratic) case of Section 3 and for which we establish convergence under somewhat <lb/>more stringent assumptions is the following <lb/>p i <lb/>jl = s i <lb/>j ; r i <lb/>l = 0; l = 1; 2; 3; j = 1; 2; 3; j 6 = l <lb/>(2.19) <lb/>Unfortunately this simple choice in the quadratic case leads to a nonsymmetric C <lb/>and hence a nonsymmetric M in (2.12). The convergence conditions for splitting non-<lb/>symmetric LCP&apos;s are quite stringent 2, Chapter 5] and not useful for our proposed <lb/>applications here. <lb/>We have therefore settled on choices for the parameters p i <lb/>jl and r i <lb/>l which are di erent <lb/>to those in (2.19), and which generate a symmetric positive semide nite M. By choosing <lb/>su ciently small, it is easily seen that (2.14) is satis ed, because by (2.9), the matrix <lb/>B is positive de nite if we assume that each A l , l = 1; 2; 3, has linearly independent rows. <lb/>This can be shown by substituting for R 1 , R 2 and R 3 from (2.10) into the de nition <lb/>of B. There are a number of choices of the p i <lb/>jl and r i <lb/>l that generate a symmetric <lb/>positive semide nite M and hence a convergent scheme. Our preliminary computational <lb/>experience does not provide a clear cut indication which is the best choice for p i <lb/>jl and <lb/>r i <lb/>l among the convergent schemes. We believe this requires further theoretical and <lb/></body>

			<page>6 <lb/></page>

			<body>computational study. However, for concreteness, we wish to present at least one speci c <lb/>choice of C that results from the following choices of p i <lb/>jl and r i <lb/>l : <lb/>r i <lb/>1 = A T 2 (s i 2 ? t i 21 ) + A T 3 (s i 3 ? t i 31 ) <lb/>r i <lb/>2 = A T 1 (s i 1 ? t i 12 ) + A T 3 (s i 3 ? t i 32 ) <lb/>r i <lb/>3 = A T 1 (s i 1 ? t i 13 ) + A T 2 (s i 2 ? t i 23 ) <lb/>(2.20) <lb/>p i 2 = t i 2 + AQ ?1 A T (s i ? t i 2 ) <lb/>p i 3 = t i 3 + AQ ?1 A T (s i ? t i 3 ) <lb/>(2.21) <lb/>We note that the r i <lb/>l substitute the latest Lagrange multiplier value s i <lb/>l obtained from <lb/>each subproblem solution for the t i <lb/>jl , both of which eventually converge to an optimal <lb/>Lagrange multiplier value. The p i <lb/>jl terms are essentially multiplier value estimates given <lb/>by t i <lb/>jl plus additional terms that converge to zero. The additional terms are added in <lb/>order to produce a symmetric C and hence a symmetric M. These above choices of p i <lb/>jl <lb/>and r i <lb/>l lead to the following matrix C de ned through the relations (2.17), (2.18) and <lb/>(2.11), <lb/>C = <lb/>2 <lb/>6 6 6 6 4 <lb/>R 2 + R 3 <lb/>?R T 2 <lb/>?R T 3 <lb/>?R 2 ?I + R 2 + R 3 <lb/>?R T 2 <lb/>?R 3 <lb/>?R 2 <lb/>?I + R 2 + R 3 <lb/>3 <lb/>7 7 7 7 5 <lb/>(2.22) <lb/>with R 2 and R 3 de ned in (2.10). Addition of the matrices B and C gives the symmetric <lb/>block{diagonal matrix M <lb/>M = <lb/>2 <lb/>6 6 6 6 4 <lb/>H 0 0 <lb/>0 H 0 <lb/>0 0 H <lb/>3 <lb/>7 7 7 7 5 <lb/>(2.23) <lb/>where <lb/>H: = R 1 + R 2 + R 3 = AQ ?1 A T <lb/>(2.24) <lb/>Note that if our original quadratic program (2.1) is feasible, then it it solvable. Hence its <lb/>Wolfe dual is solvable, which is equivalent to the solvability of the LCP (2.12) with M <lb/>as de ned in (2.23) and q as in (2.11). In fact, the LCP (2.12) constitutes a replication <lb/>of the Wolfe dual 3 times. <lb/>We are now ready to de ne the PCD algorithm for the quadratic program (2.1). <lb/></body>

			<page>7 <lb/></page>

			<body>2.1. PCD algorithm for quadratic programming. <lb/>Initialization: Start with any s 0 <lb/>l , t 0 <lb/>jl , l = 1; 2; 3, j = 1; 2; 3, j 6 = l. <lb/>Parallel iteration: In parallel, (l = 1; 2; 3), perform the following steps. <lb/>Having s i <lb/>l , t i <lb/>jl , j = 1; 2; 3, j 6 = l compute: <lb/>1. r i <lb/>l , p i <lb/>jl , j = 1; 2; 3, j 6 = l from (2.20) and (2.21). <lb/>2. ( x i+1 <lb/>l ; s i+1 <lb/>l ) 2 arg KKT(2:2) <lb/>3. t i+1 <lb/>jl = (A j x i+1 <lb/>l <lb/>? a j ) + p i <lb/>jl + , j = 1; 2; 3, j 6 = l <lb/>4. (s i+1 <lb/>l ; t i+1 <lb/>jl ) = (1 ? )(s i <lb/>l ; t i <lb/>jl ) + ( s i+1 <lb/>l ; t i+1 <lb/>jl ), j = 1; 2; 3, j 6 = l with 2 (0; 1] <lb/>satisfying (2.30) below. <lb/>2.2. Remark. We note that the subproblems (2.2) of the PCD Algorithm 2.1 di-<lb/>vide the constraints of the original quadratic program (2.1) between them in the form <lb/>of explicit constraints as well as augmented Lagrangian terms involving the remain-<lb/>ing constraints. The principal objective that has been achieved is that the explicit <lb/>constraints of each of the subproblems are a subset of the constraints of the original <lb/>problem. <lb/>2.3. Remark: Symmetric monotone LCP as dual of convex quadratic <lb/>program with nonsmooth KKT conditions. It is interesting to note that the PCD <lb/>algorithm is a matrix{splitting iterative method for a symmetric monotone LCP that <lb/>can be associated with a dual formulation of a convex program with nonsmooth Karush{ <lb/>Kuhn{Tucker conditions. Thus consider such a program: <lb/>minimize x <lb/>c T x + 1 <lb/>2 x T Qx + 1 <lb/>2 (Hx ? h) + <lb/>2 <lb/>subject to <lb/>Bx b <lb/>(2.25) <lb/>where Q is symmetric positive de nite. The necessary and su cient Karush{Kuhn{ <lb/>Tucker conditions for this problem are <lb/>c + Qx + H T (Hx ? h) + + B T s = 0 <lb/>s = (s + Bx ? b) + <lb/>(2.26) <lb/>De ning a new variable t as <lb/>t = (Hx ? h) + <lb/>(2.27) <lb/>and solving the rst Karush{Kuhn{Tucker condition for x gives <lb/>x = ?Q ?1 (H T t + B t s + c) <lb/>(2.28) <lb/></body>

			<page>8 <lb/></page>

			<body>Substituting for x in the second equation of (2.26) and in (2.27) gives the following <lb/>symmetric monotone linear complementarity problem in the variables (s; t) <lb/>2 <lb/>6 4 <lb/>v <lb/>w <lb/>3 <lb/>7 5 = <lb/>2 <lb/>6 <lb/>BQ ?1 B T BQ ?1 H T <lb/>HQ ?1 B T I + HQ ?1 H T <lb/>3 <lb/>7 5 <lb/>2 <lb/>6 4 <lb/>s <lb/>t <lb/>3 <lb/>7 5 + <lb/>6 4 <lb/>BQ ?1 c + b <lb/>HQ ?1 c + h <lb/>3 <lb/>7 5 0 <lb/>(s T ; t T ) <lb/>2 <lb/>6 4 <lb/>v <lb/>w <lb/>3 <lb/>7 5 = 0; <lb/>2 <lb/>6 4 <lb/>s <lb/>t <lb/>3 <lb/>7 5 0 <lb/>(2.29) <lb/>We then have the following duality relation between the convex program (2.25) and <lb/>the symmetric monotone LCP (2.29). For each solution (s; t) of (2.29), x de ned by <lb/>(2.28) is the unique solution of (2.25). Conversely, for each Karush{Kuhn{Tucker point <lb/>(x; s) of (2.25), the point (s; t), with t de ned by (2.27), solves (2.29). Note that the <lb/>symmetric LCP (2.29) is equivalent to the following quadratic program in (s; t): <lb/>minimize (s;t) 0 <lb/>1 <lb/>2 (s T ; t T ) <lb/>2 <lb/>6 4 <lb/>BQ ?1 B T BQ ?1 H T <lb/>HQ ?1 B T I + HQ ?1 H T <lb/>3 <lb/>7 5 <lb/>2 <lb/>6 4 <lb/>s <lb/>t <lb/>3 <lb/>7 5 + (s T ; t T ) <lb/>2 <lb/>6 4 <lb/>BQ ?1 c + b <lb/>HQ ?1 c + h <lb/>3 <lb/>7 5 <lb/>We are now ready to establish convergence of the PCD Algorithm 2.1 <lb/>Theorem 2.1 (PCD Convergence for Quadratic Programs). Let (2.1) <lb/>be feasible and let Q be symmetric positive de nite and let each of A l , l = 1; 2; 3, have <lb/>linearly independent rows. Then the sequence <lb/>n <lb/>s i <lb/>l ; t i <lb/>jl <lb/>o <lb/>, l = 1; 2; 3, j = 1; 2; 3, j 6 = l, <lb/>i = 0; 1; . . ., generated by the PCD Algorithm 2.1 converges to ( s l ; t jl ), l = 1; 2; 3, j = <lb/>1; 2; 3, j 6 = l and each of the sequences fx i <lb/>l g, l = 1; 2; 3, converges to the unique solution <lb/>x of (2.1). Furthermore, ( x; s), ( x; t 12 ; t 23 ; t 31 ) and ( x; t 13 ; t 21 ; t 32 ) are all Karush{Kuhn{ <lb/>Tucker points for (2.1), and p jl = t jl , l = 1; 2; 3, j = 1; 2; 3, j 6 = l <lb/>Proof. Let x i+1 <lb/>l , l = 1; 2; 3, be the unique solution of the subproblems (2.2). Hence <lb/>x i+1 <lb/>l <lb/>and some s i+1 <lb/>l <lb/>2 IR m l satisfy the Karush{Kuhn{Tucker conditions (2.4), or equiv-<lb/>alently, ( x i+1 <lb/>l ; s i+1 <lb/>l ) and some t i+1 <lb/>jl , l = 1; 2; 3, j = 1; 2; 3, j 6 = l satisfy (2.5). This in <lb/>turn is equivalent to z i+1 , as de ned by (2.7), satisfying the LCP (2.15). By the choice <lb/>of r i <lb/>l , p i <lb/>jl , l = 1; 2; 3, j = 1; 2; 3, j 6 = l of (2.20) and (2.21) it follows that the matrix <lb/>M = B + C, given by (2.23) and (2.24), is symmetric and positive semide nite. Fur-<lb/>thermore, B is positive de nite by virtue of the linear independence of A l , l = 1; 2; 3. <lb/>Thus if is chosen su ciently small, and speci cally such that <lb/>0 &lt; <lb/>1 and &lt; 2(min eigenvalue(B) / max eigenvalue(M)) <lb/>(2.30) <lb/>it follows that (2.13) above (which is condition (6) of 11]) and (2.14) above (which is <lb/>condition (4.1) of 9]) are satis ed. Hence, since the LCP (2.12) is solvable, the sequence <lb/>fz i g converges 9, Theorem 2 and Example 3] to a solution of the LCP (2.12), and by <lb/>z i+1 = (1 ? )z i + z i+1 , so does the sequence f z i g. It follows by (2.4), (2.5), (2.20) <lb/></body>

			<page>9 <lb/></page>

			<body>and (2.21) that in the limit we have <lb/>c + Q x l + P 3 <lb/>j=1 <lb/>j6 =l A T <lb/>j t jl + r l + A T <lb/>l s l = <lb/>s l = ( s l + (A l x l ? a l )) + <lb/>t jl = ( (A j x l ? a j ) + p jl ) + <lb/>l = 1; 2; 3; j = 1; 2; 3; j 6 = l <lb/>where <lb/>r l = P 3 <lb/>j=1 <lb/>j6 =l A T <lb/>j ( s j ? t jl ) l = 1; 2; 3 <lb/>and hence that <lb/>c + Q x l + P 3 <lb/>j=1 A T <lb/>j s j = 0 <lb/>s l = ( s l + (A l x l ? a l )) + <lb/>l = 1; 2; 3 <lb/>(2.31) <lb/>It is now clear form the nonsingularity of Q that <lb/>x 1 = x 2 = x 3 =: x <lb/>Conditions (2.31) become then the necessary and su cient conditions for x to be the <lb/>unique solution of (2.1) with multipliers as indicated in the statement of the theorem. <lb/>Furthermore, since z = ( s 1 ; s 2 ; s 3 ; t 12 ; t 23 ; t 31 ; t 13 ; t 21 ; t 32 ) solves the 3{block LCP (2.12) <lb/>with identical M and q sub{blocks as de ned by (2.23) and (2.11) respectively, it follows <lb/>that each of ( s 1 ; s 2 ; s 3 ), ( t 12 ; t 23 ; t 31 ) and ( t 13 ; t 21 ; t 32 ) solve any one of the 3 sub{blocks <lb/>of LCP (2.12) and hence 13, Corollary 2] their di erences lie in the nullspace of H. <lb/>Thus <lb/>H <lb/>0 <lb/>B B B B @ <lb/>2 <lb/>6 6 6 6 4 <lb/>s 1 <lb/>s 2 <lb/>s 3 <lb/>3 <lb/>7 7 7 7 5 <lb/>? <lb/>2 <lb/>6 6 6 6 4 <lb/>t 12 <lb/>t 23 <lb/>t 31 <lb/>3 <lb/>7 7 7 7 5 <lb/>1 <lb/>C C C C A <lb/>= 0 and H <lb/>0 <lb/>B B B B @ <lb/>2 <lb/>6 6 6 6 4 <lb/>s 1 <lb/>s 2 <lb/>s 3 <lb/>3 <lb/>7 7 7 7 5 <lb/>? <lb/>2 <lb/>6 6 6 6 4 <lb/>t 13 <lb/>t 21 <lb/>t 32 <lb/>3 <lb/>7 7 7 7 5 <lb/>1 <lb/>C C C C A <lb/>= 0 <lb/>(2.32) <lb/>Relations (2.32), and relations (2.21) in the limit, imply that p jl = t jl , l = 1; 2; 3, <lb/>j = 1; 2; 3, j 6 = l. <lb/>3. Parallel constraint distribution for convex programs. We extend our <lb/>ideas now to general convex programs with strongly convex objective functions. For <lb/>simplicity of notation we consider the 2{block problem <lb/>minimize <lb/>f(x) <lb/>subject to g 1 (x) 0; g 2 (x) 0 <lb/>(3.1) <lb/>where f: IR n ! IR, g 1 : IR n ! IR m 1 , g 2 : IR n ! IR m 2 are di erentiable convex functions <lb/>on IR n , with f strongly convex with modulus k, and g 1 , g 2 Lipschitz continuous with <lb/></body>

			<page>10 <lb/></page>

			<body>constant K on IR n . We begin with the following straightforward Lipschitz continuity <lb/>result. <lb/>Lemma 3.1. Let f, g 1 , g 2 be di erentiable convex functions on IR n with f strongly <lb/>convex with modulus k and let g 1 be Lipschitz continuous with constant K on IR n . Let <lb/>g 2 satisfy a constraint quali cation on the nonempty set fx j g 2 (x) 0g. Then <lb/>x(u 1 ): = arg min <lb/>( <lb/>f(x) + 1 <lb/>2 ( g 1 (x) + u 1 ) + <lb/>2 j g 2 (x) 0 <lb/>) <lb/>is Lipschitz continuous on IR m 1 <lb/>+ with Lipschitz constant K <lb/>2k (1 + <lb/>q 1 + 4k= K 2 ). <lb/>Proof. Let u 1 , u 1 2 IR m 1 <lb/>+ and x: = x(u 1 ) and x: = x( u 1 ). By the Karush{Kuhn{ <lb/>Tucker conditions, there exist v 2 , v 2 2 IR m 2 such that <lb/>rf(x) + ( g 1 (x) + u 1 ) T <lb/>+ rg 1 (x) + v T <lb/>2 rg 2 (x) = 0 <lb/>g 2 (x) 0; hv 2 ; g 2 (x)i = 0; v 2 0 <lb/>and <lb/>rf( x) + ( g 1 ( x) + u 1 ) T <lb/>+ rg 1 ( x) + v T <lb/>2 rg 2 ( x) = 0 <lb/>g 2 ( x) 0; h v 2 ; g 2 ( x)i = 0; v 2 0 <lb/>By the strong convexity of f we have that <lb/>k k x ? xk 2 (rf( x) ? rf(x))( x ? x) <lb/>This together with the Karush{Kuhn{Tucker conditions gives <lb/>k k x ? xk 2 <lb/>( g 1 ( x) + u 1 ) T <lb/>+ rg 1 ( x) + v T <lb/>2 rg 2 ( x) ? ( g 1 (x) + u 1 ) T <lb/>+ rg 1 (x) ? v T <lb/>2 rg 2 (x) (x ? x) <lb/>D ( g 1 (x) + u 1 ) + ? ( g 1 ( x) + u 1 ) + ; g 1 ( x) ? g 1 (x) <lb/>E + hv 2 ? v 2 ; g 2 ( x) ? g 2 (x)i <lb/>where the last inequality above follows from the following inequality <lb/>hw ? w; h(x) ? h( x)i (w T rh(x) ? w T rh( x))(x ? x) <lb/>for a convex di erentiable h: IR n ! IR k and w, w 2 IR k + . The Karush{Kuhn{Tucker <lb/>conditions allow us to drop nonpositive term hv 2 ? v 2 ; g 2 ( x) ? g 2 (x)i thus giving us <lb/>k k x ? xk 2 D ( g 1 (x) + u 1 ) + ? ( g 1 ( x) + u 1 ) + ; g 1 ( x) ? g 1 (x) <lb/>E <lb/>From the fundamental properties of the projection operator () + , we have for y, z 2 IR m <lb/>D y ? z; (y) + ? (z) + <lb/>E 0 <lb/></body>

			<page>11 <lb/></page>

			<body>so that <lb/>k k x ? xk 2 <lb/>1 D ( g 1 (x) + u 1 ) + ? ( g 1 ( x) + u 1 ) + ; u 1 ? u 1 <lb/>E <lb/>1 ( g 1 (x) + u 1 ) + ? ( g 1 ( x) + u 1 ) + ku 1 ? u 1 k <lb/>1 k g 1 (x) + u 1 ? g 1 ( x) ? u 1 k ku 1 ? u 1 k <lb/>K kx ? xk ku 1 ? u 1 k + 1 ku 1 ? u 1 k 2 <lb/>De ning d: = k x ? xk and e: = ku 1 ? u 1 k we obtain the quadratic inequality in d <lb/>kd 2 ? Ked ? 1 e 2 0 <lb/>and hence d must lie between the roots <lb/>d = <lb/>Ke <lb/>q <lb/>K 2 e 2 + 4ke 2 = <lb/>2k <lb/>Thus <lb/>d K <lb/>2k 1 + <lb/>q <lb/>1 + 4k= K 2 ]e <lb/>which gives the required Lipschitz continuity. <lb/>We are now able to state a parallel constraint distribution algorithm for the convex <lb/>program (3.1) and establish its convergence. <lb/>Theorem 3.2 (PCD Algorithm and Convergence for Convex Pro-<lb/>grams). Let f: IR n ! IR, g 1 : IR n ! IR m 1 , g 2 : IR n ! IR m 2 be continuously di eren-<lb/>tiable convex functions on IR n with f strongly convex and g 1 , g 2 Lipschitz continu-<lb/>ous on IR n . Let g(x): = <lb/>2 <lb/>6 4 <lb/>g 1 (x) <lb/>g 2 (x) <lb/>3 <lb/>7 5 and let g 1 and g 2 satisfy some constraint quali -<lb/>cation on the nonempty sets fx j g 1 (x) 0g and fx j g 2 (x) 0g respectively. De ne <lb/>s i : = <lb/>2 <lb/>6 4 <lb/>s i 1 <lb/>s i 2 <lb/>3 <lb/>7 5 2 IR m 1 +m 2 and start with s 0 1 = 0, s 0 2 = 0. Given s i determine s i+1 as <lb/>follows: <lb/>(x i+1 1 ; s i+1 1 ) 2 arg KKT(min <lb/>( <lb/>f(x) + 1 <lb/>2 <lb/>g 2 (x) + s i 2 + <lb/>2 j g 1 (x) 0 <lb/>) <lb/>) <lb/>(x i+1 2 ; s i+1 2 ) 2 arg KKT(min <lb/>( <lb/>f(x) + 1 <lb/>2 <lb/>g 1 (x) + s i 1 + <lb/>2 j g 2 (x) 0 <lb/>) <lb/>) <lb/>(3.2) <lb/>Assume that fs i+1 ? s i g ! 0, then for each accumulation point s such that fs i j g ! s, <lb/>n x i j <lb/>1 <lb/>o and <lb/>n x i j <lb/>2 <lb/>o converge to arg minff(x) j g(x) 0g. <lb/></body>

			<page>12 <lb/></page>

			<body>Proof. By Lemma 3.1, x i+1 1 : = x 1 (s i ) x i+1 2 : = x 2 (s i ) are continuous. Let fs i j g ! s. <lb/>Hence fs i j +1 g ! s, <lb/>n <lb/>x i j <lb/>1 <lb/>o ! x 1 and <lb/>n <lb/>x i j <lb/>o ! x 2 . Invoking the continuity of the <lb/>Karush{Kuhn{Tucker conditions we have at these limits <lb/>rf( x 1 ) + ( g 2 ( x 1 ) + s 2 ) T <lb/>+ rg 2 ( x 1 ) + s T 1 rg 1 ( x 1 ) = 0 <lb/>s 1 = ( g 1 ( x 1 ) + s 1 ) + <lb/>and <lb/>rf( x 2 ) + ( g 1 ( x 2 ) + s 1 ) T <lb/>+ rg 1 ( x 2 ) + s T 2 rg 2 ( x 2 ) = 0 <lb/>s 2 = ( g 2 ( x 2 ) + s 2 ) + <lb/>Hence <lb/>rf( x 1 ) + ( g 2 ( x 1 ) + s 2 ) T <lb/>+ rg 2 ( x 1 ) + ( g 1 ( x 1 ) + s 1 ) T <lb/>+ rg 1 ( x 1 ) = 0 <lb/>and <lb/>rf( x 2 ) + ( g 1 ( x 2 ) + s 1 ) T <lb/>+ rg 1 ( x 2 ) + ( g 2 ( x 2 ) + s 2 ) T <lb/>+ rg 2 ( x 2 ) = 0 <lb/>Thus <lb/>x 1 = x 2 = arg minff(x) + 1 <lb/>2 ( g(x) + s) + <lb/>2 g <lb/>because the objective of the last minimization problem is strongly convex. Hence ( x 1 ; s) <lb/>and ( x 2 ; s) satisfy the Karush{Kuhn{Tucker conditions of minff(x) j g(x) 0g and <lb/>thus x 1 = x 2 = arg minff(x) j g(x) 0g. <lb/>Without going into much detail, we note that it is possible under suitable assump-<lb/>tions to solve for x i+1 1 in terms of (s i+1 1 ; s i 2 ) and x i+1 2 in terms of (s i 1 ; s i+1 2 ), in which case <lb/>the PCD Algorithm of (3.2) can be rewritten as the following nonlinear Jacobi iteration <lb/>18] for solving a nonlinear complementarity problem <lb/>s i+1 1 = g 1 (x 1 (s i+1 1 ; s i 2 )) + s i+1 1 + <lb/>s i+1 2 = g 2 (x 2 (s i 1 ; s i+1 2 )) + s i+1 2 + <lb/>(3.3) <lb/>Improved convergence proofs (see, for example, 18]) may be possible, based on this <lb/>equivalent Jacobi iteration instead of (3.2). <lb/>4. Computational experience. We have tested out the algorithms of the previ-<lb/>ous sections on some linear programming problems. The standard form linear program <lb/>minimize <lb/>c T x <lb/>subject to <lb/>Ax = b <lb/>x 0 <lb/></body>

			<page>13 <lb/></page>

			<body>has the dual problem <lb/>maximize <lb/>b T y <lb/>subject to A T y c <lb/>(4.1) <lb/>and these problems are in precisely the form of our preceding discussion except the <lb/>objective is not strongly convex. In order to strongly convexify the objective we have <lb/>used the least two{norm formulation 15, 12], where for (0; ] for some &gt; 0, the <lb/>solution of <lb/>minimize ?b T y + 2 y T y <lb/>subject to A T y c <lb/>(4.2) <lb/>is the least two{norm solution of (4.1). For the purpose of our computation, a value of <lb/>= 10 ?6 was used. <lb/>We have split up the problems as follows: rstly, the user has speci ed the number <lb/>of processors available, and the problem has been split into that many blocks. If the <lb/>number of constraints in each block is not the same we have added to each block, <lb/>combinations of constraints from other blocks to make the number of constraints in <lb/>each block equal, with the aim of balancing the load between processors. <lb/>The PCD Algorithm 3.2 of Section 3 was implemented on the Sequent Symmetry S{ <lb/>81 shared memory multiprocessor. The subproblems were solved on each processor using <lb/>MINOS 5.3, a more recent version of 16]. The explicit constraints in each subproblem <lb/>remained xed throughout the computation but the blocks were not chosen to satisfy <lb/>the linear independence assumption. <lb/>We have used the following heuristic scheme to update the augmented Lagrangian <lb/>parameter, . Initially it is set at 10 and is increased by a factor of 4 only when the <lb/>norm of the violation of the constraints increases. <lb/>The step{length in the method (which is needed in the convergence proof) was <lb/>chosen by several techniques. One technique was to choose a xed positive step{length <lb/>&lt; 1. With a step{length of 1 we found that the algorithm did fail to converge in <lb/>several instances as the theory would suggest (see Table 4.2). We have also experimented <lb/>with a heuristic choice of the step{length . We calculated a merit function at certain <lb/>values of between 0:4 and 1:0 (depending on the number of processors available) and <lb/>took the step from among these values which minimized the merit function. The <lb/>particular form of merit function we employ is a weighted sum of two quantities, the <lb/>rst being the norm of the gradient of the standard Lagrangian for (4.2) and the second <lb/>being the di erence between the objective function values of (4.2) and its dual. This <lb/>has proven to be robust and results in a good saving in iterations (see Table 4.3). Also <lb/>the evaluation of the merit function was extremely cheap to perform (in parallel) and <lb/>did not result in any degrading of the parallel performance. <lb/>The algorithm was terminated whenever the di erence in the primal objective value <lb/>of (4.1) and its dual objective value normalized by their sum di ered by less than 10 ?5 . <lb/>The constraint violation was also required to be less than this tolerance. <lb/></body>

			<page>14 <lb/></page>

			<body>Problem No. of <lb/>No. of Iteration Count for <lb/>Variables Constraints <lb/>No. of Blocks <lb/>3 6 9 18 <lb/>Ex6 <lb/>3 <lb/>5 <lb/>10 <lb/>Ex9 <lb/>5 <lb/>11 <lb/>10 12 <lb/>Ex10 <lb/>6 <lb/>14 <lb/>11 12 13 <lb/>AFIRO <lb/>27 <lb/>51 <lb/>16 16 16 16 <lb/>ADLittle <lb/>56 <lb/>138 <lb/>14 27 19 27 <lb/>Table 4.1 <lb/>Numerical results with xed = 0:7 <lb/>Problem No. of <lb/>No. of Iteration Count for <lb/>Variables Constraints <lb/>No. of Blocks <lb/>3 6 9 18 <lb/>Ex6 <lb/>3 <lb/>5 <lb/>2 2 <lb/>Ex9 <lb/>5 <lb/>11 <lb/>4 * <lb/>Ex10 <lb/>6 <lb/>14 <lb/>4 4 4 <lb/>AFIRO <lb/>27 <lb/>51 <lb/>20 14 * <lb/>* <lb/>ADLittle <lb/>56 <lb/>138 <lb/>* * * <lb/>* <lb/>Table 4.2 <lb/>Numerical results with xed = 1:0 <lb/>Problem No. of <lb/>No. of Iteration Count for <lb/>Variables Constraints <lb/>No. of Blocks <lb/>3 6 9 18 <lb/>Ex6 <lb/>3 <lb/>5 <lb/>2 2 <lb/>Ex9 <lb/>5 <lb/>11 <lb/>4 5 <lb/>Ex10 <lb/>6 <lb/>14 <lb/>4 4 4 <lb/>AFIRO <lb/>27 <lb/>51 <lb/>13 15 15 14 <lb/>ADLittle <lb/>56 <lb/>138 <lb/>12 14 14 15 <lb/>Table 4.3 <lb/>Numerical results with variable <lb/></body>

			<page>15 <lb/></page>

			<body>Tables 4.1, 4.2 and 4.3 summarize preliminary numerical results for the PCD Algo-<lb/>rithm 3.2 on the Sequent Symmetry S{81 for 5 small linear programs reformulated as <lb/>in (4.2). The rst three are homemade test problems, while the last two, AFIRO and <lb/>ADLittle, are from the NETLIB collection 7]. In the tables, an empty column entry <lb/>signi es that we did not perform the computation. The character * signi es that the <lb/>algorithm did not terminate. Note that for the algorithm does fail when a full step is <lb/>taken (see Table 4.2) as may be expected from Theorem 2.1 where the step{size must <lb/>satisfy (2.30). The heuristic step{size outlined above performs the best (see Table 4.3). <lb/>The key observation to make is that the total number of iterations required for <lb/>accurate solutions (tolerance &lt; 10 ?5 ) can be achieved with a small number of iterations <lb/>(2{13 iterations for 3 blocks and 14{15 iterations for 18 blocks). The fact that the <lb/>number of iterations remains essentially constant for increasing number of blocks is <lb/>encouraging and leads us to believe that the PCD is worthy of additional theoretical <lb/>and computational study. <lb/></body>

			<listBibl>REFERENCES <lb/>1] D. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods, Academic Press, <lb/>New York, 1982. <lb/>2] R. Cottle, J.-S. Pang, and R. Stone, The Linear Complementarity Problem, Academic <lb/>Press, New York, 1991. <lb/>3] R. De Leone and O. Mangasarian, Parallel proximal point decomposition of linear program-<lb/>ming constraints. SIAM National Meeting, Chicago, Illinois, July 16{20, 1990. <lb/>4] A. de Pierro and A. Iusem, Convergence properties of iterative methods for symmetric pos-<lb/>itive semide nite linear complementarity problems, tech. report, Instituto de Matematica, <lb/>Elasticita e Ciencia da Computacao, Universidade Estadual de Campinas, CP 6065, Camp-<lb/>inas, 13081, SP, Brazil, 1990. <lb/>5] J. Eckstein and D. Bertsekas, On the Douglas{Rachford splitting method and the proximal <lb/>point algorithm for maximal monotone operators, Mathematical Programming, (1991). To <lb/>appear. <lb/>6] M. Ferris, Parallel constraint distribution for convex quadratic programs, Tech. Report 1009, <lb/>Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, 1991. <lb/>7] D. Gay, Electronic mail distribution of linear programming test problems, COAL Newsletter, 13 <lb/>(1985), pp. 10{12. <lb/>8] S.-P. Han, A decomposition method and its application to convex programming, Mathematics of <lb/>Operations Research, 14 (1989), pp. 237{248. <lb/>9] Z.-Q. Luo and P. Tseng, On the convergence of a matrix splitting algorithm for the symmet-<lb/>ric monotone linear complementarity problem, Tech. Report LIDS-P-1884, MIT, Cambridge, <lb/>Massachusetts, 1989. To appear, SIAM Journal on Control and Optimization. <lb/>10] O. Mangasarian, Nonlinear Programming, McGraw{Hill, New York, 1969. <lb/>11] <lb/>, Solution of symmetric linear complementarity problems by iterative methods, Journal of <lb/>Optimization Theory and Applications, 22 (1977), pp. 465{485. <lb/>12] <lb/>, Normal solutions of linear programs, Mathematical Programming Study, 22 (1984), <lb/>pp. 206{216. <lb/>13] <lb/>, A simple characterization of solution sets of convex programs, Operations Research Letters, <lb/>7 (1988), pp. 21{26. <lb/>14] <lb/>, On the convergence of iterates of an inexact matrix splitting algorithm for the symmetric <lb/>monotone linear complementarity problem, Tech. Report 917, Computer Sciences Department, <lb/>University of Wisconsin, Madison, Wisconsin 53706, Mar. 1990. To appear in SIAM Journal <lb/></listBibl>

			<page>16 <lb/></page>

			<listBibl>on Optimization, 1, 1991. <lb/>15] O. Mangasarian and R. Meyer, Nonlinear perturbation of linear programs, SIAM Journal <lb/>on Control and Optimization, 17 (1979), pp. 745{752. <lb/>16] B. Murtagh and M. Saunders, MINOS 5.0 user&apos;s guide, Technical Report SOL 83.20, Stan-<lb/>ford University, December 1983. <lb/>17] J.-S. Pang, Convergence of splitting and Newton methods for complementarity problems: An ap-<lb/>plication of some sensitivity results, department of Mathematical Sciences, The Johns Hopkins <lb/>University, Baltimore, MD 21218, Sept. 1990. <lb/>18] J.-S. Pang and D. Chan, Iterative methods for variational and complementarity problems, <lb/>Mathematical Programming, 24 (1982), pp. 284{313. <lb/>19] R. Rockafellar, Augmented Lagrange multiplier functions and duality in nonconvex program-<lb/>ming, SIAM Journal on Control, 12 (1974), pp. 268{285. <lb/>20] <lb/>, Augmented Lagrangians and applications of the proximal point algorithm in convex pro-<lb/>gramming, Mathematics of Operations Research, 1 (1976), pp. 97{116. <lb/>21] R. Rockafellar and R.-B. Wets, Scenarios and policy aggregation in optimization under <lb/>uncertainty, Mathematics of Operations Research, 10 (1991), pp. 119{147. <lb/>22] J. Spingarn, Applications of the method of partial inverses to convex programming, Mathemat-<lb/>ical Programming, 32 (1985), pp. 199{223. <lb/></listBibl>

			<page>17 </page>


	</text>
</tei>
