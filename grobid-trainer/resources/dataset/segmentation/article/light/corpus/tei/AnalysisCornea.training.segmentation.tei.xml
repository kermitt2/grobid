<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="__AnalysisCornea"/>
	</teiHeader>
	<text xml:lang="en">
			<front> On the error of computing ab + cd using <lb/>Cornea, Harrison and Tang&apos;s method <lb/> Jean-Michel Muller <lb/>CNRS, Laboratoire LIP <lb/>(CNRS, ENS Lyon, Inria, Univ. Claude Bernard Lyon 1) <lb/>jean-michel.muller@ens-lyon.fr <lb/>September 2013 <lb/> Abstract <lb/> In their book Scientific Computing on The Itanium [1], Cornea, Harrison <lb/>and Tang introduce an accurate algorithm for evaluating expressions of <lb/>the form ab + cd in binary floating-point arithmetic, assuming an FMA <lb/>instruction is available. They show that if p is the precision of the floating-<lb/>point format and if u = 2  −p  , the relative error of the result is of order <lb/> u. We improve their proof to show that the relative error is bounded by <lb/> 2u+7u  2  +6u  3  . Furthermore, by building an example for which the relative <lb/>error is asymptotically (as p → ∞ or, equivalently, as u → 0) equivalent to <lb/> 2u, we show that our error bound is asymptotically optimal. <lb/></front> 
			
			<body>1 Introduction and notation <lb/> 1.1 Computing ab + cd <lb/> Expressions of the form ab + cd, where a, b, c, d are floating-point (FP) numbers <lb/>arise naturally in many numerical computations. Typical examples are com-<lb/>plex multiplication and division; discriminant of quadratic equations; cross-<lb/>products and 2D determinants. The naive way of computing ab+cd may lead to <lb/>very inaccurate results, due to catastrophic cancellations. 1 Several algorithms <lb/>have been introduced, to overcome this problem. An algorithm attributed to <lb/>Kahan by Higham [2, p. 65] can be used when an FMA instruction is available. <lb/>It is Algorithm 1 below. <lb/>

			<note place="footnote"> 1 This is especially true when an FMA is used in a naive way: see for instance the paragraph <lb/> &quot; Multiply-Accumulate, A Mixed Blessing &quot; in Kahan&apos;s on-line document [5]. <lb/></note>

			<page> 1 <lb/></page>

			Algorithm 1 Kahan&apos;s algorithm for computing x = ab+cd with fused multiply-<lb/>adds. RN(t) means t rounded to the nearest FP number, so that RN(cd) is <lb/>the result of the floating-point multiplication c * d, assuming round-to-nearest <lb/>mode. <lb/> ˆ <lb/> w ← RN(cd) <lb/> e ← RN(cd − ˆ <lb/> w) <lb/> // this operation is exact: e = cd − ˆ <lb/> w. <lb/> ˆ <lb/> f ← RN(ab + ˆ <lb/> w) <lb/> ˆ <lb/> x ← RN( ˆ <lb/> f + e) <lb/> return  <lb/> x <lb/> Jeannerod, Louvet and Muller [4] show that in radix-β floating-point arith-<lb/>metic, the relative error of Kahan&apos;s algorithm is bounded by 2u, where u = <lb/> 1 <lb/> 2  β  1−p  is the unit roundoff. They also show that this bound is asymptotically <lb/>optimal, which means that the ratio between the largest attained relative error <lb/>and the bound goes to 1 as p goes to infinity (or, equivalently, as u goes to 0). <lb/> Another algorithm, that also requires the availability of an FMA instruction, <lb/>was introduced by Cornea, Harrison and Tang in their book Scientific Comput-<lb/>ing on The Itanium [1]. Cornea et al&apos;s algorithm is <lb/> Algorithm 2 Cornea, Harrison and Tang&apos;s algorithm for computing x = ab + cd <lb/> with fused multiply-adds. <lb/> π  1  ← RN(ab) <lb/> e  1  ← ab − π  1 <lb/> // exact with an FMA <lb/> π  2  ← RN(cd) <lb/> e  2  ← cd − π  2 <lb/> // exact with an FMA <lb/> π ← RN(π  1  + π  2  ) <lb/> e ← RN(e  1  + e  2  ) <lb/> s ← RN(π + e) <lb/> return s <lb/> Cornea, Harrison and Tang provide a quick error analysis to show that the <lb/>relative error of their algorithm is of the order of u. At the time of the pub-<lb/>lication of their book, the relative bound 2u on Kahan&apos;s algorithm was not <lb/>known, which made their algorithm a very attractive choice, although it re-<lb/>quires slightly more computation than Kahan&apos;s algorithm. Now, to choose be-<lb/>tween these two algorithms, we need to evaluate the largest possible relative <lb/>error of Cornea et al&apos;s algorithm more accurately. This is the purpose of this <lb/>paper. <lb/> 1.2 Some notation and assumtions <lb/> Throughout the paper, we assume a binary floating-point system of precision <lb/> p ≥ 2, with unbounded exponent range (that is, our results will apply to real-<lb/>life computations provided that no underflow or overflow occurs). In such a <lb/>

			<page>2 <lb/></page>

			system, a floating-point number is a number x that can be expressed in the <lb/>form <lb/> x = M  x  · 2  ex−p+1  , <lb/> where M  x  and e  x  are integers, and 2  p−1  ≤ |M  x  | ≤ 2  p  − 1. We denote u = 2  −p  . <lb/>If t is a nonzero real number, with 2  k  ≤ t &lt; 2  k+1  , we define ulp(t) as 2  k−p+1  . <lb/>We assume that an FMA instruction is available. The FMA (fused multiply-<lb/>add) evaluates expressions of the form FMA(a, b, c) = ab + c with one final <lb/>rounding only and since it is required by the 2008 revision of the IEEE 754 <lb/>standard [3], one can expect that it will soon belong to the instruction set of <lb/>most general-purpose processors. In the following we assume that the round-<lb/>ing mode is round to nearest even, and we denote RN the rounding function, so <lb/>that the result returned when computing FMA(a, b, c) is RN(ab + c). <lb/> We will frequently use the following properties [6]: <lb/>for any real number t, <lb/>
			
			(i) <lb/> |RN(t) − t| ≤  1 <lb/> 2  ulp(t) ≤ u · |t|, <lb/> (ii) |RN(t) − t| ≤ u · |RN(t)|, <lb/> (iii) ulp(t) ≤ ulp(RN(t)). <lb/>An interesting property of the FMA instruction is that it allows to quickly <lb/>compute the error of a floating-point multiplication. More precisely, if π = <lb/> RN(xy) is the result of a rounded-to-nearest FP multiplication and e = RN(xy− <lb/> π) (e is computed using one FMA), then π + e = xy. <lb/> 2 Preliminary properties of Algorithm 2 <lb/> Remark 2.1. If ab = −cd then ab + cd = 0 is exactly computed by the algorithm. <lb/>Proof. Straightforward by noticing that π  1  = −π  2  and e  1  = −e  2  . <lb/> Remark 2.2. Let cd be the product of two binary floating-point numbers of precision <lb/> p. Define π  2  = RN(cd) and e  2  = cd − π  2  . We have: <lb/> • either e  2  is a multiple of 2  −p+1  ulp(π  2  ) (which implies that it fits in p − 2 bits); <lb/> • or |cd| ≤ (2  p  − 2 + 2  −p  )ulp(π  2  ). <lb/> Proof. Since c and d are precision-p binary floating-point numbers, one has <lb/> c = M  c  · 2  ec−p+1  and d = M  d  · 2  e  d  −p+1  , <lb/> where M  c  , M  d  , e  c  , and e  d  are integers, with 2  p−1  ≤ |M  c  |, |M  d  | ≤ 2  p  − 1. The <lb/>number cd is a multiple of 2  ec+e  d  −2p+2  , hence π  2  = RN(cd) and e  2  = cd − π  2 <lb/> are multiple of 2  ec+e  d  −2p+2  too. <lb/> • if π  2  &lt; 2  ec+e  d  +1  then ulp(π  2  ) ≤ 2  ec+e  d  −p+1  , so that (since ulp(π  2  ) is a <lb/>power of 2) e  2  is a multiple of 2  −p+1  ulp(π  2  ); <lb/> 
			
			<page>3 <lb/></page> 
			
			• if π  2  ≥ 2  ec+e  d  +1  then ulp(π  2  ) = 2  ec+e  d  −p+2  , therefore <lb/> |cd| = |M  c  M  d  |·2  ec+e  d  −2p+2  ≤ (2  p  −1)  2  ·2  ec+e  d  −2p+2  = (2  p  −2+2  −p  )ulp(π  2  ). <lb/> Remark 2.3. Denote u = 2  −p  . We have, <lb/> π + e = (ab + cd)(1 + 񮽙  1  ) + γ, <lb/> with |񮽙  1  | ≤ u and |γ| ≤ 2u  2  · (|ab| + |cd|), so that <lb/> s = RN(π + e) = ((ab + cd)(1 + 񮽙  1  ) + γ) · (1 + 񮽙  3  ), <lb/> with |񮽙  3  | ≤ u. <lb/> Proof. We have, <lb/> • π  1  + e  1  = ab, |e  1  | ≤ u · |π  1  |, and |e  1  | ≤ u · |ab|; <lb/>• π  2  + e  2  = cd, |e  2  | ≤ u · |π  2  |, and |e  2  | ≤ u · |cd|; <lb/>• π = (π  1  + π  2  ) · (1 + 񮽙  1  ), with |񮽙  1  | ≤ u; <lb/> • e = (e  1  + e  2  ) · (1 + 񮽙  2  ), with |񮽙  2  | ≤ u. <lb/> Therefore, <lb/> π + e = (ab + cd)(1 + 񮽙  1  ) + γ, <lb/> with <lb/> γ = (e  1  + e  2  ) · (񮽙  2  − 񮽙  1  ), <lb/> which implies <lb/> |γ| = 2u  2  · (|ab| + |cd|) . <lb/> 3 Discussion on the various cases that occur in Al-<lb/>gorithm 2 <lb/> 3.1 If ab and cd have the same sign <lb/> In that case, |γ| ≤ 2u  2  · |ab + cd|, so that the final relative error is bounded by <lb/> 2u + 3u  2  + 2u  3  . <lb/> 3.2 If ab and cd have different signs <lb/> Without loss of generality, we assume |ab| ≥ |cd|, ab &gt; 0 and cd &lt; 0 (notice that <lb/>if ab = 0 or cd = 0 the analysis becomes straightforward). <lb/>

			<page>4 <lb/></page>

			3.2.1 If |cd| ≤  1 <lb/> 2  ab <lb/> In that case, <lb/> |ab + cd| ≥ <lb/> 1 <lb/>2 <lb/> |ab|, and |ab| + |cd| ≤ <lb/> 3 <lb/>2 <lb/> |ab|, <lb/> so that <lb/> |ab + cd| ≥ <lb/> 1 <lb/>3 <lb/>(|ab| + |cd|) , <lb/> so that |γ| ≤ 6u  2  ·|ab+cd|, which implies that the final relative error is bounded <lb/>by 2u + 7u  2  + 6u  3  . <lb/> 3.2.2 If |cd| &gt;  1 <lb/>2  ab <lb/> In that case, since function t → RN(t) is an increasing function, we easily find <lb/> 1 <lb/>2 <lb/> π  1  ≤ |π  2  | ≤ π  1  . <lb/> Applying Sterbenz Lemma [7, 6], we find that π = π  1  + π  2  exactly, so that <lb/> 񮽙  1  = 0, which gives <lb/> π + e = ab + cd + γ, <lb/> with <lb/> γ = (e  2  + e  1  )񮽙  2  , <lb/> which implies <lb/> |γ| ≤ u  2  · (|ab| + |cd|) . <lb/> 1. if |ab + cd| ≥ u · (|ab| + |cd|) , then |γ| ≤ u · |ab + cd|, so that the final <lb/>relative error is bounded by 2u + u  2  . <lb/>2. if |ab + cd| &lt; u · (|ab| + |cd|) and π  1  and π  2  have the same floating-<lb/>point exponent e. In that case, we have, <lb/> • |e  1  | ≤ (1/2)ulp(π  1  ) = 2  e−p  , <lb/> • |e  2  | ≤ (1/2)ulp(π  2  ) = 2  e−p  , <lb/> • e  1  and e  2  are multiple of 2  e−2p+1  , <lb/>Hence, e  1  + e  2  is a multiple of 2  e−2p+1  , say e  1  + e  2  = K · 2  e−2p+1  , k ∈ Z, <lb/> that satisfies <lb/> 񮽙 <lb/> 񮽙 K · 2  e−2p+1  񮽙 <lb/>񮽙 ≤ 2  e−p+1  , <lb/> i.e., |K| ≤ 2  p  . This implies that e  1  + e  2  is a floating-point number. Hence, <lb/> e = RN(e  1  +e  2  ) = e  1  +e  2  , so that 񮽙  2  = 0. As a consequence, π+e = ab+cd <lb/> exactly, and the final relative error is bounded by u. <lb/> 3. if |ab + cd| &lt; u · (|ab| + |cd|) and π  1  and π  2  do not have the same floating-<lb/>point exponent. In such a case,  1 <lb/>2  π  1  ≤ |π  2  | ≤ π  1  implies that the exponent <lb/>of π  2  is the exponent of π  1  minus one, so that ulp(π  2  ) =  1 <lb/>2  ulp(π  1  ). Let us <lb/>notice the following property <lb/>

			<page>5 <lb/></page>

			Remark 3.1. If |ab + cd| &lt; u · (|ab| + |cd|) and π  1  and π  2  do not have the same <lb/>floating-point exponent then (π  1  + π  2  ) ≤ 4ulp(π  2  ). <lb/> Proof. π  1  and π  2  are obviously multiples of ulp(π  2  ), and if we had (π  1  + <lb/> π  2  ) ≤ 4ulp(π  2  ), that would imply <lb/> |ab+cd| = |π  1  +π  2  +e  1  +e  2  | ≥ 5ulp(π  2  )−ulp(π  2  )− <lb/>1 <lb/>2 <lb/> ulp(e  2  ) = 7/2ulp(π  2  ), <lb/> whereas <lb/> |ab| + |cd| &lt; 2  p  ulp(π  1  ) + 2  p  ulp(π  2  ) = 3 · 2  p  ulp(π  2  ), <lb/> so that <lb/> |ab| + |cd| <lb/>|ab + cd| <lb/> ≤ <lb/> 6 <lb/>7 <lb/> · 2  p  = <lb/>6 <lb/>7u <lb/> , <lb/> which contradicts the assumption |ab + cd| &lt; u · (|ab| + |cd|). <lb/> The fact that π  1  and π  2  do not have the same floating-point exponent (so <lb/>

			that there is a power of 2 between them), and that (π  1  + π  2  ) ≤ 4ulp(π  2  ) implies <lb/>that there remain only a very few cases to examine. Define e  π1  as the floating-<lb/>point exponent of π  1  : <lb/> • either π  1  is the floating-point number immediately above 2  eπ  1  . In such a <lb/>case −π  2  is either 2  eπ  1  − ulp(π  2  ) or 2  eπ  1  − 2ulp(π  2  ); <lb/> • or π  1  = 2  eπ  1  . In such a case, π  2  = 2  eπ  1  − i · ulp(π  2  ), with i = 1, 2, 3, or 4. <lb/> We can even reduce further the number of cases to be considered: <lb/> • First, one can apply Remark 2.2. If e  2  is a multiple of 2  −p+1  ulp(π  2  ), then <lb/> e  1  +e  2  is a multiple of 2  −p+1  ulp(π  2  ), say e  1  +e  2  = K ·2  −p+1  ·ulp(π  2  ). Since <lb/> |e  1  + e  2  | ≤  1 <lb/>2  (ulp(π  1  ) + ulp(π  2  )) =  3 <lb/>2  ulp(π  2  ), we deduce that |K| ≤ 3 · <lb/> 2  p−2  &lt; 2  p  . This shows that e  1  + e  2  is a precision-p floating-point number. <lb/>Hence, e = RN(e  1  + e  2  ) = e  1  + e  2  , so that 񮽙  2  = 0. As a consequence, <lb/> π + e = ab + cd exactly, and the final relative error is bounded by u. <lb/> Now, Remark 2.2 tells us that If e  2  is no a multiple of 2  −p+1  ulp(π  2  ), then <lb/> |cd| ≤ (2  p  − 2 + 2  −p  )ulp(π  2  ), so that |π  2  | = |RN(cd)| ≤ 2  eπ  1  − 2ulp(π  2  ). <lb/> Hence the case π  2  = 2  eπ  1  − ulp(π  2  ) need not be considered. <lb/> • If π  1  = 2  eπ  1  , then, since π  1  = RN(ab), 2  eπ  1  −  1 <lb/>4  ulp(π  1  ) ≤ ab ≤ 2  eπ  1  + <lb/> 1 <lb/>2  ulp(π  1  ). However the case ab ≤ 2  eπ  1  is easily dealt with: in that case, <lb/>we have |e  1  | ≤  1 <lb/>2  ulp(π  2  ), so that it is very similar to a case already met: <lb/> e  1  + e  2  is a floating-point number. Hence, e = RN(e  1  + e  2  ) = e  1  + e  2  , <lb/>so that 񮽙  2  = 0. As a consequence, π + e = ab + cd exactly, and the final <lb/>relative error is bounded by u. <lb/> Therefore, we only need to consider two cases: <lb/>

			<page>6 <lb/></page>

			• Case 1 π  1  is the floating-point number immediately above 2  eπ  1  , and 2  eπ  1  − <lb/> 2ulp(π  2  ). When reasoning on the consequences of Remark 2.2, we have <lb/>seen that we can further assume that |cd| ≤ (2  p  − 2 + 2  −p  )ulp(π  2  ) = <lb/>2  eπ  1  − (2 − 2  −p  )ulp(π  2  ). This case is exemplified by Figure 1. In that case, <lb/> |ab + cd| &gt; (3 − 2  −p  )ulp(π  2  ), <lb/> and <lb/> |ab|+|cd| &lt; <lb/> 񮽙 <lb/> 2  p−1  + <lb/>3 <lb/>2 <lb/> 񮽙 <lb/> ulp(π  1  )+(2  p+1  −2+2  −p  )ulp(π  2  ) = (2  p+1  +1+2  −p  )ulp(π  2  ), <lb/> so that <lb/> γ &lt; u  2  2  p+1  + 1 + 2  −p <lb/> 3 − 2  −p <lb/> · |ab + cd|. <lb/> Elementary manipulations show that as soon as u = 2  −p  is less than 1/2 <lb/> (i.e., p ≥ 1, which always holds), the ratio <lb/> 2  p+1  + 1 + 2  −p <lb/> 3 − 2  −p <lb/> = <lb/>2 <lb/>3u <lb/>+ <lb/>5 <lb/>9 <lb/>+ <lb/>14u <lb/>27 <lb/>+ <lb/>14u  2 <lb/> 81 <lb/>+ · · · <lb/> is less than <lb/> 2 <lb/>3u <lb/>+ 1. <lb/> As a consequence, γ ≤ <lb/> 񮽙  2u <lb/>3  + u  2  񮽙 <lb/> |ab + cd|, so that the final relative error <lb/>is less than  5 <lb/>3  u +  5 <lb/>3  u  2  + u  3  . <lb/> • Case 2 π  1  = 2  eπ  1  and −π  2  is π  1  −2ulp(π  2  ), π  1  −3ulp(π  2  ), or π  1  −4ulp(π  2  ). <lb/> We have seen that we can further assume |cd| ≤ 2  eπ  1  − (2 − 2  −p  )ulp(π  2  ), <lb/> and ab &gt; 2  eπ  1  . This case is exemplified by Figure 2. In that case, <lb/> |ab + cd| &gt; (2 − 2  −p  )ulp(π  2  ), <lb/> and <lb/> |ab| + |cd| &lt; [(2  p  − 1) + (2  p  − 2 − 2  −p  ]ulp(π  2  ) = (2  p+1  − 1 + 2  −p  )ulp(π  2  ). <lb/> We deduce <lb/> γ ≤ u  2  2  p+1  − 1 + 2  −p <lb/> 2 − 2  −p <lb/> |ab + cd|. <lb/> We easily find <lb/> 2  p+1  − 1 + 2  −p <lb/> 2 − 2  −p <lb/> ≤ <lb/> 1 <lb/> u <lb/> + u, <lb/> Hence γ ≤ (u + u  3  )|ab + cd|, from which we deduce that the final relative <lb/>error is bounded by 2u + u  2  + u  3  + u  4  . <lb/>

			<page>7 <lb/></page>

			2  eπ  1 <lb/> π  1 <lb/> −π  2 <lb/> −cd is located here <lb/> ab is located here <lb/>ulp(π  1  ) = 2ulp(π  2  ) <lb/> Figure 1: Case π  1  = 2  eπ  1  · (1 + 2  −p+1  ). <lb/> π  1  = 2  eπ  1 <lb/> −π  2 <lb/> If π  2  is the largest possible, <lb/> −cd is located here <lb/> ab is located here <lb/>ulp(π  1  ) = 2ulp(π  2  ) <lb/> Figure 2: Case π  1  = 2  eπ  1  . <lb/>

			<page>8 <lb/></page>

			4 General result <lb/> The results obtained in the various cases considered in Section 3 can be sum-<lb/>marized as follows <lb/> Theorem 4.1. Provided no underflow/overflow occurs, and assuming radix-2, precision-<lb/> p floating-point arithmetic, the relative error of Cornea et al&apos;s algorithm is bounded by <lb/> 2u + 7u  2  + 6u  3  . <lb/> Now, interestingly enough, we are going to see that the bound given by <lb/>Theorem 4.1 is asymptotically optimal (as p → ∞ or, equivalently, as u → <lb/> 0). To show this, it suffices to consider, in radix-2, precision-p floating-point <lb/>arithmetic: <lb/>  <lb/>  <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> a = 2  p  − 1, <lb/> b = 2  p−3  +  1 <lb/>2  , <lb/>c = 2  p  − 1, <lb/> d = 2  p−3  +  1 <lb/>4  , <lb/> One easily checks that a, b, c, and d are precision-p FP numbers. One easily <lb/>finds: <lb/> ab + cd = 2  2p−2  + 2  p−1  −  3 <lb/>4  , <lb/> π  1 <lb/> = 2  2p−3  + 2  p−2  , <lb/>e  1 <lb/> = 2  p−3  −  1 <lb/>2  , <lb/> π  2 <lb/> = 2  2p−3  , <lb/>e  2 <lb/> = 2  p−3  −  1 <lb/>4  , <lb/> π <lb/> = 2  2p−2  , <lb/>e <lb/> = 2  p−2  −  3 <lb/>4  , <lb/>s <lb/> = 2  2p−2  . <lb/> The relative error |s − (ab + cd)|/|ab + cd| is equal to <lb/> 2  p−1  −  3 <lb/>4 <lb/> 2  2p−2  + 2  p−1  −  3 <lb/>4 <lb/> = <lb/>2u − 3u  2 <lb/> 1 + 2u − 3u  2  = 2u − 7u  2  + 20u  3  + · · · <lb/> which is asymptotically equivalent to 2u. This shows that our relative error <lb/>bound is asymptotically optimal. <lb/>In the frequent case where the considered floating-point format is the bi-<lb/>nary64/double precision format of the IEEE 754 Standard, the relative error <lb/>bound provided by Theorem 4.1 is <lb/> u × 2.000000000000000777156 · · · , <lb/> and the relative error attained with our example is <lb/> u × 1.99999999999999922284 · · · <lb/> This illustrates the tightness of the bound provided by Theorem 4.1. <lb/>

			<page>9 <lb/></page>

			Conclusion <lb/> We have provided a relative error bound for Cornea, Harrison and Tang&apos;s al-<lb/>gorithm (Algorithm 2), and we have shown that our bound is asymptotically <lb/>optimal. Since that bound is not better than the (also asymptotically optimal) <lb/>error bound for Kahan&apos;s algorithm (Algorithm 1), it is in general preferable to <lb/>use Algorithm 1. A possible exception is when one wants to always get the <lb/>same result when computing ab + cd and cd + ab (for instance to implement <lb/>a commutative complex multiplication): in this case, the natural symmetry of <lb/>Algorithm 2 will guarantee the required property, whereas it is easy to build <lb/>examples for which Algorithm 1 does not satisfy it. <lb/></body>
		
		<back>
			<listBibl> References <lb/> [1] M. Cornea, J. Harrison, and P. T. P. Tang. Scientific Computing on Itanium  R <lb/> 񮽙  -<lb/>based Systems. Intel Press, Hillsboro, OR, 2002. <lb/>[2] N. J. Higham. Accuracy and Stability of Numerical Algorithms. SIAM, <lb/>Philadelphia, 1996. <lb/>[3] IEEE Computer Society. IEEE Standard for Floating-Point Arithmetic. IEEE <lb/>Standard 754-2008, August 2008. available at http://ieeexplore. <lb/> ieee.org/servlet/opac?punumber=4610933. <lb/> [4] C.-P. Jeannerod, N. Louvet, and J.-M. Muller. Further analysis of Kahan&apos;s <lb/>algorithm for the accurate computation of 2 × 2 determinants. Mathematics <lb/>of Computation, 82, October 2013. <lb/>[5] W. Kahan. Lecture notes on the status of IEEE-754. PDF file acces-<lb/>sible at http://www.cs.berkeley.edu/ ˜ wkahan/ieee754status/ <lb/>IEEE754.PDF, 1996. <lb/>[6] Jean-Michel Muller, Nicolas Brisebarre, Florent de Dinechin, Claude-<lb/>Pierre Jeannerod, Vincent Lef evre, Guillaume Melquiond, Nathalie Revol, <lb/>Damien Stehlé, and Serge Torres. Handbook of Floating-Point Arithmetic. <lb/> Birkhäuser Boston, 2010. ACM G.1.0; G.1.2; G.4; B.2.0; B.2.4; F.2.1., ISBN <lb/>978-0-8176-4704-9. <lb/>[7] P. H. Sterbenz. Floating-Point Computation. Prentice-Hall, Englewood Cliffs, <lb/>NJ, 1974. <lb/></listBibl>

			<page>10 </page>

		</back>
	</text>
</tei>
