<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
		<front>AERA Open <lb/>January-December 2023, Vol. 9, No. 1, pp. 1 -17 <lb/>DOI:h ttps://doi.org/10.1177/23328584231165919 <lb/>Article reuse guidelines: sagepub.com/journals-permissions <lb/>© The Author(s) 2023. https://journals.sagepub.com/home/ero <lb/>Creative Commons Non Commercial CC BY-NC: This article is distributed under the terms of the Creative Commons <lb/>Attribution-NonCommercial 4.0 License (https://creativecommons.org/licenses/by-nc/4.0/) which permits non-commercial use, <lb/>reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open <lb/>Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage). <lb/>AlgebrA is a critical foundation for learning advanced math-<lb/>ematics (National Mathematics Advisory Panel, 2008). <lb/>However, many middle and high school students struggle to <lb/>understand even basic algebraic concepts (Kena et al., 2015; <lb/>Kieran, 2006). For example, students struggle to achieve a <lb/>robust understanding of how to create, transform, and <lb/>interpret algebraic expressions. This current study, therefore, <lb/>tested the impacts of three educational technology interven-<lb/>tions on algebraic understanding among students in Grade 7 <lb/>across four conditions: (a) From Here to There (FH2T), (b) <lb/>DragonBox 12+ (DragonBox), (c) Immediate Feedback, <lb/>and (d) Active Control. The FH2T and DragonBox <lb/>The Impacts of Three Educational Technologies on Algebraic <lb/>Understanding in the Context of COVID-19 <lb/>Lauren E. Decker-Woodrow <lb/>Westat Inc <lb/>Craig A. Mason <lb/>University of Maine <lb/>Ji-Eun Lee <lb/>Worcester Polytechnic Institute <lb/>Jenny Yun-Chen Chan <lb/>The Education University of Hong Kong <lb/>Adam Sales <lb/>Worcester Polytechnic Institute <lb/>Allison Liu <lb/>Worcester Polytechnic Institute <lb/>Shihfen Tu <lb/>University of Maine <lb/>The current study investigated the effectiveness of three distinct educational technologies-two game-based applications <lb/>(From Here to There and DragonBox 12+) and two modes of online problem sets in ASSISTments (an Immediate Feedback <lb/>condition and an Active Control condition with no immediate feedback) on Grade 7 students&apos; algebraic knowledge. More than <lb/>3,600 Grade 7 students across nine in-person and one virtual schools within the same district were randomly assigned to one <lb/>of the four conditions. Students received nine 30-minute intervention sessions from September 2020 to March 2021. <lb/>Hierarchical linear modeling analyses of the final analytic sample (N = 1,850) showed significantly higher posttest scores <lb/>for students who used From Here to There and DragonBox 12+ compared to the Active Control condition. No significant <lb/>difference was found for the Immediate Feedback condition. The findings have implications for understanding how game-<lb/>based applications can affect algebraic understanding, even within pandemic pressures on learning. <lb/>Keywords: algebra, classroom research, educational games, educational technology, experimental research, game-based <lb/>learning, hierarchical linear modeling, mathematics education, middle schools <lb/></front>

		1165919E ROXXX10.1177/23328584231165919Decker-Woodrow et al.Educational Technologies and Algebraic Understanding <lb/>research-article20232023 <lb/>

			<note place="headnote">Decker-Woodrow et al. <lb/></note>

		<page>2 <lb/></page>

			<body>conditions represented use of game-based applications. <lb/>Immediate Feedback entailed problem sets by using an <lb/>online homework system, ASSISTments. For the purposes <lb/>of this study, the Active Control condition mimicked tradi-<lb/>tional homework assignments while still using technology. <lb/>Although this study independently investigated each of the <lb/>three treatment conditions to the Active Control condition, it <lb/>was hypothesized that FH2T, an interactive game developed <lb/>based on theories of perceptual learning and embodied cog-<lb/>nition, might improve students&apos; algebraic understanding <lb/>through aligning their attention to, actions on, and percep-<lb/>tion of algebraic notations with high-level mathematical <lb/>concepts to a greater extent than would DragonBox or <lb/>Immediate Feedback. <lb/>Algebra Learning and Educational Technologies <lb/>Algebraic principles, such as notation and transforma-<lb/>tions, can be challenging for students to learn. Many middle <lb/>and high school students struggle to acquire basic algebraic <lb/>principles, such as which transformations are mathemati-<lb/>cally valid (Marquis, 1988; National Math Advisory Panel, <lb/>2008) and how to convert between different algebraic repre-<lb/>sentations (Koedinger &amp; Nathan, 2004). These struggles <lb/>have significant implications for students&apos; further mathemat-<lb/>ics achievement because advanced concepts are often pre-<lb/>sented in algebraic notation. <lb/>To improve mathematical understanding and perfor-<lb/>mance among middle school students, researchers and <lb/>developers have designed educational technology tools to <lb/>support learning in different ways. Some tools, such as <lb/>FH2T and DragonBox, are designed to engage students in <lb/>game-based learning, allowing students to interact with <lb/>algebraic notations and solve puzzlelike problems in a play-<lb/>ful environment. Such game-based approaches have been <lb/>found to increase students&apos; engagement and motivation as <lb/>well as problem-solving and learning (Connolly et al., 2012; <lb/>Foster, 2008; Gee, 2003; Ke, 2008; Samur &amp; Evans, 2012). <lb/>Other tools, such as ASSISTments, are designed to provide <lb/>timely support and feedback on homework, and thus the <lb/>problems resemble those in traditional mathematics text-<lb/>books. Below is an overview of the four conditions, the theo-<lb/>ries behind the design of the tools, and prior evidence of <lb/>their efficacy in improving mathematical learning. <lb/>From Here to There! (FH2T). FH2T (https://graspablemath. <lb/>com/projects/fh2t) is a dynamic research-based game that <lb/>implements theories of (a) perceptual learning and (b) <lb/>embodied cognition to address cognitive and affective fac-<lb/>tors that lead to low proficiency in mathematics. Perceptual <lb/>learning theory suggests that reasoning and learning about <lb/>mathematics are inherently perceptual (Goldstone et al., <lb/>2017; Jacob &amp; Hochstein, 2008; Kellman et al., 2010; Kirsh-<lb/>ner &amp; Awtry, 2004; Patsenko &amp; Altmann, 2010), and the <lb/>visual presentation of notation affects how students reason <lb/>about, process, understand, and learn mathematics (Braith-<lb/>waithe et al., 2016; Harrison et al., 2020; Landy &amp; Gold-<lb/>stone, 2010). For example, perceptual features, such as <lb/>spacing and color of algebraic notations, can direct students&apos; <lb/>attention to relevant information (e.g., highlighting the equal <lb/>sign with a different font color in 4 + 7 = 13 -__ to support <lb/>reasoning of equivalence; Alibali et al., 2018), and, over <lb/>time, might help students develop an automatic routine for <lb/>algebraic reasoning (e.g., balancing two sides of the equa-<lb/>tion when seeing an equal sign). Perceptual features also <lb/>affect students&apos; actions, which reflect and further influence <lb/>their learning. Embodied cognition theories contend that stu-<lb/>dents&apos; physical experiences in the world influence their cog-<lb/>nitive processes, including thinking and reasoning in <lb/>mathematics (Abrahamson et al., 2020; Alibali &amp; Nathan, <lb/>2012; Foglia &amp; Wilson, 2013; Nathan et al., 2014; Shapiro, <lb/>2010; Wilson, 2002). Given that the two theories together <lb/>suggest that learning is based in perception and action (Ali-<lb/>bali &amp; Nathan, 2012), environments within FH2T influence <lb/>how students perceive and interact with instructional materi-<lb/>als, which, in turn, is meant to inform their cognitive pro-<lb/>cesses and problem-solving. <lb/>In FH2T, algebraic notations are turned into interactive <lb/>objects that enforce mathematical rules through their physi-<lb/>cal movements. Students can dynamically manipulate and <lb/>transform expressions by using various gestures, such as <lb/>dragging and tapping, to perform operations on the screen. <lb/>The system provides a fluid visualization that allows stu-<lb/>dents to see how their actions transform algebraic expres-<lb/>sions and equations. In each problem in FH2T, students are <lb/>presented with two expressions: a starting expression, which <lb/>is an active and transformable expression, and a target goal <lb/>state, which is mathematically equivalent but perceptually <lb/>different to the starting expression. Students transform the <lb/>starting expression (e.g., &quot;9b + 27c&quot;) into the target goal <lb/>state (e.g., &quot;(3b + 9c) • 3&quot;) by using algebraically permissi-<lb/>ble actions and learned gestures. For example, students can <lb/>drag 9 on top of 27 to factor 27 into 9 • 3, and the system <lb/>transforms the starting expression into &quot;9b + 9 • 3c.&quot; As stu-<lb/>dents encounter, discover, and practice mathematical prin-<lb/>ciples in action, fluid visualizations may help direct students&apos; <lb/>attention toward structural patterns in algebraic notation and <lb/>develop their perceptual-motor routines of algebraic equa-<lb/>tion-solving (Nathan et al., 2016; Nathan &amp; Walkington, <lb/>2017). (See online supplementary material, Appendix A, for <lb/>more information.) <lb/>The effectiveness of FH2T has been examined across <lb/>several studies over the past decade. For instance, a prelimi-<lb/>nary study with 130 middle school students showed that, <lb/>after four 30-minute intervention sessions, students in the <lb/>fluid visualizations condition, which is an early version of <lb/>FH2T, improved on their procedural fluency, whereas stu-<lb/>dents in the manual calculations condition did not (Ottmar <lb/></body>

			<page>3 <lb/></page>

			<note place="headnote">Educational Technologies and Algebraic Understanding <lb/></note>

			<body>et al., 2015). Similarly, a recent randomized controlled trial <lb/>(RCT) with 475 middle school students showed that, after <lb/>four 30-minute intervention sessions, students in the FH2T <lb/>condition scored higher on mathematical equivalence com-<lb/>pared to their peers in an online problem set condition (Chan <lb/>et al., 2022). A study with an elementary version of FH2T <lb/>further showed that, among 185 Grade 2 students, complet-<lb/>ing more problems in FH2T was associated with higher <lb/>posttest scores (testing decomposition, operational strate-<lb/>gies, and basic notation), and this effect was significant <lb/>above and beyond students&apos; prior knowledge, as measured <lb/>by the pretest (Hulse et al., 2019). Together, these findings <lb/>suggest that FH2T may be effective at improving students&apos; <lb/>algebraic understanding and mathematics performance, war-<lb/>ranting a large-scale RCT to evaluate its effectiveness com-<lb/>pared to other educational technologies. <lb/>DragonBox 12+ (DragonBox). DragonBox (https://drag-<lb/>onbox.com/products/algebra-12) is an educational game that <lb/>introduces advanced algebraic concepts to students ages <lb/>12-17. For each problem, students are asked to isolate a box <lb/>containing a dragon-equivalent to solving an equation for <lb/>x. The popularity of DragonBox is reflected in the public <lb/>realm, where it is lauded as one of the best educational <lb/>games for teaching algebra (Kahoot!, 2019). Its design has <lb/>also been praised for using several research-backed design <lb/>principles, including discovery-based learning, embedded <lb/>gestures, multiple representations, immediate feedback, and <lb/>adaptive difficulty (Cayton-Hodges et al., 2015; Torres et al., <lb/>2016). One of DragonBox&apos;s major design principles is that <lb/>students should not perceive DragonBox as a math game. <lb/>Thus, at the start of the game, there are no numbers or vari-<lb/>ables; instead, students move cards with pictures of mon-<lb/>sters, following the rules of algebra. Throughout the game, <lb/>monster cards are gradually replaced by algebraic symbols; <lb/>however, cards are never explicitly connected to mathemati-<lb/>cal properties. <lb/>Despite the popularity of DragonBox, research findings <lb/>on its efficacy are mixed. In terms of supportive evidence, <lb/>the DragonBox developers and the University of Washington <lb/>Center for Game Science conducted an Algebra Challenge <lb/>with K-12 students from 70 schools across 15 districts. <lb/>They found that after 1.5 hours of playing a combined ver-<lb/>sion of DragonBox 5+ and 12+, 93% of students were able <lb/>to successfully answer three algebra problems (e.g., d • x + <lb/>m = 8) in a row (Liu et al., 2015). Similarly, Grade 8 stu-<lb/>dents have shown significant gains in algebra problem-<lb/>solving performance after using DragonBox for 8 hours <lb/>across 4 weeks (Dolonen &amp; Kluge, 2015), and its positive <lb/>impacts may extend to students&apos; confidence and attitudes <lb/>toward mathematics (Siew et al., 2016). However, Long and <lb/>Aleven (2014, 2017) find that students in Grades 7-8 who <lb/>played DragonBox for 3.5 hours across five sessions <lb/>showed no improvements in problem-solving performance <lb/>or confidence, even on problems included in the game. <lb/>Further, despite the students&apos; higher enjoyment with <lb/>DragonBox, their learning gains were significantly lower <lb/>than those of students using an intelligent tutor system. <lb/>Dolonen and Kluge (2015) also find that Grade 8 students <lb/>using DragonBox showed higher engagement but lower <lb/>learning gains compared to students who practiced prob-<lb/>lems similar to those found in standard algebra textbooks. <lb/>Contrasting DragonBox and FH2T. It is appropriate to <lb/>compare DragonBox to FH2T because both use a game-<lb/>based interface and are grounded in intuitive perceptual-<lb/>motor routines, but they also vary in important ways. First, <lb/>whereas DragonBox progressively transitions from pictures <lb/>of monsters to abstract algebraic notation as the game pro-<lb/>ceeds, FH2T aims to promote fluency in algebraic notation <lb/>by grounding perceptual-motor routines directly in notation. <lb/>Second, all of DragonBox&apos;s problems involve the same goal <lb/>(i.e., isolate the dragon to solve for x); in contrast, each prob-<lb/>lem in FH2T has a unique goal state, encouraging flexibility <lb/>in notational transformations. Third, DragonBox&apos;s opera-<lb/>tions are introduced as in-game rules independent from <lb/>mathematics (e.g., positive-and negative-color versions of <lb/>the same picture can be combined to cancel them), whereas <lb/>FH2T directly connects actions and operations to mathemat-<lb/>ical principles. These similarities and differences allowed <lb/>the study team to begin exploring relative advantages and <lb/>disadvantages of hiding mathematical materials, having <lb/>flexible goals, and explicitly grounding actions in mathe-<lb/>matical rationales. <lb/>Problem Sets With Immediate Feedback and Hints in ASSIST-<lb/>ments (Immediate Feedback). ASSISTments (https://new. <lb/>assistments.org/; Heffernan &amp; Heffernan, 2014) is an online <lb/>homework system that provides feedback to students as they <lb/>solve traditional textbook problems. The problem sets in <lb/>ASSISTments are adapted from open-source curricula, thus <lb/>resembling problems students would encounter in their text-<lb/>books and homework assignments. In ASSISTments, stu-<lb/>dents are presented with problems one at a time on their <lb/>screen; they can request hints if they need additional sup-<lb/>port on problem-solving and receive immediate correctness <lb/>feedback on their responses. Teachers receive class reports <lb/>on problem sets and students&apos; performance so they can <lb/>identify struggling students and challenging problems, and <lb/>then they use these data to inform instruction in their class-<lb/>room. (See online supplementary material, Appendix A, for <lb/>more information.) <lb/>The design of ASSISTments is informed by research on <lb/>formative assessment and timely feedback. Teachers use for-<lb/>mative assessments to gather data from students and guide <lb/>their instruction to meet students&apos; needs (Black &amp; Wiliam, <lb/>1998; Boston, 2002). When teachers adjust instruction based on <lb/>formative assessments, students show significant improvement <lb/></body>

			<note place="headnote">Decker-Woodrow et al. <lb/></note>

			<page>4 <lb/></page>

			<body>on their achievement (Bergan et al., 1991; Speece et al., 2003). <lb/>Further, timely feedback and support during learning are ben-<lb/>eficial to students (Butler &amp; Woodward, 2018; Shute, 2008). <lb/>Studies suggest that receiving feedback immediately after giv-<lb/>ing responses or completing problem sets might be effective <lb/>for improving students&apos; procedural and conceptual knowledge <lb/>(Azevedo &amp; Bernard, 1995; Corbett &amp; Anderson, 2001; <lb/>Dihoff et al., 2003; Phye &amp; Andre, 1989). <lb/>ASSISTments has had positive impacts on student learn-<lb/>ing. A preliminary study with 28 Grade 5 students found that <lb/>they learned significantly more when provided with imme-<lb/>diate feedback and on-demand hints in ASSISTments com-<lb/>pared to completing traditional paper-and-pencil homework <lb/>(Mendicino et al., 2009). Similarly, a study with 63 Grade 7 <lb/>students conducted within ASSISTments showed that stu-<lb/>dents who received immediate correctness feedback and <lb/>opportunities to reattempt problems outperformed students <lb/>who completed problems without feedback or reattempts <lb/>(Kelly et al., 2013). A large-scale RCT with 2,850 students <lb/>in Grade 7 further showed that combining ASSISTments <lb/>with teacher training on adaptive teaching significantly <lb/>improved students&apos; end-of-year performance on standard-<lb/>ized mathematics scores compared to those of a business-as-<lb/>usual control group (Roschelle et al., 2016). Together, these <lb/>findings suggest that offering online problem sets with hints <lb/>and immediate feedback plus teacher training is effective at <lb/>improving students&apos; mathematics performance. In the cur-<lb/>rent study, problem sets with immediate hints and feedback <lb/>in ASSISTments were used as a high-quality, nongamified <lb/>intervention treatment. Unlike the work of Roschelle et al. <lb/>(2016), this study did not provide teacher training on adap-<lb/>tive instruction because FH2T and DragonBox only involve <lb/>student components and do not include teacher training. <lb/>Therefore, the current study provides insights into indepen-<lb/>dent effects of the student components without teacher com-<lb/>ponents in ASSISTments. <lb/>Active Control Problem Sets With Post-Assignment Feed-<lb/>back (Active Control). The fourth condition was a version <lb/>of ASSISTments that provides post-assignment, rather than <lb/>immediate, hints and feedback. In this condition, problem <lb/>sets were administered in &quot;test mode,&quot; so students did not <lb/>receive any feedback or hints during problem-solving. <lb/>Instead, students received a report with correctness feedback <lb/>at the end of the problem set, and they could review their <lb/>responses, revisit problems, and request hints. This condi-<lb/>tion served as the active control in the study, as it mimicked <lb/>traditional homework assignments and allowed all students <lb/>involved in the study to participate via technology. <lb/>The Current Study <lb/>The goal of the current study was to independently exam-<lb/>ine the efficacy of three widely used treatment conditions <lb/>(i.e., FH2T, DragonBox, and ASSISTments with immediate <lb/>feedback) on students&apos; algebraic understanding compared to <lb/>the Active Control. The study was conducted between <lb/>September 2020 and April 2021, which was during the peak <lb/>of the COVID-19 pandemic in the United States. Given the <lb/>restrictions of physical distancing, the school district offered <lb/>students and their families a choice of classroom format <lb/>(100% in person or 100% asynchronous virtual academy) <lb/>for the 2020-2021 school year prior to the start of the fall <lb/>semester. Random assignment to study condition occurred <lb/>across formats, so the classroom format was not aligned to <lb/>any one study condition. Although their choice was intended <lb/>to be for the full school year, families were allowed to <lb/>change their selection throughout the year. Regardless of <lb/>students&apos; classroom format, all study sessions (i.e., assess-<lb/>ments and interventions) were administered online, and stu-<lb/>dents worked individually at their own pace, using a device. <lb/>Methods <lb/>Participants <lb/>A total of 52 Grade 7 mathematics teachers and their stu-<lb/>dents from 11 middle schools (10 in-person schools and one <lb/>virtual academy) were recruited from a large, suburban dis-<lb/>trict in the southeastern United States in the summer of 2020. <lb/>Together, teachers taught 190 mathematics classes and 4,092 <lb/>students. Prior to random assignment, one school declined to <lb/>participate. This school included four teachers and 377 stu-<lb/>dents. Students enrolled in resource settings were not <lb/>included in this study. This resulted in 10 schools (nine in <lb/>person and one virtual), 37 teachers, 156 classes, and 3,612 <lb/>students. The study sample was 49.1% White, 14.1% <lb/>Hispanic, and 27.9% Asian. Additionally, 42.5% began the <lb/>school year in the Virtual Academy. Student-level socioeco-<lb/>nomic data (e.g., free or reduced-price lunch status) were not <lb/>available. (All baseline and posttest sample sizes are pre-<lb/>sented in Figure 1.) <lb/>Design <lb/>This study randomly assigned students to study condi-<lb/>tions within classrooms, ensuring that study conditions were <lb/>equivalent with respect to teacher characteristics and class-<lb/>room curricula. This approach was possible because teach-<lb/>ers were able to use all four technology-based interventions <lb/>within the classroom. Students were ranked within class-<lb/>rooms based on their prior state mathematics assessment <lb/>scores, blocked into sets of five (i.e., quintets), and then ran-<lb/>domly assigned into either the FH2T (40%), DragonBox <lb/>(20%), Immediate Feedback (20%), or Active Control (20%) <lb/>condition. Because a primary goal of the RCT was designed <lb/>to test the efficacy of FH2T compared to other educational <lb/>technologies, a larger proportion of students was assigned to <lb/>FH2T. When classroom size did not allow for a quintet to be <lb/></body>

			<page>5 <lb/></page>

			<body>formed (e.g., 18 students, resulting in three excess students <lb/>not in a complete quintet), remaining students in each class-<lb/>room were placed in quintets drawn from all excess Grade 7 <lb/>students within each school. Cross-school quintets were <lb/>formed, as needed, to complete overall random assignment. <lb/>Attrition Analysis <lb/>One school dropped out following pretest assessment, <lb/>resulting in a final pool of nine schools (eight in person and <lb/>one virtual), 34 teachers, 143 classes, and 3,271 students <lb/>participating at the start of the interventions. From this pool <lb/>of 3,271 students, 1,850 had pretest and posttest assessments <lb/>and constituted the analytic sample for this study. These stu-<lb/>dents were enrolled in 127 classes across 34 teachers. Given <lb/>the larger context of the COVID-19 pandemic, the overall <lb/>attrition rate was 48.8%. <lb/>Our approach to causal inference under attrition, follow-<lb/>ing the guidelines of What Works Clearinghouse (WWC; <lb/>2020) and others, was to estimate treatment effects for the <lb/>subset of students in our analysis sample with complete pre-<lb/>test and posttest data. Under this approach, missing data <lb/>FIGURE 1. Consort figure representing attrition. <lb/></body>

			<page>6 <lb/></page>

			<body>methods, such as full-information maximum likelihood or <lb/>multiple imputation, are not necessary because no claims are <lb/>made about larger populations that include potential attritors <lb/>and non-attritors. <lb/>The threat of attrition bias was assessed in two ways-<lb/>first, by comparing attrition rates across conditions, and sec-<lb/>ond, by checking whether non-attritting students assigned to <lb/>different conditions were equivalent at baseline. As to the <lb/>first strategy, Figure 1 shows attrition for all key study con-<lb/>trasts. Importantly, no contrast exceeds attrition percentage <lb/>thresholds established by WWC. In the vernacular of WWC, <lb/>this study can be characterized as a low-attrition RCT, and <lb/>all key contrasts are eligible to &quot;Meet WWC Group Design <lb/>Standards Without Reservations&quot; (WWC, 2020, p. 5). This <lb/>characterization assumes that WWC would use the <lb/>&quot;Optimistic&quot; attrition threshold, which entails assuming that <lb/>sample loss is exogenous (i.e., unrelated) to intervention <lb/>conditions in the study. <lb/>Anecdotal evidence and exploratory data analysis (online <lb/>supplementary material, Appendix B) suggest that the rela-<lb/>tively high attrition rate for DragonBox was largely driven <lb/>by students in the Virtual Academy, due to difficulty install-<lb/>ing the extra required software. In any event, differential <lb/>attrition was not statistically significant (χ 2 (3, N = 1,849) = <lb/>3.978, p = .264), and differential as well as overall attrition <lb/>rates for active control comparisons fell within tolerable <lb/>threats of bias under optimistic assumptions (WWC, 2020). <lb/>Out of an abundance of caution, baseline equivalence of <lb/>the analysis sample was examined by using student demo-<lb/>graphics and pretest scores reported in Table 1. Differences <lb/>in covariate means for Active Control comparisons ranged in <lb/>magnitude from below 0.001 to roughly 0.22 standard devia-<lb/>tions. Therefore, the study met the WWC (2020) baseline <lb/>equivalence requirement after statistical adjustments for <lb/>covariates were made in Models 2-4, below. Baseline equiv-<lb/>alence for Active Control comparisons were tested by using <lb/>the methods of Hansen and Bowers (2008); differences were <lb/>significant compared to DragonBox (χ 2 (8) = 15.6, p = <lb/>.049), but not for the other two comparisons (Immediate <lb/>Feedback: χ 2 (8) = 6.7, p = .569 and FH2T χ 2 (8) = 11.6, p <lb/>= .17). There was evidence of higher pretest scores in each <lb/>of the experimental conditions compared to the Active <lb/>Control (Immediate Feedback: Z = 2.13, p = 0.033; <lb/>DragonBox: Z = 3.05, p = 0.002; FH2T: Z = 2.13, p = <lb/>0.033), suggesting that the association between pretest and <lb/>attrition was weaker in the Active Control and increasing the <lb/>importance of pretest adjustments in Models 2-4, below. <lb/>Procedure <lb/>This research was approved by the Institutional Review <lb/>Board at a university in the northeastern United States. This <lb/>research involved typical educational practices and thus did <lb/>not require parental consent. Instead, parents were informed <lb/>TABLE 1 <lb/>Student Demographic Information by Condition and Pretest Scores (N = 1,850) <lb/>Full Sample <lb/>N = 1,850 (100%) <lb/>FH2T <lb/>n = 753 (40.7%) <lb/>DragonBox <lb/>n = 350 (18.9%) <lb/>Immediate Feedback <lb/>n = 381 (20.6%) <lb/>Active Control <lb/>n = 366 (19.8%) <lb/>Gender <lb/>Male <lb/>932 (50.4%) <lb/>393 (52.2%) <lb/>171 (48.9%) <lb/>183 (48.0%) <lb/>185 (50.5%) <lb/>Female <lb/>918 (49.6%) <lb/>360 (47.8%) <lb/>179 (51.1%) <lb/>198 (52.0%) <lb/>181 (49.5%) <lb/>Race/Ethnicity <lb/>White <lb/>969 (52.4%) <lb/>392 (52.1%) <lb/>181 (51.7%) <lb/>210 (55.1%) <lb/>186 (50.8%) <lb/>Asian <lb/>458 (24.8%) <lb/>180 (23.9%) <lb/>82 (23.4%) <lb/>97 (25.5%) <lb/>99 (27.0%) <lb/>Hispanic <lb/>269 (14.5%) <lb/>118 (15.7%) <lb/>56 (16.0%) <lb/>44 (11.5%) <lb/>51 (13.9%) <lb/>African American <lb/>80 (4.3%) <lb/>35 (4.6%) <lb/>16 (4.6%) <lb/>14 (3.7%) <lb/>15 (4.1%) <lb/>Native American <lb/>10 (0.5%) <lb/>2 (0.3%) <lb/>3 (0.9%) <lb/>1 (0.3%) <lb/>4 (1.1%) <lb/>Pacific Islander <lb/>2 (0.1%) <lb/>0 (0.0%) <lb/>1 (0.3%) <lb/>0 (0.0%) <lb/>1 (0.3%) <lb/>Other race <lb/>62 (3.4%) <lb/>26 (3.5%) <lb/>11 (3.1%) <lb/>15 (3.9%) <lb/>10 (2.7%) <lb/>Accelerated math class <lb/>446 (24.1%) <lb/>178 (23.6%) <lb/>81 (23.1%) <lb/>94 (24.7%) <lb/>93 (25.4%) <lb/>EIP <lb/>134 (7.2%) <lb/>51 (6.8%) <lb/>29 (8.3%) <lb/>28 (7.3%) <lb/>26 (7.1%) <lb/>Gifted <lb/>309 (16.7%) <lb/>129 (17.1%) <lb/>45 (12.9%) <lb/>70 (18.4%) <lb/>65 (17.8%) <lb/>IEP <lb/>168 (9.1%) <lb/>58 (7.7%) <lb/>39 (11.1%) <lb/>35 (9.2%) <lb/>36 (9.8%) <lb/>Virtual <lb/>605 (32.7%) <lb/>249 (33.1%) <lb/>111 (31.7%) <lb/>125 (32.8%) <lb/>120 (32.8%) <lb/>Completed assignments (SD) <lb/>6.49 (2.95) <lb/>6.60 (2.86) <lb/>5.53 (3.38) <lb/>6.81 (2.68) <lb/>6.86 (2.77) <lb/>Average pretest (SD) <lb/>4.84 (2.67) <lb/>4.80 (2.70) <lb/>4.96 (2.60) <lb/>4.94 (2.70) <lb/>4.70 (2.67) <lb/>Average posttest (SD) <lb/>4.57 (2.90) <lb/>4.59 (2.96) <lb/>4.81 (2.88) <lb/>4.58 (2.90) <lb/>4.29 (2.79) <lb/>Note. EIP = Early Intervention Program; IEP = Individualized Education Program; SD = standard deviation. <lb/></body>

			<note place="headnote">Educational Technologies and Algebraic Understanding <lb/></note>

			<page>7 <lb/></page>

			<body>about the research and the data collected from the educa-<lb/>tional technologies through a letter and could opt their child <lb/>out of this study. <lb/>Students received nine 30-minute intervention sessions <lb/>across the school year, with a 2-week window to complete <lb/>each session. Four intervention sessions were administered <lb/>in the fall semester and five sessions in the spring semester. <lb/>Before and after the intervention, students received a 40-to <lb/>45-minute assessment on algebraic knowledge, mathemat-<lb/>ics anxiety, and self-efficacy. All interventions and assess-<lb/>ments were administered online, and students worked <lb/>individually at their own pace, using a device. For students <lb/>receiving instruction in person, teachers dedicated instruc-<lb/>tional periods for the study assignments in mathematics <lb/>classrooms. For students receiving virtual instruction, <lb/>teachers included the study assignments as a part of stu-<lb/>dents&apos; learning activities. <lb/>The mathematical content (arithmetic and algebraic equa-<lb/>tion solving) was aligned between the four conditions, and <lb/>all students solved algebraic problems during their interven-<lb/>tion sessions by using their assigned technology (including <lb/>the Active Control condition). Within each condition, a <lb/>countdown timer was embedded in the system to help ensure <lb/>that students used each technology for a similar amount of <lb/>time. Students could pause and resume the timer, so they <lb/>could stop for breaks and continue when they were ready. A <lb/>timer was not embedded in the assessments (i.e., pre-and <lb/>posttest), so students could take as long as needed to com-<lb/>plete the assessments. <lb/>In addition to using ASSISTments as an intervention <lb/>technology (i.e., Immediate Feedback and Active Control <lb/>conditions), it was the platform for the RCT in which assess-<lb/>ments were administered, assigned intervention conditions <lb/>were maintained within students over time, and fidelity data <lb/>were recorded for all conditions (see details in Chan et al., <lb/>2022). All students, regardless of their intervention condi-<lb/>tion, logged in to their ASSISTments account and opened <lb/>the same assignment link at the beginning of each study ses-<lb/>sion. The assignment link then directed students to their <lb/>technology intervention, and students spent 30 minutes with <lb/>their assigned technology. <lb/>Measures <lb/>Algebraic Knowledge Assessment. The algebraic knowl-<lb/>edge assessment consisted of 10 multiple-choice items from <lb/>a previously validated measure of algebraic understanding <lb/>(Star et al., 2015; Cronbach&apos;s α = .89; see the 10 items on <lb/>osf.io/bafdr). Within the 10 items, four focused on concep-<lb/>tual understanding of algebraic equation-solving (e.g., the <lb/>meaning of an equal sign), three focused on procedural skills <lb/>of equation-solving (e.g., solving for a variable), and three <lb/>focused on flexibility of equation-solving strategies (e.g., <lb/>evaluating different equation-solving strategies). These 10 <lb/>items together assessed a range of students&apos; knowledge in <lb/>algebraic equation-solving, which is the ultimate goal of <lb/>each of the education technologies tested. The study team <lb/>created the posttest assessment by substituting numbers of <lb/>similar magnitudes in the pretest questions and response <lb/>options. Each item in the pretest and posttest was scored as <lb/>correct (1) or incorrect (0), and the reliability of these items <lb/>were KR-20 (Kuder-Richardson Formula 20; a reliability <lb/>measure for binary variables) = .74 at pretest and .79 at <lb/>posttest. The total score out of 10 on the posttest scores was <lb/>included as the outcome, and the pretest score was included <lb/>as a covariate. <lb/>Assessments were administered within the ASSISTments <lb/>platform, in which students saw questions one at a time and <lb/>selected a response option by using a mouse. No feedback <lb/>was given to students about the correctness of their response. <lb/>The pretest assessment was administered in September <lb/>2020, approximately 1 week prior to the intervention ses-<lb/>sions. The posttest assessment was administered between <lb/>the end of March and the beginning of April 2021, approxi-<lb/>mately 2 weeks following completion of the intervention. <lb/>Intervention Condition. Intervention condition was a four-<lb/>level dummy-coded categorical variable indicating partici-<lb/>pation in FH2T, DragonBox, and the Immediate Feedback <lb/>condition. The Active Control condition served as the refer-<lb/>ent group. <lb/>Race/Ethnicity. Students&apos; race/ethnicity was a three-level <lb/>dummy-coded categorical variable indicating whether a stu-<lb/>dent identified as non-Hispanic White, Asian, or some other <lb/>ethnic/racial group. Non-Hispanic White was the referent <lb/>group in the analyses. <lb/>Students&apos; Biological Sex. Students&apos; biological sex was a <lb/>dummy-coded variable indicating whether a student identi-<lb/>fied as being male. Female was the reference group (0 = <lb/>female, 1 = male). <lb/>Gifted Status. Students&apos; identification as gifted was a <lb/>dummy-coded variable in the analysis (0 = not identified as <lb/>gifted, 1 = identified as gifted). <lb/>Accelerated Mathematics. All students in our study were <lb/>placed in one of two levels of mathematics classrooms by <lb/>the district: accelerated, in which the teachers implemented <lb/>more challenging course materials, or regular mathematics <lb/>classes. Student enrollment in an accelerated mathematics <lb/>class was a dummy-coded variable (0 = not in an acceler-<lb/>ated mathematics class, 1 = enrolled in an accelerated math-<lb/>ematics class). <lb/>Early Intervention Program Status. Student participation in <lb/>an early intervention program (EIP) in elementary school <lb/></body>

			<note place="headnote">Decker-Woodrow et al. <lb/></note>

			<page>8 <lb/></page>

			<body>when identified as needing extra support was a dummy-<lb/>coded variable (0 = not enrolled in EIP, 1 = enrolled in <lb/>EIP). <lb/>Individualized Education Program Status. Students&apos; indi-<lb/>vidualized education program (IEP) status was a dummy-<lb/>coded variable (0 = did not receive an IEP, 1 = received an <lb/>IEP). <lb/>Classroom Format. Students&apos; enrollment in an in-person <lb/>versus virtual classroom was a dummy-coded variable (0 = <lb/>in-person classroom, 1 = virtual classroom). <lb/>Completed Assignments. The total number of the nine inter-<lb/>vention sessions a student completed was a continuous <lb/>variable. <lb/>Intervention Conditions. In all four conditions, students <lb/>accessed their learning technology through the ASSIST-<lb/>ments platform. Students logged in to the platform with a <lb/>username and a password and then clicked the assignment <lb/>link that directed them to their assigned technology. <lb/>FH2T. FH2T consists of 14 worlds that focus on different <lb/>mathematical concepts, and each world contains 18 prob-<lb/>lems (a total of 252 problems). In this study, all students in <lb/>the FH2T condition started at World 1: Addition and worked <lb/>their way through the worlds-World 2: Multiplication, <lb/>World 3: Order of Operations + and ×, World 4: Subtrac-<lb/>tion and Negative Numbers, World 5: Mixed Practice of + <lb/>and -, World 6: Division, World 7: Order of Operations, <lb/>World 8: Equation + and -, World 9: Inverse Operations + <lb/>and -, World 10: Distribution, World 11: Factoring, World <lb/>12: Equation +, -, ×, and ÷, World 13: Inverse Operations, <lb/>and World 14: Final Review. All students were given 30 <lb/>minutes to play FH2T for each session, after which the sys-<lb/>tem would log students out of the game. The system also <lb/>saves students&apos; progress, so students could start where they <lb/>left off in each subsequent session. <lb/>DragonBox. DragonBox has 10 chapters, and each chapter <lb/>contains 20 problems (a total of 200 problems). The prob-<lb/>lems together tap the Grade 7 mathematics standards in <lb/>Common Core (Common Core State Standards for Mathe-<lb/>matics, 2010) and cover the following content: addition, <lb/>multiplication, division, parentheses, negative signs, frac-<lb/>tion addition, combining like terms, variables, factoring, and <lb/>substitution. Similar to FH2T, DragonBox students worked <lb/>through problems at their own pace and started from where <lb/>they left off in each subsequent session. <lb/>Because DragonBox is a phone-or tablet-based applica-<lb/>tion that cannot be opened on a web browser, the study team <lb/>provided tablets on which students played DragonBox in in-<lb/>person classrooms. For students in virtual classrooms, the <lb/>research team provided instructions and licenses for students <lb/>to download the game on their own device. Regardless of the <lb/>classroom format, students would log in to ASSISTments on <lb/>their laptop or Chromebook at the beginning of each session, <lb/>follow the instructions to start the timer for the session, and <lb/>then play DragonBox on a tablet or personal device. <lb/>Because DragonBox is a commercial application, access <lb/>to students&apos; progress and action-level data within the game <lb/>was not possible. Therefore, at the end of each intervention <lb/>session, students self-reported their progress-the chapter and <lb/>problem at which they stopped that day-in ASSISTments. <lb/>In case students did not report their progress at the end of the <lb/>session, they were also asked to report their previous prog-<lb/>ress at the beginning of each intervention session before they <lb/>started playing (after the first intervention session). Given <lb/>that students in the DragonBox condition had to report their <lb/>progress before and after gameplay, the timer for playing <lb/>DragonBox was set to 25 minutes. This way, students could <lb/>complete the sessions in 30 minutes without disrupting <lb/>teachers&apos; lesson plans and instruction. <lb/>Immediate Feedback. Students in the Immediate Feedback <lb/>condition solved traditional mathematics problems in <lb/>ASSISTments (Heffernan &amp; Heffernan, 2014). To ensure <lb/>that the problems aligned with traditional instruction, the <lb/>study team selected and adapted problems from three popu-<lb/>lar open-source middle-school mathematics curricula-<lb/>Engage NY (2014), Utah Math Project (2016), and <lb/>Illustrative Mathematics (2017)-that already existed in <lb/>ASSISTments. The topics covered across the nine interven-<lb/>tion sessions were (a) addition and multiplication, (b) sub-<lb/>traction and negative numbers, (c) division and fractions, (d) <lb/>order of operations, (e) addition and subtraction in equation-<lb/>solving, (f) distribution, (g) factoring, (h) multiplication and <lb/>division in equation-solving, and (i) review. <lb/>For each intervention session, students received a prob-<lb/>lem set consisting of 35-45 problems. They started on the <lb/>first problem of a problem set at the beginning of each ses-<lb/>sion and worked through the problems in the same order. <lb/>After 25 minutes, or after students had completed the prob-<lb/>lem set, the system directed students to their session report, <lb/>which showed their performance on each problem. Students <lb/>were directed to review their session report, revisit prob-<lb/>lems, and review the hints and correct answers within the <lb/>30-minute time frame. <lb/>Active Control. In the Active Control condition, the mathe-<lb/>matical problems and the study procedure were identical to <lb/>the Immediate Feedback condition. The only difference <lb/>between the two conditions was that the hints and correct-<lb/>ness feedback were available during problem-solving in the <lb/>Immediate Feedback condition, whereas the hints and cor-<lb/>rectness feedback were available only after problem-solving <lb/>(i.e., after 25 minutes or completing the problem set) in the <lb/></body>

			<page>9 <lb/></page>

			<body>Active Control condition. At this point, students could revisit <lb/>any question, review their session report, and review the <lb/>hints and correct answers within the 30-minute time frame. <lb/>Analytic Approach <lb/>Intervention effects were tested through a series of three-<lb/>level hierarchical linear models, with students (N = 1,850), <lb/>nested within classrooms (n = 127), nested within teachers <lb/>(n = 34). Classroom format (in person versus virtual) was a <lb/>Level 2 predictor, and all other predictors were Level 1 vari-<lb/>ables. The number of assignments completed and pretest <lb/>scores were centered on their respective grand means. All <lb/>other predictors were dummy-coded. The outcome measure <lb/>was student performance on the posttest assessment. The <lb/>primary analyses were registered on the Open Science <lb/>Framework (https://osf.io/2r5zs). <lb/>Results <lb/>Descriptive Statistics <lb/>Means and standard deviations for all variables were pre-<lb/>viously reported in Table 1. A correlation matrix is included <lb/>in the online supplementary material to this article (Appendix <lb/>C, Table C1); however, three sets of results within the cor-<lb/>relation matrix are worth mentioning. As noted previously, <lb/>intervention condition was related to pretest scores (χ 2 (3, N <lb/>= 1,850) = 13.303, p = .004). In addition, condition was <lb/>related to the number of assignments a student completed <lb/>(χ 2 (3, N = 1,850) = 28.657, p &lt; .001), with students in the <lb/>DragonBox condition completing fewer assignments (M = <lb/>5.526, SD = 3.382) than those in the FH2T (M = 6.603, SD <lb/>= 2.857), Immediate Feedback (M = 6.811, SD = 2.679), <lb/>and Active Control conditions (M = 6.858, SD = 2.770). <lb/>Intervention condition was unrelated to all other predictor <lb/>variables, including child race/ethnicity (χ 2 (6, N = 1,850) = <lb/>11.316, p = .079), sex (χ 2 (3, N = 1,850) = 2.322, p = .508), <lb/>gifted status (χ 2 (3, N = 1,850) = 4.408, p = .221), acceler-<lb/>ated program (χ 2 (3, N = 1,850) = 0.248, p = .969), EIP <lb/>status (χ 2 (3, N = 1,850) = 0.881, p = .830), IEP status <lb/>(χ 2 (3, N = 1,850) = 6.462, p = .091), and enrollment in a <lb/>virtual classroom (χ 2 (3, N = 1,850) = 2.540, p = .468). <lb/>Second, regarding the composition of virtual versus in-<lb/>person classrooms, 67.3% of students in in-person class-<lb/>rooms were White, with 6.8% Asian and 25.9% identified <lb/>with some other racial/ethnic group. In contrast, 61.7% of <lb/>students enrolled in a virtual classroom were Asian, with <lb/>21.7% White and 16.7% identified with some other racial/ <lb/>ethnic group. Reflecting this pattern, enrollment in a virtual <lb/>classroom was also related to most other predictors (see <lb/>Table 2). Specifically, virtual classrooms had more gifted <lb/>and accelerated students, but fewer males and fewer students <lb/>enrolled in EIP. In addition, students in virtual classrooms <lb/>completed more assignments (M = 7.321, SD = 2.766) than <lb/>those in in-person classrooms (M = 6.090, SD = 2.952, <lb/>t(92) = 2.489, p = .015) and had higher pretest scores (vir-<lb/>tual: M = 6.729, SD = 2.622, and in-person: M = 3.924, SD <lb/>= 2.172, respectively, t(92) = 6.180, p &lt; .001). <lb/>Primary Analyses <lb/>An initial model tested intervention effects with no other <lb/>covariates. This summarized the overall intervention effects <lb/>only controlling for the inherent nested design. As shown in <lb/>the first panel (i.e., Model 1) in Table 3, a significant inter-<lb/>vention effect was observed (χ 2 (3, N = 1,850) = 17.325, p <lb/>= 0.001). All three intervention conditions had higher per-<lb/>formance on the posttest than did the Active Control condi-<lb/>tion, with the largest effect seen for DragonBox (γ = .641, <lb/>Hedge&apos;s g = .240), followed by FH2T (γ = .370, Hedge&apos;s g <lb/>= .138) and the Immediate Feedback conditions (γ = .325, <lb/>Hedge&apos;s g = .122). <lb/>Model 2 controlled for various student demographic and <lb/>academic covariates existing prior to the randomization. All <lb/>were significantly related to posttest performance, with the <lb/>exception of IEP status. Intervention condition continued to <lb/>be statistically significant (χ 2 (3, N = 1,850) = 12.226, p = <lb/>0.007), with effects for FH2T (γ = .338) and DragonBox (γ <lb/>= .555) significantly larger than the Active Control condi-<lb/>tion. To produce Model 3, an indicator was included for <lb/>enrollment in an in-person or virtual classroom and a post-<lb/>randomization variable, the number of assignments com-<lb/>pleted by the student (a marker of dosage). Both were <lb/>statistically significant predictors of the posttest score, with <lb/>higher posttest scores observed for students in virtual <lb/>TABLE 2 <lb/>Percentage of Students Within Each Classroom Format Who Was Male, Gifted, in an Accelerated Program, or Enrolled in EIP or Had <lb/>an IEP <lb/>Classroom format <lb/>Male <lb/>Gifted <lb/>Accelerated <lb/>EIP <lb/>IEP <lb/>In person <lb/>53.4% <lb/>10.2% <lb/>12.7% <lb/>8.8% <lb/>10.6% <lb/>Virtual <lb/>44.1% <lb/>30.1% <lb/>47.6% <lb/>4.0% <lb/>6.0% <lb/>Classroom format t(92) = <lb/>-4.528*** <lb/>3.798*** <lb/>2.260*** <lb/>-2.295* <lb/>-1.830 <lb/>Note. *** p &lt; .001, * p &lt; .05. <lb/></body>

			<page>10 <lb/></page>

			<body>classrooms (γ = .415) and those with a higher number of <lb/>completed assignments (γ = .120). In this model, coeffi-<lb/>cients on intervention conditions were estimates of &quot;natural <lb/>direct effects&quot; (VanderWeele, 2015, p.22): Effects of inter-<lb/>vention were the number of completed assignments held <lb/>fixed (assuming no unadjusted confounding between the <lb/>number of completed assignments and posttest scores). <lb/>Direct intervention effects were statistically significant <lb/>(χ 2 (3, N = 1,850) = 20.863, p &lt; 0.001), with students in <lb/>FH2T (γ = .361, Hedge&apos;s g = .135) and DragonBox condi-<lb/>tions (γ = .719, Hedge&apos;s g = .269) significantly outperform-<lb/>ing students in the Active Control condition. <lb/>As our approach to attrition was conservative (excluding stu-<lb/>dents without pretest data), we also fit the models including stu-<lb/>dents with posttests but with missing pretests. These analyses <lb/>were conducted by imputing the global mean for missing pretest <lb/>scores, with an indicator variable for missing pretest scores. This <lb/>approach yielded similar results as those presented here. <lb/>Follow-Up Interaction Analyses <lb/>A series of follow-up analyses tested for interactions <lb/>between intervention condition and the various predictor <lb/>variables. As reported in Model 4 (see Table 3), a significant <lb/>interaction was observed between intervention condition <lb/>and pretest scores (χ 2 (3, N = 1,850) = 11.192, p = 0.011). <lb/>This reflected larger FH2T effects among students with <lb/>higher versus lower pretest performance (see Figure 2). No <lb/>other interactions with intervention condition were found to <lb/>be statistically significant: Race/Ethnicity: Asian: χ 2 (6, N = <lb/>1,850) = 5.212, p = 0.517; Male: χ 2 (3, N =1,850) = 1.193, <lb/>p = 0.755; Gifted: χ 2 (3, N = 1,850) = 2.785, p = 0.426; <lb/>Accelerated program: χ 2 (3, N = 1,850) = 1.799, p = 0.615; <lb/>EIP status: χ 2 (3, N = 1,850) = 0.196, p = 0.978; IEP status: <lb/>χ 2 (3, N = 1,850) = 1.204, p = 0.752; Virtual classroom: <lb/>χ 2 (3, N = 1,850) = 3.870, p = 0.276; Number of assign-<lb/>ments completed: χ 2 (3, N = 1,850) = 1.757, p = 0.624. <lb/>Discussion <lb/>In summary, after a 4.5-hour intervention, students in <lb/>FH2T and DragonBox conditions were found to have sig-<lb/>nificantly higher posttest scores, reflecting students&apos; higher <lb/>algebraic understanding, compared to students in the Active <lb/>Control condition. These effects remained after controlling <lb/>for students&apos; prior knowledge and demographic variables. <lb/>TABLE 3 <lb/>Summary of HLM Models Predicting Posttest Scores (N = 1,850) <lb/>Model 1 <lb/>Model 2 <lb/>Model 3 <lb/>Model 4 <lb/>Coef <lb/>95% CI <lb/>Coef <lb/>95% CI <lb/>Coef <lb/>95% CI <lb/>Coef <lb/>95% CI <lb/>Intercept <lb/>4.084*** [3.497, 4.671] <lb/>3.577*** [3.327, 3.827] <lb/>3.462*** [3.178, 3.745] <lb/>3.446*** <lb/>[3.149, 3.743] <lb/>FH2T <lb/>0.370* <lb/>[0.068, 0.672] <lb/>0.338* <lb/>[0.050, 0.627] <lb/>0.361** <lb/>[0.092, 0.630] <lb/>0.373** <lb/>[0.117, 0.630] <lb/>Dragon <lb/>0.641*** [0.328, 0.955] <lb/>0.555*** [0.243, 0.868] <lb/>0.719*** [0.408, 1.030] <lb/>0.733*** [0.432, 1.033] <lb/>Immediate <lb/>Feedback <lb/>0.325* <lb/>[0.012, 0.638] <lb/>0.240 <lb/>[-0.066, 0.547] <lb/>0.254 <lb/>[-0.037, 0.545] <lb/>0.268 <lb/>[-0.014, 0.550] <lb/>Pretest <lb/>0.313*** [0.265, 0.362] <lb/>0.279*** [0.235, 0.323] <lb/>0.199*** [0.120, 0.279] <lb/>Asian <lb/>1.032*** [0.675, 1.390] <lb/>0.824*** [0.430, 1.219] <lb/>0.827*** [0.439, 1.216] <lb/>Other race <lb/>0.167 <lb/>[-0.077, 0.411] <lb/>0.130 <lb/>[-0.109, 0.370] <lb/>0.135 <lb/>[-0.110, 0.379] <lb/>Male <lb/>-0.322*** [-0.465, -0.180] <lb/>-0.255** <lb/>[-0.409, -0.100] <lb/>-0.259*** [-0.411, -0.106] <lb/>Gifted <lb/>0.616*** [0.343, 0.889] <lb/>0.612*** [0.325, 0.899] <lb/>0.610*** [0.328, 0.893] <lb/>Accelerated <lb/>1.792*** [1.296, 2.288] <lb/>1.668*** [1.178, 2.158] <lb/>1.679*** [1.197, 2.161] <lb/>EIP <lb/>-0.228* <lb/>[-0.411, -0.046] <lb/>-0.183* <lb/>[-0.360, -0.006] <lb/>-0.182* <lb/>[-0.361, -0.002] <lb/>IEP <lb/>0.217 <lb/>[-0.078, 0.513] <lb/>0.208 <lb/>[-0.063, 0.479] <lb/>0.197 <lb/>[-0.075, 0.470] <lb/>Virtual <lb/>0.415* <lb/>[0.048, 0.782] <lb/>0.429* <lb/>[0.067, 0.791] <lb/>Assign % <lb/>0.120*** [0.088, 0.153] <lb/>0.119*** [0.087, 0.151] <lb/>FH2T × Pre <lb/>0.136** <lb/>[0.044, 0.227] <lb/>Dragon × Pre <lb/>0.048 <lb/>[-0.094, 0.191] <lb/>Immediate <lb/>Feedback × Pre <lb/>0.058 <lb/>[-0.071, 0.186] <lb/>Note. All three coefficients also remained significant at p &lt; 0.05 after Holm correction (Holm, 1979). <lb/>Model 1: Condition: χ 2 (3, N = 1,850) = 17.325, p = 0.001 <lb/>Model 2: Condition: χ 2 (3, N = 1,850) = 12.226, p = 0.007; Race Effect: χ 2 (3, N = 1,850) = 38.567, p &lt; 0.001 <lb/>Model 3: Condition: χ 2 (3, N = 1,850) = 20.863, p &lt; 0.001; Race Effect: χ 2 (3, N = 1,850) = 19.962, p &lt; 0.001 <lb/>Model 4: Condition: χ 2 (3, N = 1,850) = 22.911, p &lt; 0.001; Race Effect: χ 2 (3, N = 1,850) = 20.769, p &lt; 0.001; Cond x Pretest: χ 2 (3, N = 1,850) = 11.192, p = 0.011 <lb/>Accelerated = Student in an accelerated mathematics program; Assign = Number of assignments completed; Dragon = DragonBox; Dragon × Pre = Dragon × Pretest; EIP = <lb/>Enrolled in Early Intervention Program; FH2T = From Here to There; FH2T × Pre = FH2T × Pretest; Gifted = Identified as gifted; IEP = Has an Individual Education Program; <lb/>Immediate = Immediate Feedback; Immediate Feedback × Pre = Immediate Feedback × Pretest Other race = Race/ethnicity other than Asian or White/non-Hispanic; Pretest = <lb/>Pretest score (centered); Virtual = Enrolled in a virtual class. <lb/></body>

			<page>11 <lb/></page>

			<body>Although students in the Immediate Feedback condition also <lb/>displayed significantly higher posttest scores compared to <lb/>students in the Active Control condition, this effect was no <lb/>longer significant after controlling for covariates. Together, <lb/>the findings suggest that FH2T and DragonBox are effective <lb/>digital games to support Grade 7 students&apos; algebraic under-<lb/>standing compared to an active control condition using <lb/>online problem sets with post-assignment feedback. Below, <lb/>findings are discussed in detail. <lb/>Learning With Games: FH2T and DragonBox <lb/>In the current study, students in the two game-based <lb/>learning interventions outperformed students in the Active <lb/>Control condition on the posttest algebraic knowledge <lb/>assessment. Further, the effect size for FH2T was similar to <lb/>that reported in a prior study comparing the efficacy of FH2T <lb/>to the Immediate Feedback condition (Hedge&apos;s g = 0.16; <lb/>Chan et al., 2022), providing additional support for the con-<lb/>sistent positive effects of FH2T. Although effects were found <lb/>for both games, they were designed based on different theo-<lb/>retical perspectives. FH2T and DragonBox are grounded in <lb/>intuitive perceptual-motor routines and game-based inter-<lb/>face; however, the two games differ in important ways. For <lb/>instance, FH2T introduces algebraic notations from the start, <lb/>whereas DragonBox gradually transitions students from pic-<lb/>tures of monsters to abstract notations. Further, DragonBox <lb/>asks students to isolate variables across notational contexts, <lb/>whereas FH2T asks students to transform between mathe-<lb/>matically equivalent but perceptually different expressions <lb/>or equations (e.g., 9 • 12 + 9 • 23 and 9 • 7 • 5). It is possible <lb/>that the progression from concrete pictures to abstract sym-<lb/>bols (Fyfe et al., 2014) and the focus on isolating variables <lb/>are elements within DragonBox that effectively support stu-<lb/>dents&apos; algebra performance. However, qualitative investiga-<lb/>tions of teacher and student interactions suggest that the <lb/>design of DragonBox can make it difficult for teachers and <lb/>students to speak precisely about concepts introduced by the <lb/>game and to connect these concepts back to mathematics <lb/>(Dolonen &amp; Kluge, 2015). FH2T provides students with <lb/>opportunities to explore and learn which gesture-actions <lb/>with algebraic notations are appropriate and allowed across <lb/>mathematical contexts (Dörfler, 2003; Landy &amp; Goldstone, <lb/>2009; Nogueira de Lima &amp; Tall, 2008). However, because <lb/>game-based learning has other potential benefits, such as <lb/>increasing motivation (Hidi &amp; Renninger, 2006; Rotgans &amp; <lb/>Schmidt, 2011) and enhancing engagement (Mora et al., <lb/>2015), it may be the case that DragonBox and FH2T support <lb/>algebraic learning through engaging students with mathe-<lb/>matical content and fostering positive attitudes toward <lb/>FIGURE 2. Interaction between pretest performance and intervention condition. <lb/></body>

			<note place="headnote">Decker-Woodrow et al. <lb/></note>

			<page>12 <lb/></page>
			<body>mathematics. Although the current study does not tease apart <lb/>these potential mechanisms, it does provide a foundation for <lb/>future inquiries that could explore how various game ele-<lb/>ments differentially affect student learning from educational <lb/>technology games. For example, future analyses could use <lb/>the clickstream data within FH2T to explore the relations <lb/>between students&apos; in-game behaviors and their learning <lb/>outcomes. <lb/>Alternatively, the positive impacts of FH2T and <lb/>DragonBox may be a result of their game-based context, <lb/>which may increase students&apos; interest and engagement with <lb/>mathematics and, in turn, improve their performance. <lb/>Substantial research has highlighted the benefits of inte-<lb/>grating gamification elements, such as rewards, leveling, <lb/>challenges, supporting player skills, player controls, and <lb/>feedback, into instruction to promote learning (Garris <lb/>et al., 2002; Gee, 2003; Kalloo &amp; Mohan, 2015). <lb/>Technology-based educational games have been found to <lb/>increase engagement and motivation, which have also <lb/>increased student problem-solving, and learning (Connolly <lb/>et al., 2012; Foster, 2008; Ke, 2008; Samur &amp; Evans, 2012). <lb/>Therefore, the motivational factors related to game-based <lb/>learning may be another potential mechanism through <lb/>which FH2T and DragonBox may support students&apos; alge-<lb/>braic understanding. <lb/>Further, anecdotes from participating teachers revealed <lb/>that some students in the two problem-set conditions lacked <lb/>the motivation to solve textbook problems, knowing that <lb/>other students in the same classroom were playing games. <lb/>Although the student-level randomization maximizes the <lb/>statistical power to detect intervention effects, administering <lb/>four different conditions within each classroom may inevita-<lb/>bly highlight the differences between the interventions and <lb/>ultimately contribute to the impacts of the game-based inter-<lb/>ventions compared to the problem-set conditions. Because <lb/>of the COVID context, 32.7% of students received the inter-<lb/>vention through their virtual classroom and worked on their <lb/>study assignments at home, without the awareness of other <lb/>intervention conditions. Additional data were also collected <lb/>on students&apos; attitudes toward mathematics as well as their <lb/>engagement with the technologies. Future analyses will use <lb/>these additional data to explore whether game-based inter-<lb/>ventions supported algebraic understanding through increas-<lb/>ing students&apos; motivations or engagements with their <lb/>intervention and how the classroom context of the current <lb/>study may interact with the intervention condition to influ-<lb/>ence student learning. <lb/>Interaction Between FH2T and Pretest Scores <lb/>Study effects for FH2T and DragonBox were largely <lb/>unchanged when investigating interactions with multiple <lb/>demographic and program variables related to students. <lb/>However, results did indicate a significantly larger effect for <lb/>FH2T among students who scored higher on the pretest. This <lb/>finding suggests additional benefits of FH2T for students <lb/>who already possess foundational or basic knowledge of <lb/>algebraic concepts. In particular, the design features of <lb/>FH2T, such as introducing algebraic notations from the start <lb/>and having flexible goals, may require students to have some <lb/>foundational algebraic knowledge to further benefit from the <lb/>game. Past research on FH2T has been mixed in finding this <lb/>relation. For example, in an investigation of Grades 6-7 stu-<lb/>dents, Chan et al. (2022) do not find a significant interaction <lb/>between level of pretest and student understanding of math-<lb/>ematical equivalence. However, in a sample of Grade 2 stu-<lb/>dents, a significant interaction between pretest and progress <lb/>within FH2T is found (Hulse et al., 2019). Specifically, <lb/>among students with lower, as opposed to higher, prior <lb/>knowledge, solving more problems in FH2T is associated <lb/>with higher posttest scores on an arithmetic assessment. The <lb/>moderating effects of prior knowledge may vary, depending <lb/>on students&apos; grade level (i.e., second, sixth, seventh), the <lb/>particular FH2T variable used (i.e., intervention assign-<lb/>ment vs. progress within FH2T), and the outcome of inter-<lb/>est (i.e., algebraic knowledge, mathematical equivalence, <lb/>arithmetic). Future research should continue to explore the <lb/>extent to which prior knowledge moderates the relation-<lb/>ship between using FH2T and algebraic knowledge gain. <lb/>Doing so will advance the understanding of when and for <lb/>whom such interventions as FH2T are beneficial for learn-<lb/>ing mathematics. <lb/>Online Problem Sets <lb/>Although students in the Immediate Feedback condition <lb/>also displayed significantly higher posttest scores compared <lb/>to students in the Active Control condition, this effect was <lb/>no longer significant after controlling for several covariates. <lb/>Different from the prior findings within ASSISTments <lb/>(Kelly et al., 2013; Mendicino et al., 2009), this study did <lb/>not find consistent evidence that students in the Immediate <lb/>Feedback condition outperformed the students in the Active <lb/>Control condition with post-assignment feedback. It is <lb/>important to note that the two problem-set conditions only <lb/>involved the student aspects of ASSISTments (i.e., hints, <lb/>feedback, performance report), whereas the prior efficacy <lb/>study on ASSISTments also involved teacher training that <lb/>provided support on monitoring student progress within the <lb/>system and using student performance to inform instruc-<lb/>tional practices in the classroom (Roschelle et al., 2016). <lb/>The current findings, therefore, extend prior research and <lb/>suggest that implementing only student aspects of <lb/>ASSISTments in a homework-style intervention may not <lb/>consistently support students&apos; algebra learning. Teacher <lb/>training may be a critical component of the ASSISTments <lb/>intervention that effectively supports students&apos; mathematics <lb/>achievement. <lb/></body>

			<page>13 <lb/></page>

			<body>Study Context <lb/>It is important to note that the study was conducted within <lb/>the larger context of the COVID-19 pandemic. This context <lb/>affected many aspects of the current study. For instance, <lb/>although significant effects were found for posttest scores of <lb/>students in the FH2T and DragonBox conditions, overall, <lb/>scores were lower at posttest than at pretest. This is likely <lb/>due to interrupted learning and the overarching lack of typi-<lb/>cal gains seen across the country in mathematics due to the <lb/>pandemic (Dorn et al., 2020). For example, Kuhfeld and <lb/>Lewis (2022) find that the pandemic has had larger nega-<lb/>tive impacts on mathematics compared to reading and that <lb/>those effects have not significantly diminished as of the <lb/>2021-2022 school year. Additionally, the most recent long-<lb/>term trend National Assessment of Educational Progress <lb/>results in mathematics achievement for 13-year-olds indi-<lb/>cate a significant drop since the last data collection in 2012 <lb/>(U.S. Department of Education, n.d.). Within this context, <lb/>however, the current findings seem to suggest that FH2T <lb/>and DragonBox were able to serve as supports to reduce <lb/>the effects of interrupted learning for students in those <lb/>conditions. <lb/>Additionally, the school district offered the students and <lb/>their families options for synchronous in-person instruction <lb/>in the school buildings or asynchronous virtual instruction at <lb/>home (with the addition of synchronous help sessions). <lb/>Regardless of the instructional format, all students in the <lb/>current study received their assigned intervention online and <lb/>worked individually at their own pace, using a device. <lb/>Although the study procedure was not affected by the class-<lb/>room format, the differences in students&apos; classroom format <lb/>inadvertently introduced systematic variations in the study <lb/>contexts. Specifically, students in in-person classrooms were <lb/>aware of other intervention conditions, whereas students in <lb/>virtual classrooms were not. Although no significant interac-<lb/>tions were identified between study condition and instruc-<lb/>tional format, the presence versus absence of this awareness <lb/>may have influenced students&apos; motivation to complete their <lb/>intervention in unmeasured ways and, in turn, may have <lb/>amplified the effects of game-based learning compared to <lb/>traditional problem-solving. <lb/>Further, whether students enrolled in in-person or virtual <lb/>classrooms was not random. A higher proportion of Asian <lb/>students and high-performing students opted for virtual <lb/>instruction, whereas a higher proportion of White students <lb/>and low-performing students selected in-person instruction. <lb/>These systematic differences between in-person and virtual <lb/>classrooms may have been related to the high proportion of <lb/>multigenerational households among Asian students, their <lb/>anxiety about being bullied due to political and racial ten-<lb/>sions, or satisfaction with virtual learning (Balingit et al., <lb/>2021). Regardless of the reasons behind the families&apos; deci-<lb/>sion for enrolling students in in-person or virtual classrooms, <lb/>their choices had implications on the student compositions <lb/>within the classrooms and school buildings. Specifically, as <lb/>a higher proportion of Asian students and high-performing <lb/>students opted for virtual instruction, the heterogeneity of <lb/>the in-person classrooms was reduced through this process. <lb/>Given the important differences between in-person and vir-<lb/>tual classrooms, analyses controlled for the classroom for-<lb/>mat and found that intervention effects remained significant. <lb/>Future work will further explore how the pandemic context <lb/>affected student learning. <lb/>Limitations and Future Directions <lb/>The current study was limited in several ways. First, the <lb/>attrition in the current study was nontrivial. Among the <lb/>3,612 students who participated at the start of the interven-<lb/>tion, only 1,850 students completed pretest and posttest and <lb/>thus were included in the current analyses. Although almost <lb/>half of the students had dropped out of the study by the end <lb/>of the intervention, the dropout rate was not related to the <lb/>intervention condition, and there was minimal differential <lb/>attrition between conditions. Second, as each technology <lb/>condition functioned differently, time within the 30-minute <lb/>sessions was used differently (all practice in FH2T, while <lb/>ASSISTments included practice and review). Students in the <lb/>DragonBox condition needed to stop a few minutes early to <lb/>log their progress, which meant that those students did spend <lb/>fewer minutes engaging in the technology. Although this <lb/>step was necessary to maintain equal time within the class-<lb/>room across conditions, future studies may explore differ-<lb/>ences in timing and overall effectiveness. Third, the current <lb/>study focused on comparing each of the three treatment con-<lb/>ditions to an active control condition. Future studies will <lb/>examine differences across conditions to further inform the <lb/>ways in which these technologies relate to the conceptual, <lb/>procedural, and flexibility of algebraic thinking. Fourth, the <lb/>current study provided evidence that Grade 7 students in the <lb/>FH2T and DragonBox conditions did significantly better on <lb/>the posttest reflecting algebraic understanding, but it did not <lb/>offer insights into their areas of improvement or potential <lb/>mechanisms. Preliminary follow-up analyses have revealed <lb/>that FH2T and DragonBox improved on conceptual knowl-<lb/>edge, but not on procedural knowledge or procedural flexi-<lb/>bility (Chan et al., 2023). Future studies using the log data <lb/>will explore how these intervention conditions affect aspects <lb/>of students&apos; conceptual knowledge, whether students&apos; under-<lb/>standing was moderated by their prior knowledge or other <lb/>demographic factors, and how the fidelity of implementation <lb/>at the student level influenced student learning from each <lb/>intervention. Addressing these questions is important, as <lb/>they provide crucial information on the ways in which the <lb/>interventions support algebra learning, for whom the inter-<lb/>ventions were effective, and the amount of intervention <lb/>needed to result in learning gains. Finally, given the unique <lb/></body>

			<note place="headnote">Decker-Woodrow et al. <lb/></note>

			<page>14 <lb/></page>

			<body>context of the COVID-19 pandemic, the data set may offer <lb/>insights into how student learning was affected by the pan-<lb/>demic and inform future practices across instructional for-<lb/>mat and during stressful emergency situations. <lb/>Conclusion <lb/>Overall, the current study contributes to the efficacy of <lb/>FH2T and DragonBox as interventions for supporting alge-<lb/>braic performance. Both games are promising interventions <lb/>that focus on training students&apos; perceptual-motor routines in <lb/>algebraic reasoning, yet they differ in several important <lb/>ways. The findings have implications for future research in <lb/>delineating the effective elements within each game. Further, <lb/>they also have implications for educational practices, as they <lb/>provide evidence that gamified learning platforms are effec-<lb/>tive ways for students to explore mathematical ideas and <lb/>support their understanding. <lb/></body>

		<div type="funding">Funding <lb/>The research reported here was supported by the Institute of <lb/>Education Sciences, U.S. Department of Education, through an <lb/>Efficacy and Replication Grant (R305A180401) to Worcester <lb/>Polytechnic Institute. The opinions expressed are those of the <lb/>authors and do not represent views of the institute or the U.S. <lb/>Department of Education. The authors would like to especially <lb/>thank Kathleen, Barbara, and the teachers in the district who <lb/>gave their time to participate, without whom the study would <lb/>not have been possible. Also, thank you to the teams at <lb/>Graspable Math, DragonBox, and ASSISTments for providing <lb/>access to the technologies, as well as to Dr. Erin Ottmar for her <lb/>vision of the study. <lb/></div>

		<front>ORCID iD <lb/>Lauren E. Decker-Woodrow <lb/>https://orcid.org/0000-0001-9404-<lb/>567X <lb/></front>

			<div type="annex">Open Practices <lb/>The access to data files for this article can be found at https://osf. <lb/>io/r3nf2/ <lb/>Supplemental Material <lb/>Supplemental material for this article is available online. <lb/></div>

			<listBibl>References <lb/>Abrahamson, D., Nathan, M. J., Williams-Pierce, C., Walkington, <lb/>C., Ottmar, E. R., Soto, H., &amp; Alibali, M. W. (2020). The future <lb/>of embodied design for mathematics teaching and learning. <lb/>Frontiers in Education, 5, 147. <lb/>Alibali, M. W., Crooks, N. M., &amp; McNeil, N. M. (2018). Perceptual <lb/>support promotes strategy generation: Evidence from equation <lb/>solving. British Journal of Developmental Psychology, 36, <lb/>153-168. https://doi.org/10.1111/bjdp.12203 <lb/>Alibali, M. W., &amp; Nathan, M. J. (2012). Embodiment in mathemat-<lb/>ics teaching and learning: Evidence from learners&apos; and teach-<lb/>ers&apos; gestures. Journal of the Learning Sciences, 21(2), 247-286. <lb/>https://doi.org/10.1080/10508406.2011.611446 <lb/>Azevedo, R., &amp; Bernard, R. M. (1995, April 18-22). The effects of <lb/>computer-presented feedback on learning from computer-based <lb/>instruction: A meta-analysis [Paper presentation]. Annual <lb/>Meeting of the American Educational Research Association, <lb/>San Francisco, CA, United States. <lb/>Balingit, M., Natanson, H., &amp; Chen, Y. (2021, March 4). <lb/>As schools reopen, Asian American students are miss-<lb/>ing from classrooms. Washington Post. http://www. <lb/>washingtonpost.com/education/asian-american-students-home-<lb/>school-in-person-pandemic/2021/03/02/eb7056bc-7786-11eb-<lb/>8115-9ad5e9c02117_story.html <lb/>Bergan, J. R., Sladeczek, I. E., Schwarz, R. D., &amp; Smith, A. N. <lb/>(1991). Effects of a measurement and planning system on <lb/>kindergartners&apos; cognitive development and educational pro-<lb/>gramming. American Educational Research Journal, 28(3), <lb/>683-714. https://doi.org/10.3102/00028312028003683 <lb/>Black, P., &amp; Wiliam, D. (1998). Assessment and classroom learn-<lb/>ing. Assessment in Education: Principles, Policy and Practice, <lb/>5(1), 7-74. https://doi.org/10.1080/0969595980050102 <lb/>Boston, C. (2002). The concept of formative assessment. Practical <lb/>Assessment, Research, and Evaluation, 8(9), 1-8. <lb/>Braithwaite, D. W., Goldstone, R. L., van der Maas, H. L. J., &amp; <lb/>Landy, D. H. (2016). Non-formal mechanisms in mathematical <lb/>cognitive development: The case of arithmetic. Cognition, 149, <lb/>40-55. https://doi.org/10.1016/j.cognition.2016.01.004 <lb/>Butler, A. C., &amp; Woodward, N. R. (2018). Toward consilience <lb/>in the use of task-level feedback to promote learning. In K. <lb/>Federmeier (Ed.), Psychology of Learning and Motivation (pp. <lb/>1-38). Elsevier. https://doi.org/10.1016/bs.plm.2018.09.001 <lb/>Cayton-Hodges, G. A., Feng, G., &amp; Pan, X. (2015). Tablet-based <lb/>math assessment: What can we learn from math apps?. Journal <lb/>of Educational Technology and Society, 18(2), 3-20. <lb/>Chan, J. Y.-C., Closser, A. H., Ngo, V., Smith, H., Liu, A., &amp; <lb/>Ottmar, E. (2023). Examining shifts in conceptual knowledge, <lb/>procedural knowledge, and procedural flexibility in the context <lb/>of two game-based technologies. Journal of Computer Assisted <lb/>Learning. https://doi.org/10.1111/jcal.12798 <lb/>Chan, J. Y.-C., Lee, J.-E., Mason, C. A., Sawrey, K., &amp; Ottmar, E. <lb/>(2022). From Here to There! A dynamic algebraic notation sys-<lb/>tem improves understanding of equivalence in middle-school <lb/>students. Journal of Educational Psychology, 114(1), 56-71. <lb/>https://doi.org/10.1037/edu0000596 <lb/>Common Core State Standards for Mathematics. (2010). Common <lb/>Core State Standards for Mathematics (CCSSM). National <lb/>Governors Association Center for Best Practices and the <lb/>Council of Chief State School Officers. <lb/>Connolly, T. M., Boyle, E. A., MacArthur, E., Hainey, T., &amp; <lb/>Boyle, J. M. (2012). A systematic literature review of empiri-<lb/>cal evidence on computer games and serious games. Computers <lb/>and Education, 59(2), 661-686. https://doi.org/10.1016/J. <lb/>COMPEDU.2012.03.004 <lb/>Corbett, A. T., &amp; Anderson, J. R. (2001, March). Locus of feed-<lb/>back control in computer-based tutoring: Impact on learning <lb/></listBibl>

			<note place="headnote">Educational Technologies and Algebraic Understanding <lb/></note>

			<page>15 <lb/></page>

			<listBibl>rate, achievement and attitudes. In J. Jacko &amp; A. Sears (Eds.), <lb/>Proceedings of the SIGCHI Conference on Human Factors in <lb/>Computing Systems (pp. 245-252). Association for Computing <lb/>Machinery. <lb/>Dihoff, R. E., Brosvic, G. M., &amp; Epstein, M. L. (2003). The role <lb/>of feedback during academic testing: The delay retention effect <lb/>revisited. Psychological Record, 53(4), 533-548. <lb/>Dolonen, J. A., &amp; Kluge, A. (2015). Algebra learning through digi-<lb/>tal gaming in school. CSCL Proceedings, 252-259. <lb/>Dörfler, W. (2003). Mathematics and mathematics education: <lb/>Content and people, relation and difference. Educational <lb/>Studies in Mathematics 2003, 54(2), 147-170. https://doi. <lb/>org/10.1023/B:EDUC.0000006118.25919.07 <lb/>Dorn, E., Hancock, B., Sarakatsannis, J., &amp; Viruleg, E. (2020). <lb/>COVID-19 and student learning in the United States: The hurt <lb/>could last a lifetime. McKinsey and Company. http://www. <lb/>mckinsey.com/industries/public-and-social-sector/our-insights/ <lb/>covid-19-and-student-learning-in-the-united-states-the-hurt-<lb/>could-last-a-lifetime?cid=eml-web <lb/>Engage, NY. (2014). New York State Education Department. <lb/>https://www.engageny.org <lb/>Foglia, L., &amp; Wilson, R. A. (2013). Embodied cognition. Wiley <lb/>Interdisciplinary Reviews: Cognitive Science, 4(3). https://doi. <lb/>org/10.1002/wcs.1226 <lb/>Foster, A. (2008). Games and motivation to learn science: Personal <lb/>identity, applicability, relevance and meaningfulness. Journal <lb/>of Interactive Learning Research, 19(4), 597-614. <lb/>Fyfe, E. R., McNeil, N. M., Son, J. Y., &amp; Goldstone, R. L. (2014). <lb/>Concreteness fading in mathematics and science instruction: <lb/>A systematic review. Educational Psychology Review, 26(1), <lb/>9-25. https://doi.org/10.1007/s10648-014-9249-3 <lb/>Garris, R., Ahlers, R., &amp; Driskell, J. E. (2002). Games, moti-<lb/>vation, and learning: A research and practice model. <lb/>Simulation and Gaming, 33(4), 441-467. https://doi.org/10.11 <lb/>77/1046878102238607 <lb/>Gee, J. (2003). What videogames have to teach us about learning <lb/>and literacy. Computers in Entertainment, 1(1), 1-4. <lb/>Goldstone, R. L., Marghetis, T., Weitnauer, E., Ottmar, E. R., <lb/>&amp; Landy, D. (2017). Adapting perception, action, and tech-<lb/>nology for mathematical reasoning. Current Directions in <lb/>Psychological Science, 26(5), 434-441. https://doi.org/10.11 <lb/>77/0963721417704888 <lb/>Hansen, B. B., &amp; Bowers, J. (2008). Covariate balance in simple, <lb/>stratified and clustered comparative studies. Statistical Science, <lb/>23(2), 219-236. https://doi.org/10.1214/08-STS254 <lb/>Harrison, A., Smith, H., Hulse, T., &amp; Ottmar, E. R. (2020). Spacing <lb/>out! Manipulating spatial features in mathematical expressions <lb/>affects performance. Journal of Numerical Cognition, 6(2), <lb/>186-203. https://doi.org/10.5964/jnc.v6i2.243 <lb/>Heffernan, N. T., &amp; Heffernan, C. L. (2014). The ASSISTments <lb/>ecosystem: Building a platform that brings scientists and teach-<lb/>ers together for minimally invasive research on human learning <lb/>and teaching. International Journal of Artificial Intelligence in <lb/>Education, 24, 470-497. https://doi.org/10.1007/s40593-014-<lb/>0024-x <lb/>Hidi, S., &amp; Renninger, K. A. (2006). The four-phase model of inter-<lb/>est development. Educational Psychologist, 41(2), 111-127. <lb/>https://doi.org/10.1207/s15326985ep4102_4 <lb/>Holm, S. (1979). A simple sequentially rejective multiple test pro-<lb/>cedure. Scandinavian Journal of Statistics, 6, 65-70. <lb/>Hulse, T., Daigle, M., Manzo, D., Braith, L., Harrison, A., &amp; <lb/>Ottmar, E. R. (2019). From here to there! Elementary: A game-<lb/>based approach to developing number sense and early algebraic <lb/>understanding. Education Technology Research Development, <lb/>67, 423-441. <lb/>Illustrative Mathematics. (2017). Illustrative mathematics project. <lb/>https://www.illustrativemathematics.org <lb/>Jacob, M., &amp; Hochstein, S. (2008). Set recognition as a win-<lb/>dow to perceptual and cognitive processes. Perception and <lb/>Psychophysics, 70(7), 1165-1184. https://doi.org/10.3758/ <lb/>PP.70.7.1165 <lb/>Kahoot! (2019, May 9). Kahoot! and DragonBox join forces to cre-<lb/>ate an awesome math learning experience for all [Press release]. <lb/>https://kahoot.com/press/2019/05/09/kahoot-dragonbox-join-<lb/>forces-create-awesome-math-learning-experience-for-all/ <lb/>Kalloo, V., &amp; Mohan, P. (2015). A technique for mapping mathe-<lb/>matics content to game design. International Journal of Serious <lb/>Games, 2(4). https://doi.org/10.17083/ijsg.v2i4.95 <lb/>Ke, F. (2008). A case study of computer gaming for math: Engaged <lb/>learning from gameplay? Computers and Education, 51(4), <lb/>1609-1620. https://doi.org/10.1016/j.compedu.2008.03.003 <lb/>Kellman, P. J., Massey, C. M., &amp; Son, J. Y. (2010). Perceptual <lb/>learning modules in mathematics: Enhancing students&apos; pat-<lb/>tern recognition, structure extraction, and fluency. Topics in <lb/>Cognitive Science, 2(2), 285-305. https://doi.org/10.1111/ <lb/>j.1756-8765.2009.01053.x <lb/>Kelly, K., Heffernan, N., Heffernan, C., Goldman, S., Pellegrino, <lb/>J., &amp; Soffer-Goldstein, D. (2013). Estimating the effect of web-<lb/>based homework. In H. C. Lane, K. Yacef, J. Mostow, &amp; P. <lb/>Pavlik (Eds.), CEUR Workshop Proceedings (Vol. 1009, pp. <lb/>824-827). CEUR-WS. https://doi.org/10.1007/978-3-642-391 <lb/>12-5_122 <lb/>Kena, G., Musu-Gillette, L., Robinson, J., Wang, X., Rathbun, A., <lb/>Zhang, J., Wilkinson-Flicker, S., Barmer, A., &amp; Dunlop Velez, <lb/>E. (2015). The Condition of Education 2015 (NCES 2015-144). <lb/>U.S. Department of Education, National Center for Education <lb/>Statistics. Retrieved from http://nces.ed.gov/pubsearch <lb/>Kieran, C. (2006). Research on the learning and teaching of alge-<lb/>bra. In Á. Gutiérrez, &amp; P. Boero (Eds.), Handbook of Research <lb/>on the Psychology of Mathematics Education (pp. 11-49). <lb/>Sense Publisher. https://doi.org/10.1163/9789087901127 <lb/>Kirshner, D., &amp; Awtry, T. (2004). Visual salience of algebraic trans-<lb/>formations. Journal for Research in Mathematics Education, <lb/>35(4), 224-257. https://doi.org/10.2307/30034809 <lb/>Koedinger, K. R., &amp; Nathan, M. J. (2004). The real story behind <lb/>story problems: Effects of representations on quantitative rea-<lb/>soning. Journal of the Learning Sciences, 13(2), 129-164. <lb/>https://doi.org/10.1207/s15327809jls1302_1 <lb/>Kuhfeld, M., &amp; Lewis, K. (2022). Student achievement in 2021-<lb/>22: Cause for hope and continued urgency. NWEA. https:// <lb/>www.nwea.org/research/publication/student-achievement-in-<lb/>2021-22-cause-for-hope-and-continued-urgency <lb/>Landy, D., &amp; Goldstone, R. L. (2010). Proximity and prece-<lb/>dence in arithmetic. Quarterly Journal of Experimental <lb/>Psychology, 63(10), 1953-1968. https://doi.org/10.1080/17 <lb/>470211003787619 <lb/></listBibl>

			<note place="headnote">Decker-Woodrow et al. <lb/></note>

			<page>16 <lb/></page>

			<listBibl>Landy, D. H., &amp; Goldstone, R. L. (2009). How much of symbolic <lb/>manipulation is just symbol pushing? In N. A. Taatgen (Ed.), <lb/>Proceedings of the 31st Annual Conference of the Cognitive <lb/>Science Society (pp. 1318-1323). Cognitive Science Society. <lb/>Retrieved from http://csjarchive.cogsci.rpi.edu/Proceedings/2009/ <lb/>papers/253/paper253.pdf <lb/>Liu, Y.-E., Ballweber, C., O&apos;Rourke, E., Butler, E., Thummaphan, <lb/>P., &amp; Popović, Z. (2015). Large-scale educational campaigns. <lb/>ACM Transactions on Computer-Human Interaction, 22(2), <lb/>1-24. https://doi.org/10.1145/2699760 <lb/>Long, Y., &amp; Aleven, V. (2017). Educational game and intelligent <lb/>tutoring system: A classroom study and comparative design <lb/>analysis. ACM Transactions on Computer-Human Interaction, <lb/>24(3). https://doi.org/10.1145/3057889 <lb/>Long, Y., &amp; Aleven, V. (2014). Gamification of joint student/sys-<lb/>tem control over problem selection in a linear equation tutor. <lb/>In S. Trausan-Matu, K. E. Boyer, M. Crosby, &amp; K. Panourgia <lb/>(Eds.), Proceedings of the 12th International Conference on <lb/>Intelligent Tutoring Systems (pp. 378-387). Springer-Verlag. <lb/>https://doi.org/10.1007/978-3-319-07221-0 <lb/>Marquis, J. (1988). Common mistakes in algebra. In A. F. Coxford, <lb/>&amp; P. Shulte (Eds.), The Ideas of Algebra, K-12: National <lb/>Council of Teachers of Mathematics yearbook (pp. 204-205). <lb/>National Council of Teachers of Mathematics. <lb/>Mendicino, M., Razzaq, L., &amp; Heffernan, N. T. (2009). A compari-<lb/>son of traditional homework to computer-supported homework. <lb/>Journal of Research on Technology in Education, 41(3), 331-359. <lb/>Mora, A., Riera, D., Gonzalez, C., &amp; Arnedo-Moreno, J. (2015). <lb/>A literature review of gamification design frameworks. 2015 <lb/>7th International Conference on Games and Virtual Worlds for <lb/>Serious Applications (VS-Games), 1-8. IEEE. <lb/>Nathan, M. J., Ottmar, E. R., Abrahamson, D., Williams-Pierce, C., <lb/>Walkington, C., &amp; Nemirovsky, R. (2016). Embodied mathe-<lb/>matical imagination and cognition (EMIC) working group. In M. <lb/>B. Wood, E. E. Turner, M. Civil &amp; J. A. Eli (Eds.), Psychology <lb/>of Mathematics Education-North American Chapter confer-<lb/>ence. (pp. 1697-1697). The University of Arizona. <lb/>Nathan, M. J., &amp; Walkington, C. (2017). Grounded and embod-<lb/>ied mathematical cognition: Promoting mathematical insight <lb/>and proof using action and language. Cognitive Research: <lb/>Principles and Implications, 2(1). https://doi.org/10.1186/ <lb/>s41235-016-0040-5 <lb/>Nathan, M. J., Walkington, C., Boncoddo, R., Pier, E., Williams, C. <lb/>C., &amp; Alibali, M. W. (2014). Actions speak louder with words: <lb/>The roles of action and pedagogical language for grounding <lb/>mathematical proof. Learning and Instruction, 33. https://doi. <lb/>org/10.1016/j.learninstruc.2014.07.001 <lb/>National Mathematics Advisory Panel. (2008). Foundations <lb/>for Success: The Final Report of the National Mathematics <lb/>Advisory Panel. U.S. Department of Education. <lb/>Noguiera de Lima, R., &amp; Tall, D. (2008). Procedural embodiment and <lb/>magic in linear equations. Educational Studies in Mathematics, <lb/>67(1), 3-18. https://doi.org/10.1007/s10649-007-9086-0 <lb/>Ottmar, E. R., Landy, D., Goldstone, R., &amp; Weitnauer, E. (2015). <lb/>Getting From Here to There! Testing the effectiveness of an <lb/>interactive mathematics intervention embedding perceptual <lb/>learning. In D. C. Noelle, R. Dale, A. S. Warlaumont, J. Yoshimi, <lb/>T. Matlock, C. D. Jennings, &amp; P. P. Maglio (Eds.), Proceedings <lb/>of the Annual Conference of the Cognitive Science Society (pp. <lb/>1793-1798). Cognitive Science Society. <lb/>Patsenko, E. G., &amp; Altmann, E. M. (2010). How planful is routine <lb/>behavior? A selective-attention model of performance in the <lb/>Tower of Hanoi. Journal of Experimental Psychology: General, <lb/>139(1), 95-116. https://doi.org/10.1037/A0018268 <lb/>Phye, G. D., &amp; Andre, T. (1989). Delayed retention effect: <lb/>Attention, perseveration, or both? Contemporary Educational <lb/>Psychology, 14(2), 173-185. https://doi.org/10.1016/0361-<lb/>476X(89)90035-0 <lb/>Roschelle, J., Feng, M., Murphy, R. F., &amp; Mason, C. A. (2016). <lb/>Online mathematics homework increases student achieve-<lb/>ment. AERA Open, 2(4), 233285841667396. https://doi. <lb/>org/10.1177/2332858416673968 <lb/>Rotgans, J. I., &amp; Schmidt, H. G. (2011). Situational interest and aca-<lb/>demic achievement in the active-learning classroom. Learning <lb/>and Instruction, 21(1), 58-67. https://doi.org/10.1016/j.learnin-<lb/>struc.2009.11.001 <lb/>Samur, Y., &amp; Evans, M. A. (2012, April 13-17). The effects of seri-<lb/>ous games on performance and engagement: A review of the lit-<lb/>erature (2001-2011). In [Conference presentation]. American <lb/>Educational Research Association Conference, Vancouver, <lb/>British Columbia, Canada. <lb/>Shapiro, L. (2010). Embodied cognition. Routledge. https://doi. <lb/>org/10.4324/9781315180380 <lb/>Shute, V. J. (2008). Focus on formative feedback. Review <lb/>of Educational Research, 78(1), 153-189. https://doi. <lb/>org/10.3102/0034654307313795 <lb/>Siew, N. M., Geofrey, J., &amp; Lee, B. N. (2016). Students&apos; algebraic <lb/>thinking and attitudes toward algebra: The effects of game-<lb/>based learning using Dragonbox 12+ app. Research Journal of <lb/>Mathematics and Technology, 5(1), 66-79. <lb/>Speece, D. L., Molloy, D. E., &amp; Case, L. P. (2003). Responsiveness <lb/>to general education instruction as the first gate to learning dis-<lb/>abilities identification. Learning Disabilities: Research and <lb/>Practice, 18(3), 147-156. <lb/>Star, J. R., Pollack, C., Durkin, K., Rittle-Johnson, B., Lynch, K., <lb/>Newton, K., &amp; Gogolen, C. (2015). Learning from comparison <lb/>in algebra. Contemporary Educational Psychology, 40, 41-54. <lb/>https://doi.org/10.1016/j.cedpsych.2014.05.005 <lb/>Torres, R., Toups, Z. O., Wiburg, K., Chamberlin, B., Gomez, C., <lb/>&amp; Ozer, M. A. (2016). Initial design implications for early alge-<lb/>bra games. In A. Cox, &amp; Z. O. Toups (Eds), Proceedings of <lb/>the 2016 Annual Symposium on Computer-Human Interaction <lb/>in Play Companion Extended Abstracts (pp. 325-333). ACM. <lb/>https://doi.org/10.1145/2968120.2987748 <lb/>U.S. Department of Education, Institute of Education Sciences, <lb/>National Center for Education Statistics, National Assessment <lb/>of Educational Progress. (n. d.). Long-term trend reading and <lb/>mathematics assessments. https://www.nationsreportcard.gov/ <lb/>ltt/?age=9#overall-results-mathematics <lb/>Utah Math Project. (2016). The Utah middle school math project. <lb/>http://utahmiddleschoolmath.org <lb/>VanderWeele, T. J. (2015). Explanation in causal inference: <lb/>Methods for mediation and interaction. Oxford University <lb/>Press. <lb/></listBibl>

			<note place="headnote">Educational Technologies and Algebraic Understanding <lb/></note>

			<page>17 <lb/></page>

		<listBibl>What Works Clearinghouse. (2020). What Works Clearinghouse <lb/>standards handbook, Version 4.1. U.S. Department of <lb/>Education, Institute of Education Sciences, National Center for <lb/>Education Evaluation and Regional Assistance. Available at <lb/>https://ies.ed.gov/ncee/wwc/handbook <lb/>Wilson, M. (2002). Six views of embodied cognition. Psychonomic <lb/>Bulletin and Review, 9(4), 625-636. https://doi.org/10.3758/ <lb/>BF03196322 <lb/></listBibl>

		<div type="annex">Authors <lb/>LAUREN E. DECKER-WOODROW is a principal research asso-<lb/>ciate in the Education Studies practice at Westat; email: lauren-<lb/>woodrow@westat.com. She is a research methodologist with inter-<lb/>ests in teacher and classroom instructional quality and supports that <lb/>influence learning and growth for children. <lb/>CRAIG A. MASON is a professor of education and applied quan-<lb/>titative methods at the University of Maine; email: craig.mason@ <lb/>maine.edu. He is a research methodologist with broad interests in <lb/>growth modeling as a tool for identifying factors that influence <lb/>developmental trajectories in children. <lb/>JI-EUN LEE is a postdoctoral research scientist at Worcester Polytechnic <lb/>Institute; email: jlee13@wpi.edu. Her research interests focus on applying <lb/>learning analytics and educational data-mining techniques to improve <lb/>instructional design and student learning in online learning environments. <lb/>JENNY YUN-CHEN CHAN is an assistant professor of Early <lb/>Childhood Education at the Education University of Hong Kong; <lb/>email: chanjyc@eduhk.hk. Her research focuses on (a) the effects <lb/>of perceptual and contextual features in the environment on learn-<lb/>ing mathematics and (b) the influences of language and executive <lb/>function skills on mathematical thinking. <lb/>ADAM SALES is an assistant professor at Worcester Polytechnic <lb/>Institute; email: asales@wpi.edu. His research interests focus on <lb/>methods for causal inference using large, administrative data <lb/>sets, primarily with applications in learning sciences and social <lb/>sciences. <lb/>ALLISON LIU is a postdoctoral scholar at Worcester Polytechnic <lb/>Institute; email: aliu2@wpi.edu. Her research focuses on under-<lb/>standing how learner characteristics and task design features influ-<lb/>ence STEM learning across the life-span, with the goal of inform-<lb/>ing instructional practices and interventions. <lb/>SHIHFEN TU is a professor of education and applied quantitative <lb/>methods and director of the School of Learning and Teaching at <lb/>the University of Maine; email: shihfen.tu@maine.edu. She spe-<lb/>cializes in linking and analyzing large population databases to <lb/>study developmental issues in the fields of health, education, and <lb/>developmental disabilities and is also interested in concept forma-<lb/>tion associated with STEM learning and using technology to teach <lb/>mathematics. </div>


	</text>
</tei>
