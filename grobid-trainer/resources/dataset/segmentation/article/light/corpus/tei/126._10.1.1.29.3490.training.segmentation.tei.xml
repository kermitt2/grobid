<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="_0"/>
	</teiHeader>
	<text xml:lang="en">
			<titlePage>CS-TR-3692 <lb/>Sept. 1996 <lb/>Putting Visualization to Work: <lb/>ProgramFinder for Youth Placement <lb/>Jason Ellis, Anne Rose, Catherine Plaisant <lb/>Human-Computer Interaction Laboratory <lb/>Institute for Advanced Computer Studies <lb/>University of Maryland, College Park, MD 20742-3255 <lb/>http://www/cs.umd.edu/projects/hcil/ <lb/>{jellis, rose, plaisant}@cs.umd.edu <lb/>Abstract <lb/>The Human-Computer Interaction Laboratory (HCIL) and the Maryland Department of Juvenile Justice (DJJ) have <lb/>been working together to develop the ProgramFinder, a tool for choosing programs for a troubled youth from drug <lb/>rehabilitation centers to secure residential facilities. The seemingly straightforward journey of the ProgramFinder <lb/>from an existing user interface technique to a product design required the development of five different prototypes <lb/>which involved user interface design, prototype implementation, and selecting search criterion. While HCIL&apos;s effort <lb/>focused primarily on design and implementation, DJJ&apos;s attribute selection process was the most time consuming and <lb/>difficult task. We also found that a direct link to DJJ&apos;s workflow was needed in the prototypes to generate the <lb/>necessary &quot;buy-in&quot;. This paper analyzes the interaction between the efforts of HCIL and DJJ and the amount of &quot;buy-<lb/>in&quot; by DJJ staff and management. Lesson learned are presented for developers. <lb/>__________________ <lb/></titlePage>

			<page>2 <lb/></page>

			<front>Putting Visualization to Work: <lb/>ProgramFinder for Youth Placement <lb/>Jason Ellis, Anne Rose, Catherine Plaisant <lb/>Human-Computer Interaction Laboratory <lb/>University of Maryland Institute for Advanced Computer Studies <lb/>College Park, MD 20742-3255 <lb/>{jellis, rose, plaisant}@cs.umd.edu <lb/>http://www.cs.umd.edu/projects/hcil/ <lb/>ABSTRACT <lb/>The Human-Computer Interaction Laboratory (HCIL) <lb/>and the Maryland Department of Juvenile Justice (DJJ) <lb/>have been working together to develop the <lb/>ProgramFinder, a tool for choosing programs for a <lb/>troubled youth from drug rehabilitation centers to <lb/>secure residential facilities. <lb/>The seemingly <lb/>straightforward journey of the ProgramFinder from an <lb/>existing user interface technique to a product design <lb/>required the development of five different prototypes <lb/>which involved user interface design, prototype <lb/>implementation, and selecting search criterion. While <lb/>HCIL&apos;s effort focused primarily on design and <lb/>implementation, DJJ&apos;s attribute selection process was <lb/>the most time consuming and difficult task. We also <lb/>found that a direct link to DJJ&apos;s workflow was needed <lb/>in the prototypes to generate the necessary &quot;buy-in&quot;. <lb/>This paper analyzes the interaction between the efforts <lb/>of HCIL and DJJ and the amount of &quot;buy-in&quot; by DJJ <lb/>staff and management. Lesson learned are presented <lb/>for developers. <lb/>KEYWORDS: technology transfer, visualization, <lb/>dynamic query, legal systems, matching <lb/></front>

			<body>INTRODUCTION <lb/>For the past two years, the Human-Computer <lb/>Interaction Laboratory (HCIL) has been working with <lb/>the Maryland Department of Juvenile Justice (DJJ) to <lb/>redesign the user interface of their information system <lb/>which is used to process approximately 50,000 juvenile <lb/>complaints per year. The first year consisted of <lb/>performing 22 field visits, administering the <lb/>Questionnaire for User Interaction Satisfaction (QUIS) <lb/>to 332 DJJ personnel, and making short and long term <lb/>user interface recommendations (Rose, Shneiderman &amp; <lb/>Plaisant, 1995; Slaughter, Norman &amp; Shneiderman, <lb/>1995). In the second year, we continued with extensive <lb/>prototyping with an emphasis on supporting DJJ&apos;s <lb/>workflow related to youth case management. <lb/>One case management function involves placing youths <lb/>in a variety of programs that meet their individual <lb/>needs, ranging from community-based drug treatment <lb/>programs to secure residential facilities. Currently, DJJ <lb/>chooses from about 250 programs. This process <lb/>involves searching through a 4-inch manual to find the <lb/>best program. Not only is very time consuming but <lb/>there is also the potential bias of choosing the first <lb/>program found, as opposed to the one best suited to the <lb/>needs of the youth. It was immediately obvious to us <lb/>that HCIL&apos;s earlier dynamic query (DQ) research could <lb/>be applied here since they had been designed to solve <lb/>problems before. The ProgramFinder was designed to <lb/>allow DJJ to quickly and easily select the best <lb/>program(s) for a youth from among all the programs <lb/>matching the set criterion. <lb/>BUILDING ON EXISTING TECHNIQUES <lb/>One of the original dynamic query prototypes was the <lb/>HomeFinder, a tool for browsing homes for sale in an <lb/>area (Figure 1). Dynamic query (DQ) applications <lb/>support fast and easy exploration of data by allowing <lb/>users to make queries by adjusting sliders and selecting <lb/>buttons while the search results are continuously <lb/>updated in a visual display (e.g., x/y scatterplot, map, <lb/>etc.) (Williamson &amp; Shneiderman, 1992; Ahlberg &amp; <lb/>Shneiderman, 1993). Instead of the HomeFinder&apos;s map <lb/>of Washington, D.C, the ProgramFinder plots the <lb/>available programs on a map of Maryland (Figure 2). <lb/>Adjusting the placement controls updates the display <lb/>which shows a dot for each program that matches. A <lb/>click on a program provides more details and the press <lb/>of a button generates the appropriate paperwork. <lb/>This paper describes the seemingly straightforward <lb/>conversion of the ProgramFinder from a research <lb/>prototype to a real product, and analyzes the interaction <lb/>between the efforts of HCIL and DJJ and the amount of <lb/>&quot;buy-in&quot; of DJJ staff and management (i.e., how <lb/></body>

			<page>3 <lb/></page>

			<body>excited they seemed to be about the prototype). We <lb/>found that many levels of prototyping were still needed <lb/>(5) and that the choice of the search criterion was the <lb/>most time consuming (and the most conflict <lb/>generating) task. A direct link to the workflow was <lb/>also needed in the prototypes to generate the necessary <lb/>&quot;buy-in&quot;. <lb/>Figure 1. Original HomeFinder research prototype <lb/>Figure 2. Final ProgramFinder prototype <lb/>DESIGN PROCESS <lb/>The process of evolving the ProgramFinder design <lb/>from the original HomeFinder concept (Figure 1) to the <lb/>final design (Figure 2) involved five different <lb/>prototypes: <lb/>• IVEE prototype <lb/>• Initial Customization prototype <lb/>• Comparison prototype <lb/>• Testing prototype <lb/>• Final prototype <lb/>The primary effort involved in developing each <lb/>prototype consisted of customizing the user interface <lb/>design, implementing the prototype, and deciding on <lb/>the search criterion. The level of effort in each of these <lb/>categories varied significantly by prototype and so did <lb/>the amount of user &quot;buy-in&quot;. <lb/>The initial IVEE prototype was developed in a few <lb/>hours to illustrate the ProgramFinder concept to DJJ. <lb/>With DJJ&apos;s go ahead to continue, the Initial <lb/>Customization prototype was then developed. This is <lb/>when development started to focus more intently on <lb/>DJJ&apos;s workflow and as a result DJJ&apos;s &quot;buy-in&quot; <lb/>increased dramatically. DJJ also began working harder <lb/>on choosing the selection criterion. It became obvious <lb/>that staff had vastly different opinions than <lb/>management. A comparison prototype was developed <lb/>to illustrate the worker&apos;s ideas to management. After <lb/>considerable debate, management decided on a set of <lb/>criterion to use and a testing prototype was developed <lb/>so preliminary usability testing could be performed. <lb/>After the attributes were selected, the design effort <lb/>increased because DJJ started to really react to all the <lb/>details in the prototype and requested many <lb/>modifications. <lb/>The testing prototype required <lb/>increased implementation effort because there was little <lb/>working functionality in the previous prototypes. <lb/>IVEE Prototype <lb/>The first prototype (Figure 3) was built in a few hours <lb/>using the Information Visualization and Exploration <lb/>Environment (IVEE) (Ahlberg &amp; Wistrand, 1995). <lb/>IVEE automatically creates DQ interfaces for given <lb/>datasets. <lb/>The dataset used to generate the <lb/>ProgramFinder was entirely mocked up by HCIL. <lb/>The major drawback of using IVEE was that it ran on <lb/>Sun workstations and DJJ only uses PCs. <lb/>For <lb/>demonstration purposes, we resorted to using a slide <lb/>show of IVEE screens, in conjunction with a live demo <lb/>of the HomeFinder to show the smooth DQ interaction. <lb/>DJJ&apos;s initial reactions were positive and they asked us <lb/>to continue. <lb/>Initial Customization Prototype <lb/>The implementation and attribute selection efforts <lb/>increased during this phase. DJJ started to get more <lb/>involved. They provided us with more detailed <lb/>information about their placement process and <lb/>proposed a set of attributes. These attributes allowed <lb/>users to specify the &quot;best&quot; value (i.e., the ideal value <lb/>for the youth) within a range of values. This had the <lb/>advantage of allowing the selected programs to be rank <lb/>ordered. However, this required a few modifications to <lb/>the standard range slider behavior: <lb/>• Best Values: The range sliders were modified so a <lb/>click on a value underneath the slider selected that <lb/>value as the &quot;best&quot; one for that attribute and <lb/></body>

			<page>4 <lb/></page>

			<body>Figure 3. IVEE prototype <lb/>Figure 4. First Delphi prototype <lb/></body>

			<page>5 <lb/></page>

			<body>marked it with a yellow star. The matching <lb/>programs are then color-coded from bright yellow <lb/>(for best) to dark red (for worst) with respect to <lb/>how close they are to the best values. <lb/>• Current Values: Vertical black lines were also <lb/>added to the range sliders to show the actual value <lb/>of the selected program. <lb/>The second prototype was developed using Borland&apos;s <lb/>Delphi on PCs (Figure 4). This gave us the advantage <lb/>of being able to run on DJJ&apos;s machines plus IVEE did <lb/>not allow us to design a more customized interface for <lb/>DJJ. A &quot;Send Packet…&quot; button was added to <lb/>demonstrate how users could select a program and then <lb/>automatically generate the required paperwork. <lb/>DJJ&apos;s &quot;buy-in&quot; jumped dramatically when they were <lb/>shown the prototype. They were very excited to see a <lb/>DJJ document popup when the &quot;Send Packet …&quot; <lb/>button was pressed. <lb/>They immediately started <lb/>discussing how they could market it to other juvenile <lb/>justice agencies. They also started envisioning other <lb/>ways the ProgramFinder could be used. For example, <lb/>they suggested adding a referral log so the acceptance <lb/>and rejection patterns of different programs could be <lb/>monitored. Having a strict set of criteria for what a <lb/>program provides would allow DJJ to hold the <lb/>programs to that standard. DJJ was quickly moving <lb/>away from a casual exploratory effort to a more serious <lb/>product design effort. Ironically, the ProgramFinder <lb/>was not even a tool DJJ anticipated needing initially. <lb/>Now there were discussions about how it could be <lb/>used to explore programs than currently possible. By <lb/>coordinating with other MD agencies, they could <lb/>choose from over 2000 programs. We believe this shift <lb/>was primarily due to the customization effort, even <lb/>though it was relatively simple. <lb/>Comparison Prototype <lb/>Shifting toward a more serious design effort, we started <lb/>working more closely with the DJJ staff and discussing <lb/>their needs in detail. Up to this point, our discussions <lb/>had been primarily with middle and upper <lb/>management. While the users were pleased with the <lb/>general concept of the ProgramFinder, they were <lb/>concerned that the proposed attributes were too limiting <lb/>and did not correspond to how they currently did their <lb/>jobs. The comparison prototype was developed to <lb/>illustrate their proposed changes to management <lb/>(Figure 5). <lb/>The major effort involved with this prototype was <lb/>deciding on the new data attributes which were <lb/>significantly different from the attributes proposed by <lb/>management. Another difference was how to specify <lb/>the value of an attribute. Instead of specifying a range <lb/>of values, the workers wanted to rank each value on a <lb/>scale from not important to required. This required the <lb/>design of an attribute ranker widget. Some other minor <lb/>changes included reducing the number of program <lb/>types in the legend, adding more fields to the details <lb/>area, and creating a &quot;Show Referral Log&quot; button. <lb/>There were now two significantly different prototypes, <lb/>in terms of attributes, that needed to be brought to some <lb/>consensus. Management was presented with both <lb/>prototypes and the strengths and weaknesses of each <lb/>were discussed. <lb/>After a month of deliberation, <lb/>management chose the Initial Customization prototype. <lb/>The rationale was that the checklist attributes in the <lb/>Comparison prototype did not engage the user in the <lb/>selection process as much as the range sliders. There <lb/>was also the concern that users might ignore critical <lb/>areas in the lengthy checklists which would greatly <lb/>effect the level of service a youth receives. DJJ <lb/>decided that it was preferable to provide a few <lb/>attributes with broad implications and ask users to <lb/>consider all of them. The decision not to use the <lb/>worker&apos;s criterion (Comparison prototype) decreased <lb/>the their &quot;buy-in&quot; temporarily. <lb/>While the <lb/>ProgramFinder would still help the staff perform their <lb/>jobs, it had also become the vehicle by which their jobs <lb/>were being redefined. <lb/>Testing Prototype <lb/>Management&apos;s decision to use the original data <lb/>attributes spawned additional discussions about the set <lb/>of attributes to be included in the testing prototype, <lb/>which was used to perform initial usability testing. <lb/>Interestingly, after working with management&apos;s <lb/>attributes for a few hours, the workers decided they <lb/>were sufficient after all. However, they did propose a <lb/>few new attributes to include in the testing prototype. <lb/>Management requested that the color coding not be <lb/>included in the testing prototype because they felt it <lb/>would unduly bias the selection process. The concern <lb/>was that workers might just select the highest ranked <lb/>program (e.g., the one with the &quot;best&quot; color) and not <lb/>take into account other suitable programs. DJJ wanted <lb/>to avoid creating a tool that gives the &quot;perfect answer.&quot; <lb/>They wanted the ProgramFinder to narrow down the <lb/>number of programs and then require the workers to <lb/>examine each of the remaining programs in-depth to <lb/>select the best one. <lb/>HCIL&apos;s major effort in developing the testing prototype <lb/>(Figure 6) was implementation since there was still <lb/>very little working functionality in the previous <lb/></body>

			<page>6 <lb/></page>

			<body>Figure 5. Comparison prototype <lb/>Figure 6. Testing prototype <lb/></body>

			<page>7 <lb/></page>

			<body>prototypes. The slider implementation in particular <lb/>required the most time. Similar controls are available <lb/>in the public domain but none had all the functionality <lb/>DJJ needed. <lb/>Final Prototype <lb/>During testing, several changes were proposed to the <lb/>prototype. The result was the final prototype design <lb/>(Figure 2). <lb/>INFORMAL USABILITY TESTING <lb/>Preliminary usability testing was conducted on the <lb/>testing prototype. The goal was to give users hands-on <lb/>experience while HCIL gained valuable design <lb/>feedback. The testing consisted of two sessions with a <lb/>total of seven users, a limited but representative group <lb/>of users. Each session was divided into four sections: <lb/>training, testing with representative tasks, filling out a <lb/>questionnaire, and discussion. <lb/>Screen mockups <lb/>illustrating solutions to problems discovered during the <lb/>first session were presented to users in the second <lb/>session for their feedback. <lb/>Users&apos; reaction was very positive. This was not <lb/>surprising since they had been involved in the design <lb/>from the early stages. However, several usability <lb/>issues did emerge during testing that were incorporated <lb/>into the final design (Figure 2). <lb/>1-Addition of Textual Display -Users noted that <lb/>plotting the programs on a map was not very useful <lb/>since the location of a program is not normally taken <lb/>into account when placing a youth. A textual display <lb/>was added (showing a list instead of a map) as a more <lb/>effective way to review the few best matches once the <lb/>filtering was done. <lb/>2-Reinstate the &quot;Best&quot; Values -DJJ reversed its <lb/>decision about color coding with respect to &quot;best&quot; <lb/>values. Although they were initially concerned that <lb/>ranking programs might bias the selection process, after <lb/>using the system they realized the color coding could <lb/>assist workers when there is no program that matches a <lb/>youth&apos;s needs fully. In addition, assigning &quot;best&quot; <lb/>values would provide a clearer picture of what sorts of <lb/>programs are needed. <lb/>3-More Integrated Help -Users in the first session <lb/>recommended allowing selections from the help facility <lb/>which provided more detailed information about the <lb/>slider values. A sample screen illustrating how users <lb/>could select the maximum and minimum values via <lb/>Figure 7. Help facility supporting range selection <lb/>check boxes in the help facility was presented to the <lb/>users in the second session (Figure 7). The new <lb/>interface met with mild approval but the users felt the <lb/>range sliders would be more convenient to use once <lb/>they learned how to use them. <lb/>4-Attaching Notes -while the workers were using the <lb/>ProgramFinder, they found that they wanted to record <lb/>comments about their settings. A small icon above <lb/>each slider was added that when clicked would display <lb/>the portion of the placement packet related to that <lb/>particular attribute. <lb/>5-Simplifying Range Sliders -several users expressed <lb/>difficulty using the range sliders. They were frustrated <lb/>using the sliders when they knew the exact range they <lb/>wanted. One suggestion was to enhance the range <lb/>sliders to allow users to select a range by dragging their <lb/>mouse across the values shown below the slider. This <lb/>would only require one action as opposed to the two <lb/>drags required by the standard range slider. <lb/>6-Reordering Sliders -the order and categorization of <lb/>the attributes was raised as an important issue. The <lb/>decision was to present the controls by workflow and <lb/>allow users to redisplay them alphabetically. <lb/>LESSONS LEARNED <lb/>After several months of effort and five different <lb/>prototype designs, we learned several important (and <lb/>sometimes surprising) lessons that could benefit future <lb/>developers. <lb/>Search criterion selection can be difficult -We <lb/>initially anticipated that it would be a simple task, but <lb/>choosing the search attributes for this visualization <lb/>required the highest level of effort and caused the most <lb/>conflict inside of DJJ. The Comparison prototype was <lb/></body>

			<page>8 <lb/></page>

			<body>developed solely for the purpose of exploring <lb/>alternative attributes. <lb/>Customization Increases &quot;buy-in&quot; -We were <lb/>surprised how much DJJ&apos;s &quot;buy-in&quot; increased after the <lb/>Initial Customization prototype was developed. To us, <lb/>it was merely a re-implementation of the IVEE <lb/>prototype for the PCs and the customization added was <lb/>very minor (a few buttons and scanned forms) but it <lb/>had a dramatic impact on DJJ&apos;s ability to understand <lb/>how the ProgramFinder could help them and to start <lb/>planning for novel uses. <lb/>Interface design can initiate changes in work <lb/>processes -In the case of the ProgramFinder, <lb/>management decided to use a set of attributes that will <lb/>significantly change how their workers select <lb/>programs. <lb/>Presentation of similar applications stimulates early <lb/>interest -Even though it is less effective than building <lb/>a customized prototype, showing &quot;live&quot; demos of <lb/>similar systems (e.g., HomeFinder) helps focus user <lb/>thinking and bootstrap management &quot;buy-in&quot;. <lb/>Creating alternative designs helps engage users -<lb/>Illustrating functional differences through the creation <lb/>of several prototypes is a very powerful tool. Users <lb/>who initially expressed no came forward with strong <lb/>ideas once concrete choices were presented. <lb/>SUMMARY AND CONCLUSION <lb/>The level of effort to convert an existing interface <lb/>technique into a product design is significant. The <lb/>entire process of designing the ProgramFinder involved <lb/>six months of effort and five different prototypes and <lb/>there are still issues to resolve (Figure 8). <lb/>Estimated Effort Level <lb/>Design <lb/>DJJ <lb/>Attribute <lb/>selection <lb/>Management <lb/>HCIL <lb/>DJJ Buy-In <lb/>Staff <lb/>IVEE <lb/>Initial <lb/>Customization Comparison <lb/>Testing <lb/>Final <lb/>Attribute decision <lb/>(process change) <lb/>Conflict <lb/>Prototypes <lb/>Implementation <lb/>Figure 8. ProgramFinder Design Process. Line thickness <lb/>indicates the relative amount of effort and &quot;buy-in&quot; and the <lb/>length approximates the amount of time involved. <lb/>Selecting the search criterion was the most time <lb/>consuming <lb/>and <lb/>conflict <lb/>generating <lb/>task. <lb/>Demonstrating similar applications early on and adding <lb/>custom workflow hooks to the prototypes increased <lb/>&quot;buy-in&quot;. <lb/>Alternative designs were presented to <lb/>increase user involvement. This effort also served as <lb/>the catalyst for DJJ to redesign their work practice. <lb/></body>

			<div type="acknowledgement">ACKNOWLEDGMENTS <lb/>We would like to thank Walt Wirshing and Dave <lb/>Brimm from DJJ for their overall assistance. <lb/>Additional thanks are due to all the DJJ personnel who <lb/>took time out of their busy schedules to work with us. <lb/>The preparation of this report was supported by funding <lb/>from the Maryland Department of Juvenile Justice. <lb/></div>

			<listBibl>REFERENCES <lb/>Ahlberg, C., Shneiderman, B. (1993) &quot;Visual <lb/>Information Seeking: Tight coupling of dynamic query <lb/>filters with starfield displays,&quot; ACM CHI 94 <lb/>Conference Proc. (Boston, MA, April 24-28, 1994), <lb/>313-317. Also appears in Readings in Human-<lb/>Computer Interaction: Toward the Year 2000, Baeker, <lb/>R.M., Gruden, J. , Buxton, W.A.S. &amp; Greenberg, S., <lb/>Eds., Morgan Kaufmann Pubs., Inc., (1995), 450-456. <lb/>Ahlberg, C., Wistrand, E. (1995) &quot;IVEE: An <lb/>Information <lb/>Visualization <lb/>&amp; <lb/>Exploration <lb/>Environment,&quot; Proceedings of IEEE Visualization &apos;95 <lb/>(Atlanta, October 1995), 66-73. <lb/>Rose, A., Shneiderman, B., Plaisant, C. (1995) &quot;An <lb/>applied ethnographic method for redesigning user <lb/>interfaces,&quot; ACM Proc. of DIS &apos;95, Symposium on <lb/>Designing Interactive Systems: Processes, Practices, <lb/>Methods &amp; Techniques (Ann Arbor, MI, Aug 23-25, <lb/>1995), 115-122. <lb/>Slaughter, L., Norman, K., Shneiderman, B., (1995), <lb/>&quot;Assessing Users&apos; Subjective Satisfaction with the <lb/>Information System for Youth Services (ISYS),&quot; <lb/>Proceedings of Third Annual Mid-Atlantic Human <lb/>Factors Conference (Blacksburg, VA, March 26-28, <lb/>1995), 164-170. <lb/>Williamson, C., Shneiderman, B. (1992) &quot;The dynamic <lb/>HomeFinder: Evaluating dynamic queries in a real-<lb/>estate information exploration system,&quot; Proceedings <lb/>ACM SIGIR &apos;92 (Copenhagen, June 21-24, 1992), 338-<lb/>346. Also appears in Sparks of Innovation in Human-<lb/>Computer Interaction, Shneiderman, B., Ed., Ablex <lb/>(June 1993), 295-307. </listBibl>


	</text>
</tei>
