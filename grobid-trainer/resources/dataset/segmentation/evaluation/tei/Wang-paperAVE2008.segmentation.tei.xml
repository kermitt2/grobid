<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Information Synthesis for Answer Validation<lb/> Rui Wang 1 and Günter Neumann 2<lb/> 1 Saarland University<lb/> 66123 Saarbrücken, Germany<lb/> rwang@coli.uni-sb.de<lb/> 2 LT-Lab, DFKI<lb/> Stuhlsatzenhausweg 3, 66123 Saarbrücken, Germany<lb/> neumann@dfki.de<lb/> Abstract. This report is about our participation in the Answer Validation Exercise (AVE2008).<lb/> Our system casts the AVE task into a Recognizing Textual Entailment (RTE) problem and uses an<lb/> existing RTE system to validate answers. Additional information from named-entity (NE)<lb/> recognizer, question analysis component, and so on, is also considered as assistances to make the<lb/> final decision. In all, we have submitted two runs, one run for English and the other for German.<lb/> They have achieved f-measures of 0.64 and 0.61 respectively. Compared with our system last year,<lb/> which purely depends on the output of the RTE system, the extra information does show its<lb/> effectiveness.<lb/> Keywords: Answer Validation, Recognizing Textual Entailment, Information Synthesis<lb/></front>

			<body>
				
			1 Introduction and Related Work<lb/>

			Answer Validation is an important step for Question Answering (QA) systems, which aims to validate the<lb/> answers extracted from natural language texts, and select the most proper answers for the final output.<lb/>

			Using Recognizing Textual Entailment (RTE-1 – Dagan et al., 2006; RTE-2 – Bar-Haim et al., 2006) to do<lb/> answer validation has shown a great success (Peñas et al., 2007). We also developed our own RTE system and<lb/> participated in AVE2007. The RTE system proposed a new sentence representation extracted from the<lb/> dependency structure, and utilized the Subsequence Kernel method (Bunescu and Mooney, 2006) to perform<lb/> machine learning. We have achieved fairly high results on both the RTE-2 data set (Wang and Neumann, 2007a)<lb/> and the RTE-3 data set (Wang and Neumann, 2007b), especially on Information Extraction (IE) and QA pairs.<lb/> However, on the AVE data sets, we still found much space for the improvement. Therefore, based on the<lb/> system we developed last year, our motivation this year is to see whether using extra information, e.g. named-<lb/>entity (NE) recognition, question analysis, etc., can make further improvement on the final results.<lb/> This report will start with a brief introduction of our RTE system and then followed by the whole AVE<lb/> system. The results of our two submission runs will be shown in section 4, and in the end, we will summarize<lb/> our work.<lb/>

			2 The RTE System<lb/>

			The RTE system (Wang and Neumann, 2007a; Wang and Neumann, 2007b) is developed for RTE-3 Challenge<lb/> (Giampiccolo et al., 2007). The system contains a main approach with two backup strategies. The main approach<lb/> extracts parts of the dependency structures to form a new representation, named Tree Skeleton, as the feature<lb/> space and then applies Subsequence Kernels to represent TSs and perform Machine Learning. The backup<lb/> strategies will deal with the T-H pairs which cannot be solved by the main approach. One backup strategy is<lb/> called Triple Matcher, as it calculates the overlapping ratio on top of the dependency structures in a triple<lb/> representation; the other is simply a Bag-of-Words (BoW) method, which calculates the overlapping ratio of<lb/> words in T and H.<lb/>

			The main approach starts with processing H, since it is usually textually shorter than T, and the dependency<lb/> structure also simpler. Tree skeletons are extracted based on the dependency structures derived by Minipar (Lin,<lb/> 1998) for English and SMES (Neumann and Piskorski, 2002) for German. There are nouns in the lower part of<lb/> the parse tree, and they share a common parent node, which is (usually) a verb in the upper part. Since content<lb/> words usually convey most of the meaning of the sentence, we will mark the nouns as Topic Words and the verb<lb/> as the Root Node. Together with the dependency paths in between, they form a subtree of the original<lb/> dependency structure, which can be viewed as an extended version of Predicate-Argument Structure (Gildea and<lb/> Palmer, 2002). We call the subtree Tree Skeleton, the topic words Foot Nodes, and the dependency path from the<lb/> noun to the root node Spine. If there are two foot nodes, the corresponding spines will be the Left Spine and the<lb/> Right Spine.<lb/>

			On top of the tree skeleton of H, the tree skeleton of T can also be extracted. We assume that if the entailment<lb/> holds from T to H, at least, they will share the same topics. Since in practice, there are different expressions for<lb/> the same entity, we have applied some fuzzy matching techniques to correspond the topic words in T and H, like<lb/> initialism, partial matching, etc. Once we successfully identify the topic words in T, we trace up along the<lb/> dependency parse tree to find the lowest common parent node, which will be marked as the root node of the tree<lb/> skeleton of T 1 .<lb/>

			After some generalizations, we merge the two tree skeletons by 1) excluding the longest common prefixes for<lb/> left spines and 2) excluding the longest common suffixes for right spines. Finally, we will get the dissimilarity of<lb/> the two tree skeletons and we call it Spine Differences, i.e. Left Spine Difference (LSD) and Right Spine<lb/> Difference (RSD). Then, since all the remaining symbols are POS tags and (generalized) dependency relation<lb/> tags, they altogether form a Closed-Class Symbol (CCS) set. The spine difference is thus a sequence of CCSs. To<lb/> represent it, we have utilized a Subsequence Kernel and a Collocation Kernel (Wang and Neumann, 2007a).<lb/>

			We have also considered the comparison between root nodes and their adjacent dependency relations. We<lb/> have observed that some adjacent dependency relations of the root node (e.g. &lt;SUBJ&gt;or &lt;OBJ&gt;) can play<lb/> important roles in predicting the entailment relationship. For instance, the verb &quot;sell&quot; has a direction of the action<lb/> from the subject to the object. In addition, the verb &quot;sell&quot; and &quot;buy&quot; convey totally different semantics.<lb/> Therefore, we assign them two extra simple kernels named Verb Consistence (VC) and Verb Relation<lb/> Consistence (VRC). The former indicates whether two root nodes have a similar meaning, and the latter<lb/> indicates whether the relations are contradictive (e.g. &lt;SUBJ&gt; and &lt;OBJ&gt; are contradictive).<lb/>

			Finally, the main approach is assisted by two backup strategies: one is called the Triple Similarity and the<lb/> other is called the BoW Similarity. Chief requirements for the backup strategy are robustness and simplicity.<lb/> Accordingly, we construct a similarity function, which operates on two triple (dependency structure represented<lb/> in the form of &lt;head, relation, modifier&gt;) sets and determines how many triples of H are contained in T. The<lb/> core assumption here is that the higher the number of matching triple elements, the more similar both sets are,<lb/> and the more likely it is that T entails H. The function uses an approximate matching function. Different cases<lb/> (i.e. ignoring either the parent node or the child node, or the relation between nodes) might provide different<lb/> indications for the similarity of T and H. We then sum them up using different weights and divide the result by<lb/> the cardinality of H for normalization. The BoW similarity score is calculated by dividing the number of<lb/> overlapping words between T and H by the total number of words in H after a simple tokenization according to<lb/> the space between words.<lb/>

			<note place="footnote">1 The Root Node of T is not necessary to be a verb, instead, it could be a noun, a preposition, or even a dependency relation.<lb/></note>

			3<lb/> The AVE System<lb/>

				Fig. 1. 
				Our AVE system uses the RTE system (Tera – Textual Entailment Recognition for Application) as a core component.<lb/> The preprocessing module mainly adapts questions, their corresponding answers, and supporting documents into Text (T)-<lb/>Hypothesis (H) pairs, assisted by some manually designed patterns. The post-processing module (i.e. the Answer Validation<lb/> in the picture) will validate each answer and select a most proper one based on the output of the RTE system. The new<lb/> modules added are the NE Recognition and Question Analysis. Thus, we will have extra information like NEs in the answers,<lb/> Expected Answer Types (EATs), etc.<lb/>

			3.1 Preprocessing and Post-processing<lb/>

			Since the input of the AVE task is a list of questions, their corresponding answers and the documents containing<lb/> these answers, we need to adapt them into T-H pairs for the RTE system. For instance, the question is,<lb/> How many &quot;Superside&quot; world championships did Steve Webster win between 1987 and 2004?<lb/> (id=87) 2<lb/>

			The QA system gives out several candidate answers to this question, as follows,<lb/> ten (id=87_1)<lb/> 24 (id=87_2)<lb/> …

			Each answer will have one supporting document where the answer comes from, like this,<lb/> The most successful sidecar racer in Superside has been Steve Webster MBE, who has won ten<lb/> world championships between 1987 and 2004. (id=87_1)<lb/>

			The assumption here is that if the answer is relevant to the question, the document which contains the answer<lb/> should entail the statement derived by combining the question and the answer. This section will mainly focus on<lb/> the combination of the question and the answer and in the next sections the RTE system and how to deal with the<lb/> output of the system will be described.<lb/>

			In order to combine the question and the answer into a statement, we need some language patterns. Normally,<lb/> we have different types of questions, such as Who-questions asking about persons, What-questions asking about<lb/> definitions, etc. Therefore, we manually construct some language patterns for the input questions. For the<lb/> example given above (id=87), we will apply the following pattern,<lb/> Steve Webster won &lt;Answer&gt; &quot;Superside&quot; world championships between 1987 and 2004.<lb/> (id=87)<lb/>

			Consequently, we substitute the &lt;Answer&gt; by each candidate answer to form Hs – hypotheses. Since the<lb/> supporting documents are naturally the Ts – texts, the T-H pairs are built up accordingly,<lb/> Id: 87_1<lb/> Entailment: Unknown<lb/> Text: The most successful sidecar racer in Superside has been Steve Webster MBE, who has<lb/> won ten world championships between 1987 and 2004.<lb/> Hypothesis: Steve Webster won ten &quot;Superside&quot; world championships between 1987 and 2004.<lb/>

			These T-H pairs can be the input for any generic RTE systems. In practice, after applying our RTE system, if<lb/> the T-H pairs are covered by our main approach, we will directly use the answers; if not, we will use a threshold<lb/>

			<note place="footnote">2 The &quot;id&quot; comes from AVE 2008 test data, i.e. &quot;AVE2008-annotated-test-EN.xml&quot;.<lb/></note>

			to decide the answer based on the two similarity scores. Therefore, every T-H pair has a triple similarity score<lb/> and a BoW similarity score, and for some of the T-H pairs, we directly know whether the entailment holds. The<lb/> post-processing is straightforward, the &quot;YES&quot; entailment cases will be validated answers and the &quot;NO&quot;<lb/> entailment cases will be rejected answers. In addition, the selected answers (i.e. the best answers) will naturally<lb/> be the pairs covered by our main approach or (if not,) with the highest similarity scores.<lb/>

			3.2 Additional Components<lb/>

			The RTE system is used as a core component of the AVE system. Based on the error analysis of last year&apos;s<lb/> results, this year we use additional components to filter out noisy candidates. Therefore, two extra components<lb/> are added to the architecture, the NE recognizer and the question analyzer. For NE recognition, we use<lb/> StanfordNER (Finkel et al., 2005) for English and SPPC (Neumann and Piskorski, 2002) for German; and for<lb/> question analysis, we use the SMES system (Neumann and Piskorski, 2002). The detailed workflow is as follows,<lb/>

			1. Annotate NEs in H, store them in an NE list; if the answer is an NE, store the NE type as A&apos;_Type;<lb/>

			2. Analyze the question and obtain expected answer type, store it as A_Type;<lb/>

			3. Synthesize all the information, i.e. NE list, A_Type, A&apos;_Type, BoW similarity, Triple similarity, etc.<lb/> As for the example mentioned above (id=87), the additional information will be,<lb/> NE list: Steve Webster (person), 1987 (date), 2004 (date);<lb/> A_Type: Number<lb/> A&apos;_Type: Number<lb/>

			Then, heuristic rules are straightforward to be applied, e.g. checking the consistence between<lb/> A_Type and A&apos;_Type, checking whether all (or how many of) the NEs also appear in the documents,<lb/> etc. All these results together with the outputs of the RTE system will be synthesized to make the final<lb/> decision.<lb/>

			4 Results<lb/>

			We have submitted two runs for this year&apos;s AVE tasks, one for English and one for German. In the following, we<lb/> will first show the table of the results and then present an error analysis.<lb/>

				Table 1. Results of our submissions compared with last year&apos;s<lb/>

				Submission Runs<lb/> Recall Precision<lb/> F-measure<lb/> Estimated QA<lb/> Performance<lb/> QA Accuracy<lb/> 100% VALIDATED (EN)<lb/> 1 0.08<lb/> 0.14<lb/> N/A<lb/> N/A<lb/> 50%VALIDATED (EN)<lb/> 0.5<lb/> 0.08<lb/> 0.13<lb/> N/A<lb/> N/A<lb/> Perfect Selection (EN)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.56<lb/> 0.34<lb/> Best QA System (EN)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.21<lb/> 0.21<lb/> dfki07-run1 (EN)<lb/> 0.62<lb/> 0.37<lb/> 0.46<lb/> N/A<lb/> 0.16<lb/> dfki07-run2 (EN)<lb/> 0.71<lb/> 0.44<lb/> 0.55<lb/> N/A<lb/> 0.21<lb/> dfki08run1 (EN)<lb/> 0.78<lb/> 0.54<lb/> 0.64<lb/> 0.34<lb/> 0.24<lb/> 100% VALIDATED(DE)<lb/> 1 0.12<lb/> 0.21<lb/> N/A<lb/> N/A<lb/> 50% VALIDATED (DE)<lb/> 0.5<lb/> 0.12<lb/> 0.19<lb/> N/A<lb/> N/A<lb/> Perfect Selection (DE)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.77<lb/> 0.52<lb/> Best QA System (DE)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.38<lb/> 0.38<lb/> dfki08run1 (DE)<lb/> 0.71<lb/> 0.54<lb/> 0.61<lb/> 0.52<lb/> 0.43<lb/>

			In the table, we notice that both for English and German, our validation system outperforms the best QA<lb/> systems, which suggests the necessity of the validation step. Although there is a gap between the system<lb/> performance and the perfect selection, the results are quite satisfactory. If we compare this year&apos;s results with<lb/> last year&apos;s, the additional information does improve the results significantly.<lb/>

			Comparing the recall and precision, for both languages, the latter is worse. Therefore, we did some error<lb/> analysis to see whether there is still some space for improvements. An interesting example in the English data is<lb/> as follows,<lb/> Question: What is the name of the best known piece by Jeremiah Clarke? (id=0011)<lb/> Answer: a rondo (id=0011_7)<lb/> Document: The most famous piece known by that name, however, is a composition by<lb/> Jeremiah Clarke, properly a rondo for keyboard named Prince of Denmark&apos;s March.<lb/>

			Our system wrongly validated this answer, because &quot;a rondo&quot; is not the name of that music work. In fact,<lb/> what we need here is a special proper name recognizer which can differentiate whether the noun is a name for a<lb/> music work.<lb/>

			In the German data, other kinds of errors occur. For instance,<lb/> Question: Wer war Russlands Verteidigungsminister 1994? (id=0020 3 )<lb/> Answer: Pawel Gratschow (id=0020_6)<lb/> Document: Wie der russische Verteidigungsminister Pawel Gratschow am Mittwoch in Tiflis<lb/> weiter bekanntgab, will Rußland insgesamt fünf Militärstützpunkte in den Kaukasus-<lb/>Republiken Georgien, Armenien und Aserbaidschan einrichten. 1994-02-02<lb/>

			The key problem here is that the year &quot;1994&quot; in the document might not be the year when the event happened,<lb/> but the year of the report. This asks us to further synthesize the information we have, i.e. NE annotation and<lb/> dependency parsing, to make better use of them.<lb/>

			5 Conclusion and Future Work<lb/>

			To sum up, in this paper, we described our participation of AVE 2008. Based on the experience of last year&apos;s<lb/> participation, apart from the RTE core system, we add two extra components, NE recognizer and question<lb/> analyzer, to further improve the results. The strategy is quite successful according to the comparison of system<lb/> performances.<lb/>

			However, the problem has not been fully solved. Due to the noisy web data, filtering some documents in the<lb/> preprocessing step could be even more effective than working on the post-processing phase. Another direction<lb/> considered by us is to take a closer look at the different performances between different languages.<lb/>
			
			</body>
			<back>

			<listBibl>
			
			References<lb/>
			
			<bibl> <label>1.</label> Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B. and Szpektor, I. 2006. The Second PASCAL<lb/> Recognising Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on<lb/> Recognising Textual Entailment, Venice, Italy.<lb/></bibl>

			<bibl> <label>2.</label> Bunescu, R. and Mooney, R. 2006. Subsequence Kernels for Relation Extraction. In Advances in Neural Information<lb/> Processing Systems 18. MIT Press.<lb/></bibl>

			<bibl> <label>3.</label> Dagan, I., Glickman, O., and Magnini, B. 2006. The PASCAL Recognising Textual Entailment Challenge. In Quiñonero-<lb/>Candela et al., editors, MLCW 2005, LNAI Volume 3944, pages 177-190. Springer-Verlag.<lb/></bibl>

			<bibl> <label>4.</label> Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into<lb/> Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for<lb/> Computational Linguistics (ACL 2005), pp. 363-370.<lb/></bibl>

			<bibl> <label>5.</label> Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. 2007. The Third PASCAL Recognizing Textual Entailment<lb/> Challenge. In Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague, June 2007.<lb/></bibl>

			<bibl> <label>6.</label> Gildea, D. and Palmer, M. 2002. The Necessity of Parsing for Predicate Argument Recognition. In Proceedings of the<lb/> 40th Meeting of the Association for Computational Linguistics (ACL 2002):239-246, Philadelphia, PA.<lb/></bibl>

			<bibl> <label>7.</label> Lin, D. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems.<lb/></bibl>

			<bibl> <label>8.</label> Neumann, G. and Piskorski, J. 2002. A Shallow Text Processing Core Engine. Journal of Computational Intelligence,<lb/> Volume 18, Number 3, 2002, pages 451-476.<lb/></bibl>

			<bibl> <label>9.</label> Anselmo Peñas, Álvaro Rodrigo, Felisa Verdejo. 2007. Overview of the Answer Validation Exercise 2007. In the CLEF<lb/> 2007 Working Notes.<lb/></bibl>

			<bibl> <label>10.</label> Wang, R. and Neumann, G. 2007a. Recognizing Textual Entailment Using a Subsequence Kernel Method. In Proc. of<lb/> AAAI 2007.<lb/></bibl>

			<bibl> <label>11.</label> Wang, R. and Neumann, G. 2007b. Recognizing Textual Entailment Using Sentence Similarity based on Dependency<lb/> Tree Skeletons. In Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 36–41, Prague, June<lb/> 2007.<lb/></bibl>

			<bibl> <label>12.</label> Wang, R. and Neumann, G. 2007c. DFKI–LT at AVE 2007: Using Recognizing Textual Entailment for Answer<lb/> Validation. In online proceedings of CLEF 2007 Working Notes, ISBN: 2-912335-31-0, September 2007, Budapest,<lb/> Hungary.<lb/></bibl>
			
			</listBibl>

			<note place="footnote">3 This &quot;id&quot; comes from &quot;AVE2008-annotated-test-DE.xml&quot;.</note>
			
			</back>

	</text>
</tei>
