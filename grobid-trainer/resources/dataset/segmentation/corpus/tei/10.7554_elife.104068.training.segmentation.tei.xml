<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
    <!-- not sure about the annex -->
	<text xml:lang="en">
			<front>Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/>1 of 32 <lb/>Neuroscience <lb/>Determinantal Point Process Attention <lb/>Over Grid Cell Code Supports Out of <lb/>Distribution Generalization <lb/>Shanka Subhra Mondal <lb/>, Steven Frankland <lb/>, Taylor W. Webb <lb/>, Jonathan D. Cohen <lb/>Department of Electrical and Computer Engineering, Princeton University ‚Ä¢ Princeton Neuroscience Institute, <lb/>Princeton University ‚Ä¢ Department of Psychology, University of California, Los Angeles <lb/>https://en.wikipedia.org/wiki/Open_access <lb/>Copyright information <lb/>Abstract <lb/>Deep neural networks have made tremendous gains in emulating human-like intelligence, <lb/>and have been used increasingly as ways of understanding how the brain may solve the <lb/>complex computational problems on which this relies. However, these still fall short of, and <lb/>therefore fail to provide insight into how the brain supports strong forms of generalization of <lb/>which humans are capable. One such case is out-of-distribution (OOD) generalization-<lb/>successful performance on test examples that lie outside the distribution of the training set. <lb/>Here, we identify properties of processing in the brain that may contribute to this ability. We <lb/>describe a two-part algorithm that draws on specific features of neural computation to <lb/>achieve OOD generalization, and provide a proof of concept by evaluating performance on <lb/>two challenging cognitive tasks. First we draw on the fact that the mammalian brain <lb/>represents metric spaces using grid cell code (e.g., in the entorhinal cortex): abstract <lb/>representations of relational structure, organized in recurring motifs that cover the <lb/>representational space. Second, we propose an attentional mechanism that operates over the <lb/>grid cell code using Determinantal Point Process (DPP), that we call DPP attention (DPP-A) -a <lb/>transformation that ensures maximum sparseness in the coverage of that space. We show <lb/>that a loss function that combines standard task-optimized error with DPP-A can exploit the <lb/>recurring motifs in the grid cell code, and can be integrated with common architectures to <lb/>achieve strong OOD generalization performance on analogy and arithmetic tasks. This <lb/>provides both an interpretation of how the grid cell code in the mammalian brain may <lb/>contribute to generalization performance, and at the same time a potential means for <lb/>improving such capabilities in artificial neural networks. <lb/>eLife assessment <lb/>This important modeling work demonstrates out-of-distribution generalization using <lb/>a grid cell coding scheme combined with an attentional mechanism that operates <lb/>over these representations (Determinantal Point Process Attention). The simulations <lb/>provide compelling evidence that the model can improve generalization <lb/>performance for analogies, addition, and multiplication. The paper is significant in <lb/>demonstrating how neural grid codes can support human-like generalization <lb/>capabilities in analogy and arithmetic tasks, which has been a challenge for prior <lb/>models. <lb/>Reviewed Preprint <lb/>Revised by authors after peer <lb/>review. <lb/>About eLife&apos;s process <lb/>Reviewed preprint version 2 <lb/>March 21, 2024 (this version) <lb/>Reviewed preprint version 1 <lb/>August 23, 2023 <lb/>Sent for peer review <lb/>June 17, 2023 <lb/>Posted to preprint server <lb/>May 28, 2023 <lb/></front>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>2 of 32 <lb/></page>

			<body>1 Introduction <lb/>Deep neural networks now meet, or even exceed, human competency in many challenging task <lb/>domains (He et al., 2016 ; Silver et al., 2017 ; Wu et al., 2016 ; He et al., 2017 ). Their success <lb/>on these tasks, however, is generally limited to the narrow set of conditions under which they <lb/>were trained, falling short of the capacity for strong forms of generalization that is central to <lb/>human intelligence (Barrett et al., 2018 ; Lake &amp; Baroni, 2018 ; Hill et al., 2019 ; Webb et al., <lb/>2020 ), and hence fail to provide insights into how our brain supports them. One such case is out-<lb/>of-distribution (OOD) generalization where the test data lies outside the distribution of the training <lb/>data. Here, we consider two challenging cognitive problems that often require a capacity for OOD <lb/>generalization: a) analogy and b) arithmetic. What enables the human brain to successfully <lb/>generalize on these tasks, and how might we better realize that ability in deep learning systems? <lb/>To address the problem, we focus on two properties of processing in the brain that we hypothesize <lb/>are useful for OOD generalization: a) the abstract representations of relational structure, in which <lb/>relations are preserved across transformations like translation and scaling (such as observed for <lb/>grid cells in mammalian medial entorhinal cortex (Hafting et al., 2005 )); and b) an attentional <lb/>objective inspired from Determinantal Point Processes (DPPs), which are probabilistic models of <lb/>repulsion arising in quantum physics (Macchi, 1975 ), to attend to abstract representations that <lb/>have maximum variance and minimum correlation among them, over the training data. We refer <lb/>to this as DPP attention or DPP-A. The net effect of these two properties is to normalize the <lb/>representations of training and testing data in a way that preserves their relational structure, and <lb/>allows the network to learn that structure in a form that can be applied well beyond the domain <lb/>over which it was trained. <lb/>In previous work, it has been shown that such OOD generalization can be accomplished in a <lb/>neural network by providing it with a mechanism for temporal context normalization (Webb et al., <lb/>2020 ) 1 , a technique that allows neural networks to preserve the relational structure between <lb/>the inputs in a local temporal context, while abstracting over the differences between contexts. <lb/>Here, we test whether the same capabilities can be achieved using a well-established, biologically <lb/>plausible embedding scheme -grid cell code -and an adaptive form of normalization that is based <lb/>strictly on the statistics of the training data in the embedding space. We show that when deep <lb/>neural networks are presented with data that exhibits such relational structure, grid cell code <lb/>coupled with an error-minimizing/attentional objective promotes strong OOD generalization. We <lb/>unpack each of these theoretical components in turn before describing the tasks, modeling <lb/>architectures, and results. <lb/>Abstract Representations of Relational Structure. The first component of the proposed <lb/>framework relies on the idea that a key element underlying human-like OOD generalization is the <lb/>use of low-dimensional representations that emphasize the relational structure between data <lb/>points. Empirical evidence suggests that, for spatial information, this is accomplished in the brain <lb/>by encoding the organism&apos;s spatial position using a periodic code consisting of different <lb/>frequencies and phases (akin to a Fourier transform of the space). Although grid cells were <lb/>discovered for representations of space (Hafting et al., 2005 ; Sreenivasan &amp; Fiete, 2011 ; Mathis <lb/>et al., 2012 ) and used for guiding spatial behaviour (Erdem &amp; Hasselmo, 2014 ; Bush et al., <lb/>2015 ), they have since been identified in non-spatial domains, such as auditory tones (Aronov et <lb/>al., 2017 ), odor (Bao et al., 2019 ), episodic memory (Chandra et al., 2023 ), and conceptual <lb/>dimensions (Constantinescu et al., 2016 ). These findings suggest that the coding scheme used by <lb/>grid cells may serve as a general representation of metric structure that may be exploited for <lb/>reasoning about the abstract conceptual dimensions required for higher-level reasoning tasks, <lb/>such as analogy and mathematics (McNamee et al., 2022 ). Of interest here, the periodic response <lb/>function displayed by grid cells belonging to a particular frequency is invariant to translation by <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>3 of 32 <lb/></page>

			<body>its period, and increasing the scale of a higher frequency response gives a lower frequency <lb/>response and vice versa, making it invariant to scale across frequencies. This is particularly <lb/>promising for prospects of OOD generalization: downstream systems that acquire parameters over <lb/>a narrow training region may be able to successfully apply those parameters across <lb/>transformations of translation or scale, given the shared structure (which can also be learned <lb/>(Cueva &amp; Wei, 2018 ; Banino et al., 2018 ; Whittington et al., 2020 )). <lb/>DPP attention (DPP-A). The second component of our proposed framework is a novel attentional <lb/>objective that uses the statistics of the training data to sculpt the influence of grid cells on <lb/>downstream computation. Despite the use of a relational encoding metric (i.e., grid cell code), <lb/>generalization may also require identifying which aspects of this encoding that could potentially <lb/>be shared across training and test distributions. Here, we implement this by identifying, and <lb/>restricting further processing to those grid cell embeddings that exhibit the greatest variance, but <lb/>are least redundant (that is, pairwise uncorrelated) over the training data. Formally, this is <lb/>captured by maximizing the determinant of the covariance matrix of the grid cell embeddings <lb/>computed over the training data (Kulesza &amp; Taskar, 2012 ). To avoid overfitting the training data, <lb/>we attend to a subset of grid cell embeddings that maximize the volume in the representational <lb/>space, diminishing the influence of low-variance codes (irrelevant), or codes with high-similarity <lb/>to other codes (redundant), which decrease the determinant of the covariance matrix. <lb/>DPP-A is inspired by mathematical work in statistical physics using DPPs that originated for <lb/>modeling the distribution of fermions at thermal equilibrium (Macchi, 1975 ). DPPs have since <lb/>been adopted in machine learning for applications in which diversity in a subset of selected items <lb/>is desirable, such as recommender systems (Kulesza &amp; Taskar, 2012 ). Recent work in <lb/>computational cognitive science has shown DPPs naturally capture inductive biases in human <lb/>inference, such as some word-learning and reasoning tasks (e.g., one noun should only refer to <lb/>one object) while also serving as an efficient memory code (Frankland &amp; Cohen, 2020 ). In that <lb/>context, the learner is biased to find a set of possible word-meaning pairs whose representations <lb/>exhibit the greatest variance and lowest covariance on a task-relevant dataset. DPPs also provide a <lb/>formal objective for the type of orthogonal coding that has been proposed to be characteristic of <lb/>representations in mammalian hippocampus, and integral for episodic memory (McClelland et al., <lb/>1995 ). Thus, using the DPP objective to govern attention over the grid cell code, known to be <lb/>implemented in the entorhinal cortex (Hafting et al., 2005 ; Barry et al., 2007 ; Stensola et al., <lb/>2012 ; Giocomo et al., 2011 ; Brandon et al., 2011 )(one synapse upstream of the <lb/>hippocampus), aligns with the function and organization of cognitive and neural systems <lb/>underlying the capability for abstraction. <lb/>Taken together, the representational and attention mechanisms outlined above define a two-<lb/>component framework of neural computation for OOD generalization, by minimizing task-specific <lb/>error subject to: i) embeddings that encode relational structure among the data (grid cell code), <lb/>and ii) attention to those embeddings that maximize the &quot;volume&quot; of the representational space <lb/>that is covered, while minimizing redundancy (DPP-A). Below, we demonstrate proof of concept by <lb/>showing that these mechanisms allow artificial neural networks to learn representations that <lb/>support OOD generalization on two challenging cognitive tasks and therefore serve as a <lb/>reasonable starting point for examining the properties of interest in these networks. <lb/>2 Approach <lb/>Figure 1 <lb/>illustrates the general framework. Task inputs, corresponding to points in a metric <lb/>space, are represented as a set of grid cell embeddings x t=1..T , that are then passed to the inference <lb/>module R. The embedding of each input is represented by the pattern of activity of grid cells that <lb/>respond selectively to different combinations of phases and frequencies. Attention over these is a <lb/>learned gating g of the grid cells, the gated activations of which (x ‚äô g) are passed to the inference <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>4 of 32 <lb/></page>

			<body>module (R). The parameterization of g and R are determined by backpropagation of the error <lb/>signal obtained by two loss functions over the training set. Note that learning of parameter g <lb/>occurs only over the training space and is not further modified during testing (i.e. over the test <lb/>spaces). The first loss function, ùìõ DPP favors attentional gatings over the grid cells that maximize <lb/>the DPP-A objective; that is, the &quot;volume&quot; of the representational space covered by the attended <lb/>grid cells. The second loss function, ùìõ task is a standard task error term (e.g., the cross entropy of <lb/>targets y and task outputs over the training set). We describe each of these components in the <lb/>following sections. <lb/>Figure 1 <lb/>Schematic of the overall framework. Given a task (e.g., an analogy to solve), inputs <lb/>(denoted as {A, B, C, D}) are represented by the grid cell code, consisting of units <lb/>(&quot;grid cells&quot;) representing different combinations of frequencies and phases. <lb/>Grid cell embeddings (x A , x B , x C , x D ) are multiplied elementwise (represented as a Hadamard product ‚äô) by a set of <lb/>learned attention gates g, then passed to the inference module R. The attention gates g are optimized using ùìõ DPP , <lb/>which encourages attention to grid cell embeddings that maximize the volume of the representational space. The <lb/>inference module outputs a score for each candidate analogy (consisting of A, B, C and a candidate answer choice D). <lb/>The scores for all answer choices are passed through a softmax to generate an answer , which is compared against <lb/>the target y to generate the task loss ùìõ task . <lb/>2.1 Task setup <lb/>2.1.1 Analogy task <lb/>We constructed proportional analogy problems with four terms, of the form A : B :: C : D, where <lb/>the relation between A and B was the same as between C and D. Each of A, B, C, D was a point in <lb/>the integer space ‚Ñ§ 2 , with each dimension sampled from the range [0, M -1], where M denotes the <lb/>size of the training region. To form an analogy, two pairs of points (A, B) and (C, D) were chosen <lb/>such that the vectors AB and CD were equal. Each analogy problem also contained a set of 6 foil <lb/>items sampled in the range [0, M -1] 2 excluding D, such that they didn&apos;t form an analogy with A, <lb/>B, C. The task was, given A, B and C, to select D from a set of multiple choices consisting of D and <lb/>the 6 foil items. During training, the networks were exposed to sets of points sampled uniformly <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>5 of 32 <lb/></page>

			<body>over locations in the training range, and with pairs of points forming vectors of varying length. <lb/>The network was trained on 80% of all such sets of points in the training range, with 20% held out <lb/>as the validation set. <lb/>To study OOD generalization, we created two cases of test data, that tested for OOD generalization <lb/>in translation and scale. For the translation invariance case (Figure 2(a) ), the constituents of the <lb/>training analogies were translated along both dimensions by the same integer value 2 KM <lb/>(obtained by multiplying K and M, both of which are integer values) such that the test analogies <lb/>were in the range [KM, (K +1)M -1] 2 after translation. Non-overlapping test regions were <lb/>generated for K ‚àà [1, 9]. Similar to the translation OOD generalization regime of Webb et al. <lb/>(2020) , this allowed the graded evaluation of OOD generalization to a series of increasingly <lb/>remote test domains as the distance from the training region increased. For example a training <lb/>analogy A : B :: C : D after translation by KM, would be A + KM : B + KM :: C + KM : D + KM. <lb/>Figure 2 <lb/>Generation of test analogies from training analogies (region marked in blue) <lb/>by: a) translating both dimension values of A, B, C, D by the same amount; <lb/>and b) scaling both dimension values of A, B, C, D by the same amount. <lb/>Since both dimension values are transformed by the same amount, each input gets transformed along the diagonal. <lb/>For the scale invariance case (Figure 2(b) ), we scaled each constituent of the training analogies <lb/>by K so that the test analogies after scaling were in the range [0, KM -1] 2 . Thus, an analogy A : B :: <lb/>C : D after scaling by K, would be KA : KB :: KC : KD. By varying the value of K from 1 to 9, we <lb/>scaled the training analogies to occupy increasingly distant and larger regions of the test space. It <lb/>is worth noting that while humans can exhibit complex and sophisticated forms of analogical <lb/>reasoning (Holyoak, 2012 ; Webb et al., 2023 ; Lu et al., 2022 ), here we focused on a relatively <lb/>simple form, that was inspired by Rumelhart&apos;s parallelogram model of analogy (Rumelhart &amp; <lb/>Abrahamson, 1973 ; Mikolov et al., 2013 ) that has been used to explain traditional human <lb/>verbal analogies (e.g., &quot;king is to what as man is to woman?&quot;). Our model, like that one, seeks to <lb/>explain analogical reasoning in terms of the computation of simple Euclidean distances (i.e., A -B <lb/>= C -D, where A, B, C, D are vectors in 2D space). <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>6 of 32 <lb/></page>

			<body>2.1.2 Arithmetic task <lb/>We tested two types of arithmetic operations, corresponding to the translation and scaling <lb/>transformations used in the analogy tasks: elementwise addition and multiplication of two inputs <lb/>A and B, each a point in ‚Ñ§ 2 , for which C was the point corresponding to the answer (i.e., C = A + B <lb/>or C = A * B). As with the analogy task, each arithmetic problem also contained a set of 6 foil items <lb/>sampled in the range [0, M -1] 2 , excluding C. The task was to select C from a set of choices <lb/>consisting of C and the 6 foil items. Similar to the analogy task, training data was constructed from <lb/>a uniform distribution of points and vector lengths in the training range, with 20% held out as the <lb/>validation set. To study OOD generalization, we created testing data corresponding to K = 9 non-<lb/>overlapping regions, such that C ‚àà [M, 2M -1] 2 , [2M, 3M -1] 2 , ‚Ä¶[KM, (K + 1)M -1] 2 . <lb/>2.2 Architecture <lb/>2.2.1 Grid cell code <lb/>As discussed above, the grid cell code is found in the mammalian neocortex, that support <lb/>structured, low-dimensional representations of task-relevant information. For example, an <lb/>organism&apos;s location in 2D allocentric space (Hafting et al., 2005 ), the frequency of 1D auditory <lb/>stimuli (Aronov et al., 2017 ), and conceptual knowledge in two continuous dimensions (Doeller <lb/>et al., 2010 ; Constantinescu et al., 2016 ) have all been shown to be represented as unique, <lb/>similarity-preserving combinations of frequencies and phases. Here, these codes are of interest <lb/>because the relational structure in the input is preserved in the code across translation and scale. <lb/>This offers a promising metric that can be used to learn structure relevant to the processing of <lb/>analogies (Frankland et al., 2019 ) and arithmetic over a restricted range of stimulus values, and <lb/>then used to generalize such processing to stimuli outside of the domain of task training. <lb/>To derive the grid cell code for stimuli, we follow the analytic approach described by Bicanski &amp; <lb/>Burgess (2019) <lb/>3 . Specifically, the grid cell embedding for a particular stimulus location A is <lb/>given by: <lb/>where, <lb/>The spatial frequencies of grids (F) begin at a value of 0.0028 * 2œÄ. Wei et al. (2015) <lb/>have shown <lb/>that, to minimize the number of variables needed to represent an integer domain of size S, the <lb/>firing rate widths and frequencies should scale geometrically in a range ( <lb/>), closely matching <lb/>empirically observed scaling in entorhinal cortex (Stensola et al., 2012 ). We choose a scaling <lb/>factor of <lb/>to efficiently tile the space. One consequence of this efficiency is that the total number <lb/>of discrete frequencies in the entorhinal cortex is expected to be small. Empirically, it has been <lb/>estimated to be between 8-12 (Moser et al., 2015 ) 4 . Here, we choose N f = 9 (dimension of F) as <lb/>the number of frequencies. A refers to a particular location in a two dimensional space, and 100 <lb/>offsets (A of f set ) are used for each frequency to evenly cover a space of 1000√ó1000 locations using <lb/>900 grid cells. These offsets represent different phases within each frequency and since there are <lb/>100 of them, N p = 100. Hence N p √ó N f = 900, which denotes the number of grid cells. Each point <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>7 of 32 <lb/></page>

			<body>from the set of 2D points for the stimuli in a task (described in Section 2.1 ), was represented <lb/>using the firing rate of 900 grid cells which constituted the grid cell embedding for that point to <lb/>form the inputs to our model. <lb/>2 2 2 dpp a <lb/>We hypothesize that the use of a relational encoding metric (i.e., grid cell code) is extremely useful, <lb/>but not sufficient for a system to achieve strong generalization, which requires attending to <lb/>particular aspects of the encoding that can capture the same relational structure across the <lb/>training and test distributions. Toward this end, we propose an attentional objective that uses the <lb/>statistics of the training data to attend to grid cell embeddings that can induce the inference <lb/>module to achieve strong generalization. Our objective, which we describe in detail below, seeks to <lb/>identify those grid cell embeddings that exhibit the greatest variance but are least redundant <lb/>(pairwise uncorrelated over the training data). Formally, this is captured by maximizing the <lb/>determinant of the covariance matrix of the grid cell embeddings computed over the training data <lb/>(Kulesza &amp; Taskar, 2012 ). Although in machine learning, DPPs have been particularly influential <lb/>in work on recommender systems (Chen et al., 2018 ), summarization (Gong et al., 2014 ; <lb/>Perez-Beltrachini &amp; Lapata, 2021 ), neural network pruning (Mariet &amp; Sra, 2015 ), here, we <lb/>propose to use maximization of the determinant of the covariance matrix as an attentional <lb/>mechanism to limit the influence of grid cell embeddings with low-variance (which are less <lb/>relevant) or with high-similarity to other grid cell embeddings (which are redundant). <lb/>For the specific tasks that we study here, we have assumed the grid cell embeddings to be pre-<lb/>learned to represent the entire space of possible test data points, and we are simply focused on the <lb/>problem of how to determine which of these are most useful in enabling generalization for a task-<lb/>optimized network trained on a small fraction of that space (Figure 2 ). That is, we look for a <lb/>way to attend to a subset of gridcells frequencies whose embeddings capture recurring task-<lb/>relevant relational structure. We find that grid cell embeddings corresponding to the higher <lb/>spatial frequency grid cells exhibit greater variance over the training data than the lower <lb/>frequency embeddings, while at the same time the correlations among those grid cell embeddings <lb/>are lower than the correlations among the lower frequency grid cell embeddings. The determinant <lb/>of the covariance matrix of the grid cell embeddings is maximized when the variances of the grid <lb/>cell embeddings are high (they are &quot;expressive&quot;) and the correlation among the grid cell <lb/>embeddings is low (they &quot;cover the representational space&quot;). As a result, the higher frequency grid <lb/>cell embeddings more efficiently cover the representational space of the training data, allowing <lb/>them to efficiently capture the same relational structure across training and test distributions <lb/>which is required for OOD generalization. <lb/>Formally, we treat obtaining ùìõ DPP as an approximation of a determinantal point process (DPP). A <lb/>DPP ùí´ defines a probability measure on all subsets of a set of items ùí∞ = {1, 2, ‚Ä¶N}. For every u ‚äÜ <lb/>ùí∞, P (u) ‚àù det(V u ). Here V is a positive semidefinite covariance matrix and V u = [V ij ] i,j‚ààu denotes <lb/>the matrix V restricted to the entries indexed by elements of u. The maximum a posteriori (MAP) <lb/>problem argmax u det(V u ) is NP-hard (Ko et al., 1995 ). However f (u) = log(det(V u )) satisfies the <lb/>property of a submodular function, and various algorithms exist for approximately maximizing <lb/>them. One common way is to approximate this discrete optimization problem by replacing the <lb/>discrete variables with continuous variables and extending the objective function to the <lb/>continuous domain. Gillenwater et al. (2012) <lb/>proposed a continuous extension that is efficiently <lb/>computable and differentiable: <lb/>We use the following theorem from Gillenwater et al. (2012) <lb/>to construct ùìõ DPP : <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>8 of 32 <lb/></page>

			<body>Theorem 2.1. For a positive semidefinite matrix V and w ‚àà [0, 1] N : <lb/>We propose an attention mechanism that uses ùìõ DPP to attend to subsets of grid cell embeddings for <lb/>further processing. Algorithm 1 <lb/>describes the training procedure with DPP-A which consists of <lb/>two steps, using ùìõ DPP as the first step. This step maximizes the objective function: <lb/>using stochastic gradient ascent for <lb/>epochs, which is equivalent to minimizing ùìõ DPP , as <lb/>. It involves attending to grid cell embeddings that exhibit the greatest <lb/>within frequency variance but are least redundant (that is, that are least also pairwise <lb/>uncorrelated) over the training data. This is achieved by maximizing the determinant of the <lb/>covariance matrix over the within frequency grid cell embeddings of the training data, and <lb/>Equation 6 <lb/>is obtained by applying log on both sides of the Theorem 2.1, and in our case ùí∞ <lb/>refers to grid cells of a particular frequency. Here g are the attention gates corresponding to each <lb/>grid cell, and N f is the number of distinct frequencies. The matrix V captures a measure of the <lb/>covariance of the grid cell embeddings over the training region. We used the synth_kernel function <lb/>5 <lb/>to construct V, where in our case m are the variances of the grid cell embeddings S computed <lb/>over the training region M, N is the number of grid cells and w m , b are hyperparameters with <lb/>values of 1 and 0.1 respectively. The dimensionality of V is N f N p √ó N f N p (900 √ó 900). g f are the gates <lb/>of the grid cells belonging to the fth frequency, so g f = g[f N p : (f + 1)N p ], where N p is the number of <lb/>phases for each frequency. V f = V [f N p : (f + 1)N p , f N p : (f + 1)N p ] is the restriction of V to the grid <lb/>cell embeddings for fth frequency, so it captured the covariance of the grid cell embeddings <lb/>belonging to the fth frequency. œÉ is sigmoid non-linearity applied to g f , defined as <lb/>, <lb/>so that their values are between 0 and 1. diag(œÉ(g f )) converts vector œÉ(g f ) into a matrix with œÉ(g f ) <lb/>as the diagonal of the matrix and the rest entries are zero, which is multiplied with V -I, which <lb/>results in elementwise multiplication of œÉ(g f ) with the column vectors of V -I. Equation 6 <lb/>which <lb/>involved summation of the logarithm of the determinant of the gated covariance matrix of grid <lb/>cell embeddings within each frequency, over N f frequencies was used to compute the negative of <lb/>ùìõ DPP . Maximizing <lb/>gave the approximate maximum within frequency log determinant for each <lb/>frequency f ‚àà [1, N f ], which we denote for the fth frequency as <lb/>. In the second step of the <lb/>training procedure, we used the <lb/>grid cell frequency, where <lb/>. In <lb/>other words, we used the grid cell embeddings for the frequency which had the maximum within-<lb/>frequency log determinant at the end of the first step, which we find are best at capturing the <lb/>relational structure across the training and testing data, thereby promoting out-of-distribution <lb/>generalization. In this step, we trained the inference module minimizing ùìõ task over <lb/>epochs. <lb/>More details can be found in Appendix 7.10 . <lb/>2.2.3 Inference module <lb/>We implemented the inference module R in two forms, one using an LSTM (Hochreiter &amp; <lb/>Schmidhuber, 1997 ) and the other using a transformer (Vaswani et al., 2017 ) architecture. <lb/>Separate networks were trained for the analogy and arithmetic tasks using each form of inference <lb/>module. For each task, the attended grid cell embeddings of each stimuli obtained from the DPP-A <lb/>process ( <lb/>), were provided to R as its inputs, which we denote as x R for brevity. For the <lb/>arithmetic task, we also concatenated to x R a one-hot tensor of dimension 2, before feeding to R <lb/>that specified which computation to perform (addition or multiplication). As proposed by Hill et al. <lb/>(2019 ), we treated both the analogy and arithmetic tasks as scoring (i.e., multiple choice) <lb/>problems. For each analogy, the inference module was presented with multiple problems, each <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>9 of 32 <lb/></page>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>9 of 32 <lb/></page>

			<body>Algorithm 1 <lb/>Training with DPP-A <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>10 of 32 <lb/></page>

			<body>consisting of three stimuli, A, B, C, and a candidate completion from the set containing D (the <lb/>correct completion) and six foil completions. For each instance of the arithmetic task, it was <lb/>presented with two stimuli, A, B, and a candidate completion from the set containing C (the correct <lb/>completion) and six foil completions. Stimuli were presented sequentially for R constructed using <lb/>an LSTM, which consists of three gates, and computations using them defined as below: <lb/>where W ii ,W hi ,W if ,W hf ,W io ,W ho ,W ic ,W hc are weight matrices and b ii , b hi , b if , b hf , b io , b ho , b ic , <lb/>b hc are bias vectors. h[t] and c[t] are the hidden state and the cell state at time t respectively. The <lb/>hidden state for the last timestep was passed through a linear layer with a single output unit to <lb/>generate a score for the candidate completions for each problem. We used a single layered LSTM <lb/>of 512 hidden units, which corresponds to the size of the hidden state, cell state, bias vectors, and <lb/>weight matrices. The hidden and cell state of the LSTM was reinitialized at the start of each <lb/>sequence for each candidate completion. <lb/>For R constructed using a transformer, we used the standard multi-head self attention (MHSA) <lb/>mechanism followed by a multi-layered perceptron (MLP) -a feedforward neural network with <lb/>one hidden layer, with layer normalization (Ba et al., 2016 ) (LN) to transform x R at each layer of <lb/>the transformer, defined as below: <lb/>where Q, K, and V are called the query, key, and value matrices respectively, d k is the dimension of <lb/>the matrices, W Q , W K , and W V are the corresponding projection matrices, W O is the output <lb/>projection matrix which is applied to the concatenation of self attention (SA) output for each head, <lb/>and H is the number of heads. The softmax function is used to convert real valued vector inputs <lb/>into a probability distribution, defined as <lb/>. We used a transformer with 6 <lb/>layers, each of which had 8 heads, d k = 32, and MLP hidden layer dimension of 512. The stimuli <lb/>were presented together and projected into 128 dimensions to be more easily divisible by the <lb/>number of heads, followed by layer normalization. Since a transformer is naturally invariant to <lb/>the order of the stimuli, to make use of the order we added to x R a learnable positional encoding <lb/>(Kazemnejad, 2019 ), which is the linear projection of one-hot tensors denoting the position of <lb/>stimuli in the sequence. We then concatenated a learned CLS (short for &quot;classification&quot;) token <lb/>(analogous to the CLS token in Devlin et al. (2018) ) to x R , before transforming with Equation <lb/>14 . We took the transformed value ( <lb/>) corresponding to the CLS token, and passed it to a <lb/>linear layer with 1 output unit to generate a score for each candidate completion. This procedure <lb/>was repeated for each candidate completion. <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>11 of 32 <lb/></page>

			<body>The seven scores (one for the correct completion and for six foil completions) were normalized <lb/>using a softmax function, such that a higher score would correspond to a higher probability and <lb/>vice versa, and the probabilities sum to 1. The inference module was trained using the task <lb/>specific cross entropy loss (ùìõ task = cross-entropy) between the softmax-normalized scores and the <lb/>index for the correct completion (target), defined as -log(softmax(scores)[target]). It is worth <lb/>noting that the properties of equivariance hold, since the probability distribution after applying <lb/>softmax remains the same when the transformation (translation or scaling) is applied to the scores <lb/>for each of the answer choices obtained from the output of the inference module, and when the <lb/>same transformation is applied to the stimuli for the task and all the answer choices before <lb/>presenting as input to the inference module to obtain the scores. We also tried formulating the <lb/>tasks as regression problems, the details of which can be found in Appendix 7.7 . <lb/>While our model is not construed to be as specific about the implementation of the R module, we <lb/>assume that -as a standard deep learning component -it is likely to map onto neocortical <lb/>structures that interact with the entorhinal cortex and, in particular, regions of the prefrontal-<lb/>posterior parietal network widely believed to be involved in abstract relational processes (Waltz et <lb/>al., 1999 ; Christoff et al., 2001 ; Knowlton et al., 2012 ; Summerfield et al., 2020 ). In <lb/>particular, the role of the prefrontal cortex in the encoding and active maintenance of abstract <lb/>information needed for task performance (such as rules and relations) has often been modeled <lb/>using gated recurrent networks, such as LSTMs (Frank et al., 2001 ; Braver &amp; Cohen, 2000 ), and <lb/>the posterior parietal cortex has long been known to support &quot;maps&quot; that may provide an <lb/>important substrate for computing complex relations (Summerfield et al., 2020 ). <lb/>3 Related work <lb/>A body of recent computational work has shown that representations similar to grid cells can be <lb/>derived using standard analytical techniques for dimensionality reduction (Dordek et al., 2016 ; <lb/>Stachenfeld et al., 2017 ), as well as from error-driven learning paradigms (Cueva &amp; Wei, 2018 ; <lb/>Banino et al., 2018 ; Whittington et al., 2020 ; Sorscher et al., 2022 ). Previous work has also <lb/>shown that grid cells emerge in networks trained to generalize to novel location/object <lb/>combinations, and support transitive inference (Whittington et al., 2020 ). Here, we show that <lb/>grid cells enable strong OOD generalization when coupled with the appropriate attentional <lb/>mechanism. Our proposed method is thus complementary to these previous approaches for <lb/>obtaining grid cell representations from raw data. <lb/>In the field of machine learning, DPPs have been used for supervised video summarization (Gong <lb/>et al., 2014 ), diverse recommendations (Chen et al., 2018 ), selecting a subset of diverse <lb/>neurons to prune a neural network without hurting performance (Mariet &amp; Sra, 2015 ), and <lb/>diverse minibatch attention for stochastic gradient descent (Zhang et al., 2017 ). Recently, Mariet <lb/>et al. (2019) <lb/>generated approximate DPP samples by proposing an inhibitive attention <lb/>mechanism based on transformer networks as a proxy for capturing the dissimilarity between <lb/>feature vectors, and Perez-Beltrachini &amp; Lapata (2021) <lb/>used DPP-based attention with seq-to-seq <lb/>architectures for diverse and relevant multi-document summarization. To our knowledge, <lb/>however, DPPs have not previously been combined with the grid cell code that we employ here, <lb/>and have not been used to enable OOD generalization. <lb/>Various approaches have been proposed to prevent deep learning systems from overfitting, and <lb/>enable them to generalize. A commonly employed technique is weight decay (Krogh &amp; Hertz, <lb/>1992 ). Srivastava et al. (2014) <lb/>proposed dropout, a regularization technique which reduces <lb/>overfitting by randomly zeroing units from the neural network during training. Recently, Webb et <lb/>al. (2020) <lb/>proposed temporal context normalization (TCN) in which a normalization similar to <lb/>batch normalization (Ioffe &amp; Szegedy, 2015 ) was applied over the temporal dimension instead of <lb/>the batch dimension. However, unlike these previous approaches, the method reported here <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>12 of 32 <lb/></page>

			<body>achieves nearly perfect OOD generalization when operating over the appropriate representation, <lb/>as we show in the results. Our proposed method also has the virtue of being based on a well <lb/>understood, and biologically plausible, encoding scheme (grid cell code). <lb/>4 Experiments <lb/>4.1 Experimental details <lb/>For each task, the sequence of stimuli for a given problem was encoded using grid cell code (see <lb/>Section 2.2.1 ), that were then modulated by DPP-A (see Section 2.2.2 ), and passed to the <lb/>inference module R (see Section 2.2.3 ). We trained 3 networks using each type of inference <lb/>module. For networks using an LSTM in the inference module, we trained each network for <lb/>number of epochs for optimizing DPP attention <lb/>, number of epochs for optimizing task <lb/>loss <lb/>, on analogy problems, and for <lb/>, on arithmetic problems <lb/>with a batch size of 256, using the ADAM optimizer (Kingma &amp; Ba, 2014 ), and a learning rate of <lb/>1e -3 . For networks using a transformer in the inference module, we trained with a batch size of <lb/>128 on analogy with a learning rate of 5e -4 , and on arithmetic problems with a learning rate of <lb/>5e -5 . More details can be found in Appendix 7.2 . <lb/>4.2 Comparison models <lb/>To evaluate how the grid cell code coupled with DPP-A compares with other architectures and <lb/>approaches to generalization, and the extent to which each of these components contributed to the <lb/>performance of the model, we compared it with several alternative models for performing the <lb/>analogy and arithmetic tasks. First, we compared it with the temporal context normalization (TCN) <lb/>model (Webb et al., 2020 ) (see Section 3 ), but modified so as to use the grid cell code as input. <lb/>We passed the grid cell embeddings for each input through a shared feedforward encoder which <lb/>consisted of two fully connected layers with 256 units per layer. ReLU nonlinearities were used in <lb/>both the layers. The final embedding was generated with a linear layer of 256 units. TCN was <lb/>applied to these embeddings and then passed as a sequence for each candidate completion to the <lb/>inference module. This implementation of TCN involved a learned encoder on top of the grid cell <lb/>embeddings, so it is closely analogous to the original TCN. <lb/>Next, we compared our model to one that used variational dropout (Gal &amp; Ghahramani, 2016 ), <lb/>which is shown to be more effective in generalization compared to naive dropout (Srivastava et al., <lb/>2014 ). We randomly sampled a dropout mask (50% dropout), zeroing out the contribution of <lb/>some of the grid cell code in the input to the inference module. We then use that locked dropout <lb/>mask for every time step in the sequence. We also compared DPP-A to a model that had an <lb/>additional penalty (L1 regularization) proportional to the absolute sum of the attention gates g <lb/>along with the task-specific loss. We experimented with different values of Œª, which controlled the <lb/>strength of the penalty relative to the cross-entropy loss. We report accuracy values for Œª that <lb/>achieved the best performance on the validation set. Accuracy values for various Œªs are provided <lb/>in the Appendix 7.8 . Dropout and L1 regularization were chosen as a proxy for DPP-A and hence <lb/>we used the same input data for fair comparison. Finally, we compared to a model that used the <lb/>complete grid cell code, i.e. no DPP-A. <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>13 of 32 <lb/></page>

			<body>5 Results <lb/>5.1 Analogy <lb/>We first present results on the analogy task for two types of testing data, translation and scaling <lb/>using two types of inference module, LSTM and transformer. We trained 3 networks for each <lb/>method and report mean accuracy along with standard error of the mean. Figure 3 <lb/>shows the <lb/>results for the analogy task using an LSTM in the inference module. The left panel shows results <lb/>for the translation regime, and the right panel shows results for the scaling regime. Both plots <lb/>show accuracy on the training and validation sets, and on a series of 9 (increasingly distant) OOD <lb/>generalization test regions. DPP-A (shown in blue) achieves nearly perfect accuracy on all of the <lb/>test regions, considerably outperforming the other models. <lb/>Figure 3 <lb/>Results on analogy on each region for translation <lb/>and scaling using LSTM in the inference module. <lb/>For the case of translation, using all the grid cell code with no DPP-A (shown in purple) led to the <lb/>worst OOD generalization performance, overfitting on the training set. Locked dropout (denoted <lb/>by green) and L1 regularization (denoted by red) reduced overfitting and demonstrated better <lb/>OOD generalization performance than no DPP-A but still performed considerably worse than DPP-<lb/>A. For the case of scaling, locked dropout and L1 regularization performed slightly better than <lb/>TCN, achieving marginally higher test accuracy, but DPP-A still substantially outperformed all <lb/>other models, with a nearly 70% improvement in average test accuracy. <lb/>To test the generality of the grid cell code and DPP-A across network architectures, we also tested a <lb/>transformer (Vaswani et al., 2017 ) in place of the LSTM in the inference module. Previous work <lb/>has suggested that transformers are particularly useful for extracting structure in sequential data <lb/>and have been used for OOD generalization (Saxton et al., 2019 ). Figure 4 <lb/>shows the results <lb/>for the analogy task using a transformer in the inference module. With no explicit attention (no <lb/>DPP-A) over the grid cell code (shown in orange), the transformer did poorly on the analogies on <lb/>the test regions. This suggests that simply using a more sophisticated architecture with standard <lb/>forms of attention is not sufficient to enable OOD generalization based on the grid cell code. With <lb/>DPP-A (shown in blue), the OOD generalization performance of the transformer is nearly perfect <lb/>for both translation and scaling. These results also demonstrate that grid cell code coupled with <lb/>DPP-A can be exploited for OOD generalization effectively by different architectures. <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>14 of 32 <lb/></page>

			<body>Figure 4 <lb/>Results on analogy on each region for translation and <lb/>scaling using the transformer in the inference module. <lb/>5.2 Arithmetic <lb/>We next present results on the arithmetic task for two types of operations, addition and <lb/>multiplication using two types of inference module, LSTM and transformer. We trained 3 networks <lb/>for each method and report mean accuracy along with standard error of the mean. <lb/>Figure 5 <lb/>shows the results for arithmetic problems using an LSTM in the inference module. The <lb/>left panel shows results for addition problems, and the right panel shows results for multiplication <lb/>problems. DPP-A achieves higher accuracy for addition than multiplication on the test regions. <lb/>However, in both cases DPP-A significantly outperforms the other models, achieving nearly perfect <lb/>OOD generalization for addition, and 65% accuracy for multiplication, compared with under 20% <lb/>accuracy for all the other models. We found that grid cell embeddings obtained after the first step <lb/>in Algorithm 1 <lb/>aren&apos;t able to fully preserve the relational structure for multiplication problems <lb/>on the test regions (more details in Appendix 7.3 ), but still it affords superior capacity for OOD <lb/>generalization than any of the other models. Thus, while it does not match the generalizability of a <lb/>genuine algorithmic (i.e., symbolic) arithmetic function, it may be sufficient for some tasks (e.g., <lb/>approximate multiplication ability in young children (Qu et al., 2021 )). <lb/>Figure 5 <lb/>Results on arithmetic on each region using LSTM in the inference module. <lb/>Figure 6 <lb/>shows the results for arithmetic problems using a transformer in the inference <lb/>module. With no DPP-A over the grid cell code the transformer did poorly on addition and <lb/>multiplication on the test regions, achieving around 20-30% accuracy. With DPP-A, the OOD <lb/>generalization performance of the transformer shows a pattern similar to that for LSTM: it is <lb/>nearly perfect for addition and, though not as good on multiplication, nevertheless shows <lb/>approximately 40% better performance than the transformer multiplication. <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>15 of 32 <lb/></page>

			<body>Figure 6 <lb/>Results on arithmetic on each region using the transformer in the inference module. <lb/>5.3 Ablation study <lb/>To determine the individual importance of the grid cell code and the DPP-A attention objective, we <lb/>carried out several ablation studies. First, to evaluate the importance of grid cell code, we <lb/>analyzed the effect of DPP-A with other embeddings, using either one-hot or smoothed one-hot <lb/>embeddings (similar to place cell coding) with standard deviations of 1, 10, and 100, each passed <lb/>through a learned feedforward encoder, which consisted of two fully connected layers with 1024 <lb/>units per layer, and ReLU nonlinearities. The final embedding was generated with a fully <lb/>connected layer with 1024 units and sigmoid nonlinearity. Since these embeddings don&apos;t have a <lb/>frequency component, the training procedure with DPP-A consisted of only one step: minimizing <lb/>the loss function <lb/>. We tried different values of Œª (0.001, 0.01, 0.1, 1, 10, 100, 1000, <lb/>10000). For each type of embedding (one-hots and smoothed one-hots with each value of standard <lb/>deviation), we trained 3 networks and report for the model that achieved best performance on the <lb/>validation set. Note that, given the much higher dimensionality and therefore memory demands of <lb/>embeddings based on one-hots and smoothed one-hots, we had to limit the evaluation to a subset <lb/>of the total space, resulting in evaluation on only two test regions (i.e., K ‚àà [1, 3]). <lb/>Figure 7 <lb/>shows the results for the analogy task (results for the arithmetic task are in Appendix <lb/>7.6 , Figure 11 ) using an LSTM in the inference module. The average accuracy on the test <lb/>regions for translation and scaling using smoothed one-hots passed through an encoder (shown in <lb/>green) is nearly 30% better than simple one-hot embeddings passed through an encoder (shown in <lb/>orange), but both still achieve significantly lower test accuracy than grid cell code which support <lb/>perfect OOD generalization. <lb/>Figure 7 <lb/>Results on analogy on each region using DPP-A, an LSTM in the inference <lb/>module, and different embeddings (grid cell code, one-hots, and smoothed one-<lb/>hots passed through a learned encoder) for translation (left) and scaling (right). <lb/>Each point is mean accuracy over three networks, and bars show standard error of the mean. <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>16 of 32 <lb/></page>

			<body>With respect to the importance of the DPP-A, we note that the simulations reported earlier show <lb/>that replacing the DPP-A mechanism with either other forms of regularization (dropout and L1 <lb/>regularization; see Section 4.2 ) or a transformer (Figure 4 <lb/>in Section 5.1 <lb/>for analogy and <lb/>Figure 6 <lb/>in Section 5.2 <lb/>for arithmetic tasks) failed to achieve the same level of OOD <lb/>generalization as the network that used DPP-A. The results using a transformer are particularly <lb/>instructive, as that incorporates a powerful mechanism for learned attention, but, even when <lb/>provided with grid cell embeddings, failed to produce results comparable to DPP-A, suggesting that <lb/>the latter provides a simple but powerful form of attention objective, at least when used in <lb/>conjunction with grid cell embeddings. <lb/>Finally, for completeness, we also carried out a set of simulations that examined the performance <lb/>of networks with various embeddings (grid cell code, and one-hots or smoothed one-hots with or <lb/>without a learned feedforward encoder), but no attention or regularization (i.e., neither DPP-A, <lb/>transformer, nor TCN, L1 Regularization, or Dropout). Figure 8 <lb/>shows the results for the <lb/>different embeddings on the analogy task (results for the arithmetic task are in Appendix 7.6 , <lb/>Figure 12 ). For translation (left), the average accuracy over the test regions using grid cell code <lb/>(shown in blue) is nearly 25% more compared to other embeddings, which all yield performance <lb/>near chance (‚àº 15%). For scaling (right), although other embeddings achieve higher performance <lb/>than chance (except smoothed one-hots), they still achieve lower test accuracy than grid cell code. <lb/>More ablation studies can be found in Appendix 7.4 , 7.9 , and Figure 13 . <lb/>Figure 8 <lb/>Results on analogy on each region using different embeddings (grid cell <lb/>code, and one-hots or smoothed one-hots with and without an encoder) <lb/>and an LSTM in the inference module, but without DPP-A, TCN, L1 <lb/>Regularization, or Dropout for translation (left) and scaling (right). <lb/>6 Discussion and future directions <lb/>We have identified how particular properties of processing observed in the brain can be used to <lb/>achieve strong OOD generalization, and introduced a two-component algorithm to promote OOD <lb/>generalization in deep neural networks. The first component is a structured representation of the <lb/>training data, modeled closely on known properties of grid cells -a population of cells that <lb/>collectively represent abstract position using a periodic code. However, despite their intrinsic <lb/>structure, we find that grid cell code and standard error-driven learning alone are insufficient to <lb/>promote OOD generalization, and standard approaches for preventing overfitting offer only <lb/>modest gains. This is addressed by the second component, using DPP-A to implement attentional <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>17 of 32 <lb/></page>

			<body>regularization over the grid cell code. DPP-A allows only a relevant and diverse subset of cells to <lb/>influence downstream computation in the inference module using the statistics of the training <lb/>data. For proof of concept, we started with two challenging cognitive tasks (analogy and <lb/>arithmetic), and showed that the combination of grid cell code and DPP-A promotes OOD <lb/>generalization across both translation and scale when incorporated into common architectures <lb/>(LSTM and transformer). It is worth noting that the DPP-A attentional mechanism is different from <lb/>the attentional mechanism in the transformer module, and both are needed for strong OOD <lb/>generalization in the case of transformers. Use of the standard self-attention mechanism in <lb/>transformers over the inputs (i.e., A, B, C, and D for the analogy task) in place of DPP-A would lead <lb/>to weightings of grid cell embeddings over all frequencies and phases. The objective function for <lb/>the DPP-A represents an inductive bias, that selectively assigns the greatest weight to all grid cell <lb/>embeddings (i.e., for all phases) of the frequency for which the determinant of the covariance <lb/>matrix is greatest computed over the training space. The transformer inference module then <lb/>attends over the inputs with the selected grid cell embeddings based on the DPP-A objective. <lb/>The current approach has some limitations and presents interesting directions for future work. <lb/>First, we derive the grid cell code from known properties of neural systems, rather than obtaining <lb/>the code directly from real-world data. Here, we are encouraged by the body of work providing <lb/>evidence for grid cell code in the hidden layers of neural networks in a variety of task contexts <lb/>and architectures (Wei et al., 2015 ; Cueva &amp; Wei, 2018 ; Banino et al., 2018 ; Whittington et al., <lb/>2020 ). This suggests reason for optimism that DPP-A may promote strong generalization in cases <lb/>where grid cell code naturally emerge: for example, navigation tasks (Banino et al., 2018 ) and <lb/>reasoning by transitive inference (Whittington et al., 2020 ). Integrating our approach with <lb/>structured representations acquired from high-dimensional, naturalistic datasets remains a <lb/>critical next step which would have significant potential for broader future practical applications. <lb/>So too does application to more complex transformations beyond translation and scale, such as <lb/>rotation, and complex forms of representations, and analogical reasoning tasks (Holyoak, 2012 ; <lb/>Webb et al., 2023 ; Lu et al., 2022 ). Second, it is not clear how DPP-A might be implemented in <lb/>a neural network. In that regard, Bozkurt et al. (2022) <lb/>recently proposed a biologically plausible <lb/>neural network algorithm using a weighted similarity matrix approach to implement a <lb/>determinant maximization criterion, which is the core idea underlying the objective function we <lb/>use for DPP-A (Equation 6 ), suggesting that the DPP-A mechanism we describe may also be <lb/>biologically plausible. This could be tested experimentally by exposing individuals (e.g., rodents or <lb/>humans) to a task that requires consistent exposure to a subregion, and evaluating the distribution <lb/>of activity over the grid cells. Our model predicts that high frequency grid cells should increase <lb/>their firing rate more than low frequency cells, since the high frequency grid cells maximize the <lb/>determinant of the covariance matrix of the grid cell embeddings. It is also worth noting that <lb/>Frankland &amp; Cohen (2020 ) have suggested that the use of DPPs may also help explain a mutual <lb/>exclusivity bias observed in human word learning and reasoning. While this is not direct evidence <lb/>of biological plausibility, it is consistent with the idea that the human brain selects representations <lb/>for processing that maximize the volume of the representational space, which can be achieved by <lb/>maximizing the DPP-A objective function defined in Equation 6 . Third, we compared grid cell <lb/>code to only one-hots and place cell code. Future work could compare to a broader range of <lb/>potential biological coding schemes for the overall space, e.g. boundary vector cell coding (Barry et <lb/>al., 2006 ), band cell coding, or egocentric boundary cell coding (Hinman et al., 2019 ). <lb/>Finally, we focus on analogies in linear spaces which limits the generality of our approach in <lb/>nonlinear spaces. In that regard, there are at least two potential directions that could be pursued. <lb/>One is to directly encode nonlinear structures (such as trees, rings, etc.) with grid cells, to which <lb/>DPP-A could be applied as described in our model. The TEM model (Whittington et al., 2020 ) <lb/>suggests that grid cells in the medial entorhinal may form a basis set that captures structural <lb/>knowledge for such nonlinear spaces, such as social hierarchies and transitive inference when <lb/>formalized as a connected graph. Another would be to use eigen-decomposition of the successor <lb/>representation (Dayan, 1993 ), a learnable predictive representation of possible future states that <lb/></body>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>18 of 32 <lb/></page>

            <body>has been shown by Stachenfeld et al. (2017) <lb/>to provide an abstract structured representation of <lb/>a space that is analogous to the grid cell code. This general-purpose mechanism could be applied to <lb/>represent analogies in nonlinear spaces (Frankland et al., 2019 ), for which there may not be a <lb/>clear factorization in terms of grid cells (i.e., distinct frequencies and multiple phases within each <lb/>frequency). Since the DPP-A mechanism, as we have described it, requires representations to be <lb/>factored in this way it would need to be modified for such purpose. Either of these approaches, if <lb/>successful, would allow our model to be extended to domains containing nonlinear forms of <lb/>structure. To the extent that different coding schemes (i.e., basis sets) are needed for different <lb/>forms of structure, the question of how these are identified and engaged for use in a given setting <lb/>is clearly an important one, that is not addressed by the current work. We imagine that this is <lb/>likely subserved by monitoring and selection mechanisms proposed to underlie the capacity for <lb/>selective attention and cognitive control (Shenhav et al., 2013 ), though the specific <lb/>computational mechanisms that underlie this function remain an important direction for future <lb/>research. <lb/></body>

            <div type="annex">7 Appendix <lb/></div>

            <div type="availability">7.1 Code and data availability <lb/>The code and the data can be downloaded from https://github.com/Shanka123/DPP-Attention_Grid-<lb/>Cell-Code . <lb/></div>

            <div type="annex">7.2 More experimental details <lb/>The size of the training region, M was 100. For the analogy task, we used 653216 training samples, <lb/>163304 validation samples, and 20000 testing samples for each of the nine regions. For the <lb/>arithmetic task, we used 80000 training samples, 20000 validation samples, and 20000 testing <lb/>samples for each of the nine regions with equal number of addition and multiplication problems. <lb/>We used the PyTorch library (Paszke et al., 2017 ) for all experiments. For each network, the <lb/>training epoch that achieved the best validation accuracy was used to report performance <lb/>accuracy for the training stimulus sets, validation sets (held out stimuli from the training range), <lb/>and OOD generalization test sets (from regions beyond the range of the training data). <lb/>7.3 Why is OOD generalization performance <lb/>worse for the multiplication task? <lb/>In an effort to understand why DPP-A achieved around 65% average test accuracy on <lb/>multiplication compared to nearly perfect accuracy for addition and analogy tasks, we analyzed <lb/>the distribution of the grid cell embeddings for the frequency which had the maximum within-<lb/>frequency determinant at the end of the first step in Algorithm 1 . More specifically for A, B and <lb/>the correct answer C, we analyzed the distribution of each grid cell for the training and the nine <lb/>test regions. Note that since the total number of grid cells was 900 and there were nine <lb/>frequencies, the dimension of the grid cell embeddings corresponding to <lb/>grid cell <lb/>frequency was 100. To quantify the similarity between training and the test distributions, we <lb/>computed cosine distance (1 -cosine similarity), and averaged it over the 100 dimensions and nine <lb/>test regions. We found that the average cosine distance is 5x greater for multiplication than <lb/>addition problem (0.0002 for addition: 0.001 for multiplication). In this respect, grid cell code does <lb/>not perfectly preserve the relational structure of the multiplication problem, which we would <lb/>expect to limit DPP-A&apos;s OOD generalization ability in that task domain. <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>19 of 32 <lb/></page>

			<div type="annex">7.4 Ablation study on choice of frequency <lb/>Figure 9 <lb/>Results on analogy on each region using LSTM in the inference <lb/>module for choosing top K frequencies with <lb/>in Algorithm 1 . <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/>7.5 Baseline using dynamic attention across frequencies <lb/>Figure 10 <lb/>Results on analogy on each region for translation and <lb/>scaling using the transformer in the inference module. <lb/>7.6 Ablation study on arithmetic task <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>20 of 32 <lb/></page>

			<div type="annex">Figure 11 <lb/>Results on arithmetic with different embeddings <lb/>(with DPP-A) using LSTM in the inference module. <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/>Figure 12 <lb/>Results on arithmetic with different embeddings (without DPP-A, TCN, <lb/>L1 Regularization, or Dropout) using LSTM in the inference module. <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/>Figure 13 <lb/>Results on arithmetic for increasing number of grid cell <lb/>frequencies N f on each region using LSTM in the inference module. <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>21 of 32 <lb/></page>

			<div type="annex">7.7 Regression formulation <lb/>We also tried formulating the analogy and arithmetic tasks as regression instead of classification <lb/>via a scoring mechanism. For DPP-A, the inference module was trained to generate the grid cell <lb/>embeddings belonging to the <lb/>frequency, which had the maximum within-frequency <lb/>determinant at the end of the first step in Algorithm 1 <lb/>for the correct completion, given as input <lb/>the <lb/>frequency grid cell embeddings for A, B, C for the analogy task and A, B for the <lb/>arithmetic task. A linear layer with 100 units and sigmoid activation was used to generate the <lb/>output of the inference module and was trained to minimize the mean squared error with the <lb/>frequency grid cell embeddings of the correct completion. We compared DPP-A with a <lb/>version that didn&apos;t use the attentional objective (no DPP-A), where the inference module was <lb/>trained to generate the grid cell embeddings for all the frequencies, but was evaluated on only the <lb/>frequency grid cell embeddings for fair comparison with the DPP-A version. Figure 14 <lb/>shows the results for the analogy task using an LSTM in the inference module. For both the <lb/>translation (left) and scaling (right) regimes, DPP-A achieves nearly zero mean squared error on <lb/>all the test regions, considerably outperforming the no DPP-A which achieves a much higher error. <lb/>Figure 15 <lb/>shows the results for arithmetic problems using an LSTM in the inference module. <lb/>For addition problems, shown on the left, DPP-A achieves nearly zero mean squared error on the <lb/>test regions. For multiplication problems, shown on the right, DPP-A achieves a lower mean <lb/>squared error on the test regions, 0.11, compared to no DPP-A which achieves around 0.17. <lb/>Figure 14 <lb/>Results for regression on analogy using LSTM in the inference module. <lb/>Results show mean squared error on each region averaged over 3 trained networks along with errorbar (standard <lb/>error of the mean). <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>22 of 32 <lb/></page>

			<div type="annex">Figure 15 <lb/>Results for regression on arithmetic on each <lb/>region using LSTM in the inference module. <lb/>Results show mean squared error on each region averaged over 3 trained networks along with errorbar (standard <lb/>error of the mean). <lb/>7.8 Effect of L1 Regularization strength (Œª) <lb/>Figure 16 <lb/>Results on analogy for L1 regularization for various Œªs for <lb/>translation and scaling using LSTM in the inference module. <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/>Figure 17 <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>23 of 32 <lb/></page>

			<div type="annex">Results on arithmetic for L1 regularization for <lb/>various Œªs using LSTM in the inference module. <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/>7.9 Ablation on DPP-A <lb/>The proposed DPP-A method (Algorithm 1 ) consists of two steps with ùìõ DPP in the first step and <lb/>ùìõ task in the second step. We considered two ablation experiments which consists of a single step. <lb/>In one case we maximized the objective function, <lb/>, over the grid cell <lb/>embeddings of all frequencies and phases (instead of summing <lb/>corresponding to the grid cell <lb/>embeddings from each frequency independently as done in the first step of Algorithm 1 ), using <lb/>stochastic gradient ascent, along with minimizing ùìõ task , which would use all the attended grid cell <lb/>embeddings (instead of using <lb/>frequency grid cell embeddings as done in the second step of <lb/>Algorithm 1 ). So total loss, <lb/>. <lb/>We refer to this ablation experiment as one step DPP-A over the complete grid cell code. The <lb/>results on the analogy for this ablation experiment is shown in Figure 18 . We see that the <lb/>accuracy on test analogies for translation for various Œªs are around 30-60%, and for scaling around <lb/>20-40%, which is much lower than the nearly perfect accuracy achieved by the proposed DPP-A <lb/>method. In the other case we maximized the objective function <lb/>, using stochastic gradient ascent, <lb/>which is same as ùìõ DPP in the first step of Algorithm 1 , along with minimizing ùìõ task , which <lb/>would use all the attended grid cell embeddings. So total loss, <lb/>. We <lb/>refer to this ablation experiment as one step DPP-A within frequencies. As shown in Figure 19 , <lb/>the accuracy on test analogies for both translation and scaling for various Œªs are in a similar range <lb/>to one step DPP-A over the complete grid cell code, and is much lower than the nearly perfect <lb/>accuracy achieved by the proposed DPP-A method. <lb/>Figure 18 <lb/>Results on analogy for one step DPP-A over the complete grid cell code for <lb/>various Œªs for translation and scaling using LSTM in the inference module. <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>24 of 32 <lb/></page>

			<div type="annex">Figure 19 <lb/>Results on analogy for one step DPP-A within frequencies for various <lb/>Œªs for translation and scaling using LSTM in the inference module. <lb/>Results show mean accuracy on each region averaged over 3 trained networks along with errorbar (standard error of <lb/>the mean). <lb/>7.10 DPP-A attentional modulation <lb/>The gates within each frequency were optimized (independent of the task inputs), according to <lb/>Equation 6 , to compute the approximate maximum within frequency log determinant of the <lb/>covariance matrix over the grid cell embeddings individually for each frequency. We then used <lb/>the grid cell embeddings belonging to the frequency that had the maximum within-frequency log <lb/>determinant for training the inference module, which always happened to be grid cells within the <lb/>top three frequencies. Figure 20 <lb/>shows the approximate maximum log determinant (on the y-<lb/>axis) for the different frequencies (on the x-axis). The intuition behind why DPP-A identified grid <lb/>cell embeddings corresponding to the highest spatial frequencies, and why this produced the best <lb/>OOD generalization (i.e., extrapolation on our analogy tasks), is because those grid cell embeddings <lb/>exhibited greater variance over the training data than the lower frequency embeddings, while at <lb/>the same time the correlations among those grid cell embeddings were lower than the correlations <lb/>among the lower frequency grid cell embeddings. The determinant of the covariance matrix of the <lb/>grid cell embeddings is maximized when the variances of the grid cell embeddings are high (they <lb/>are &quot;expressive&quot;) and the correlation among the grid cell embeddings is low (they &quot;cover the <lb/>representational space&quot;). As a result, the higher frequency grid cell embeddings more efficiently <lb/>covered the representational space of the training data, allowing them to efficiently capture the <lb/>same relational structure across training and test distributions which is required for OOD <lb/>generalization. <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>25 of 32 <lb/></page>

			<div type="annex">Figure 20 <lb/>Approximate maximum log determinant of the covariance <lb/>matrix over the grid cell embeddings (y-axis) for each <lb/>frequency (x-axis), obtained after maximizing Equation 6 . <lb/>To demonstrate that the higher grid cell frequencies more efficiently cover the representational <lb/>space, we analyzed in Figure 21 , the results after the summation of the multiplication of the <lb/>grid cell embeddings over the 2d space of 1000 √ó 1000 locations, with their corresponding gates for <lb/>3 representative frequencies (left, middle, and right panels showing results for the lowest, middle <lb/>and highest grid cell frequencies, respectively, of the 9 used in the model), obtained after <lb/>maximizing Equation 6 <lb/>for each grid cell frequency. The color code indicates the <lb/>responsiveness of the grid cells to different X and Y locations in the input space (lighter color <lb/>corresponding to greater responsiveness). Note that the dark blue area (denoting regions of least <lb/>responsiveness to any grid cell) is greatest for the lowest frequency and nearly zero for the highest <lb/>frequency, illustrating that grid cell embeddings belonging to the highest frequency more <lb/>efficiently cover the representational space which allows them to capture the same relational <lb/>structure across training and test distributions as required for OOD generalization. <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>26 of 32 <lb/></page>

			<div type="annex">Figure 21 <lb/>Each panel shows the results after summation of the multiplication <lb/>of the grid cell embeddings over the 2d space of 1000√ó1000 locations, <lb/>with their corresponding gates for a particular frequency, obtained <lb/>after maximizing Equation 6 <lb/>for each grid cell frequency. <lb/>The left, middle, and right panels show results for the lowest, middle, and highest grid cell frequencies, respectively, <lb/>of the 9 used in the model. Lighter color in each panel corresponds to greater responsiveness of grid cells at that <lb/>particular location in the 2d space. <lb/></div>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>27 of 32 <lb/></page>

			<listBibl>References <lb/>Aronov Dmitriy, Nevers Rhino, Tank David W (2017) Mapping of a non-spatial dimension by <lb/>the hippocampal-entorhinal circuit Nature 543:719-722 <lb/>Ba Jimmy Lei, Kiros Jamie Ryan, Hinton Geoffrey E (2016) Layer normalization arXiv preprint <lb/>arXiv:1607.06450 <lb/>Banino Andrea et al. (2018) Vector-based navigation using grid-like representations in <lb/>artificial agents Nature 557:429-433 <lb/>Bao Xiaojun, Gjorgieva Eva, Shanahan Laura K, Howard James D, Kahnt Thorsten, Gottfried Jay <lb/>A (2019) Grid-like neural representations support olfactory navigation of a two-<lb/>dimensional odor space Neuron 102:1066-1075 <lb/>Barrett David, Hill Felix, Santoro Adam, Morcos Ari, Lillicrap Timothy (2018) Measuring <lb/>abstract reasoning in neural networks International Conference on Machine Learning :511-520 <lb/>Barry Caswell, Lever Colin, Hayman Robin, Hartley Tom, Burton Stephen, O&apos;Keefe John, Jeffery <lb/>Kate, Burgess N (2006) The boundary vector cell model of place cell firing and spatial <lb/>memory Reviews in the Neurosciences 17:71-98 <lb/>Barry Caswell, Hayman Robin, Burgess Neil, Jeffery Kathryn J (2007) Experience-dependent <lb/>rescaling of entorhinal grids Nature Neuroscience 10:682-684 <lb/>Bicanski Andrej, Burgess Neil (2019) A computational model of visual recognition memory <lb/>via grid cells Current Biology 29:979-990 <lb/>Bozkurt Bariscan, Pehlevan Cengiz, Erdogan Alper (2022) Biologically-plausible determinant <lb/>maximization neural networks for blind separation of correlated sources Advances in <lb/>Neural Information Processing Systems :13704-13717 <lb/>Brandon Mark P, Bogaard Andrew R, Libby Christopher P, Connerney Michael A, Gupta Kishan, <lb/>Hasselmo Michael E (2011) Reduction of theta rhythm dissociates grid cell spatial <lb/>periodicity from directional tuning Science 332:595-599 <lb/>Braver Todd S, Cohen Jonathan D (2000) On the control of control: The role of dopamine in <lb/>regulating prefrontal function and working memory Attention and Performance 18:712-737 <lb/>Bush Daniel, Barry Caswell, Manson Daniel, Burgess Neil (2015) Using grid cells for <lb/>navigation Neuron 87:507-520 <lb/>Chandra Sarthak, Sharma Sugandha, Chaudhuri Rishidev, Fiete Ila (2023) High-capacity <lb/>flexible hippocampal associative and episodic memory enabled by prestructured <lb/>&quot;spatial&quot; representations BioRxiv :2023-11 <lb/>Chen Laming, Zhang Guoxin, Zhou Hanning (2018) Proceedings of the 32nd International <lb/>Conference on Neural Information Processing Systems Fast greedy map inference for <lb/>determinantal point process to improve recommendation diversity :5627-5638 <lb/></listBibl>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>28 of 32 <lb/></page>

			<listBibl>Christoff Kalina, Prabhakaran Vivek, Dorfman Jennifer, Zhao Zuo, Kroger James K, Holyoak <lb/>Keith J, Gabrieli John DE (2001) Rostrolateral prefrontal cortex involvement in relational <lb/>integration during reasoning Neuroimage 14:1136-1149 <lb/>Constantinescu Alexandra O, O&apos;Reilly Jill X, Behrens Timothy EJ (2016) Organizing conceptual <lb/>knowledge in humans with a gridlike code Science 352:1464-1468 <lb/>Cueva Christopher J, Wei Xue-Xin (2018) Emergence of grid-like representations by training <lb/>recurrent neural networks to perform spatial localization arXiv preprint arXiv:1803.07770 <lb/>Dayan Peter (1993) Improving generalization for temporal difference learning: The <lb/>successor representation Neural Computation 5:613-624 <lb/>Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina (2018) Bert: Pre-training of <lb/>deep bidirectional transformers for language understanding arXiv preprint arXiv:1810.04805 <lb/>Doeller Christian F, Barry Caswell, Burgess Neil (2010) Evidence for grid cells in a human <lb/>memory network Nature 463:657-661 <lb/>Dordek Yedidyah, Soudry Daniel, Meir Ron, Derdikman Dori (2016) Extracting grid cell <lb/>characteristics from place cell inputs using non-negative principal component analysis <lb/>Elife 5 <lb/>Erdem Uƒüur M, Hasselmo Michael E (2014) A biologically inspired hierarchical goal directed <lb/>navigation model Journal of Physiology-Paris 108:28-37 <lb/>Frank Michael J, Loughry Bryan, O&apos;Reilly Randall C (2001) Interactions between frontal cortex <lb/>and basal ganglia in working memory: a computational model Cognitive, Affective, &amp; <lb/>Behavioral Neuroscience 1:137-160 <lb/>Frankland Steven, Cohen Jonathan (2020) Determinantal point processes for memory and <lb/>structured inference CogSci <lb/>Frankland Steven, Webb Taylor W, Petrov Alexander A, O&apos;Reilly Randall C, Cohen Jonathan <lb/>(2019) Extracting and utilizing abstract, structured representations for analogy CogSci <lb/>:1766-1772 <lb/>Gal Yarin, Ghahramani Zoubin (2016) A theoretically grounded application of dropout in <lb/>recurrent neural networks Advances in Neural Information Processing Systems 29:1019-1027 <lb/>Gillenwater Jennifer, Kulesza Alex, Taskar Ben (2012) Near-optimal map inference for <lb/>determinantal point processes Advances in Neural Information Processing Systems :2744-2752 <lb/>Giocomo Lisa M, Hussaini Syed A, Zheng Fan, Kandel Eric R, Moser May-Britt, Moser Edvard I <lb/>(2011) Grid cells use hcn1 channels for spatial scaling Cell 147:1159-1170 <lb/>Gong Boqing, Chao Wei-Lun, Grauman Kristen, Sha Fei (2014) Diverse sequential subset <lb/>selection for supervised video summarization Advances in Neural Information Processing <lb/>Systems 27:2069-2077 <lb/>Hafting Torkel, Fyhn Marianne, Molden Sturla, Moser May-Britt, Moser Edvard I (2005) <lb/>Microstructure of a spatial map in the entorhinal cortex Nature 436:801-806 <lb/></listBibl>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>29 of 32 <lb/></page>

			<listBibl>He Kaiming, Zhang Xiangyu, Ren Shaoqing, Sun Jian (2016) Proceedings of the IEEE <lb/>Conference on Computer Vision and Pattern Recognition Deep residual learning for image <lb/>recognition :770-778 <lb/>He Kaiming, Gkioxari Georgia, Doll√°r Piotr, Girshick Ross (2017) Proceedings of the IEEE <lb/>International Conference on Computer Vision Mask r-cnn :2961-2969 <lb/>Hill Felix, Santoro Adam, Barrett David GT, Morcos Ari S, Lillicrap Timothy (2019) Learning to <lb/>make analogies by contrasting abstract relational structure arXiv preprint arXiv:1902.00120 <lb/>Hinman James R, Chapman G William, Hasselmo Michael E (2019) Neuronal representation of <lb/>environmental boundaries in egocentric coordinates Nature Communications 10 <lb/>Hochreiter Sepp, Schmidhuber J√ºrgen (1997) Lstm can solve hard long time lag problems <lb/>Advances in Neural Information Processing Systems :473-479 <lb/>Holyoak Keith J (2012) Analogy and relational reasoning The Oxford Handbook of Thinking and <lb/>Reasoning :234-259 <lb/>Howard Marc W, Fotedar Mrigankka S, Datey Aditya V, Hasselmo Michael E (2005) The <lb/>temporal context model in spatial navigation and relational learning: toward a common <lb/>explanation of medial temporal lobe function across domains Psychological Review 112 <lb/>Ioffe Sergey, Szegedy Christian (2015) Batch normalization: Accelerating deep network <lb/>training by reducing internal covariate shift International Conference on Machine Learning <lb/>:448-456 <lb/>Kazemnejad Amirhossein (2019) Transformer architecture: The positional encoding <lb/>Kazemnejad&apos;s blog <lb/>Kingma Diederik P, Ba Jimmy (2014) Adam: A method for stochastic optimization arXiv <lb/>preprint arXiv:1412.6980 <lb/>Knowlton Barbara J, Morrison Robert G, Hummel John E, Holyoak Keith J (2012) A <lb/>neurocomputational system for relational reasoning Trends in Cognitive Sciences 16:373-381 <lb/>Ko Chun-Wa, Lee Jon, Queyranne Maurice (1995) An exact algorithm for maximum entropy <lb/>sampling Operations Research 43:684-691 <lb/>Krogh Anders, Hertz John A (1992) A simple weight decay can improve generalization <lb/>Advances in Neural Information Processing Systems :950-957 <lb/>Kulesza Alex, Taskar Ben (2012) Determinantal point processes for machine learning arXiv <lb/>preprint arXiv:1207.6083 <lb/>Lake Brenden, Baroni Marco (2018) Generalization without systematicity: On the <lb/>compositional skills of sequence-to-sequence recurrent networks International Conference <lb/>on Machine Learning :2873-2882 <lb/>Lu Hongjing, Ichien Nicholas, Holyoak Keith J (2022) Probabilistic analogical mapping with <lb/>semantic relation networks Psychological Review <lb/>Macchi Odile (1975) The coincidence approach to stochastic point processes Advances in <lb/>Applied Probability 7:83-122 <lb/></listBibl>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>30 of 32 <lb/></page>

			<listBibl>Mariet Zelda, Sra Suvrit (2015) Diversity networks: Neural network compression using <lb/>determinantal point processes arXiv preprint arXiv:1511.05077 <lb/>Mariet Zelda, Ovadia Yaniv, Snoek Jasper (2019) Dppnet: Approximating determinantal point <lb/>processes with deep networks arXiv preprint arXiv:1901.02051 <lb/>Mathis Alexander, Herz Andreas VM, Stemmler Martin (2012) Optimal population codes for <lb/>space: grid cells outperform place cells Neural Computation 24:2280-2317 <lb/>McClelland James L, McNaughton Bruce L, O&apos;Reilly Randall C (1995) Why there are <lb/>complementary learning systems in the hippocampus and neocortex: insights from the <lb/>successes and failures of connectionist models of learning and memory Psychological <lb/>Review 102 <lb/>McNamee Daniel C, Stachenfeld Kimberly L, Botvinick Matthew M, Gershman Samuel J (2022) <lb/>Compositional sequence generation in the entorhinal-hippocampal system Entropy 24 <lb/>Mikolov Tomas, Chen Kai, Corrado Greg, Dean Jeffrey (2013) Efficient estimation of word <lb/>representations in vector space arXiv preprint arXiv:1301.3781 <lb/>Moser May-Britt, Rowland David C, Moser Edvard I (2015) Place cells, grid cells, and memory <lb/>Cold Spring Harbor Perspectives in Biology 7 <lb/>Paszke Adam, Gross Sam, Chintala Soumith, Chanan Gregory, Yang Edward, DeVito Zachary, <lb/>Lin Zeming, Desmaison Alban, Antiga Luca, Lerer Adam (2017) Automatic differentiation in <lb/>pytorch <lb/>Perez-Beltrachini Laura, Lapata Mirella (2021) Multi-document summarization with <lb/>determinantal point process attention Journal of Artificial Intelligence Research 71:371-399 <lb/>Qu Chuyan, Szkudlarek Emily, Brannon Elizabeth M (2021) Approximate multiplication in <lb/>young children prior to multiplication instruction Journal of Experimental Child Psychology <lb/>207 <lb/>Rumelhart David E, Abrahamson Adele A (1973) A model for analogical reasoning Cognitive <lb/>Psychology 5:1-28 <lb/>Saxton David, Grefenstette Edward, Hill Felix, Kohli Pushmeet (2019) Analysing mathematical <lb/>reasoning abilities of neural models arXiv preprint arXiv:1904.01557 <lb/>Shenhav Amitai, Botvinick Matthew M, Cohen Jonathan D (2013) The expected value of <lb/>control: an integrative theory of anterior cingulate cortex function Neuron 79:217-240 <lb/>Silver David et al. (2017) Mastering the game of go without human knowledge Nature <lb/>550:354-359 <lb/>Sorscher Ben, Mel Gabriel C, Ocko Samuel A, Giocomo Lisa M, Ganguli Surya (2022) A unified <lb/>theory for the computational and mechanistic origins of grid cells Neuron <lb/>Sreenivasan Sameet, Fiete Ila (2011) Grid cells generate an analog error-correcting code for <lb/>singularly precise neural computation Nature Neuroscience 14:1330-1337 <lb/></listBibl>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>31 of 32 <lb/></page>

			<listBibl>Srivastava Nitish, Hinton Geoffrey, Krizhevsky Alex, Sutskever Ilya, Salakhutdinov Ruslan (2014) <lb/>Dropout: a simple way to prevent neural networks from overfitting The Journal of Machine <lb/>Learning Research 15:1929-1958 <lb/>Stachenfeld Kimberly L, Botvinick Matthew M, Gershman Samuel J (2017) The hippocampus as <lb/>a predictive map Nature Neuroscience 20:1643-1653 <lb/>Stensola Hanne, Stensola Tor, Solstad Trygve, Fr√∏land Kristian, Moser May-Britt, Moser Edvard I <lb/>(2012) The entorhinal grid map is discretized Nature 492:72-78 <lb/>Summerfield Christopher, Luyckx Fabrice, Sheahan Hannah (2020) Structure learning and the <lb/>posterior parietal cortex Progress in Neurobiology 184 <lb/>Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N, <lb/>Kaiser ≈Åukasz, Polosukhin Illia (2017) Attention is all you need Advances in Neural Information <lb/>Processing Systems :5998-6008 <lb/>Waltz James A, Knowlton Barbara J, Holyoak Keith J, Boone Kyle B, Mishkin Fred S, de Menezes <lb/>Santos Marcia, Thomas Carmen R, Miller Bruce L (1999) A system for relational reasoning in <lb/>human prefrontal cortex Psychological Science 10:119-125 <lb/>Webb Taylor, Dulberg Zachary, Frankland Steven, Petrov Alexander, O&apos;Reilly Randall, Cohen <lb/>Jonathan (2020) Learning representations that support extrapolation International <lb/>Conference on Machine Learning :10136-10146 <lb/>Webb Taylor, Fu Shuhao, Bihl Trevor, Holyoak Keith J, Lu Hongjing (2023) Zero-shot visual <lb/>reasoning through probabilistic analogical mapping Nature Communications 14 <lb/>Wei Xue-Xin, Prentice Jason, Balasubramanian Vijay (2015) A principle of economy predicts <lb/>the functional architecture of grid cells Elife 4 <lb/>Whittington James CR, Muller Timothy H, Mark Shirley, Chen Guifen, Barry Caswell, Burgess <lb/>Neil, Behrens Timothy EJ (2020) The tolman-eichenbaum machine: Unifying space and <lb/>relational memory through generalization in the hippocampal formation Cell 183:1249-<lb/>1263 <lb/>Wu Yonghui et al. (2016) Google&apos;s neural machine translation system: Bridging the gap <lb/>between human and machine translation arXiv preprint arXiv:1609.08144 <lb/>Zhang Cheng, Kjellstrom Hedvig, Mandt Stephan (2017) Determinantal point processes for <lb/>mini-batch diversification <lb/></listBibl>

			<front>Article and author information <lb/>Shanka Subhra Mondal <lb/>Department of Electrical and Computer Engineering, Princeton University <lb/>For correspondence: smondal@princeton.edu <lb/>Steven Frankland <lb/>Princeton Neuroscience Institute, Princeton University <lb/>For correspondence: s.m.frankland@gmail.com <lb/></front>

			<note place="headnote">Shanka Subhra Mondal et al., 2024 eLife. https://doi.org/10.7554/eLife.89911.2 <lb/></note>

			<page>32 of 32 <lb/></page>

			<front>Taylor W. Webb <lb/>Department of Psychology, University of California, Los Angeles <lb/>For correspondence: taylor.w.webb@gmail.com <lb/>Jonathan D. Cohen <lb/>Princeton Neuroscience Institute, Princeton University <lb/>For correspondence: jdc@princeton.edu <lb/>Copyright <lb/>¬© 2023, Mondal et al. <lb/>This article is distributed under the terms of the Creative Commons Attribution License, <lb/>which permits unrestricted use and redistribution provided that the original author and <lb/>source are credited. <lb/>Editors <lb/>Reviewing Editor <lb/>Anna Schapiro <lb/>University of Pennsylvania, Philadelphia, United States of America <lb/>Senior Editor <lb/>Timothy Behrens <lb/>University of Oxford, Oxford, United Kingdom <lb/>Reviewer #1 (Public Review): <lb/>This paper presents a cognitive model of out-of-distribution generalisation, where the <lb/>representational basis is grid-cell codes. In particular, the authors consider the tasks of <lb/>analogies, addition, and multiplication, and the out-of-distribution tests are shifting or scaling <lb/>the input domain. The authors utilise grid cell codes, which are multi-scale as well as <lb/>translationally invariant due to their periodicity. To allow for domain adaptation, the authors <lb/>use DPP-A which is, in this context, a mechanism of adapting to input scale changes. The <lb/>authors present simulations results demonstrating this model can perform out-of-distribution <lb/>generalisation to input translations and re-scaling, whereas other models fail. <lb/>This paper makes the point it sets out to -that there are some underlying representational <lb/>bases, like grid cells, that when combined with a domain adaptation mechanism, like DPP-A, <lb/>can facilitate out-of-generalisation. I don&apos;t have any issues with the technical details. <lb/>The paper nicely demonstrates how neural codes can be transformed into a common <lb/>representational space so that analogies, and presumably other useful tasks/computations, <lb/>can be performed. <lb/>https://doi.org/10.7554/eLife.89911.2.sa0 </front>


	</text>
</tei>
