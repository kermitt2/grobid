<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Hierarchical Inter-Domain Routing Protocol <lb/>with On-Demand ToS and Policy Resolution <lb/>Cengiz Alaettino glu, A. Udaya Shankar <lb/>Institute for Advanced Computer Studies <lb/>Department of Computer Science <lb/>University of Maryland <lb/>College Park, Maryland 20742 <lb/>CS-TR-3299 <lb/>June 20, 1994 <lb/>Abstract <lb/>Traditional inter-domain routing protocols based on superdomains maintain either \strong&quot; <lb/>or \weak&quot; ToS and policy constraints for each visible superdomain. With strong constraints, <lb/>a valid path may not be found even though one exists. With weak constraints, an invalid <lb/>domain-level path may be treated as a valid path. <lb/>We present an inter-domain routing protocol based on superdomains, which always nds <lb/>a valid path if one exists. Both strong and weak constraints are maintained for each visible <lb/>superdomain. If the strong constraints of the superdomains on a path are satis ed, then the <lb/>path is valid. If only the weak constraints are satis ed for some superdomains on the path, the <lb/>source uses a query protocol to obtain a more detailed \internal&quot; view of these superdomains, <lb/>and searches again for a valid path. Our protocol handles topology changes, including node/link <lb/>failures that partition superdomains. Evaluation results indicate our protocol scales well to large <lb/>internetworks. <lb/>Categories and Subject Descriptors: C.2.1 Computer-Communication Networks]: Network Archi-<lb/>tecture and Design|packet networks; store and forward networks; C.2.2 Computer-Communication Net-<lb/>works]: Network Protocols|protocol architecture; C.2.m Routing Protocols]; F.2.m Computer Network <lb/>Routing Protocols]. <lb/>This work is supported in part by ARPA and Philips Labs under contract DASG60-92-0055 to Department <lb/>of Computer Science, University of Maryland, and by National Science Foundation Grant No. NCR 89-04590. The <lb/>views, opinions, and/or ndings contained in this report are those of the author(s) and should not be interpreted as <lb/>representing the o cial policies, either expressed or implied, of the Advanced Research Projects Agency, PL, NSF, <lb/>or the U.S. Government. Computer facilities were provided in part by NSF grant CCR-8811954. <lb/></front>

			<div type="toc">Contents <lb/>1 Introduction <lb/>2 Preliminaries <lb/>3 Superdomain-Level Views with Gateways <lb/>4 Edge-Costs and Topology Changes <lb/>5 View-Query Protocol <lb/>6 View-Update Protocol <lb/>7 Evaluation <lb/>7.1 Evaluation Model : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 16 <lb/>7.2 Application to Superdomain Query Protocol : : : : : : : : : : : : : : : : : : : : : : : 19 <lb/>8 Related Work <lb/>9 Conclusion <lb/>A Results for Other Internetworks <lb/>i <lb/></div>

			<body>1 Introduction <lb/>A computer internetwork, such as the Internet, is an interconnection of backbone networks, regional <lb/>networks, metropolitan area networks, and stub networks (campus networks, o ce networks and <lb/>other small networks) 1 . Stub networks are the producers and consumers of the internetwork tra c, <lb/>while backbones, regionals and MANs are transit networks. Most of the networks in an internetwork <lb/>are stub networks. Each network consists of nodes (hosts, routers) and links. A node that has a <lb/>link to a node in another network is called a gateway. Two networks are neighbors when there is <lb/>one or more links between gateways in the two networks (see Figure 1). <lb/>M <lb/>man G <lb/>regional D <lb/>backbone A <lb/>backbone B <lb/>backbone C <lb/>man F <lb/>man K <lb/>L <lb/>N <lb/>O <lb/>P <lb/>Q <lb/>R <lb/>man H <lb/>regional E <lb/>Figure 1: A portion of an internetwork. (Circles represent stub networks.) <lb/>An internetwork is organized into domains 2 . A domain is a set of networks (possibly consisting <lb/>of only one network) administered by the same agency. Domains are typically subject to policy <lb/>constraints, which are administrative restrictions on inter-domain tra c 7, 11, 8, 5]. The policy <lb/>constraints of a domain U are of two types: transit policies, which specify how other domains <lb/>can use the resources of U (e.g. $0.01 per packet, no tra c from domain V ); and source policies, <lb/>which specify constraints on tra c originating from U (e.g. domains to avoid/prefer, acceptable <lb/>connection cost). Transit policies of a domain are public (i.e. available to other domains), whereas <lb/>source policies are usually private. <lb/>Within each domain, an intra-domain routing protocol is executed that provides routes between <lb/>source and destination nodes in the domain. This protocol can be any of the typical ones, i.e., <lb/>next-hop or source routes computed using distance-vector or link-state algorithms. To satisfy <lb/></body>

			<note place="footnote">1 For example, NSFNET, MILNET are backbones, and Suranet, CerfNet are regionals. <lb/></note>

			<note place="footnote">2 Also referred to as routing domains or administrative domains. <lb/></note>

			<page>1 <lb/></page>

			<body>type-of-service (ToS) constraints of applications (e.g. low delay, high throughput, high reliability, <lb/>minimum monetary cost), each node maintains a cost for each outgoing link and ToS. The intra-<lb/>domain routing protocol should choose optimal paths based on these costs. <lb/>Across all domains, an inter-domain routing protocol is executed that provides routes between <lb/>source and destination nodes in di erent domains, using the services of the intra-domain routing <lb/>protocols within domains. This protocol should have the following properties: <lb/>(1) It should satisfy the policy constraints of domains. To do this, it must keep track of the <lb/>policy constraints of domains 5]. <lb/>(2) An inter-domain routing protocol should also satisfy ToS constraints of applications. To do <lb/>this, it must keep track of the ToS services o ered by domains 5]. <lb/>(3) An inter-domain routing protocol should scale up to very large internetworks, i.e. with a very <lb/>large number of domains. Practically this means that processing, memory and communication <lb/>requirements should be much less than linear in the number of domains. It should also <lb/>handle non-hierarchical domain interconnections at any level 8] (e.g. we do not want to <lb/>hand-con gure special routes as \back-doors&quot;). <lb/>(4) An inter-domain routing protocol should automatically adapt to link cost changes and node/link <lb/>failures and repairs, including failures that partition domains 13]. <lb/>A Straight-Forward Approach <lb/>A straight-forward approach to inter-domain routing is domain-level source routing with link-state <lb/>approach 7, 5]. In this approach, each router 3 maintains a domain-level view of the internetwork, <lb/>i.e., a graph with a vertex for every domain and an edge between every two neighbor domains. <lb/>Policy and ToS information is attached to the vertices and the edges of the view. <lb/>When a source node needs to reach a destination node, it (or a router 4 in the source&apos;s domain) <lb/>rst examines this view and determines a domain-level source route satisfying ToS and policy <lb/>constraints, i.e., a sequence of domain ids starting from the source&apos;s domain and ending with the <lb/>destination&apos;s domain. Then packets are routed to the destination using this domain-level source <lb/>route and the intra-domain routing protocols of the domains crossed. <lb/>For example, consider the internetwork of Figure 2 (each circle is a domain, and each thin line <lb/></body>

			<note place="footnote">3 Not all nodes maintain routing tables. A router is a node that maintains a routing table. <lb/></note>

			<note place="footnote">4 referred to as the policy server in 7] <lb/></note>

			<page>2 <lb/></page>

			<body>is a domain-level interconnection). Suppose a node in d1 desires a connection to a node in d7. <lb/>Suppose the policy constraints of d3 and d19 do not allow transit tra c originating from d1. Every <lb/>node maintains this information in its view. Thus the source node can choose a valid path from <lb/>source domain d1 to destination domain d7 avoiding d3 and d19 (e.g. thick line in the gure). <lb/>d1 <lb/>d2 <lb/>d3 <lb/>d4 <lb/>d5 <lb/>d6 <lb/>d7 <lb/>d8 <lb/>d9 <lb/>d10 <lb/>d11 d12 <lb/>d13 <lb/>d14 <lb/>d15 <lb/>d16 <lb/>d17 <lb/>d18 <lb/>d19 <lb/>d20 <lb/>d21 <lb/>d22 <lb/>d23 d24 <lb/>d25 <lb/>d26 d27 d28 <lb/>Source <lb/>Destination <lb/>valid path <lb/>Figure 2: An example interdomain topology. <lb/>The disadvantage of this straightforward scheme is that it does not scale up for large internet-<lb/>works. The storage at each router is proportional to N D E D , where N D is the number of domains <lb/>and E D is the average number of neighbor domains to a domain. The communication cost for <lb/>updating views is proportional to N R E R , where N R is the number of routers in the internetwork <lb/>and E R is the average router neighbors of a router (topology changes are ooded to all routers in <lb/>the internetwork). <lb/>The Superdomain Approach <lb/>To achieve scaling, several approaches based on hierarchically aggregating domains into superdo-<lb/>mains have been proposed 16, 14, 6]. Here, each domain is a level 1 superdomain, \close&quot; level 1 <lb/>superdomains are grouped into level 2 superdomains, \close&quot; level 2 superdomains are grouped into <lb/>level 3 superdomains, and so on (see Figure 3). Each router x maintains a view that contains the <lb/>level 1 superdomains in x&apos;s level 2 superdomain, the level 2 superdomains in x&apos;s level 3 superdomain <lb/>(excluding the x&apos;s level 2 superdomain), and so on. Thus a router maintains a smaller view than <lb/>it would in the absence of hierarchy. For the superdomain hierarchy of Figure 3, the views of two <lb/></body>

			<page>3 <lb/></page>

			<body>d1 <lb/>d2 <lb/>d3 <lb/>d4 <lb/>d5 <lb/>d6 <lb/>d7 <lb/>d8 <lb/>d9 <lb/>d10 <lb/>d11 d12 <lb/>d13 <lb/>d14 <lb/>d15 <lb/>d16 <lb/>d17 <lb/>d18 <lb/>d19 <lb/>d20 <lb/>d21 <lb/>d22 <lb/>d23 d24 <lb/>d25 <lb/>d26 d27 d28 <lb/>A <lb/>B <lb/>C <lb/>D <lb/>E <lb/>F <lb/>G <lb/>H <lb/>I <lb/>J <lb/>Source <lb/>Destination <lb/>valid path <lb/>level 1 superdomain <lb/>level 2 superdomain <lb/>level 3 superdomain <lb/>Figure 3: An example of superdomain hierarchy. <lb/>routers (one in domain d1 and one in domain d16) are shown in Figures 4 and 5. <lb/>d3 <lb/>d1 <lb/>d2 <lb/>B <lb/>C <lb/>G <lb/>J <lb/>Figure 4: View of a router in d1. <lb/>D <lb/>J <lb/>F <lb/>d15 <lb/>d13 <lb/>d14 <lb/>d17 <lb/>d16 <lb/>d18 <lb/>Figure 5: View of a router in d16. <lb/>The superdomain approach has several problems. One problem is that the aggregation results <lb/>in loss of domain-level ToS and policy information. A superdomain is usually characterized by a <lb/>single set of ToS and policy constraints derived from the ToS and policy constraints of the domains <lb/>in it. Routers outside the superdomain assume that this set of constraints applies uniformly to <lb/>each of its children (and by recursion to each domain in the superdomain). If there are domains <lb/>with di erent (possibly contradictory) constraints in a superdomain, then there is no good way of <lb/>deriving the ToS and policy constraints of the superdomain. <lb/>The usual technique 16] of obtaining ToS and policy constraints of a superdomain is to obtain <lb/>either a strong set of constraints or a weak set of constraints 5 from the ToS and policy constraints of <lb/></body>

			<note place="footnote">5 &quot;strong&quot; and &quot;weak&quot; are referred to respectively as &quot;union&quot; and &quot;intersection&quot; in 16] <lb/></note>

			<page>4 <lb/></page>

			<body>the children superdomains in it. If strong (weak) constraints are used for policies, the superdomain <lb/>enforces a policy constraint if that policy constraint is enforced by some (all) of its children. If <lb/>strong (weak) constraints are used for ToS constraints, the superdomain is assumed to support a <lb/>ToS if that ToS is supported by all (some) of its children. The intention is that if strong (weak) <lb/>constraints of a superdomain are (are not) satis ed then any (no) path through that superdomain <lb/>is valid. <lb/>Each approach has problems. Strong constraints can eliminate valid paths, and weak constraints <lb/>can allow invalid paths. For example in Figure 3, d16 allows transit tra c from d1 while d19 does <lb/>not; with strong constraints G would not allow transit tra c from d1, and with weak constraints <lb/>G would allow transit tra c from d1 to be routed via d19. <lb/>Other problems of the superdomain approach are that the varying visibilities of routers compli-<lb/>cates superdomain-level source routing and handling of node/link failures (especially those that par-<lb/>tition superdomains). The usual technique for solving these problems is to augment superdomain-<lb/>level views with gateways 16] (see Section 3). <lb/>Our Contribution <lb/>In this paper, we present an inter-domain routing protocol based on superdomains, which nds <lb/>a valid path if and only if one exists. Both strong and weak constraints are maintained for each <lb/>visible superdomain. If the strong constraints of the superdomains on a path are satis ed, then <lb/>the path is valid. If only the weak constraints are satis ed for some superdomains on the path, the <lb/>source uses a query protocol to obtain a more detailed \internal&quot; view of these superdomains, and <lb/>searches again for a valid path. <lb/>We use superdomain-level views with gateways and a link-state view update protocol to handle <lb/>topology changes including failures that partition superdomains. The storage cost is O(log N D <lb/>log N D ) without the query protocol. We demonstrate the scaling properties of the query protocol <lb/>by giving evaluation results based on simulations. Our evaluation results indicate that the query <lb/>protocol can be performed using 15% extra space. <lb/>Our protocol consists of two subprotocols: a view-query protocol for obtaining views of <lb/>greater resolution when needed; and a view-update protocol for disseminating topology changes <lb/>to the views. <lb/></body>

			<page>5 <lb/></page>

			<body>Several approaches to scalable inter-domain routing have been proposed, based on the super-<lb/>domain hierarchy 1, 14, 16, 9, 6], and the landmark hierarchy 18, 17]. Some of these approaches <lb/>su er from loss of ToS and policy information (and hence may not nd a valid path which exists). <lb/>Others are still in a preliminary stage. (Details in Section 8.) <lb/>One important di erence between these approaches and ours is that ours uses a query mechanism <lb/>to obtain ToS and policy details whenever needed. In our opinion, such a mechanism is needed <lb/>to obtain a scalable solution. Query protocols are also being developed to enhance the protocols <lb/>in 9, 6]. Reference 2] presents protocols based on a new kind of hierarchy, referred to as the <lb/>viewserver hierarchy (more details in Section 8). <lb/>A preliminary version of the view-query protocol was proposed in reference 1]. That version <lb/>di ers greatly from the one in this paper. Here, we augment superdomain-level views with gate-<lb/>ways. In 1], we augmented superdomain-level views with superdomain-to-domain edges (details in <lb/>Section 8). Both versions have the same time and space complexity, but the protocols in this paper <lb/>are much simpler conceptually. Also the view-update protocol is not in reference 1]. <lb/>Organization of the paper <lb/>In Section 2, we present some de nitions used in this paper. In Section 3, we de ne the view data <lb/>structures. In Section 4, we describe how views are a ected by topology changes. In Section 5, we <lb/>present the view-query protocol. In Section 6, we present the view-update protocol. In Section 7, <lb/>we present our evaluation model and the results of its application to the superdomain hierarchy. <lb/>In Section 8, we survey recent approaches to inter-domain routing. In Section 9, we conclude and <lb/>describe cacheing and heuristic schemes to improve performance. <lb/>2 Preliminaries <lb/>Each domain has a unique id. Let DomainIds denote the set of domain-ids. Each node has a <lb/>unique id. Let NodeIds denote the set of node-ids. For a node x, we use domainid(x) to denote <lb/>the domain-id of x&apos;s domain. <lb/>The superdomain hierarchy de nes the following parent-child relationship: a level i, i &gt; 1, <lb/>superdomain is the parent of each level i ? 1 superdomain it contains. Top-level superdomains <lb/></body>

			<page>6 <lb/></page>

			<body>have no parents. Level 1 superdomains, which are just domains, have no children. For any two <lb/>superdomains X and Y , X is a sibling of Y i X and Y have the same parent. X is an ancestor <lb/>(descendant) of Y i X = Y or X is an ancestor (descendant) of Y &apos;s parent (child). <lb/>Each router maintains information about a subset of superdomains, referred to as its visible <lb/>superdomains. The visible superdomains of a router x are (1) x&apos;s domain itself, (2) siblings of x&apos;s <lb/>domain, and (3) siblings of ancestors of x&apos;s domain. In Figure 3, the visible superdomains of a <lb/>router in d1 are d1; d2; d3; B; C; G; J (these are shown in Figure 4). Note that if a superdomain U <lb/>is visible to a router, then no ancestor or descendant of U is visible to the router. <lb/>Each superdomain has a unique id, i.e. unique among all superdomains regardless of level. Let <lb/>SuperDomainIds denote the set of superdomain-ids. DomainIds is a subset of SuperDomainIds. <lb/>For a superdomain U, let level(U ) denote the level of U in the hierarchy, let Ancestors(U ) denote <lb/>the set of ids of ancestor superdomains of U in the hierarchy, and let Children(U ) denote the set <lb/>of ids of child superdomains of U in the hierarchy. <lb/>For a router x, let VisibleSuperDomains(x) denote the set of ids of superdomains visible from <lb/>x. <lb/>We extend the above de nitions by allowing their arguments to be nodes, in which case the node <lb/>stands for its domain. For example, if x is a node in domain d, Ancestors(x) denotes Ancestors(d). <lb/>3 Superdomain-Level Views with Gateways <lb/>For routing purposes, each domain (and node) has an address, de ned as the concatenation of the <lb/>superdomain ids starting from the top level and going down to the domain (node). For example in <lb/>Figure 3, the address of domain d15 is G:E:d15, and the address of a node h in d15 is G:E:d15:h. <lb/>When a source node needs to reach a destination node, it rst determines the visible superdo-<lb/>main in the destination address and then by examining its view determines a superdomain-level <lb/>source route (satisfying ToS and policy constraints) to this superdomain. However, since routers <lb/>in di erent superdomains maintain views of di erent sets of superdomains, this superdomain-level <lb/>source route can be meaningless at some intermediate superdomain&apos;s router x because the next <lb/>superdomain in this source route is not visible to x. For example in Figure 4, superdomain-level <lb/>source route hd2; B; G; Ci created at a router in d2 becomes meaningless once the packet is in G, <lb/>where C is not visible. <lb/></body>

			<page>7 <lb/></page>

			<body>The usual technique of solving this problem is to augment superdomain-level views with gate-<lb/>ways and edges between these gateways. <lb/>De ne the pair U:g to be an sd-gateway i U is a superdomain and g is a node that is in U and <lb/>has a link to a node outside U. Equivalently, we say that g is a gateway of U. <lb/>De ne hU:g; hi to be an actual-edge i U:g is an sd-gateway, h is a gateway not in U, and there <lb/>is a link from g to h. <lb/>De ne hU:g; hi to be a virtual-edge i U:g and U:h are sd-gateways and g 6 = h (note that there <lb/>may not be a link between g and h). <lb/>hU:g; hi is an edge i it is an actual-edge or a virtual-edge. An edge hU:g; hi is also said to be <lb/>an outgoing edge of U:g. De ne edges of U:g to be the set of edges outgoing from U:g. De ne edges <lb/>of U to be the set of edges outgoing from any gateway of U. <lb/>Let Gateways(U ) denote the set of node-ids of gateways of U. Let Edges(U :g) denote the edges <lb/>of U:g. Note that we never use \edge&quot; as a synonym for link. <lb/>A gateway g of a domain can generate many sd-gateways, speci cally, U:g for every ancestor U <lb/>of g&apos;s domain such that g has a link to a node outside U. A link hg; hi where g and h are gateways <lb/>in di erent domains, can generate many actual-edges; speci cally, actual-edge hU:g; hi for every <lb/>ancestor U of g&apos;s domain such that U is not an ancestor of h&apos;s domain. <lb/>For the internetwork topology of Figure 2, the corresponding gateway-level connections are <lb/>shown in Figure 6 where black rectangles are gateways. For the hierarchy of Figure 3, gateway <lb/>g in Figure 6 generates sd-gateways d16:g, E:g, and G:g. The link hg; hi in Figure 6 generates <lb/>actual-edges hd16:g; hi, hE:g; hi, hG:g; hi. <lb/>To a router, at most one of the sd-gateways generated by a gateway g is visible, namely U:g <lb/>where U is an ancestor of g&apos;s domain and U is visible to the router. At most one of the actual-edges <lb/>generated by a link hg; hi between two gateways in di erent domains is visible to the router, namely <lb/>edge hU:g; hi where U:g is visible to the router. None of the actual-edges are visible to the router <lb/>if g and h are inside a visible superdomain. For example in Figure 3, of the actual-edges generated <lb/>by link hg; hi, only hG:g; hi is visible to a router in d1, and only hd16:g; hi is visible to a router in <lb/>d16. <lb/>A router maintains a view consisting of the visible sd-gateways and their outgoing actual-and <lb/>virtual-edges. An edge hU:g; hi in the view of a router connects the sd-gateway U:g to the sd-<lb/></body>

			<page>8 <lb/></page>

			<body>d1 <lb/>d2 <lb/>d3 <lb/>d4 <lb/>d5 <lb/>d6 <lb/>d7 <lb/>d8 <lb/>d9 <lb/>d10 <lb/>d11 <lb/>d12 <lb/>d13 <lb/>d14 <lb/>d15 <lb/>d16 <lb/>d17 <lb/>d18 <lb/>d19 <lb/>d20 <lb/>d21 <lb/>d22 <lb/>d23 <lb/>d24 <lb/>d25 <lb/>d26 <lb/>d27 d28 <lb/>gateway h gateway g <lb/>Figure 6: Gateway-level connections of internetwork of Figure 2. <lb/>gateway V :h such that V :h is visible to the router. For the superdomain-level views of Figures 4 <lb/>and 5, the new views are shown in Figures 7 and 8, respectively. <lb/>B <lb/>C <lb/>G <lb/>J <lb/>d1 <lb/>d2 <lb/>d3 <lb/>gateway G:g <lb/>gateway B:h <lb/>Figure 7: View of a router in d1. <lb/>D <lb/>J <lb/>F <lb/>d13 <lb/>d14 <lb/>d15 <lb/>d16 <lb/>d17 <lb/>d18 <lb/>gateway d16:g <lb/>gateway D:h <lb/>Figure 8: View of a router in d16. <lb/>The view of a router x contains, for each superdomain U that is visible to x or is an ancestor <lb/>of x, the strong and weak constraints of U and a set referred to as Gateways&amp;Edges x (U). This <lb/>set contains, for each gateway y of U, the edges of U:y and their costs. The reason for storing <lb/>information about ancestor superdomains is given in Section 5. The cost eld is used to satisfy ToS <lb/>constraints and is described in Section 4. The timestamp eld is described in Section 6. Formally, <lb/>the view of x is de ned as follows: <lb/></body>

			<page>9 <lb/></page>

			<body>V iew x : View of x. <lb/>= fhU; strong constraints(U ); weak constraints(U ); Gateways&amp;Edges x (U)i : <lb/>U 2 VisibleSuperDomains(x) Ancestors(x) g <lb/>where <lb/>Gateways&amp;Edges x (U): Sd-gateways and edges of U. <lb/>= fhy; timestamp; fhz; costi : hU:y; zi 2 Edges(U :y)gi : y 2 Gateways(U ) g. <lb/>ToS and policy constraints can also be speci ed for each sd-gateway and edge. Our protocols <lb/>can be extended to handle such constraints, but we have not done so here in order to keep their <lb/>descriptions simple. <lb/>A superdomain-level source route is now a sequence of sd-gateway ids. With this de nition, it <lb/>is easy to verify that whenever the next superdomain in a superdomain-level source route is not <lb/>visible to a router, there is an actual-edge (hence a link) between the router and the next gateway <lb/>in this route. <lb/>4 Edge-Costs and Topology Changes <lb/>A cost is associated with each edge. The cost of an edge equals a vector of values if the edge is up; <lb/>each cost value indicates how expensive it is to cross the edge according to some ToS constraint. <lb/>The cost equals 1 if the edge is an actual-edge and it is down, or the edge is a virtual-edge hU:g; hi <lb/>and h can not be reached from g without leaving U. <lb/>Since an actual-edge represents a physical link, its cost can be determined from measured link <lb/>statistics. The cost of a virtual-edge hU:g; hi is an aggregation of the cost of physical links in <lb/>U and is calculated as follows: If U is a domain, the cost of hU:g; hi is calculated as the maxi-<lb/>mum/minimum/average cost of the routes within U from g to h 4]. For higher level superdomains <lb/>U, the cost of hU:g; hi is derived from the costs of edges between the gateways of children super-<lb/>domains of U. <lb/>Link cost changes and link/node failures and repairs correspond to cost changes, failures and <lb/>repairs of actual-and virtual-edges. Thus the attributes of edges in the views of routers must be <lb/>regularly updated. For this, we employ a view-update protocol (see Section 6). <lb/></body>

			<page>10 <lb/></page>

			<body>Link/node failures can also partition a superdomain into cells, where a cell of a superdomain <lb/>is de ned to be a maximal subset of nodes of the superdomain that can reach each other without <lb/>leaving the superdomain. Superdomain partitions can occur at any level in the hierarchy. For <lb/>example, suppose U is a domain and V is its parent superdomain. U can be partitioned into cells <lb/>without V being partitioned (i.e. if the cells of U can reach each other without leaving V ). The <lb/>opposite can also happen: if all links between U and the other children of V fail, then V becomes <lb/>partitioned but U does not. Or both U and V can be partitioned. In the same way, link/node <lb/>repairs can merge cells into bigger cells. <lb/>We handle superdomain partitioning as follows: A router detects that a superdomain U is <lb/>partitioned when a virtual-edge of U in the router&apos;s view has cost 1. When a router forwards <lb/>a packet to a destination for which the visible superdomain, say U, in the destination address is <lb/>partitioned into cells, a copy of the packet is sent to each cell by sending a copy of the packet to <lb/>each gateway of U; the id U in the destination address is \marked&quot; in the packet so that subsequent <lb/>routers do not create new copies of the packet for U. <lb/>5 View-Query Protocol <lb/>When a source node wants a superdomain-level source route to a destination, a router in its domain <lb/>examines its view and searches for a valid path (i.e. superdomain-level source route) using the <lb/>destination address 6 . We refer to this router as the source router. Even though the source router <lb/>does not know the constraints of the individual domains that are to be crossed in each superdomain, <lb/>it does know the strong and weak constraints of the superdomains. We refer to a superdomain <lb/>whose strong constraints are satis ed as a valid superdomain. If a superdomain&apos;s weak constraints <lb/>are satis ed but strong constraints are not satis ed, then there may be a valid path through this <lb/>superdomain. We refer to such a superdomain as a candidate superdomain. <lb/>A path is valid if it involves only valid superdomains. A path cannot be valid if it involves <lb/>a superdomain which is neither valid nor candidate. We refer to a path involving only valid and <lb/>candidate superdomains as a candidate path. <lb/></body>

			<note place="footnote">6 We assume that the source has the destination&apos;s address. If that is not the case, it would rst query the name <lb/>servers to obtain the address for the destination. Querying the name servers can be done the same way it is done <lb/>currently in the Internet. It requires nodes to have a set of xed addresses to name servers. This is also su cient in <lb/>our case. <lb/></note>

			<page>11 <lb/></page>

			<body>If the source router&apos;s view contains a candidate path hU 0 :g 0 0 ; . . .; U 0 :g 0n 0 ; U 1 :g 1 0 ; . . .; U 1 :g 1n 1 ; ; <lb/>U m :g m 0 ; . . .; U m :g mn m i to the destination (and does not contain a valid path), then for each candi-<lb/>date superdomain U i on this path, the source router queries gateway g i 0 of U i for the internal view of <lb/>U i . This internal view consists of the constraints, sd-gateways and edges of the child superdomains <lb/>of U i . <lb/>When a router x receives a request for the internal view of an ancestor superdomain U, it <lb/>returns the following data structure: <lb/>IView x (U): Internal view of U at router x. <lb/>= fhV; strong constraints(V ); weak constraints(V ); Gateways&amp;Edges x (V )i 2 V iew x : <lb/>V 2 Children(U )g <lb/>It is to simplify the construction of IView x (U) that we store information about ancestor su-<lb/>perdomains in the view of router x. Instead of storing this information, router x could construct <lb/>IView x (U) from the constraints, sd-gateways and edges of the visible descendants of U. We did <lb/>not choose this alternative because the extra information does not increase storage complexity. <lb/>When the source router receives the internal view of a superdomain U, it does the following: <lb/>(1) it removes the sd-gateways and edges of U from its view; (2) it adds the sd-gateways and edges <lb/>of children superdomains in the internal view of U; and (3) searches for a valid path again. If there <lb/>is still no valid path but there are candidate paths, the process is repeated. <lb/>For example, consider Figure 3. For a router in superdomain d1 (see Figure 7), G is visible and <lb/>is a candidate domain. The internal view of G is shown in Figure 9, and the resulting merged view <lb/>is shown in Figure 10. The valid path through G (visiting d16 and avoiding d19) can be discovered <lb/>using this merged view (since the strong constraints of E are satis ed). <lb/>Consider a candidate route to a destination: hU 0 :g 0 0 ; . . .; U 0 :g 0n 0 ; U 1 :g 1 0 ; . . .; U 1 :g 1n 1 ; ; <lb/>U m :g m 0 ; . . .; U m :g mn m i. If superdomain U i is partitioned into cells, it may re-appear later in the <lb/>candidate path (i.e. for some j 6 = i, U j = U i ). In this case both gateways g i 0 and g j 0 are queried. <lb/>Timestamps are used to resolve con icts between the information reported by these gateways. <lb/>The view-query protocol uses two types of messages as follows: <lb/>(RequestIView; sdid; gid; s address; d address) <lb/></body>

			<page>12 <lb/></page>

			<note place="headnote">E <lb/></note>

			<body>F <lb/>Figure 9: Internal view of G. <lb/>B <lb/>C <lb/>J <lb/>d1 <lb/>d2 <lb/>d3 <lb/>E <lb/>F <lb/>Figure 10: Merged view at d1. <lb/>Sent by a source router to gateway gid to obtain the internal view of superdomain sdid. <lb/>s address is the address of the source router. d address is the address of the destination <lb/>node (of the desired route). <lb/>(ReplyIView; sdid; gid; iview; d address) <lb/>where iview is the internal view of superdomain sdid, and other parameters are as in the <lb/>RequestIView message. It is sent by gateway gid to the source router. <lb/>The state maintained by a source router x is listed in Figure 15. PendingReq x is used to <lb/>avoid sending new request messages before receiving all outstanding reply messages. WV iew x and <lb/>PendingReq x are allocated and deallocated on demand for each destination. <lb/>The events of router x are speci ed in Figure 15. In the gure, * is a wild-card matching any <lb/>value. TimeOut x event is executed after a time-out period from the execution of Request x event to <lb/>indicate that the request has not been satis ed. The source host can then repeat the same request <lb/>afterwards. <lb/>The procedure search x uses an operation \ReliableSend(m) to v&quot;, where m is the message being <lb/>sent and v is either an address of an arbitrary router or an id of a gateway of a visible superdomain. <lb/>ReliableSend is asynchronous. The message is delivered to v as long as there is a sequence of up <lb/>links between u and v. 7 (Note that an address is not needed to obtain an inter-domain route to a <lb/>gateway of a visible superdomain.) <lb/>Router Failure Model: A router can undergo failures and recoveries at anytime. We <lb/>assume failures are fail-stop (i.e. a failed router does not send erroneous messages). When a router <lb/>x recovers, the variables WV iew x and PendingReq x are lost for all destinations. The cost of each <lb/>edge in V iew x is set to 1. It becomes up-to-date as the router receives new information from other <lb/></body>

			<note place="footnote">7 This involves time-outs, retransmissions, etc. It requires a transport protocol support such as TCP. <lb/></note>

			<page>13 <lb/></page>

			<body>routers. <lb/>6 View-Update Protocol <lb/>A gateway g, for each ancestor superdomain U, informs other routers of topology changes (i.e. <lb/>failures, repairs and cost changes) a ecting U:g&apos;s edges. The communication is done by ooding <lb/>messages. The ooding is restricted to the routers in the parent superdomain of U, since U is <lb/>visible only to these routers. <lb/>Due to the nature of ooding, a router can receive information out of order from a gateway. In <lb/>order to avoid old information replacing new information, each gateway includes increasing time <lb/>stamps in the messages it sends. Routers maintain for each gateway the highest received time <lb/>stamp (in the timestamp eld in V iew x ), and discard messages with smaller timestamps. Time <lb/>stamps do not have to be real-time clock values. <lb/>Due to superdomain partitioning, messages sent by a gateway may not reach all routers within <lb/>the parent superdomain, resulting in some routers having out-of-date information. This out-of-date <lb/>information can cause inconsistencies when the partition is repaired. To eliminate inconsistencies, <lb/>when a link recovers, the two routers at the ends of the link exchange their views and ood any new <lb/>information. As usual, information about a superdomain U is ooded over U&apos;s parent superdomain. <lb/>The view-update protocol uses messages of the following form: <lb/>(Update; sdid; gid; timestamp; edge-set) <lb/>Sent by the gateway gid to inform other routers about current attributes of edges of sdid:gid. <lb/>timestamp indicates the time stamp of gid. edge-set contains a cost for each edge. <lb/>The state maintained by a router x is listed in Figure 16. Note that AdjLocalRouters x or <lb/>AdjForeignGateways x can be empty. IntraDomainRT x contains a route (next-hop or source) 8 for <lb/>every reachable node of the domain. We assume that consecutive reads of Clock x returns increasing <lb/>values. <lb/>Routers also receive and ood messages containing edges of sd-gateways of their ancestor su-<lb/>perdomains. This information is used by the query protocol (see Section 5). Also the highest <lb/>timestamp received from a gateway g of an ancestor superdomain is needed to avoid exchanging <lb/></body>

			<note place="footnote">8 IntraDomainRTx is a view in case of a link-state routing protocol or a distance table in case of a distance-vector <lb/>routing protocol. <lb/></note>

			<page>14 <lb/></page>

			<body>the messages of g in nitely during ooding. <lb/>The events of router x are speci ed in Figure 16. We use Ancestor i (U) to denote the superdomain-<lb/>id of the ith ancestor of U, where Ancestor 0 (U) = U. In the view-update protocol, a node u uses <lb/>send operations of the form \Send(m) to v&quot;, where m is the message being sent and v is the <lb/>destination-id. Here, nodes u and v are neighbors, and the message is sent over the physical link <lb/>hu; vi. If the link is down, we assume that the packet is dropped. <lb/>7 Evaluation <lb/>In the superdomain hierarchy (without the query protocol), the number of superdomains in a view <lb/>is logarithmic in the number of superdomains in the internetwork 10]. 9 However, the storage <lb/>required for a view is proportional not to the number of superdomains in it but to the number of <lb/>sd-gateways in it. As we have seen, there can be more than one sd-gateway for a superdomain in <lb/>a view. <lb/>In fact, the superdomain hierarchy does not scale-up for arbitrary internetworks; that is, the <lb/>number of sd-gateways in a view can be proportional to the number of domains in the internetwork. <lb/>For example, if each domain in a superdomain U has a distinct gateway with a link to outside U, <lb/>the number of sd-gateways of U would be linear in the number of domains in U. <lb/>The good news is that the superdomain hierarchy does scale-up for realistic internetwork topolo-<lb/>gies. A su cient condition for scaling is that each superdomain has at most log N D sd-gateways; <lb/>this condition is satis ed by realistic internetworks since most domain interconnections are \hier-<lb/>archical connections&quot; i.e. between backbones and regionals, between regionals and MANs, and so <lb/>on. <lb/>In this section, we present an evaluation of the scaling properties of the superdomain hierarchy <lb/>and the query protocol. To evaluate any inter-domain routing protocol, we need a model in which <lb/>we can de ne internetwork topologies, policy/ToS constraints, inter-domain routing hierarchies, <lb/>and evaluation measures (e.g. memory and time requirements). We have recently developed such <lb/>a model 3]. We rst describe our model, and then use it to evaluate our superdomain hierarchy. <lb/>Our evaluation measures are the amount of memory required at the routers, and the amount of <lb/></body>

			<note place="footnote">9 Even though the results in 10] were for intra-domain routing, it is easy to show that the analysis there holds <lb/>for inter-domain routing as well. <lb/></note>

			<page>15 <lb/></page>

			<body>time needed to construct a path. <lb/>7.1 Evaluation Model <lb/>We rst describe our method of generating topologies and policy/ToS constraints. We then describe <lb/>the evaluation measures. <lb/>Generating Internetwork Topologies <lb/>For our purposes, an internetwork topology is a directed graph where the nodes correspond to <lb/>domains and the edges correspond to domain-level connections. However, an arbitrary graph will <lb/>not do. The topology should have the characteristics of a real internetwork, like the Internet. <lb/>That is, it should have backbones, regionals, MANS, LANS, etc.; there should be hierarchical <lb/>connections, but some \non-hierarchical&quot; connections should also be present. <lb/>For brevity, we refer to backbones as class 0 domains, regionals as class 1 domains, metropolitan-<lb/>area domains and providers as class 2 domains, and campus and local-area domains as class 3 <lb/>domains. A (strictly) hierarchical interconnection of domains means that class 0 domains are <lb/>connected to each other, and for i &gt; 0, class i domains are connected to class i ? 1 domains. <lb/>As mentioned above, we also want some \non-hierarchical&quot; connections, i.e., domain-level edges <lb/>between domains irrespective of their classes (e.g. from a campus domain to another campus <lb/>domain or to a backbone domain). <lb/>In reality, domains span geographical regions and domain-level edges are often between do-<lb/>mains that are geographically close (e.g. University of Maryland campus domain is connected to <lb/>SURANET regional domain which are both in the east coast). We also want some edges that are <lb/>between far domains. A class i domain usually spans a larger geographical region than a class i + 1 <lb/>domain. To generate such interconnections, we associate a \region&quot; attribute to each domain. The <lb/>intention is that two domains with the same region are geographically close. <lb/>The region of a class i domain has the form r 0 :r 1 : :r i , where the r j &apos;s are integers. For <lb/>example, the region of a class 3 domain can be 1.2.3.4. For brevity, we refer to the region of a <lb/>class i domain as a class i region. <lb/>Note that regions have their own hierarchy which should not be confused with the superdomain <lb/>hierarchy. Class 0 regions are the top level regions. We say that a class i region r 0 :r 1 : :r i?1 :r i <lb/></body>

			<page>16 <lb/></page>

			<body>is contained in the class i ?1 region r 0 :r 1 : :r i?1 (where i &gt; 0). Containment is transitive. Thus <lb/>region 1.2.3.4 is contained in regions 1.2.3, 1.2 and 1. <lb/>A <lb/>B <lb/>C <lb/>D <lb/>E <lb/>F <lb/>G <lb/>H <lb/>I <lb/>J <lb/>K <lb/>L <lb/>M <lb/>N <lb/>O <lb/>P <lb/>Q <lb/>1 <lb/>1.1 <lb/>1.2 <lb/>1.2.1 <lb/>1.2.2 <lb/>1.2.1.1 <lb/>1.2.1.2 <lb/>1.2.1.3 <lb/>1.2.2.1 <lb/>2 <lb/>Figure 11: Regions <lb/>Given any pair of domains, we classify them as local, remote or far, based on their regions. <lb/>Let X be a class i domain and Y a class j domain, and without loss of generality let i j. X <lb/>and Y are local if they are in the same class i region. For example in Figure 11, A is local to <lb/>B; C; J; K; M; N; O; P, and Q. X and Y are remote if they are not in the same class i region but <lb/>they are in the same class i ? 1 region, or if i = 0. For example in Figure 11, some of the domains <lb/>A is remote to are D; E; F, and L. X and Y are far if they are not local or remote. For example <lb/>in Figure 11, A is far to I. <lb/>We refer to a domain-level edge as local (remote, or far) if the two domains it connects are local <lb/></body>

			<page>17 <lb/></page>

			<body>(remote, or far). <lb/>We use the following procedure to generate internetwork topologies: <lb/>We rst specify the number of domain classes, and the number of domains in each class. <lb/>We next specify the regions. Note that the number of region classes equals the number of <lb/>domain classes. We specify the number of class 0 regions. For each class i &gt; 0, we specify a <lb/>branching factor, which creates that many class i regions in each class i ? 1 region. (That is, <lb/>if there are two class 0 regions and the class 1 branching factor equals three, then there are <lb/>six class 1 regions.) <lb/>For each class i, we randomly map the class i domains into the class i regions. Note that <lb/>several domains can be mapped to the same region, and some regions may have no domain <lb/>mapped into them. <lb/>For every class i and every class j, j i, we specify the number of local, remote and far <lb/>edges to be introduced between class i domains and class j domains. The end points of the <lb/>edges are chosen randomly (within the speci ed constraints). <lb/>We ensure that the internetwork topology is connected by ensuring that the subgraph of class <lb/>0 domains is connected, and each class i domain, for i &gt; 0, is connected to a local class i ? 1 <lb/>domain. <lb/>Each domain has one gateway. So all neighbors of a domain are connected via this gateway. <lb/>This is for simplicity. <lb/>Choosing Policy/ToS Constraints <lb/>We chose a simple scheme to model policy/ToS constraints. Each domain is assigned a color: green <lb/>or red. For each domain class, we specify the percentage of green domains in that class, and then <lb/>randomly choose a color for each domain in that class. <lb/>A valid route from a source to a destination is one that does not visit any red intermediate <lb/>domains; the source and destination domains are allowed to be red. <lb/>This simple scheme can model many realistic policy/ToS constraints, such as security constraints <lb/>and bandwidth requirements. It cannot model some important kinds of constraints, such as delay <lb/>bounds. <lb/></body>

			<page>18 <lb/></page>

			<body>Computing Evaluation Measures <lb/>The evaluation measures of most interest for an inter-domain routing protocol are its memory, time <lb/>and communication requirements. We postpone the precise de nitions of the evaluation measures <lb/>to the next subsection. <lb/>The only analysis method we have at present is to numerically compute the evaluation measures <lb/>for a variety of source-destination pairs. Because we use internetwork topologies of large sizes, it <lb/>is not feasible to compute for all possible source-destination pairs. We randomly choose a set <lb/>of source-destination pairs that satisfy the following conditions: (1) the source and destination <lb/>domains are di erent stub domains, and (2) there exists a valid path from the source domain to the <lb/>destination domain in the internetwork topology. (Note that the straight-forward scheme would <lb/>always nd such a path.) <lb/>7.2 Application to Superdomain Query Protocol <lb/>We use the above model to evaluate our superdomain query protocol for several di erent super-<lb/>domain hierarchies. For each hierarchy, we de ne a set of superdomain-ids and a parent-child <lb/>relationship on them. <lb/>The rst superdomain hierarchy scheme is referred to as child-domains. Each domain d (re-<lb/>gardless of its class) is a level-1 superdomain, also identi ed as d. In addition, for each backbone d, <lb/>we create a distinct level-4 superdomain referred to as d-4. For each regional d, we create a distinct <lb/>level-3 superdomain d-3 and make it a child of a randomly chosen level-4 superdomain e-4 such <lb/>that d and e are local and connected. For each MAN d, we create a distinct level-2 superdomain <lb/>d-2 and make it a child of a randomly chosen level-3 superdomain e-3 such that d and e are local <lb/>and connected. Please see Figure 12. <lb/>We next describe how the level-1 superdomains (i.e. the domains) are placed in the hierarchy. <lb/>A backbone d is placed in, i.e. as a child of, d-4. A regional d is placed in d-3. A MAN d is placed <lb/>in d-2. A stub d is placed in e-2 such that d and e are local and connected. Please see Figure 12. <lb/>The second superdomain hierarchy scheme is referred to as sibling-domains. It is identical <lb/>to child-domains except for the placement of level-1 superdomains corresponding to backbones, <lb/>regionals and MANs. In sibling-domains, a backbone d is placed as a sibling of d-4. A regional d <lb/>is placed as a sibling of d-3. A MAN d is placed as a sibling of d-2. Please see Figure 13. <lb/></body>

			<page>19 <lb/></page>

			<body>backbone A <lb/>regional E <lb/>regional B <lb/>man C <lb/>D <lb/>man F <lb/>backbone G <lb/>A−4 <lb/>B−3 <lb/>C−2 <lb/>F−2 <lb/>E−3 <lb/>G−4 <lb/>Figure 12: child-domains <lb/>backbone A <lb/>regional E <lb/>regional B <lb/>man C <lb/>D <lb/>man F <lb/>backbone G <lb/>A−4 <lb/>B−3 <lb/>C−2 <lb/>F−2 <lb/>E−3 <lb/>G−4 <lb/>Figure 13: sibling-domains <lb/>The third superdomain hierarchy scheme is referred to as leaf-domains. It is identical to child-<lb/>domains except for the placement of level-1 superdomains corresponding to backbones and regionals. <lb/></body>

			<page>20 <lb/></page>

			<body>In leaf-domains, backbones and regionals are placed in some level-2 superdomain, as follows. A <lb/>regional d, if superdomain d-3 has a child superdomain e-2, is placed in e-2. Otherwise, a new level-<lb/>2 superdomain d-2 is created and placed in d-3. d is placed in d-2. A backbone d, if superdomain <lb/>d-4 has a child superdomain f-3, is placed in the level-2 superdomain containing the regional f. <lb/>Otherwise, a new level-3 superdomain d-3 is created and placed in d-4, a new level-2 superdomain <lb/>d-2 is created and placed in d-3. d is placed in d-2. Please see Figure 14. <lb/>Note that in leaf-domains, all level-1 superdomains are placed under level-2 superdomains. <lb/>Whereas other schemes allow some level-1 superdomains to be placed under higher level superdo-<lb/>mains. <lb/>backbone A <lb/>regional E <lb/>regional B <lb/>man C <lb/>D <lb/>man F <lb/>backbone G <lb/>A−4 <lb/>B−3 <lb/>C−2 <lb/>F−2 <lb/>E−3 <lb/>G−4 <lb/>Figure 14: leaf-domains <lb/>The fourth superdomain hierarchy scheme is referred to as regions. In this scheme, the super-<lb/>domain hierarchy corresponds exactly to the region hierarchy used to generate the internetwork <lb/>topology. That is, for a class 1 region x there is a distinct level 5 (top level) superdomain x-5. For <lb/>a class 2 region x.y there is a distinct level 4 superdomain x:y-4 placed under level 5 superdomain <lb/>x-5, and so on. Each domain is placed under the superdomain of its region. Please see Figure 11. <lb/></body>

			<page>21 <lb/></page>

			<body>Results for Internetwork 1 <lb/>The parameters of the rst internetwork topology, referred to as Internetwork 1, are shown in <lb/>Table 1. <lb/>Class i No. of Domains No. of Regions 10 % of Green Domains Edges between Classes i and j <lb/>Class j Local Remote Far <lb/>0 <lb/>10 <lb/>4 <lb/>0.80 <lb/>0 <lb/>8 <lb/>6 <lb/>1 <lb/>100 <lb/>16 <lb/>0.75 <lb/>0 <lb/>190 <lb/>20 <lb/>1 <lb/>26 <lb/>5 <lb/>2 <lb/>1000 <lb/>64 <lb/>0.70 <lb/>0 <lb/>100 <lb/>0 <lb/>1 <lb/>1060 <lb/>40 <lb/>2 <lb/>200 <lb/>40 <lb/>3 <lb/>10000 <lb/>256 <lb/>0.20 <lb/>0 <lb/>100 <lb/>0 <lb/>1 <lb/>100 <lb/>0 <lb/>2 <lb/>10100 <lb/>50 <lb/>3 <lb/>50 <lb/>50 <lb/>50 <lb/>Table 1: Parameters of Internetwork 1. <lb/>Our evaluation measures were computed for a (randomly chosen but xed) set of 100,000 source-<lb/>destination pairs. For a source-destination pair, we refer to the length of the shortest valid path in <lb/>the internetwork topology as the shortest-path length, or spl in short. The minimum spl of these <lb/>pairs was 2, the maximum spl was 15, and the average spl was 6.84. <lb/>For each source-destination pair, the set of candidate paths is examined in shortest-rst order <lb/>until either a valid path was found or the set was exhausted and no valid paths were found. <lb/>For each candidate path, RequestIView messages are sent to all candidate superdomains on this <lb/>path in parallel. All ReplyIView messages are received in time proportional to the round-trip <lb/>time to the farthest of these superdomains. Hence, total time requirement is proportional to the <lb/>number of candidate paths queried multiplied by the round-trip time to the farthest superdomain <lb/>in these paths. Let msgsize denote the sum of average RequestIView message size and average <lb/></body>

			<note place="footnote">10 Branching factor is 4 for all region classes. <lb/></note>

			<page>22 <lb/></page>

			<body>Scheme <lb/>No query needed Candidate Paths Candidate Superdomains <lb/>child-domains <lb/>220 <lb/>3.31/13 <lb/>7.35/38 <lb/>sibling-domains <lb/>220 <lb/>3/10 <lb/>6.17/22 <lb/>leaf-domains <lb/>219 <lb/>6.31/24 <lb/>15.94/66 <lb/>regions <lb/>544 <lb/>3.70/12 <lb/>7.79/30 <lb/>Table 2: Queries for Internetwork 1. <lb/>ReplyIView message size. The number of candidate superdomains queried times msgsize indicates <lb/>the communication capacity required to ship the RequestIView and ReplyIView messages. <lb/>Table 2 lists for each superdomain scheme the average and maximum number of candidate paths <lb/>and candidate superdomains queried. As apparent from the table, sibling-domains is superior to <lb/>other schemes and leaf-domains is much worse than the rest. This is because in leaf-domains, even <lb/>if only one domain d in a superdomain U is actually going to be crossed, all descendants of U <lb/>containing d may need to be queried to obtain a valid path (e.g. to cross backbone A in Figure 14, <lb/>it may be necessary to query for superdomain A-4, then B-3, then C-2). <lb/>Initial view size <lb/>Merged view size <lb/>Scheme <lb/>in sd-gateways in superdomains in sd-gateways in superdomains <lb/>child-domains <lb/>964/1006 <lb/>42/60 <lb/>1089/1282 <lb/>100/298 <lb/>sibling-domains 1167/1269 <lb/>70/99 <lb/>1470/2190 <lb/>148/337 <lb/>leaf-domains <lb/>963/1006 <lb/>40/60 <lb/>1108/1322 <lb/>130/411 <lb/>regions <lb/>492/715 <lb/>85/163 <lb/>1042/2687 <lb/>158/369 <lb/>Table 3: View sizes for Internetwork 1. <lb/>Table 3 lists for each superdomain scheme the average and maximum of the initial view size <lb/>and of the merged view size. The initial view size indicates the memory requirement at a router <lb/>without using the query protocol (i.e. assuming the initial view has a valid path). The merged view <lb/>size indicates the memory requirement at a router during the query protocol (after nding a valid <lb/></body>

			<page>23 <lb/></page>

			<body>path). The memory requirement at a router is O(view size in number of sd-gateways E G ) where <lb/>E G is the average number of edges of an sd-gateway. Note that the source does not need to store <lb/>information about red and non-transit domains in the merged views (other than the ones already <lb/>in the initial view). The numbers for the merged view sizes in Table 3 take advantage of this. <lb/>As apparent from the table, leaf-domains, child-domains and regions scale better than sibling-<lb/>domains. There are two reasons for this. First, placing a backbone (regional or MAN) domain d as a <lb/>sibling to d-4 (d-3 or d-2) doubles the number of level 4 (3 or 2) superdomains in the views of routers. <lb/>Second, since these domains have many edges to the domains in their associated superdomains, the <lb/>end points of each of these edges become sd-gateways of the associated superdomains. Note that <lb/>regions scales much superior to the other schemes in the initial view size. This is because most <lb/>edges are local (i.e. contained within regions), thus contained completely in superdomains. Hence, <lb/>their end points are not sd-gateways. <lb/>Overall, the child-domains and regions schemes scale best in space, time and communication <lb/>requirements. We have repeated the above evaluations for two other internetworks and obtained <lb/>similar conclusions. The results are in Appendix A. <lb/>8 Related Work <lb/>In this section, we survey recently proposed inter-domain routing protocols that support ToS and <lb/>policy routing for large internetworks. <lb/>Nimrod 6] and IDPR 16] use the link-state approach with domain-level source routing to <lb/>enforce policy and ToS constraints and superdomains to solve scaling problem. Nimrod is still in <lb/>a design stage. Both protocols su er from loss of policy and ToS information as mentioned in the <lb/>introduction. A query protocol for Nimrod is being developed to obtain more detailed policy, ToS <lb/>and topology information. <lb/>BGP 12] and IDRP 14] are based on a path-vector approach 15]. Here, for each destination <lb/>domain a router maintains a set of paths, one through each of its neighbor routers. ToS and policy <lb/>information is attached to these paths. Each router requires O(N D N D E R ) space, where N D <lb/>is the average number of neighbor domains for a domain and N R is the number of routers in the <lb/>internetwork. For each destination, a router exchanges its best valid path with its neighbor routers. <lb/>However, a path-vector algorithm may not nd a valid path from a source to the destination even <lb/></body>

			<page>24 <lb/></page>

			<body>if such a route exists 16] 11 (i.e. detailed ToS and policy information may be lost). By exchanging k <lb/>paths to each destination, the probability of detecting a valid path for each source can be increased. <lb/>But to guarantee detection, either all possible paths should be exchanged (exponential number of <lb/>paths in the worst case) or source policies should be made public and routers should take this into <lb/>account when exchanging routes. However, this x increases space and communication requirements <lb/>drastically. <lb/>IDRP 14] uses superdomains to solve the scaling problem. It exchanges all paths between <lb/>neighbor routers subject to the following constraint: a router does not inform a neighbor router <lb/>of a route if usage of the route by the neighbor would violate some superdomain&apos;s constraint on <lb/>the route. IDRP also su ers from loss of ToS and policy information. To overcome this problem, <lb/>it uses overlapping superdomains: that is, a domain and superdomain can be in more than one <lb/>parent superdomain. If a valid path over a domain can not be discovered because the constraints <lb/>of a parent superdomain are violated, the same path may be discovered through another parent <lb/>superdomain whose constraints are not violated. However, handling ToS and policy constraints <lb/>in general requires more and more combinations of overlapping superdomains, resulting in more <lb/>storage requirement. <lb/>Reference 9] combines the bene ts of path-vector approach and link-state approach by having <lb/>two modes: An NR mode, which is an extension of IDRP and is used for the most common ToS <lb/>and policy constraints; and a SDR mode, which is like IDPR and is used for less frequent ToS and <lb/>policy requests. This study does not address the scalability of the SDR mode. Ongoing work by <lb/>this group considers a new SDR mode which is not based on IDPR. <lb/>Reference 19] suggests the use of multiple addresses for each node, one for each ToS and Policy. <lb/>This scheme does not scale up. In fact, it increases the storage requirement, since a router maintains <lb/>a route for each destination address, and there are more addresses with this scheme. <lb/>The landmark hierarchy 18, 17] is another approach for solving scaling problem. Here, each <lb/>router is a landmark with a radius, and routers which are at most radius away from the landmark <lb/>maintain a route for it. Landmarks are organized hierarchically, such that radius of a landmark <lb/>increases with its level, and the radii of top level landmarks include all routers. Addressing and <lb/></body>

			<note place="footnote">11 For example, suppose a router u has two paths P1 and P2 to the destination. Let u have a router neighbor v, <lb/>which is in another domain. u chooses and informs v of one of the paths, say P1. But P1 may violate source policies <lb/>of v&apos;s domain, and P2 may be a valid path for v. <lb/></note>

			<page>25 <lb/></page>

			<body>packet forwarding schemes are introduced. Link-state algorithms can not be used with the landmark <lb/>hierarchy, and a thorough study of enforcing ToS and policy constraints with this hierarchy has <lb/>not been done. <lb/>In 1], we provided an alternative solution to loss of policy and ToS information that is perhaps <lb/>more faithful to the original superdomain hierarchy. To handle superdomain-level source routing <lb/>and topology changes, we augmented each superdomain-level edge (U; V ) with the address of an <lb/>\exit&quot; domain u in U and an \entry&quot; domain v in V . To obtain internal views, we added for <lb/>each visible superdomain U the edges from U to domains outside the parent of U. Surprisingly, <lb/>this approach and the gateway-level view approach have the same memory and communication <lb/>requirements. However, the rst approach results in much more complicated protocols. <lb/>Reference 2] presents interdomain routing protocols based on a new kind of hierarchy, referred <lb/>to as the viewserver hierarchy. This approach also scales well to large internetworks and does <lb/>not lose detail ToS and policy information. Here, special routers called viewservers maintain the <lb/>view of domains in a surrounding precinct. Viewservers are organized hierarchically such that <lb/>for each viewserver, there is a domain of a lower level viewserver in its view, and views of top <lb/>level viewservers include domains of other top level viewservers. Appropriate addressing and route <lb/>discovery schemes are introduced. <lb/>9 Conclusion <lb/>We presented a hierarchical inter-domain routing protocol which satis es policy and ToS con-<lb/>straints, adapts to dynamic topology changes including failures that partition domains, and scales <lb/>well to large number of domains. <lb/>Our protocol achieves scaling in space requirement by using superdomains. Our protocol main-<lb/>tains superdomain-level views with sd-gateways and handles topology changes by using a link-state <lb/>view update protocol. It achieves scaling in communication requirement by ooding topology <lb/>changes a ecting a superdomain U over U&apos;s parent superdomain. <lb/>Our protocol does not lose detail in ToS, policy and topology information. It stores both a <lb/>strong set of constraints and a weak set of constraints for each visible superdomain. If the weak <lb/>constraints but not the strong constraints of a superdomain U are satis ed (i.e. the aggregation has <lb/>resulted in loss of detail in ToS and policy information), then some paths through U may be valid. <lb/></body>

			<page>26 <lb/></page>

			<body>Our protocol uses a query protocol to obtain a more detailed \internal&quot; view of such superdomains, <lb/>and searches again for a valid path. Our evaluation results indicate that the query protocol can be <lb/>performed using 15% extra space. <lb/>One drawback of our protocols is that to obtain a source route, views are merged at or prior <lb/>to the connection setup, thereby increasing the setup time. This drawback is not unique to our <lb/>scheme 7, 16, 6, 9]. There are several ways to reduce this setup overhead. First, source routes <lb/>to frequently used destinations can be cached. Second, the internal views of frequently queried <lb/>superdomains can be cached at routers close to the source domain. Third, better heuristics to <lb/>choose candidate paths and candidate superdomains to query can be developed. <lb/>We also described an evaluation model for inter-domain routing protocols. This model can be <lb/>applied to other inter-domain routing protocols. We have not done so because precise de nitions of <lb/>the hierarchies in these protocols are not available. For example, to do a fair evaluation of IDPR 16], <lb/>we need precise guidelines for how to group domains into superdomains, and how to choose between <lb/>the strong and weak methods when de ning policy/ToS constraints of superdomains. In fact, these <lb/>protocols have not been evaluated in a way that we can compare them to the superdomain hierarchy. <lb/></body>

			<listBibl>References <lb/>1] C. Alaettino glu and A. U. Shankar. Hierarchical Inter-Domain Routing Protocol with On-Demand <lb/>ToS and Poicy Resolution. In Proc. IEEE International Conference on Networking Protocols &apos;93, San <lb/>Fransisco, California, October 1993. <lb/>2] C. Alaettino glu and A. U. Shankar. Viewserver Hierarchy: A New Inter-Domain Routing Protocol and <lb/>its Evaluation. Technical Report UMIACS-TR-93-98, CS-TR-3151, Department of Computer Science, <lb/>University of Maryland, College Park, October 1993. Earlier version CS-TR-3033, February 1993. <lb/>3] C. Alaettino glu and A. U. Shankar. Viewserver Hierarchy: A New Inter-Domain Routing Protocol. In <lb/>Proc. IEEE INFOCOM &apos;94, Toronto, Canada, June 1994. To appear. <lb/>4] A. Bar-Noy and M. Gopal. Topology Distribution Cost vs. E cient Routing in Large Networks. In <lb/>Proc. ACM SIGCOMM &apos;90, pages 242{252, Philadelphia, Pennsylvania, September 1990. <lb/>5] L. Breslau and D. Estrin. Design of Inter{Administrative Domain Routing Protocols. In Proc. ACM <lb/>SIGCOMM &apos;90, pages 231{241, Philadelphia, Pennsylvania, September 1990. <lb/>6] I. Castineyra, J. N. Chiappa, C. Lynn, R. Ramanathan, and M. Steenstrup. The Nimrod Routing Archi-<lb/>tecture. Internet Draft., March 1994. Available by anonymous ftp from research.ftp.com:pub/nimrod. <lb/>7] D.D. Clark. Policy routing in Internet protocols. Request for Comment RFC-1102, Network Information <lb/>Center, May 1989. <lb/>8] D. Estrin. Policy requirements for inter Administrative Domain routing. Request for Comment RFC-<lb/>1125, Network Information Center, November 1989. <lb/></listBibl>

			<page>27 <lb/></page>

			<listBibl>9] D. Estrin, Y. Rekhter, and S. Hotz. Scalable Inter-Domain Routing Architecture. In Proc. ACM <lb/>SIGCOMM &apos;92, pages 40{52, Baltimore, Maryland, August 1992. <lb/>10] L. Kleinrock and F. Kamoun. Hierarchical Routing for Large Networks. Computer Networks and ISDN <lb/>Systems, (1):155{174, 1977. <lb/>11] B.M. Leiner. Policy issues in interconnecting networks. Request for Comment RFC-1124, Network <lb/>Information Center, September 1989. <lb/>12] K. Lougheed and Y. Rekhter. Border Gateway Protocol (BGP). Request for Comment RFC-1105, <lb/>Network Information Center, June 1989. <lb/>13] R. Perlman. Hierarchical Networks and Subnetwork Partition Problem. Computer Networks and ISDN <lb/>Systems, 9:297{303, 1985. <lb/>14] Y. Rekhter. Inter-Domain Routing Protocol (IDRP). Available from the author., 1992. T.J. Watson <lb/>Research Center, IBM Corp. <lb/>15] K. G. Shin and M. Chen. Performance Analysis of Distributed Routing Strategies Free of Ping-Pong-<lb/>Type Looping. IEEE Transactions on Computers, 1987. <lb/>16] M. Steenstrup. An Architecture for Inter-Domain Policy Routing. Request for Comment RFC-1478, <lb/>Network Information Center, July 1993. <lb/>17] P. F. Tsuchiya. The Landmark Hierarchy: Description and Analysis, The Landmark Routing: Ar-<lb/>chitecture Algorithms and Issues. Technical Report MTR-87W00152, MTR-87W00174, The MITRE <lb/>Corporation, McLean, Virginia, 1987. <lb/>18] P. F. Tsuchiya. The Landmark Hierarchy:A New Hierarchy For Routing In Very Large Networks. In <lb/>Proc. ACM SIGCOMM &apos;88, August 1988. <lb/>19] P. F. Tsuchiya. E cient and Robust Policy Routing Using Multiple Hierarchical Addresses. In Proc. <lb/>ACM SIGCOMM &apos;91, pages 53{65, Zurich, Switzerland, September 1991. <lb/></listBibl>

			<div type="annex">A Results for Other Internetworks <lb/>Results for Internetwork 2 <lb/>The parameters of the second internetwork topology, referred to as Internetwork 2, are the same as <lb/>the parameters of Internetwork 1 but a di erent seed is used for the random number generation. <lb/>Our evaluation measures were computed for a set of 100,000 source-destination pairs. The <lb/>minimum spl of these pairs was 1, the maximum spl was 14, and the average spl was 7.13. <lb/>Table 5 and Table 4 shows the results. Similar conclusions as in the case of Internetwork 1 hold. <lb/>Results for Internetwork 3 <lb/>The parameters of the third internetwork topology, referred to as Internetwork 3, are shown in <lb/>Table 6. Internetwork 3 is more connected, more class 0, 1 and 2 domains are green, and more <lb/>class 3 domains are red. Hence, we expect bigger view sizes in number of sd-gateways. <lb/></div>

			<page>28 <lb/></page>

			<div type="annex">Scheme <lb/>No query needed Candidate Paths Candidate Superdomains <lb/>child-domains <lb/>205 <lb/>4.52/20 <lb/>10.22/47 <lb/>sibling-domains <lb/>205 <lb/>3.01/8 <lb/>6.50/21 <lb/>leaf-domains <lb/>205 <lb/>8.80/32 <lb/>21.34/82 <lb/>regions <lb/>640 <lb/>3.52/10 <lb/>7.85/28 <lb/>Table 4: Queries for Internetwork 2. <lb/>Initial view size <lb/>Merged view size <lb/>Scheme <lb/>in sd-gateways in superdomains in sd-gateways in superdomains <lb/>child-domains <lb/>958/1012 <lb/>43/60 <lb/>1079/1269 <lb/>118/306 <lb/>sibling-domains 1153/1283 <lb/>72/101 <lb/>1480/2169 <lb/>160/324 <lb/>leaf-domains <lb/>956/1009 <lb/>41/58 <lb/>1095/1281 <lb/>156/387 <lb/>regions <lb/>624/1024 <lb/>110/231 <lb/>1356/3578 <lb/>206/435 <lb/>Table 5: View sizes for Internetwork 2. <lb/>Our evaluation measures were computed for a set of 100,000 source-destination pairs. The <lb/>minimum spl of these pairs was 1, the maximum spl was 11, and the average spl was 5.95. <lb/>Table 8 and Table 7 shows the results. Similar conclusions as in the cases of Internetwork 1 <lb/>and 2 hold. <lb/></div>

			<note place="footnote">12 Branching factor is 4 for all domain classes. <lb/></note>

			<page>29 <lb/></page>

			<div type="annex">Class i No. of Domains No. of Regions 12 % of Green Domains Edges between Classes i and j <lb/>Class j Local Remote Far <lb/>0 <lb/>10 <lb/>4 <lb/>0.85 <lb/>0 <lb/>8 <lb/>7 <lb/>1 <lb/>100 <lb/>16 <lb/>0.80 <lb/>0 <lb/>190 <lb/>20 <lb/>1 <lb/>50 <lb/>20 <lb/>2 <lb/>1000 <lb/>64 <lb/>0.75 <lb/>0 <lb/>500 <lb/>50 <lb/>1 <lb/>1200 <lb/>100 <lb/>2 <lb/>200 <lb/>40 <lb/>3 <lb/>10000 <lb/>256 <lb/>0.10 <lb/>0 <lb/>300 <lb/>50 <lb/>1 <lb/>250 <lb/>100 <lb/>2 <lb/>10250 150 <lb/>50 <lb/>3 <lb/>200 <lb/>150 100 <lb/>Table 6: Parameters of Internetwork 3. <lb/>Scheme <lb/>No query needed Candidate Paths Candidate Superdomains <lb/>child-domains <lb/>142 <lb/>3.99/29 <lb/>7.70/43 <lb/>sibling-domains <lb/>142 <lb/>2.95/10 <lb/>5.39/22 <lb/>leaf-domains <lb/>142 <lb/>9.65/70 <lb/>18.99/103 <lb/>regions <lb/>676 <lb/>3.47/17 <lb/>6.25/21 <lb/>Table 7: Queries for Internetwork 3. <lb/></div>

			<page>30 <lb/></page>

			<div type="annex">Initial view size <lb/>Merged view size <lb/>Scheme <lb/>in sd-gateways in superdomains in sd-gateways in superdomains <lb/>child-domains <lb/>2160/2239 <lb/>43/60 <lb/>2354/2647 <lb/>107/348 <lb/>sibling-domains 2365/2504 <lb/>72/101 <lb/>2606/3314 <lb/>148/356 <lb/>leaf-domains <lb/>2159/2236 <lb/>41/58 <lb/>2386/2645 <lb/>160/648 <lb/>regions <lb/>1107/1644 <lb/>110/231 <lb/>1850/3559 <lb/>194/436 <lb/>Table 8: View sizes for Internetwork 3. <lb/></div>

			<page>31 <lb/></page>

			<div type="annex">Variables: <lb/>V iew x : Dynamic view of x. <lb/>WV iew x (d address): Temporary view of x. d address is the destination address. <lb/>Used for merging internal views of superdomains to the view of x. <lb/>PendingReq x (d address): Integer. d address is the destination address. <lb/>Number of outstanding request messages. <lb/> Events: <lb/>Request x (d address) <lb/>fExecuted when x wants a valid domain-level source routeg <lb/>allocate WV iew x (d address) := V iew x ; allocate PendingReq x (d address) := 0; <lb/>search x (d address); <lb/>where <lb/>search x (d address) <lb/>if there is a valid path to d address in WV iew x (d address) then <lb/>result := shortest valid path; <lb/>deallocate WV iew x (d address); PendingReq x (d address); <lb/>return result; <lb/>else if there is a candidate path to d address in WV iew x (d address) then <lb/>Let cpath = hU 0 :g 00 ; . . .; U 0 :g 0n 0 ; U 1 :g 10 ; . . .; U 1 :g 1n 1 ; ; U m :g m0 ; . . .; U m :g mn m i <lb/>be the shortest candidate path; <lb/>for U i in cpath such that U i is candidate do <lb/>ReliableSend(RequestIView; U i ; g i0 ; address(x); d address) to g i0 <lb/>PendingReq x (d address) := PendingReq x (d address) + 1; <lb/>else <lb/>deallocate WV iew x (d address); PendingReq x (d address); <lb/>return failure; <lb/>endif <lb/>endif <lb/>TimeOut x (d address) <lb/>fExecuted after a time-out period and PendingReq x (d address) 6 = 0.g <lb/>deallocate WV iew x (d address); PendingReq x (d address); <lb/>return failure; <lb/>Figure 15: view-query protocol: State and events of a router x. (Figure continued on next page.) <lb/></div>

			<page>32 <lb/></page>

			<div type="annex">Receive x (RequestIView; sdid; x; s address; d address) <lb/>ReliableSend(ReplyIView; sdid; x; IView x (U); d address) to s address; <lb/>Receive x (ReplyIView; sdid; gid; iview; d address) <lb/>if PendingReq x (d address) 6 = 0 then <lb/>fNo time-out happenedg <lb/>PendingReq x (d address) := PendingReq x (d address) ? 1; <lb/>fmerge internal viewg <lb/>delete hsdid; ; ; i from WV iew x ; <lb/>for hchild; scons; wcons; gateway-seti in iview do <lb/>if :9hchild; ; ; i 2 WV iew x then <lb/>insert hchild; scons; wcons; gateway-seti in WV iew x ; <lb/>else <lb/>for hgid; ts; edge-seti in gateway-set do <lb/>if 9hgid; timestamp; i 2 Gateways&amp;Edges x (child)^ts &gt; timestamp then <lb/>delete hgid; ; i from Gateways&amp;Edges x (child); <lb/>endif; <lb/>if :9hgid; ; i 2 Gateways&amp;Edges x (child) then <lb/>insert hgid; ts; edge-seti to Gateways&amp;Edges x (child); <lb/>endif <lb/>endif <lb/>if PendingReq x (d address) = 0 then <lb/>fAll pending replies are receivedg <lb/>search x (d address); <lb/>endif <lb/>endif <lb/>Figure 15: view-query protocol: State and events of a router x. (cont.) <lb/></div>

			<page>33 <lb/></page>

			<div type="annex">Constants: <lb/>AdjLocalRouters x : ( NodeIds). Set of neighbor routers in x&apos;s domain. <lb/>AdjForeignGateways x : ( NodeIds). Set of neighbor routers in other domains. <lb/>Ancestor i (x): ( SuperDomainIds). ith ancestor of x. <lb/>Variables: <lb/>V iew x : Dynamic view of x. <lb/>IntraDomainRT x : Intra-domain routing table of x. Initially contains no entries. <lb/>Clock x : Integer. Clock of x. <lb/>Events: <lb/>Receive x (Update; sdid; gid; ts; edge-set) from sender <lb/>if 9hgid; timestamp; i 2 Gateways&amp;Edges x (sdid)^ts &gt; timestamp then <lb/>delete hgid; ; i from Gateways&amp;Edges x (sdid); <lb/>endif; <lb/>if :9hgid; ; i 2 Gateways&amp;Edges x (sdid) then <lb/>flood x ((Update; sdid; gid; ts; edge-set)); <lb/>insert hgid; ts; edge-seti to Gateways&amp;Edges x (sdid); <lb/>update parent domains x (level(sdid) + 1); <lb/>endif <lb/>where <lb/>update parent domains x (startinglevel) <lb/>for level := startinglevel to number of levels in the hierarchy do <lb/>sdid := Ancestor level (x); <lb/>if x 2 Gateways(sdid) then <lb/>edge-set := aggregate edges of sdid:x using V iew x ; IntraDomainRT x and links of x; <lb/>timestamp = Clock x ; <lb/>flood x ((Update; sdid; x; timestamp; edge-set)); <lb/>delete hx; ; i from Gateways&amp;Edges x (sdid); <lb/>insert hx; timestamp; edge-seti to Gateways&amp;Edges x (sdid); <lb/>endif <lb/>Do Update x <lb/>fExecuted periodically and upon a change in IntraDomainRT x or links of xg <lb/>update parent domains x (1) <lb/>Link Recovery x (y) <lb/>fhx; yi is a link. Executed when hx; yi recovers.g <lb/>for all hsdid; ; ; i in V iew x do <lb/>if 9i : Ancestor i (y) = Ancestor 1 (sdid) then <lb/>for all hgid; timestamp; edge-seti in Gateways&amp;Edges x (sdid) do <lb/>Send((Update; sdid; gid; timestamp; edge-set)) to y; <lb/>endif <lb/>flood x (packet) <lb/>for all y 2 AdjLocalRouters x do <lb/>Send(packet) to y; <lb/>for all y 2 AdjForeignGateways x^9 i : Ancestor i (y) = Ancestor 1 (packet:sdid) do <lb/>Send(packet) to y; <lb/>Figure 16: view-update protocol: State and events of a router x. <lb/></div>

			<page>34 </page>


	</text>
</tei>
