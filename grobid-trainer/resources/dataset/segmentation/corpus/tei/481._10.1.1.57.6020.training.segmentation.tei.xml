<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<titlePage>A Survey of Collective Communication in <lb/>Wormhole-Routed Massively Parallel Computers <lb/>Philip K. McKinley, Yih-jia Tsai, and David F. Robinson <lb/>Technical Report <lb/>MSU-CPS-94-35 <lb/>June 1994 <lb/>Communications Research Group <lb/>Michigan State University <lb/>Submitted for publication, June 1994. <lb/></titlePage>

			<front>A Survey of Collective Communication in <lb/>Wormhole-Routed Massively Parallel Computers <lb/>Philip K. McKinley, Yih-jia Tsai, and David F. Robinson <lb/>Department of Computer Science <lb/>Michigan State University <lb/>East Lansing, Michigan 48824 <lb/>fmckinley, tsaiyi, robinsodg@cps.msu.edu <lb/>June 1994 <lb/>Abstract <lb/>Massively parallel computers (MPC) are characterized by the distribution of memory <lb/>among an ensemble of nodes. Since memory is physically distributed, MPC nodes com-<lb/>municate by sending data through a network. In order to program an MPC, the user may <lb/>directly invoke low-level message passing primitives, may use a higher-level communications <lb/>library, or may write the program in a data parallel language and rely on the compiler to <lb/>translate language constructs into communication operations. Whichever method is used, <lb/>the performance of communication operations directly a ects the total computation time <lb/>of the parallel application. Communication operations may be either point-to-point, which <lb/>involves a single source and a single destination, or collective, in which more than two <lb/>processes participate. <lb/>This paper discusses the design of collective communication operations for current <lb/>systems that use the wormhole routing switching strategy, in which messages are divided <lb/>into small pieces and pipelined through the network. Compared to the store-and-forward <lb/>switching method that was used in early multicomputers, wormhole routing often reduces <lb/>the e ect of path length on communication latency. Over the past several years, a number <lb/>of researchers have exploited this property in the design of new collective communication <lb/>algorithms, which di er fundamentally from their store-and-forward predecessors. This <lb/>paper discusses the signi cant issues involved in wormhole-routed collective communication <lb/>and presents the major classes of solutions that have been proposed to address the problem. <lb/>Keywords: Collective communication, parallel processing, wormhole routing, distributed <lb/>memory, massively parallel computers. <lb/>This work was supported in part by the NSF grants MIP-9204066, CDA-9121641, CDA9222901, by DOE grant <lb/>DE-FG02-93ER25167, and by an Ameritech Faculty Fellowship. <lb/></front>

			<page>ii <lb/></page>

			<body>1 Introduction <lb/>A recent trend in supercomputing has been towards the use of parallel processing to solve <lb/>computationally-intensive problems. Several so-called scalable parallel architectures, which o er cor-<lb/>responding increases in performance as the number of processors is increased, have been designed <lb/>in the last few years. Massively parallel computers (MPCs) are characterized by the distribution of <lb/>memory among an ensemble of computing nodes. Many such systems interconnect nodes through a <lb/>direct network, in which each node has a connection to a set of other nodes, called neighbors. <lb/>Since memory is physically distributed, MPC nodes communicate by sending messages through the <lb/>network. Historically, the programmer of a distributed-memory system has invoked various system <lb/>primitives to send messages among processes executing on di erent nodes, resulting in a message-<lb/>passing program. While such low-level control over communication allows the user to exploit charac-<lb/>teristics of the architecture, this type of programming is often tedious and error-prone. Furthermore, <lb/>parallel software development has long been plagued by the large variety of parallel architectures <lb/>available; such diversity often has implied that a new version of a particular algorithm had to be <lb/>developed for each new architecture. One method that has been used to address these problems is <lb/>to construct communication libraries 1], which hide the details of the underlying architecture and <lb/>vendor-speci c interfaces from the user but provide a common interface across multiple platforms, <lb/>permitting user code to be more easily ported among machines. In order to further simplify the <lb/>programmer&apos;s task and improve code portability, an alternative approach to parallel programming is <lb/>to use a data parallel language, such as High Performance Fortran (HPF) 2], which provides the user <lb/>with control over data alignment and realignment, but which hides the communication calls from the <lb/>user. For a distributed-memory system, the compiler for such a language must translate high-level <lb/>data parallel language constructs into appropriate low-level communication primitives 3, 4]. <lb/>Whether communication operations are programmed by the user, contained in a library, or gener-<lb/>ated by a compiler, their latency directly a ects the total computation time of the parallel application. <lb/>Communication operations may be either point-to-point, which involve a single source and a single <lb/>destination, or collective, in which more than two processes participate. Collective communication <lb/>operations are particularly important in scienti c computing, where large data arrays are typically <lb/>partitioned and distributed over the local memories of the nodes that are executing the program. In <lb/>such applications, nodes use collective operations to distribute, gather, and exchange data, to per-<lb/>form global compute operations on distributed data, and to synchronize with one another at speci c <lb/>points in program ow. The growing interest in collective operations is evidenced by their inclusion <lb/>in Message Passing Interface (MPI) 5], an emerging standard for communication routines used by <lb/>message-passing programs, and by their increasing role in supporting various programming constructs <lb/>in HPF 4]. A set of standard collective operations is reviewed in Section 2. <lb/>E cient implementation of collective communication operations depends on the underlying ar-<lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>1 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>chitecture of the MPC. While there has been little consensus on some aspects of communication <lb/>architectures, such as network topology, there has been a good deal of agreement on the way in which <lb/>messages are switched through the network. Speci cally, many new generation MPCs employ wormhole <lb/>routing 6], where each message is divided into small pieces that are pipelined through the network <lb/>by way of routers at each node. Compared to the store-and-forward switching method that was used <lb/>in early multicomputers, wormhole routing often reduces the e ect of path length on communication <lb/>latency 7]. In the absence of channel contention, which occurs when two messages simultaneously <lb/>require the same channel, the measured latency of a wormhole-routed message has been shown to <lb/>be nearly independent of the distance between the source and destination nodes 7]. However, in <lb/>situations where multiple messages exist in the network concurrently, channel contention among those <lb/>messages may be exacerbated by the use of wormhole routing, in which blocked messages hold some <lb/>communication channels while waiting for other messages. The invocation of a collective operation, <lb/>whose implementation may involve many messages, poses precisely such a situation. <lb/>This paper addresses the design of collective communication operations for wormhole-routed net-<lb/>works. Ni and McKinley 7] previously surveyed wormhole routing techniques for direct networks; that <lb/>paper focused on switching architectures, performance comparisons with other switching strategies, <lb/>deadlock prevention methods, and adaptive routing algorithms. In the years since that paper was <lb/>originally written, a relatively large body of work has been published in the area of collective commu-<lb/>nication algorithms for wormhole-routed systems. Most notably, many new collective algorithms have <lb/>been designed that exploit the relative distance-insensitivity of wormhole routing, di ering fundamen-<lb/>tally from their store-and-forward counterparts. Other architectural properties, such as the network <lb/>topology and the number of ports connecting each node to the network, are also important to their <lb/>design and performance. This paper is intended to present the main issues involved in that research <lb/>and to describe the major classes of solutions that have been proposed. It is not, however, intended <lb/>to be an exhaustive survey of the literature. <lb/>Following a review of collective communication operations, we brie y describe wormhole-routed <lb/>architectures, focusing on those architectural characteristics that are particularly important to col-<lb/>lective communication. We then devote a signi cant amount of discussion to the implementation <lb/>of collective communication operations in software. These approaches are designed for systems that <lb/>support only point-to-point, or unicast, communication in hardware. In these environments, collective <lb/>operations must be implemented by sending multiple unicast messages; such implementations are <lb/>called unicast-based 8]. We rst describe the implementation of tree-like arrangements of messages <lb/>in various network topologies; these structures are used in several of the \elementary&quot; collective op-<lb/>erations, such as broadcast and global compute operations. We next discuss the so-called \all-to-all&quot; <lb/>collective operations, in which many nodes are both sources and recipients of data. Finally, we describe <lb/>how some collective operations may be supported directly in hardware. We conclude the paper with <lb/>the discussion of several open issues in this area of research. <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>2 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>2 Collective Communication Operations <lb/>Collective operations are usually de ned in terms of a group of processes. The operation is executed <lb/>by having all processes in the group call the communication routine with matching parameters. The <lb/>group, which may constitute all or a subset of the processes in the parallel application, is assumed to <lb/>have been previously de ned and is identi ed in one of the parameters to the collective operation 5]. <lb/>Collective operations can operate synchronously or asynchronously, depending on whether a calling <lb/>process can return before other processes in the group have called the routine. <lb/>De nitions of Operations. Collective operations may be used for process control, data movement, <lb/>or global operations; examples are listed in Table 1. Data movement operations include broadcast, in <lb/>which one process sends the same message to all other group members; scatter, in which one process <lb/>sends a di erent message to each of a set of destinations; and gather, in which one process receives a <lb/>message from each of a group of processes. These basic operations can be extended and combined to <lb/>form more complex operations. In all-to-all broadcast, every process sends a message to all members <lb/>of the group. In all-to-all scatter-gather, also referred to as complete exchange, every member of a <lb/>group sends di erent data to every other node in the group. Barrier synchronization de nes a logical <lb/>point in the control ow of an algorithm at which all the members of the group must arrive before <lb/>any of the processes in the subset is allowed to proceed further. <lb/>Table 1. Examples of collective communication primitives <lb/>Category <lb/>Primitive <lb/>Description <lb/>broadcast <lb/>one member sends same message to all members <lb/>scatter <lb/>one member sends di erent message to each member <lb/>data movement gather <lb/>every member sends a message to a single member <lb/>all-to-all broadcast <lb/>every member performs a broadcast <lb/>all-to-all scatter-gather every member performing scatter <lb/>process control barrier syncrhonization all members must reach point before any can proceed <lb/>global operation reduction <lb/>perform a global operation on distributed data <lb/>scan (parallel pre x) <lb/>\partial&quot; reduction based upon relative process number <lb/>Global compute operations include both reduction and scan (also known as parallel pre x). In <lb/>reduction, an associative and commutative operation is applied across data items from each member <lb/>of the group. Example operations include sum, max, min, bitwise operations, and so on. In an <lb/>N=1 reduction operation, the resultant data resides at a single process, called the root. In an N=N <lb/>reduction operation, every process involved in the operation obtains a copy of the reduced data. <lb/>In scan operations, given processes P 0 ; P 1 ; : : :; P n and data items d 0 ; d 1 ; : : :; d n , an associative and <lb/>commutative operator ` &apos; is applied such that the result at process p i is d 0 d 1 : : : d i . <lb/>Figure 1 depicts examples of collective operations among a group of four processes; the actual <lb/>messages sent between processes may di er from those shown, depending on the underlying algorithm <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>3 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>and computing platform. Figures 1(a) through 1(e), which depict the data movement operations, <lb/>are self-explanatory. Figure 1(f) illustrates the execution of barrier synchronization in a distributed <lb/>memory environment. In this particular implementation, process P0 plays the role of a barrier process. <lb/>In the rst phase of the operation, each process that reaches the barrier sends a message indicating this <lb/>fact to process P0. As soon as P0 has received messages from all the other processes, it broadcasts a <lb/>message to the group, indicating to each member that all processes have reached the barrier and that <lb/>they may proceed. Figure 1(g) shows a special case of reduction (with a generic operator, denoted by <lb/>` &apos;) in which the result R of the operation resides at a single process, in this case P0. In other cases, <lb/>the result may be distributed to some or all of the processes involved. A scan operation, using the <lb/>same operator ` &apos;, is shown in Figure 1(h). <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>a0 <lb/>a0 <lb/>a0 <lb/>(a) broadcast <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>a0 <lb/>b0 <lb/>c0 <lb/>(b) scatter <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>a1 <lb/>a2 <lb/>a3 <lb/>(c) gather <lb/>a1 <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>a0 <lb/>a0 <lb/>a0 <lb/>a1 <lb/>a1 <lb/>a2 <lb/>a2 <lb/>a2 <lb/>a3 <lb/>a3 <lb/>a3 <lb/>(d) all−to all broadcast <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>[1] <lb/>[2] <lb/>[1] <lb/>[1] <lb/>[2] <lb/>[2] <lb/>(f) barrier sync. <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>a1 <lb/>a2 <lb/>a3 <lb/>R: (a0*a1*a2*a3) <lb/>(g) reduction (4/1) <lb/>a1 <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>a0 <lb/>a0 <lb/>a0 <lb/>a1 <lb/>a2 <lb/>R: a0 <lb/>R: (a0*a1) <lb/>R: (a0*a1*a2) <lb/>R: (a0*a1*a2*a3) <lb/>(h) scan <lb/>P1 <lb/>P3 <lb/>P2 <lb/>P0 <lb/>a0 <lb/>a2 <lb/>a3 <lb/>b0 <lb/>c0 <lb/>b1 <lb/>c1 <lb/>d1 <lb/>d2 <lb/>d3 <lb/>c2 <lb/>b3 <lb/>(e) all−to−all <lb/>scatter−gather <lb/>Figure 1. Semantics of various collective operations among four processes <lb/>It is important to distinguish between the process view and the node view of a collective operation. <lb/>For example, when one node in a network sends a message to a proper subset of the nodes, this is <lb/>usually referred to as multicast 8]. However, the MPI standard does not explicitly discuss multicast. <lb/>Rather, MPI would describe multicast as a broadcast to a group of processes that happen to reside <lb/>on only a proper subset of the nodes in the network. While de ning collective operations in terms of <lb/>process groups is very useful for studying their semantics, it is less appropriate for the study of their <lb/>performance, which depends on the physical relationships between the group members and the system <lb/>architecture. For example, the number of processes per node and the distribution of processes in the <lb/>network a ect the performance of collective operations executed within that group. Therefore, in this <lb/>paper, we will discuss collective implementations in terms of the physical network architecture and <lb/>the speci c messages that constitute the operation. <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>4 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>Uses of Collective Operations. Collective communication is used in all three major methods used <lb/>to program distributed-memory computers: message-passing, translation of shared-memory code to <lb/>message-passing code, and execution of shared-memory code atop distributed shared-memory. <lb/>Collective communication operations are found in many algorithms designed for message-passing <lb/>systems. In fact, it was their frequent use that led to their inclusion in several commercial commu-<lb/>nications libraries, and eventually to the standardization of their syntax and semantics in MPI 5]. <lb/>Collective operations are used in numerous sorting, graph, and search algorithms 9]. Perhaps the <lb/>largest class of message-passing applications that can take advantage of e cient collective operations <lb/>is parallel numerical algorithms. Collective operations are used in a wide variety of matrix-related <lb/>algorithms, including solving of linear systems, nding eigenvalues, and performing transform oper-<lb/>ations 9]. Many of these numerical algorithms have themselves been organized into libraries. For <lb/>example, the ScaLAPACK project 10] is targeted to producing a distributed-memory version of LA-<lb/>PACK, a popular sequential library for problems in numerical linear algebra. In order to improve <lb/>portability, ScaLAPACK uses a library called BLACS (Basic Linear Algebra Communication Subpro-<lb/>grams) 11], which provides basic matrix-related communications operations. In turn, BLACS can be <lb/>implemented using architecture-speci c implementations of collective operations. <lb/>In spite of the presence of communication libraries and numerical libraries, many users prefer <lb/>a shared-memory programming paradigm to a message-passing paradigm. A compiler translates a <lb/>program written in a data parallel language into lower-level communication operations. Li and Chen 3] <lb/>have studied this translation process. The communication operations generated in their approach <lb/>include all those listed in Table 1, as well as others, such as permutation. More recently, a coalition <lb/>of industrial and academic groups has standardized High Performance Fortran 2], which is designed <lb/>for distributed-memory platforms. When an HPF program is compiled, many of those communication <lb/>operations generated by the compiler may be collective in nature. For example, HPF contains so-<lb/>called intrinsics, which perform reduction and scan, rearranging and reshaping, and scatter and gather <lb/>operations on data arrays. HPF also allows dynamic redistribution of arrays which, depending on their <lb/>present distribution, may involve any of broadcast, scatter, gather, and their all-to-all counterparts. <lb/>Furthermore, since entire arrays may be operated on without explicitly looping across the elements, <lb/>even a statement as common as A = B + C, where A, B, and C are arrays, may produce underlying <lb/>reduction and scatter operations when compiled and executed. <lb/>Finally, it is possible to execute a shared-memory program directly atop a distributed-memory <lb/>platform, so long as the system dynamically translates remote memory accesses into communication <lb/>operations. The distributed shared memory paradigm provides a virtual address space that is shared <lb/>among processes located on processing nodes with their own local memories. In order to improve the <lb/>performance of local reads, most systems allow data to be replicated across multiple nodes, which <lb/>implies that the system must take certain measures in order to maintain memory coherence. A wide <lb/>variety of coherence protocols have been proposed 12]; many of them can be classi ed as either write-<lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>5 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>update or write-invalidate. Both approaches rely on broadcast to send invalidations and updates, <lb/>respectively. In addition to being able to access shared memory, the other main form of interaction <lb/>among processes is synchronization, such as barriers and semaphores. Again, e cient collective com-<lb/>munication operations can be used to implement these constructs so that their overhead is minimized, <lb/>thereby improving performance. <lb/>3 Architectural Issues <lb/>Varying levels of support for collective operations may be provided in an MPC. In some systems, <lb/>certain collective operations are supported directly in hardware, in which single dedicated instructions <lb/>may be invoked by a process executing on a node. Many existing MPCs, however, support only <lb/>unicast communication in hardware. In these environments, all communication operations must be <lb/>implemented in software by sending one or more unicast messages; such implementations are called <lb/>unicast-based 8]. Finally, operations may be partially supported in hardware. For example, a sys-<lb/>tem may provide some elementary collective operations, such as multicast, in hardware, with several <lb/>instances combined in software to implement a more complex operations. Whether implemented in <lb/>hardware, software, or a combination of the two, the design and performance of collective operations <lb/>is in uenced by several characteristics of the system, as described below. <lb/>Switching and Network Latency. One of the most important architectural characteristics, which <lb/>a ects all types of communication, is the switching strategy, which determines how data is removed <lb/>from one channel and placed on another channel along the path from source to destination. In store-<lb/>and-forward switching, which was used in early hypercube systems, this task was relegated to the local <lb/>processors along the path from the source to the destination. Each intermediate node received the <lb/>entire message before forwarding it on towards the destination. In wormhole-routed systems, on the <lb/>other hand, all such communication-related tasks are handled by a separate router, or communications <lb/>co-processor, located at each node. As shown in Figure 2, the message is divided into small pieces, <lb/>called its, that are pipelined through the network by way of small bu ers at the routers. The presence <lb/>of routers allows any set of local processors to communicate among one another without a ecting any <lb/>of the local processors outside the set. As we shall see later, this capability allows much more exibility <lb/>than store-and-forward switching in the design of collective operations. <lb/>The switching strategy directly a ects the network latency, t n , which equals the elapsed time after <lb/>the head of a (unicast) message has entered the network at the source until the tail of the message <lb/>emerges from the network at the destination. In store-and-forward switched systems, t n was linear <lb/>in the path length between the source and destination. In wormhole-routed systems, network latency <lb/>is given by t p d + `tf , where t p is the delay at the individual nodes encountered on the path, d is the <lb/>number of nodes traversed, or distance, `is the length of the message, and t f is the time required to <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>6 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>Router <lb/>Processor <lb/>Source <lb/>Destination <lb/>Flit Buffer <lb/>Figure 2. Pipelining operation of wormhole routing <lb/>transmit a it between routers. For relatively long messages, the value of t p d becomes small compared <lb/>to that of `tf 7]. <lb/>The start-up latency, t s , is the time required for the system to handle the message at both the <lb/>source and destination nodes. Its value is mainly dependent on the design of system software and the <lb/>interface between local processors and routers. Startup latency can be further decomposed into sending <lb/>latency, t snd , and receiving latency, t rcv , the start-up latencies incurred at the sending node and the <lb/>receiving node, respectively. For some systems, t s may be an order of magnitude greater than t n for <lb/>small messages 8]. Thus, the latency of short messages is distance-insensitive due to startup latency, <lb/>while the latency of long messages is distance-insensitive due to the pipelining e ect of wormhole <lb/>routing. Under these circumstances, all pairs of nodes can be considered to be essentially equidistant <lb/>apart with respect to time, which allows a great deal of exibility in the de nition and scheduling of <lb/>constituent messages of collective operations. Therefore, when evaluating the performance of collective <lb/>operations in this paper, we will often ignore the contribution of t p d to network latency. <lb/>Despite this property, a wormhole-routed network may not be modeled as a complete graph, <lb/>especially in the context of collective communication, because messages sent concurrently through the <lb/>network may contend for communication channels. Collective operations must be designed so that they <lb/>not only minimize the number of message-passing steps, but also minimize, or preferably eliminate, <lb/>contention among constituent messages. This task involves both the physical network topology and <lb/>the underlying hardware routing algorithm. <lb/>Network Topology. Several MPC topologies are depicted in Figure 3. The topologies of many <lb/>commercial MPCs are special cases of either n-dimensional meshes or k-ary n-cubes. These classes of <lb/>topologies, which include hypercubes, meshes, and tori as special cases, are popular in part because <lb/>they lend themselves to very simple routing algorithms. Early systems that used store-and-forward <lb/>switching often adopted a hypercube topology because of the relatively dense interconnection network, <lb/>which resulted in shorter message paths. With the advent of wormhole routing, in which internode <lb/>distance has less e ect on communication delay, low-dimensional meshes and tori have attracted larger <lb/>followings due to their simpler physical layouts and better scalability 7]. <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>7 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>(c) 3x3x2 3D mesh <lb/>(a) 4−cube <lb/>(b) 5x4 2D torus <lb/>Figure 3. MPC topologies <lb/>Notable exceptions to mesh-based direct networks include the fat tree, which is used in the TMC <lb/>CM-5, and switch-based interconnection networks, as used in the IBM SP1. The issue of collective <lb/>communication is also important in these architectures and has recently drawn attention from the <lb/>research community; we review some of that work later in the paper. However, we will concentrate <lb/>primarily on wormhole-routed systems with hypercube, mesh, and torus topologies, for which the <lb/>solutions to collective communication design possess a certain degree of similarity. Commercial and <lb/>research MPCs using these network topologies include the nCUBE-2 (hypercube), the Intel Paragon <lb/>(2D mesh), the MIT J-machine (3D mesh), and the Cray T3D (3D torus). <lb/>Hardware Routing Algorithm. The routing algorithm determines the path selected by a message <lb/>in order to reach its destination. Since wormhole-routed messages can hold some channels while <lb/>waiting for others, the routing algorithm is designed to prevent deadlock among messages. Although <lb/>research in adaptive wormhole routing is very promising, most routing algorithms presently used in <lb/>commercial systems are deterministic, that is, the path between a given source and destination is <lb/>xed. For example, dimension-ordered routing, which has been adopted in many wormhole-routed n-<lb/>dimensional mesh systems, avoids deadlock by enforcing a strictly monotonic order on the dimensions <lb/>of the network traversed by each message. Designing collective operations that avoid contention among <lb/>constituent messages is complicated by the use of deterministic routing, since contending messages <lb/>cannot be dynamically rerouted along alternate paths. <lb/>Port Model. Each router is connected to its local processor/memory by one or more pairs of internal <lb/>channels, or ports. One channel of each pair is for input, the other for output. The port model of a <lb/>system refers to the number of internal channels at each node. If each node possesses exactly one pair <lb/>of internal channels, then the result is a so-called \one-port communication architecture.&quot; Figure 4(a) <lb/>shows a one-port node/router pair in a 2D mesh. A major consequence of a one-port architecture is <lb/>that the local processor must transmit (receive) messages sequentially. Architectures with multiple <lb/>ports reduce this bottleneck. In the case of an all-port system, shown in Figure 4(b), every external <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>8 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>channel has a corresponding internal channel, thus allowing the node to send and receive on all external <lb/>channels simultaneously. It is also possible for a system to possess a k-port architecture, where the <lb/>number of ports is greater than one but less than the number of external channels. The port model of <lb/>the system can be important to collective operations, most of which involve sending multiple messages, <lb/>receiving multiple messages, or both. <lb/>west out channel <lb/>east out channel <lb/>west in channel <lb/>south out channel <lb/>north in channel <lb/>south in channel <lb/>north out channel <lb/>east in channel <lb/>ports from local processor <lb/>ports to local processor <lb/>west out channel <lb/>east out channel <lb/>west in channel <lb/>south out channel <lb/>north in channel <lb/>south in channel <lb/>north out channel <lb/>east in channel <lb/>single port from local processor <lb/>single port to local processor <lb/>(a) one−port router for 2D mesh <lb/>(b) all−port router for 2D mesh <lb/>Figure 4. Two di erent port models for a 2D mesh <lb/>Virtual Channels. In meshes, the use of dimension-ordered routing is su cient to prevent dead-<lb/>lock. In torus networks, however, the presence of wraparound channels can lead to routing cycles <lb/>among messages, even under dimension-ordered routing. Channel-dependence cycles can be broken <lb/>by multiplexing virtual channels on a single physical communication channel. Each virtual channel <lb/>has its own it bu er and control lines. Virtual channels may be used in a variety of ways to eliminate <lb/>deadlock, but how they are used determines potential message contention, thereby a ecting the design <lb/>of e cient collective communication operations. <lb/>Intermediate Reception. The intermediate reception (IR) capability, also referred to as path-based <lb/>routing, is a hardware feature that allows a router to deliver an incoming message to the local host <lb/>while simultaneously forwarding it to another router, as depicted in Figure 5. Since implementing IR <lb/>requires a relatively minor modi cation to existing routers used in MPCs, it is receiving increasing <lb/>attention, particularly among researchers involved in collective communication. IR can be used to <lb/>deliver a message to multiple destinations in nearly the same time that is needed to send a message <lb/>to a single destination. Application of IR to multicast, broadcast, and other collective operations will <lb/>be described later. <lb/>Software-Suppported Collective Communication. Even if collective operations are imple-<lb/>mented in software, their performance can often be improved by arranging the constituent messages <lb/>so as to make better use of the underlying hardware features. Many collective operations can be <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>9 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>Local Host <lb/>message replication <lb/>flit buffer <lb/>Router <lb/>to current destination <lb/>to subsequent destinations <lb/>Figure 5. Operation of intermediate reception <lb/>implemented using a tree of messages. Distribution-type operations, such as broadcast, multicast, and <lb/>scatter, involve sending messages from the root of the tree to its leaves. In receiving-type operations, <lb/>such as gather and reduction, messages are sent towards the root from the leaves. If the communication <lb/>patterns of two operations are essentially identical except that the direction of the messages is reversed, <lb/>then these operations are called duals of one another 9]. Which type of tree to use depends on the <lb/>underlying architecture and which nodes are involved in the operation. Let us de ne a broadcast <lb/>tree as one that involves all nodes in the network, and a multicast tree as one that involves only a <lb/>partial subset of the nodes. In the next three sections, we describe various tree structures and the <lb/>architectures and situations for which they are best suited. <lb/>4 Broadcast Trees in Hypercubes <lb/>Since the introduction of the rst hypercube systems in the 1980&apos;s, the study of collective commu-<lb/>nication primitives for this topology has been extensive. Many of these results concern broadcast <lb/>communication, and hence several types of broadcast trees have been proposed. Since rst-generation <lb/>systems used store-and-forward switching, many such proposals are based on nearest-neighbor commu-<lb/>nication, although the distance-insensitivity of wormhole routing allows this constraint to be relaxed. <lb/>Nearest-neighbor trees. Perhaps the simplest broadcast \tree&quot; in any network is a Hamiltonian <lb/>path. From a given source node in a hypercube, a Hamiltonian path can be constructed by using <lb/>the re ected gray code and the exclusive-or of the source address. Figure 6(a) illustrates such a <lb/>Hamiltonian path, indicated with bold arrows, in a 4-cube. The path begins with node 0000 and <lb/>ends with node 1000. In either store-and-forward or wormhole-routed networks, the time needed to <lb/>complete this operation is linear in the total number of nodes, since each intermediate node must <lb/>receive the entire message before forwarding it to the next node in the path. Speci cally, the delay is <lb/>N t u , where N is the number of nodes in the network and t u is the time required to send a unicast <lb/>message between two neighboring nodes. <lb/>An approach that makes better use of the dense interconnection of the hypercube topology is <lb/>the well-known spanning binomial tree (SBT) algorithm 13], which is illustrated for a 3-cube in <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>10 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>0000 <lb/>0001 <lb/>0010 <lb/>0011 <lb/>0100 <lb/>0101 <lb/>0110 <lb/>0111 <lb/>1000 <lb/>1001 <lb/>1010 <lb/>1011 <lb/>1100 <lb/>1101 <lb/>1110 <lb/>1111 <lb/>(a) Hamiltonian path <lb/>0000 <lb/>0001 <lb/>0010 <lb/>0011 <lb/>0100 <lb/>0101 <lb/>0110 <lb/>0111 <lb/>[1] <lb/>[2] <lb/>[3] <lb/>[3] <lb/>[3] <lb/>[3] <lb/>[2] <lb/>(b) E-cube tree <lb/>001 <lb/>010 <lb/>011 <lb/>100 <lb/>110 <lb/>111 <lb/>101 <lb/>101 <lb/>001 <lb/>010 <lb/>011 <lb/>100 <lb/>110 <lb/>111 <lb/>000 <lb/>001 <lb/>010 <lb/>011 <lb/>100 <lb/>101 <lb/>110 <lb/>111 <lb/>(c) Edge-disjoint spanning trees <lb/>Figure 6. Hypercube broadcast trees based on nearest-neighbor communication <lb/>Figure 6(b). In the rst step of the algorithm, the source node sends the message to its neighbor <lb/>whose address di ers from its own in the lowest (alternatively highest) bit position. Next, these two <lb/>nodes send to their respective neighbors in the second dimension. This recursive doubling process <lb/>continues until, in the last step, half of the nodes in the network forward the message to the other half <lb/>through the highest dimension. This algorithm requires n message-passing steps to reach all nodes in <lb/>an n-cube, with the last node receiving the message at time nt u . <lb/>In order to reduce broadcast latency in hypercubes, Johnsson and Ho 14] proposed a tree structure <lb/>that uses n edge-disjoint spanning trees (EDST) of the hypercube. In order to implement a broadcast <lb/>operation using this method, the message is partitioned into n segments, each of which is transmitted <lb/>along a di erent spanning tree, as illustrated in Figure 6(c). This algorithm employs the channels more <lb/>e ciently than the SBT broadcast algorithm. The algorithm completes the broadcast of a message of <lb/>length `in time of O((`=n) log 2 N) = O(`). Since the spanning trees are disjoint, this algorithm does <lb/>not incur channel contention. However, the algorithm does require that the message be reconstructed <lb/>at every destination. Moreover, if the architecture does not support a su cient number of input and <lb/>output ports, then this approach can incur contention at the ports. For example, in Figure 6(c), node <lb/>111 may be required to receive three message segments at approximately the same time. <lb/>Trees designed for wormhole routing. In the case of one-port systems, wormhole routing has <lb/>little e ect on the performance of broadcast trees, since a node can send (receive) only one message <lb/>at a time; implementations using only nearest-neighbor communication can distribute (gather) data <lb/>as fast as any other algorithm. Even the presence of multiple ports may not necessarily improve the <lb/>overall performance of an operation that uses only nearest-neighbor communication. For example, <lb/>Figure 7 shows the steps of the SBT algorithm in a 4-cube (either store-and-forward or wormhole-<lb/>routed). If the system is a one-port architecture, then the four step solution shown in Figure 7(a) is <lb/>optimal; given that a node can send only one message at a time, four steps are necessary and su cient <lb/>to reach all other nodes. If the SBT algorithm is executed on a 4-port 4-cube, as shown in Figure 7(b), <lb/>some nodes receive the message earlier than in a one-port architecture. This result occurs because <lb/>nodes are able to transmit multiple messages in parallel as long as they are routed through di erent <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>11 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>dimensions. However, the algorithm still requires four steps to reach all the nodes. The maximum <lb/>broadcast latency is the same in both cases, 4t u , because the SBT algorithm was not designed to take <lb/>advantage of either multiple ports or wormhole routing. <lb/>0000 <lb/>0001 <lb/>0010 <lb/>0100 <lb/>0101 <lb/>0110 <lb/>1000 <lb/>1001 <lb/>1010 <lb/>1100 <lb/>1101 <lb/>1110 <lb/>0011 <lb/>0111 <lb/>1011 <lb/>1111 <lb/>[4] <lb/>[3] <lb/>[4] <lb/>[2] <lb/>[3] <lb/>[4] <lb/>[3] <lb/>[4] <lb/>[1] <lb/>[2] <lb/>[3] <lb/>[4] <lb/>[4] <lb/>[4] <lb/>[4] <lb/>(a) SBT on a 1-port 4-cube <lb/>0000 <lb/>0001 <lb/>0010 <lb/>0100 <lb/>0101 <lb/>0110 <lb/>1000 <lb/>1001 <lb/>1010 <lb/>1100 <lb/>1101 <lb/>1110 <lb/>0011 <lb/>0111 <lb/>1011 <lb/>1111 <lb/>[3] <lb/>[2] <lb/>[3] <lb/>[2] <lb/>[3] <lb/>[1] <lb/>[2] <lb/>[3] <lb/>[4] <lb/>[1] <lb/>[1] <lb/>[1] <lb/>[2] <lb/>[2] <lb/>[2] <lb/>(b) SBT on a 4-port 4-cube <lb/>Figure 7. SBT forwarding in 4-cubes <lb/>For multiple-port systems, however, there exist better solutions that use other than nearest-<lb/>neighbor communication. McKinley and Tre tz 15] proposed a simple variation on the SBT called <lb/>the Double Tree (DT) algorithm, which is designed for all-port, wormhole-routed hypercubes. The DT <lb/>algorithm begins with the source node s sending the message to node s, whose address is the bitwise <lb/>complement of s. Subsequently, nodes s and s become the roots of partial spanning binomial trees. <lb/>In this manner, the number of message passing steps required to reach all nodes in an n-dimensional <lb/>hypercube is dn=2e. Figure 8 illustrates the steps of the DT algorithm as executed on a 4-cube; for <lb/>clarity, some links are not shown. Node 0000 (0) rst sends the message concurrently to neighboring <lb/>nodes 0001, 0010, 0100, and to node 1111, which is reached by way of routers at nodes 1000, 1100, <lb/>and 1110. Nodes 0000 and 1111 then become the roots of forward and backward trees, respectively. <lb/>Addresses in the rst tree are resolved by changing 0&apos;s to 1&apos;s, while addresses in the backward tree are <lb/>resolved by changing 1&apos;s to 0&apos;s. On a 4-port 4-cube, the broadcast is complete after only two message-<lb/>passing steps, which is optimal without partitioning the message. The DT algorithm requires three <lb/>steps in a 6-cube, which is also optimal. Experiments on an nCUBE-2 hypercube, which possesses <lb/>a multiple-port architecture, con rm the advantage of this approach over a spanning binomial tree, <lb/>particularly for large messages 15]. <lb/>Although the DT algorithm reduces broadcast latency compared to the SBT algorithm, it is not <lb/>optimal for larger hypercubes. Ho and Kao 16] discovered a more general solution in which the <lb/>network is recursively divided into subcubes of nearly equal size; the DT algorithm is used to nish <lb/>the broadcast operation in small subcubes. The approach uses the concept of a dimension-simple path. <lb/>A path P = v 0 ; v 1 ; : : :; v d in an n-dimensional hypercube is called dimension-simple if there exists a <lb/>sequence i 1 ; i 2 ; : : :; i d of distinct cube dimensions such that for all v j , j 1, v j is obtained from v j?1 <lb/>by complementing the bit at dimension i j . The path P is called ascending (respectively, descending) <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>12 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>1110 <lb/>1111 <lb/>1011 <lb/>1001 <lb/>1101 <lb/>1010 <lb/>1000 <lb/>0001 <lb/>0101 <lb/>0111 <lb/>0011 <lb/>0110 <lb/>0010 <lb/>0000 <lb/>0100 <lb/>(a) step 1 <lb/>1110 <lb/>1100 <lb/>1111 <lb/>1001 <lb/>1101 <lb/>1010 <lb/>1000 <lb/>0001 <lb/>0101 <lb/>0111 <lb/>0011 <lb/>0110 <lb/>0010 <lb/>0000 <lb/>0100 <lb/>1011 <lb/>(b) step 2 <lb/>1100 <lb/>Figure 8. Example of DT broadcast in a 4-cube <lb/>if i 1 &lt; i 2 &lt; : : : &lt; i d (respectively, i 1 &gt; i 2 &gt; : : : &gt; i d ). For example, an ascending dimension-simple <lb/>path from node 0000000 in a 7-cube is <lb/>0000000 ! 0000001 ! 0000011 ! 0000111 ! 0001111 ! 0011111 ! 0111111 ! 1111111 <lb/>This particular path happens to traverse all the dimensions of the hypercube, that is, d = 7. In <lb/>an all-port wormhole-routed hypercube in which dimension-ordered routing is performed by resolving <lb/>addresses from top-to-bottom, a single node can send a message to all the nodes along an ascending <lb/>path simultaneously, except for any message staggering due to sending latency. In this manner, an <lb/>n-dimensional cube can be partitioned into n + 1 subcubes such that each subcube contains one node <lb/>on the dimension-simple path. Ho and Kao 16] have shown that any cube can be partitioned equally <lb/>or nearly equally, which implies that the number of steps required by their algorithm is optimal to <lb/>within a multiplicative constant, speci cally, the time required is ( n <lb/>log 2 (n+1) ). <lb/>Figure 9 depicts the operation of the Ho-Kao broadcast algorithm in a 7-cube. In the rst step, <lb/>the source node 0000000 sends a copy of the message to the seven nodes that lie along an ascending <lb/>dimension-simple path. Each of these nodes, plus the source, lies in a disjoint 4-cube; those eight <lb/>4-cubes contain all the nodes in the network. The eight intermediate destination nodes denoted in the <lb/>gure complete the broadcast operation using the DT algorithm within their individual 4-cube. <lb/>Support for Other Collective Operations. As we mentioned earlier, tree structures are suitable <lb/>for supporting other collective operations besides broadcast. For example, a tree can also be used <lb/>to perform a scatter operation, as shown in Figure 10(a) for an 8-node system in which nodes have <lb/>one-port architectures; the number in brackets represents the number of the message-passing step. <lb/>Messages for individual nodes are sent from the source down the appropriate branch; each node stores <lb/>its own messages and forwards the rest, as necessary, to its children. The dual operation, gather, is <lb/>implemented by simply reversing the direction of message transmission, as shown in Figure 10(b). The <lb/>same structure can be used to implement all-to-one reduction, in which case the reduction operation <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>13 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<page>0000000 <lb/></page>

			<body>0000001 <lb/>0000011 <lb/>0000111 <lb/>0001111 <lb/>0011111 <lb/>0111111 <lb/>1111111 <lb/>0000010 <lb/>0000100 <lb/>0001000 <lb/>0010000 <lb/>0100000 <lb/>1000000 <lb/>0000110 <lb/>0001100 <lb/>0011000 <lb/>0110000 <lb/>1100000 <lb/>0001110 <lb/>0011100 <lb/>0111000 <lb/>1110000 <lb/>0011110 <lb/>0111100 <lb/>1111000 <lb/>0111110 <lb/>1111100 <lb/>1111110 <lb/>xxx0x00 <lb/>xxx001x <lb/>xxx0x01 <lb/>xxx011x <lb/>x001xxx <lb/>x011xxx <lb/>01x1xxx <lb/>11x1xxx <lb/>Step 1. Deliver message to <lb/>one node in each of <lb/>eight disjoint 4−cubes <lb/>Steps 2−3. Each node holding the <lb/>message delivers the <lb/>message to its 4−cube <lb/>using the DT Algorithm <lb/>4−cube <lb/>router forwarding message <lb/>destination node <lb/>source node <lb/>Figure 9. Ho-Kao broadcast algorithm, as executed on an all-port 7-cube <lb/>is performed pairwise on data items as they proceed towards the root. Should the result need to be <lb/>distributed back to all the nodes, then the tree structure in Figure 10(a) could be used, but with only <lb/>the single result message being broadcast. Other approaches to such N=N reduction operations are <lb/>described in Section 7. Finally, barrier synchronization can also be implemented using both trees, <lb/>as shown in Figure 10(c). The root processor collects notices from nodes that they have reached the <lb/>barrier and, after hearing from all other nodes, informs the nodes that they may proceed past the <lb/>barrier. <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>4 <lb/>M <lb/>5 <lb/>M <lb/>6 <lb/>M <lb/>7 <lb/>M <lb/>6 <lb/>M <lb/>7 <lb/>M <lb/>7 <lb/>M <lb/>3 <lb/>M <lb/>M 1 <lb/>5 <lb/>M <lb/>3 <lb/>M <lb/>2 <lb/>M <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>4 <lb/>M <lb/>5 <lb/>M <lb/>6 <lb/>M <lb/>7 <lb/>M <lb/>6 <lb/>M <lb/>7 <lb/>M <lb/>7 <lb/>M <lb/>3 <lb/>M <lb/>M 1 <lb/>5 <lb/>M <lb/>3 <lb/>M <lb/>2 <lb/>M <lb/>(a) scatter <lb/>(b) gather/reduction <lb/>barrier processor <lb/>(c) barrier synchronization <lb/>[1] <lb/>[2] <lb/>[3] <lb/>[2] <lb/>[2] <lb/>[2] <lb/>[1] <lb/>[1] <lb/>[1] <lb/>[1] <lb/>[3] <lb/>[3] <lb/>[3] <lb/>[3] <lb/>Figure 10. Use of broadcast trees in other collective operations <lb/>5 Broadcast and Multicast in One-Port Meshes and Tori <lb/>Many of the studies of broadcast communication in wormhole-routed meshes and tori are based on <lb/>counterparts of hypercube algorithms, but exploit wormhole routing to send messages across multiple-<lb/>hop paths in a single step. As with hypercubes, the solution to such problems depends on the port <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>14 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>model. In this section, we describe algorithms for both broadcast and multicast in one-port systems; <lb/>multiple-port architectures are addressed in Section 6. <lb/>Dimensional Broadcast Tree Approach. The one-to-all broadcast problem in one-port <lb/>wormhole-routed mesh networks was rst studied by Barnett et al 17]. The algorithm presented <lb/>in 17] operates in a recursive manner, similar to the spanning binomial tree, within each dimension of <lb/>the mesh. Suppose, for example, that the source node is the leftmost node in a linear array topology, as <lb/>illustrated in Figure 11. In the rst message-passing step, the source node sends the message halfway <lb/>across the linear array, partitioning the network into two subnetworks. In subsequent steps, each node <lb/>holding a copy of the message forwards it to a node in its partition that has not yet received the <lb/>message. By employing non-nearest-neighbor communication, this approach takes advantage of the <lb/>pipelining e ect of wormhole routing. <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <lb/>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <lb/>(a) step 1 <lb/>(b) step 2 <lb/>(c) step 3 <lb/>(d) step 4 <lb/>node holding message <lb/>node not yet holding message <lb/>Figure 11. Recursive doubling broadcast in a 16-node wormhole-routed linear array <lb/>The algorithm may be generalized to higher-dimensional meshes by applying the above procedure <lb/>to one dimension at a time. As illustrated in Figure 12, which depicts the operation of this algorithm <lb/>in an 8 8 2D mesh, the source node need not be positioned at the end or corner of the mesh. <lb/>An important feature of this algorithm is that by partitioning the network in this manner, channel <lb/>contention among the messages is avoided. Therefore, in the absence of contention from other network <lb/>tra c, the total time required for broadcast in a 2 r 2 c 2D mesh is (r+s)(t s +`t f ). The same approach <lb/>can be applied to meshes of higher-dimension as well. In general, this algorithm requires P d i=0 dlog w i e <lb/>message-passing steps in a d-dimensional mesh in which the width of dimension i is w i . <lb/>Contention-Free Multicast. In the examples above, the width of each dimension is assumed to <lb/>be a power of 2. This assumption does not hold in many commercial systems, however. If one or <lb/>more of the widths is not a power of two, then direct application of the above approach can lead to <lb/>contention among the constituent messages. It is desirable to nd an algorithm that achieves the lower <lb/>bound on the number of steps needed to broadcast on a one-port architecture (without partitioning <lb/>the message); that lower bound is dlog 2 ( d i=0 w i )e = dlog 2 (N)e. <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>15 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>(a) step 1 <lb/>(b) step 2 <lb/>(c) step 3 <lb/>(d) step 4 <lb/>(e) step 5 <lb/>(f) step 6 <lb/>node holding message <lb/>node not yet holding message <lb/>Figure 12. Dimensional broadcast in an 8 8 mesh <lb/>It turns out that the broadcast problem is solved by a suite of algorithms that were originally <lb/>designed for the general multicast problem, but which can also be applied to the degenerate case <lb/>of broadcast. We now describe these multicast algorithms. As mentioned in Section 2, collective <lb/>operations are often performed among a proper subset of the nodes in the network, rather than among <lb/>all the nodes. Such situations arise whenever the operation involves only a subset of the processes in the <lb/>application, or when the nodes allocated to a particular application do not form a contiguous, regular <lb/>subnetwork. Designing a multicast tree is more di cult than designing a broadcast tree, since the <lb/>latter is a special case in which any node may be called upon to relay messages. A multicast tree, on the <lb/>other hand, should not involve any local processors other than the source and destinations; although <lb/>the routers at other nodes may be required to route messages, interrupting their local processors wastes <lb/>computing resources. <lb/>The multicast algorithms described below are all based on recursive doubling and apply to one-port, <lb/>n-dimensional hypercubes, meshes, and tori that use dimension-ordered routing. For each topology, <lb/>there is a corresponding algorithm, and hence the algorithms are named U-cube, U-mesh, and U-torus. <lb/>Recursive doubling can be viewed in many ways; one possible view is depicted in Figure 13, where <lb/>we assume for simplicity that m, the number of nodes involved in the multicast, is a power of 2. <lb/>Figure 13 shows all unicasts occurring in the rst three steps of an optimal multicast, as well as two <lb/>of the unicasts that occur in the nal step. <lb/>The key to avoiding contention among the constituent messages is the ordering of the destinations. <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>16 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>d <lb/>0 <lb/>d <lb/>1 <lb/>d <lb/>m/4 <lb/>d <lb/>m/2 <lb/>d <lb/>5m/8 <lb/>d <lb/>3m/2 <lb/>d <lb/>m−1 <lb/>m−2 <lb/>d <lb/>7m/8 <lb/>d <lb/>m/8 <lb/>d <lb/>d <lb/>3m/8 <lb/>Figure 13. A minimum-time multicast <lb/>The multicast algorithms take advantage of several simple results regarding message contention and <lb/>lexicographical ordering, by dimension, of arbitrary sets of nodes. In a 2D mesh, for example, such an <lb/>ordering is simply a column-wise or row-wise listing of the node addresses. Such an ordering is also <lb/>called dimension order, denoted by the symbol &lt; d . A sequence of node addresses that are ordered <lb/>with respect to dimension order is referred to as a dimension-ordered chain 8]. Three properties of <lb/>dimension-ordered chains, depicted in Figure 14, are used in these multicast algorithms. Consider any <lb/>four nodes, u, v, x, and y, in a hypercube or mesh. If u &lt; d v &lt; d x &lt; d y, then dimension-ordered routes <lb/>P(u; v) and P(x; y) in Figure 14(a) have been shown to be arc-disjoint 8]. That is to say, the routes <lb/>do not share any channels (although they may use two, oppositely directed channels which, in some <lb/>systems, may be multiplexed on the same link). Two other pairs of routes, depicted in Figures 14(b) <lb/>and 14(c), are also arc-disjoint under dimension-ordered routing. <lb/>P(y,x) <lb/>P(v,u) <lb/>dimension order <lb/>u <lb/>v <lb/>x <lb/>y <lb/>P(u,v) <lb/>P(x,y) <lb/>dimension order <lb/>u <lb/>v <lb/>y <lb/>dimension order <lb/>u <lb/>v <lb/>x <lb/>y <lb/>P(x,y) <lb/>P(v,u) <lb/>x <lb/>dimension order <lb/>u <lb/>v <lb/>y <lb/>P(u,v) <lb/>P(u,y) <lb/>dimension order <lb/>u <lb/>v <lb/>y <lb/>P(u,v) <lb/>P(u,y) <lb/>x <lb/>(a) case 1, arc−disjoint <lb/>(b) case 2, arc−disjoint <lb/>(c) case 3, arc−disjoint <lb/>(d) message (u,y) later than (u,v) <lb/>(d) message (x,y) later than (u,v) <lb/>Figure 14. Arc-disjoint paths in dimension-ordered chains <lb/>If we assume that the source and destinations in Figure 13 are ordered lexicographically from <lb/>left to right, then we can see that all the messages generated by the algorithm are contention-free, as <lb/>follows: Since all pairs of messages sent in the same step match the pattern shown in Figure 14(a), they <lb/>are guaranteed to be arc-disjoint, and therefore contention-free. Messages sent in di erent steps are <lb/>contention-free by the nature of the algorithm when executed on a one-port architecture. Speci cally, <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>17 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>all pairs of messages that do not match the pattern in Figure 14(a), must match one of the patterns <lb/>in Figure 14(d) or 14(e). In either case, transmission of the inner message must occur after that of <lb/>the outer message, so the two messages are contention-free. As described above, this chain algorithm <lb/>is only applicable to those cases in which the source address is less than or greater than (with respect <lb/>to &lt; d ) all the destination addresses. Clearly, this situation is not true in general, however, minor <lb/>variations on this approach solve the problem in each of the three topologies. <lb/>For a hypercube network in which E-cube routing is used, the symmetry of the topology e ectively <lb/>allows the source node to play the role of the rst node in a dimension-ordered chain. The exclusive-<lb/>or operation, denoted , is used to carry out this task. A sequence d 1 ; d 2 ; : : :; d m?1 of hypercube <lb/>addresses is called a d 0 -relative dimension-ordered chain if and only if d 0 d 1 ; d 0 d 2 ; : : :; d 0 d m?1 <lb/>is a dimension-ordered chain. The source can easily sort the m ? 1 destinations into a d 0 -relative <lb/>dimension-ordered chain, = d 1 ; d 2 ; : : :; d m?1 . The source may then execute the chain algorithm <lb/>using instead of the original addresses. An example is shown in Figure 15. Notice that this U-cube <lb/>multicast algorithm degenerates to the well known spanning tree 14] for the special case of <lb/>broadcast. <lb/>0000 <lb/>0001 <lb/>0010 <lb/>0011 <lb/>0100 <lb/>0101 <lb/>0110 <lb/>0111 <lb/>1000 <lb/>1001 <lb/>1010 <lb/>1011 <lb/>1100 <lb/>1101 <lb/>1110 <lb/>1111 <lb/>1000 <lb/>1101 <lb/>1110 <lb/>0011 <lb/>1001 <lb/>0010 <lb/>0101 <lb/>1101 <lb/>1111 <lb/>0110 <lb/>0110 <lb/>0010 <lb/>0110 <lb/>0100 <lb/>0000 <lb/>0001 <lb/>0100 <lb/>0111 <lb/>1010 <lb/>1011 <lb/>1100 <lb/>1001− relative dimension−ordered chain <lb/>(a) multicast problem in 4−cube <lb/>(b) U−cube multicast tree <lb/>1100 <lb/>source <lb/>destination <lb/>other node <lb/>router used <lb/>Figure 15. U-cube multicast solution in a 4-cube <lb/>In meshes, symmetry cannot be used for cases in which the source address lies in the middle of <lb/>a dimension-ordered chain. However, another relatively simple method, based on Figure 14(c), may <lb/>be used to address this problem. In the U-mesh algorithm, the source and destination addresses are <lb/>sorted into a dimension-ordered chain, denoted . The source node successively divides in half. If <lb/>the source is in the lower half, then it sends a copy of the message to the smallest node (with respect <lb/>to &lt; d ) in the upper half. That node will be responsible for delivering the message to the other nodes <lb/>in the upper half, using the chain algorithm. If the source is in the upper half, then it sends a copy of <lb/>the message to the largest node in the lower half. The source continues this procedure until contains <lb/>only its own address. Figure 16(a) depicts a multicast problem in a 6 6 2D mesh. <lb/>The U-mesh algorithm is not contention-free when executed on a torus; however, a simple extension <lb/>of the dimension-ordered chain can be used to avoid contention in a torus (and, incidentally, also <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>18 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>5,4 <lb/>4,3 <lb/>1,0 <lb/>3,2 <lb/>0,0 <lb/>0,1 <lb/>0,2 <lb/>0,3 <lb/>0,4 <lb/>0,5 <lb/>1,1 <lb/>1,2 <lb/>1,3 <lb/>1,4 <lb/>1,5 <lb/>2,0 <lb/>2,1 <lb/>2,2 <lb/>2,3 <lb/>2,4 <lb/>2,5 <lb/>3,0 <lb/>3,1 <lb/>3,3 <lb/>3,4 <lb/>3,5 <lb/>4,0 <lb/>4,1 <lb/>4,2 <lb/>4,4 <lb/>4,5 <lb/>5,0 <lb/>5,1 <lb/>5,5 <lb/>5,2 <lb/>5,3 <lb/>1,0 <lb/>1,2 <lb/>1,5 <lb/>2,1 <lb/>3,2 <lb/>3,4 <lb/>4,3 <lb/>4,5 <lb/>5,1 <lb/>5,4 <lb/>5,4 <lb/>1,2 <lb/>1,3 <lb/>1,4 <lb/>2,2 <lb/>4,2 <lb/>4,4 <lb/>5,5 <lb/>5,2 <lb/>5,3 <lb/>1,4 <lb/>1,3 <lb/>2,2 <lb/>3,3 <lb/>5,2 <lb/>5,3 <lb/>1,1 <lb/>source node <lb/>destination node <lb/>other node <lb/>intermediate router <lb/>(a) multicast example in 6x6 mesh <lb/>(b) U−mesh multicast tree solution <lb/>[1] <lb/>[3] <lb/>[4] <lb/>[2] <lb/>[3] <lb/>[4] <lb/>[2] <lb/>[3] <lb/>[3] <lb/>Figure 16. U-mesh multicast solution in a 6 6 mesh <lb/>in a mesh) 18]. The source x s and destinations are sorted into a dimension-ordered chain = <lb/>fx 0 ; x 1 ; x 2 ; : : :; x s , : : :x m?1 g. Next, is \rotated&quot; so that x s becomes the rst element in the chain. <lb/>The resultant chain 0 = fx s ; x s+1 ; : : :; x m?1 ; x 0 ; x 1 ; : : :; x s?1 g is called an R-chain with respect to x s . <lb/>The U-torus algorithm takes an R-chain as input and uses the recursive doubling algorithm described <lb/>earlier. Figure 17 illustrates the application of the U-torus algorithm. In this example, the 6 6 torus <lb/>shown in Figure 17(a) is assumed to be a unidirectional torus; messages may be routed only towards <lb/>the right or the top of the torus. The U-torus algorithm also applies to bidirectional tori, in which <lb/>messages can be sent in either direction in each dimension 18]. <lb/>5,4 <lb/>4,3 <lb/>1,0 <lb/>3,2 <lb/>0,0 <lb/>0,1 <lb/>0,2 <lb/>0,3 <lb/>0,4 <lb/>0,5 <lb/>1,1 <lb/>1,2 <lb/>1,3 <lb/>1,4 <lb/>1,5 <lb/>2,0 <lb/>2,1 <lb/>2,2 <lb/>2,3 <lb/>2,4 <lb/>2,5 <lb/>3,0 <lb/>3,1 <lb/>3,3 <lb/>3,4 <lb/>3,5 <lb/>4,0 <lb/>4,1 <lb/>4,2 <lb/>4,4 <lb/>4,5 <lb/>5,0 <lb/>5,1 <lb/>5,5 <lb/>5,2 <lb/>5,3 <lb/>1,0 <lb/>1,2 <lb/>1,5 <lb/>2,1 <lb/>3,2 <lb/>3,4 <lb/>4,3 <lb/>4,5 <lb/>5,1 <lb/>5,4 <lb/>source node <lb/>destination node <lb/>other node <lb/>intermediate router <lb/>[1] <lb/>[3] <lb/>[4] <lb/>[2] <lb/>[3] <lb/>[4] <lb/>[2] <lb/>[3] <lb/>[3] <lb/>4,2 <lb/>5,3 <lb/>5,2 <lb/>4,2 <lb/>3,3 <lb/>4,4 <lb/>5,5 <lb/>5,0 <lb/>1,4 <lb/>0,4 <lb/>0,4 <lb/>1,4 <lb/>2,5 <lb/>2,0 <lb/>1,0 <lb/>1,1 <lb/>1,5 <lb/>1,5 <lb/>1,3 <lb/>1,4 <lb/>(b) U−torus multicast tree solution <lb/>(a) multicast example in 6x6 (unidirectional) torus <lb/>same link, but different <lb/>virtual channels <lb/>Figure 17. Example multicast using the U-torus algorithm <lb/>The reader may notice that two of the messages sent in step three appear to require the same <lb/>channel from node (1; 4) to node (1; 5). However, not shown in Figure 17 are the virtual channels <lb/>required to prevent deadlock in a unidirectional torus. Figure 18 shows the virtual channels connecting <lb/>the nodes in the second column of the torus. The p channels are used by message that will eventually <lb/>use the wraparound channel, in this case c p5 . The h channels are used by messages that will not <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>19 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>require the wraparound link and by messages that have already used the wraparound link. In the <lb/>example shown in Figure 17, the message from (5; 4) to (1; 0) uses the p channel on link ((1,4),(1,5)), <lb/>while the message from (1; 2) to (1; 5) uses the h channel on that link. <lb/>1,0 <lb/>1,1 <lb/>1,2 <lb/>1,3 <lb/>1,4 <lb/>1,5 <lb/>c h1 <lb/>c h2 <lb/>c h3 <lb/>c h4 <lb/>h0 <lb/>c <lb/>p5 <lb/>c <lb/>p1 <lb/>c <lb/>p3 <lb/>c <lb/>p4 <lb/>c <lb/>p2 <lb/>c <lb/>Figure 18. Virtual channels for second column in 6 6 torus <lb/>The U-cube, U-mesh, and U-torus algorithms exhibit several desirable properties. They require <lb/>dlog 2 (m+1)e message passing steps to reach m destinations, regardless of their location, the dimension <lb/>of the topology, or the shape of the topology (in the case of mesh and torus). When used for the special <lb/>case of broadcast, each requires dlog 2 (N)e message-passing steps, where N is the number of nodes <lb/>in the network. Moreover, they avoid channel contention among the constituent messages. As with <lb/>the broadcast trees described earlier, these algorithms can be used to implement other collective <lb/>operations, such as scatter, gather, reduction, and barrier synchronization, among arbitrary subsets <lb/>of nodes. Another advantage is that these algorithms do not require partitioning of the message, <lb/>resulting in simpler code and fewer communication startups. For very long messages, however, message <lb/>partitioning may be worthwhile, as described below. <lb/>Message Partitioning Approaches to Broadcast. As in the EDST broadcast algorithm de-<lb/>scribed in Section 4, it is possible to take advantage of the fact that all nodes participate in the <lb/>broadcast operation and are available to relay and replicate the message. Such approaches involve <lb/>partitioning the message and sending parts of it along separate subnetworks, with every node eventu-<lb/>ally collecting all the pieces. These algorithms exploit redundancy in the network topology in order <lb/>to reduce the time of the operation. An extension of this approach is pipelining, in which each piece <lb/>of the message assigned to a particular distribution tree is further partitioned as it is sent through the <lb/>tree. <lb/>One algorithm that uses this approach in 2D mesh networks is similar to the EDST algorithm <lb/>for hypercubes, and is appropriately named edge-disjoint spanning fences (EDSF) 19]. The EDSF <lb/>algorithm embeds two disjoint spanning trees, or fences, in the mesh, as illustrated in Figure 19. The <lb/>source node partitions the message and alternately pipelines the pieces through the two fences. For <lb/>clarity, the source in Figure 19 is the node in the upper-left corner. Beginning in step 0, the source <lb/>node alternates between sending segments east and south, the two pipelines. The can be <lb/>considered to be colored black and white in a checkerboard pattern. Black nodes forward segments to <lb/>the east in even numbered steps, and to the south in odd-numbered steps. Conversely, white nodes <lb/>forward segments to the south in even numbered steps, and to the east in odd-numbered steps. To <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>20 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>accommodate any node as the source, the mesh is actually considered as a logical torus, with some <lb/>segments being wormholed across the network to the west and north. (Therefore, the same algorithm <lb/>could of course be used on an actual torus, as well.) Compared to the single tree approaches described <lb/>earlier, this approach involves many more message startups that accompany message partitioning. <lb/>Despite these additional costs, however, this approach has been shown to provide better performance, <lb/>for relatively long messages, than a non-pipelined tree 19]. <lb/>source <lb/>source <lb/>flow of segments in either fence <lb/>send east in even steps, <lb/>south in odd steps <lb/>send south in even steps, <lb/>east in odd steps <lb/>(a) vertical fence <lb/>(b) horizontal fence <lb/>segment wormholed across network <lb/>on &apos;&apos;logicall&apos;&apos; wraparound channel <lb/>Figure 19. Two edge-disjoint spanning fences in a 6 6 mesh <lb/>Another broadcast method that involves message partitioning, called scatter-collect, is also pro-<lb/>posed in 19]. Figure 20 illustrates the operation of this algorithm in an 8 8 2D mesh. As the name <lb/>implies, the source node partitions the message and performs a scatter operation across its row, with <lb/>each node receiving a particular segment of the message. (This operation could be implemented using <lb/>the approach in Figure 10, which would require 3 steps to reach 8 nodes.) Next, each node in the row <lb/>of the source performs a scatter in its column. Finally, every node collects the remaining segments. <lb/>In the 2D mesh, the collect operation is implemented as two operations: collecting in rows and then <lb/>in columns. Speci cally, the nodes in each row form a logical ring; nodes circulate segments around <lb/>the ring until all nodes hold all segments in that row. Next, the nodes in each column form a logical <lb/>ring, and nodes circulate segments around the ring until all nodes hold all segments. This algorithm <lb/>avoids channel contention. As with the EDSF algorithm, the pipelining of messages provides good <lb/>performance for broadcast of long messages 19]. <lb/>The scatter-collect algorithm can also be used to implement multicast in hypercubes, meshes, and <lb/>tori, with the aid of the multicast tree algorithms described earlier. Speci cally, the source node <lb/>can use a multicast tree to scatter the data to all the nodes in the group. Next, the nodes in the <lb/>group can form a logical ring, according to a dimension-ordered chain, and circulate the message <lb/>segments until all destinations have received all segments. For example, suppose that the U-mesh <lb/>tree depicted in Figure 16 were used to scatter segments of the message to the destinations, by <lb/>recursively partitioning the message as in Figure 10(a). The destinations could then circulate the <lb/>segments around a logical ring, as shown in Figure 21, until all destinations hold all the segments. <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>21 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>1/8 of message <lb/>1/64 of message <lb/>entire message <lb/>scatter operation <lb/>wormhole across network <lb/>(a) scatter in row − 3 steps <lb/>(b) scatter in columns − 3 steps <lb/>(c) collect in rows − 8 steps <lb/>(d) collect in columns − 8 steps <lb/>Figure 20. Scatter-collect broadcast operation in an 8 8 2D mesh <lb/>This circulation process is contention-free, although some pairs of messages may simultaneously use <lb/>two oppositely-directed channels on the same link. As with the broadcast operation, this approach <lb/>may provide better performance than simple recursive doubling for long messages. <lb/>1,0 <lb/>1,2 <lb/>1,5 <lb/>2,1 <lb/>3,2 <lb/>3,4 <lb/>4,3 <lb/>4,5 <lb/>5,1 <lb/>5,4 <lb/>5,4 <lb/>5,5 <lb/>5,2 <lb/>5,3 <lb/>1,4 <lb/>1,3 <lb/>3,3 <lb/>5,2 <lb/>5,3 <lb/>1,1 <lb/>2,2 <lb/>2,5 <lb/>2,4 2,3 <lb/>3,1 <lb/>4,4 <lb/>4,4 <lb/>4,4 <lb/>3,4 <lb/>2,4 <lb/>1,4 <lb/>1,3 <lb/>1,2 <lb/>destination node <lb/>other node <lb/>intermediate router <lb/>original <lb/>source node <lb/>1,1 <lb/>Figure 21. Collect phase of scatter-collect multicast (see Figure 16) <lb/>6 Broadcasting in Multi-Port Meshes and Tori <lb/>In addition to the pipelining e ect of wormhole routing, we saw in Section 4 that multiple ports <lb/>can be exploited in broadcast implementations for hypercubes. Similar techniques can be applied <lb/>in the design of unicast-based collective communication operations in multi-port meshes and tori. <lb/>Multiple ports allow a local processor to simultaneously transmit (receive) messages to (from) any of <lb/>its external channels. Of course, high sending latency can cause message transmission to be staggered, <lb/>even serialized if the message is small enough. However, our experiences with the nCUBE-2 15] have <lb/>shown that its all-port architecture allows considerable overlap among messages sent in succession <lb/>from a given node, even though the startup latency of the nCUBE-2 is relatively high, between 80 and <lb/>100 microseconds for sending messages. Moreover, newly announced wormhole-routed systems, such <lb/>as the nCUBE-3 and Cray T3D, claim much lower startup latencies of a few microseconds. <lb/>The EDN Approach. The Extended Dominating Node (EDN) model 20] has been developed <lb/>recently to systematically construct unicast-based collective operations for multi-port wormhole-routed <lb/>networks. The model is based on the concept of dominating sets from graph theory. A dominating set <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>22 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>D of a graph G is a set of vertices in G such that every vertex in G is either in D or is adjacent to at least <lb/>one vertex in D. Figure 22 illustrates the concept of node domination, using a 2D mesh. Figure 22(a) <lb/>shows a set of 4 dominating nodes for the 4 4 mesh. As illustrated with bold arrows, these four <lb/>nodes can send a message to the remaining 12 nodes in a single message-passing step, by sending to <lb/>their appropriate neighbors. Figure 22(b) shows an abstract representation of the dominating nodes. <lb/>The EDN model broadens the concept of node domination to include nodes reachable in a single <lb/>communication step under a given routing algorithm (XY routing in the case of 2D mesh networks). <lb/>Figure 22(c) illustrates how four di erent nodes can dominate the other 12 nodes in the 4 4 mesh. <lb/>The EDN approach uses multiple levels of EDNs to de ne multiple message-passing steps of collective <lb/>communication algorithms. The key issue in applying the EDN method to the development of collective <lb/>operations lies in nding levels of EDNs that form regular and recursive patterns. <lb/>message transmission <lb/>dominating node <lb/>other node <lb/>(b) abstract representation <lb/>(a) set of 4 dominating nodes <lb/>(c)extended dominating nodes <lb/>Figure 22. Dominating nodes in a 4 4 2D mesh <lb/>Figure 23 illustrates how the EDN method can be used to implement broadcast in a 16 16 2D <lb/>mesh network under XY routing 20]. In the rst two message-passing steps, called startup steps, <lb/>the source node delivers the message to the highest-level EDNs, that is, the level-3 EDNs; there are <lb/>four such nodes. The level-3 EDNs proceed to \dominate&quot; the twelve level-2 EDNs by delivering the <lb/>message to them in step 3. By the end of step 4, every level-1 EDN has received the message from <lb/>either a level-2 or level-3 EDN. In step 5, which is not shown, the message is delivered to all remaining <lb/>nodes by having the EDNs send to their appropriate neighboring nodes following the communication <lb/>pattern shown in Figure 22(a). In general, this method can complete the broadcast operation in a <lb/>mesh of size 2 k 2 k in at most (k + 2) steps, while avoiding channel contention among the constituent <lb/>unicast messages. <lb/>By taking advantage of multiple ports, this method is able to approach a lower bound on the <lb/>number of message-passing steps for broadcast in an all-port 2D mesh networks. Speci cally, <lb/>the maximum number of nodes that can hold the message after t steps is 5 t . Given a mesh of size <lb/>n n, a lower bound on the time complexity is derived to be T dlog 5 (n 2 )e. (It should be noted <lb/>that in most cases this bound is not achievable because of the limitations imposed by XY routing.) <lb/>Figure 24 compares this method to a recursive doubling algorithm (U-mesh) when executed on an <lb/>all-port architecture 2D mesh. Figure 24(a) compares the algorithms in terms of the number of steps <lb/>on meshes with up to 2 16 nodes. Using a recursive doubling approach, the number of steps required <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>23 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>(b) step 3 <lb/>(c) step 4 (penultimate step) <lb/>(a) startup steps (1 and 2) <lb/>message transmission <lb/>level−0 node <lb/>level−3 EDN <lb/>level−2 EDN <lb/>level−1 EDN <lb/>source node <lb/>Figure 23. EDN broadcast in a 16 16 mesh <lb/>is equal to dlog 2 (n 2 )e, regardless of the location of the source node. The number of steps required by <lb/>the EDN algorithm depends on the source, but the maximum number of steps is at most one more <lb/>than the minimum number. As shown in Figure 24(a), in some cases (n = 5; 6; 7; 12; 14; 28; 56) the <lb/>EDN algorithm actually achieves the lower bound. Figure 24(b) compares the maximum broadcast <lb/>latency of the EDN algorithm and a recursive doubling broadcast algorithm for meshes of size 8 8, <lb/>16 16, and 32 32. The message size is varied from 32 bytes to 2048 bytes. Although the recursive <lb/>doubling algorithm will sometimes inadvertently take advantage of the multiple-port architecture, the <lb/>advantage of the EDN algorithm is clear. <lb/>2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>12 <lb/>14 <lb/>16 <lb/>0 <lb/>32 <lb/>64 <lb/>96 <lb/>128 <lb/>160 <lb/>192 <lb/>224 <lb/>256 <lb/>Mesh width n (number of nodes = n x n) <lb/>Steps <lb/>U-mesh <lb/>EDN max <lb/>EDN min <lb/>Lower bound <lb/>(a) number of steps for broadcast <lb/>0 <lb/>2000 <lb/>4000 <lb/>6000 <lb/>8000 <lb/>10000 <lb/>12000 <lb/>14000 <lb/>16000 <lb/>0 <lb/>256 <lb/>512 <lb/>768 <lb/>1024 <lb/>1280 <lb/>1536 <lb/>1792 <lb/>2048 <lb/>Message Size (bytes) <lb/>Maximum <lb/>Latency <lb/>(microseconds) <lb/>EDN (32x32) <lb/>EDN (16x16) <lb/>EDN (8x8) <lb/>U-mesh (32x32) <lb/>U-mesh (16x16) <lb/>UM-mesh (8x8) <lb/>(b) simulation comparison <lb/>Figure 24. Performance of EDN and recursive doubling broadcast algorithms <lb/>Besides broadcast, the EDN model can be applied to other collective operations besides broadcast. <lb/>For example, the EDN approach has also been applied to reduction and gather, which are duals of <lb/>broadcast, to the problem of matrix transposition, an example of a permutation operation 20], and <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>24 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>to other topologies, including higher-dimension meshes and tori. Other research has addressed the <lb/>problem of multicast in all-port hypercubes 21]. However, numerous problems in the area of collective <lb/>communication in multi-port systems remain to be explored. <lb/>7 All-to-All Communication <lb/>The complex collective operations, such as complete exchange and all-to-all broadcast, can be im-<lb/>plemented by having multiple nodes simply invoke other operations, such as broadcast, scatter, and <lb/>gather. However, more e cient solutions that involve less channel contention are possible when the <lb/>complex operation is treated as an atomic unit in which all group members participate in the entire <lb/>operation, rather than just a suboperation. Several new algorithms, designed for wormhole-routed <lb/>architectures, have been proposed for these operations. <lb/>Complete Exchange. The complete exchange operation requires that each node transmit a unique <lb/>message to each of the other nodes; in other words, every pair of nodes must exchange messages. Com-<lb/>plete exchange is sometimes referred to as all-to-all personalized communication or all-to-all scatter-<lb/>gather. The operation involves N 2 messages, M i;j , for 0 i; j N ?1. Usually, it is assumed that all <lb/>messages are the same length, `. Initially, each message M i;j is stored at node i; after the operation, <lb/>M i;j is to be stored at node j. <lb/>In order to realize a complete exchange in store-and-forward systems, the algorithm must utilize <lb/>indirect communication, in which one or more intermediate nodes receive and then relay a message <lb/>traveling between two non-adjacent nodes. To save communication startup time, the algorithm may <lb/>combine messages into larger message blocks to be transmitted as a single unit between adjacent <lb/>nodes. The standard exchange algorithm for hypercubes 14] is an example of a complete exchange <lb/>algorithm designed for one-port, store-and-forward systems. As shown in Figure 25, the standard <lb/>exchange algorithm consists of n steps; during each step, the cube is recursively divided into halves, <lb/>and messages are exchanged across this new division, between pairs of corresponding nodes. Not <lb/>shown in the gure is the local rearrangement of data between each step, which allow each message to <lb/>be transmitted from a contiguous block of memory. In the rst step, node i sends to node (i 2 0 ) all <lb/>messages residing at node i that are destined for the half-cube containing node (i 2 0 ). During the <lb/>next step node i sends to node (i 2 1 ) all messages residing at node i that are destined for the quarter-<lb/>cube containing node (i 2 1 ). This process continues recursively. By combining individual messages <lb/>into larger message blocks, the standard exchange algorithm is able to accomplish a complete exchange <lb/>in only n communication steps while using only nearest-neighbor communication. Each transmitted <lb/>message is of size (N=2)`, where `is the size of each message and local data permutation is required <lb/>after each step. <lb/>The same message-passing pattern can be used for other operations involving all nodes in the <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>25 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>(a) step 1 <lb/>(b) step 2 <lb/>(c) step 3 <lb/>1 <lb/>2 <lb/>0 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>Figure 25. Standard exchange in a 3-cube <lb/>hypercube, although the size of the messages in each step di ers 9]. In each step of all-to-all broadcast, <lb/>every node sends its own message as well as all those it has received from other nodes; the message <lb/>size in step i is m2 i?1 . For N=N reduction, every node performs a reduction after each step and <lb/>forwards the result in the next step; therefore the size of the message sent in every step is `. After <lb/>the nth step, all nodes will hold the reduced value. Since the standard exchange algorithm uses <lb/>only nearest-neighbor communication, its performance will be approximately the same on either a <lb/>store-and-forward or wormhole-routed architecture. <lb/>The direct algorithm for hypercubes 22], is a complete exchange algorithm designed to take ad-<lb/>vantage of the distance-insensitive communication of one-port, wormhole-routed hypercubes. The <lb/>algorithm is very simple: there are N ?1 communication steps; in each step k = 1; 2; : : : ; N ?1, node <lb/>i sends message M i;j to node j = i k and receives message M j;i from node j. Thus, all communication <lb/>occurs directly between the original source node of a message and the nal destination node of that <lb/>message. For communication between non-adjacent nodes only the routers are required to relay the <lb/>message at the intermediate nodes on the path. The direct algorithm is stepwise contention-free, in <lb/>that no two messages transmitted during any particular step will require a common communication <lb/>channel. <lb/>In contrast to the standard exchange algorithm, each node in the direct algorithm transmits only a <lb/>single message during each step. The direct algorithm is thus more e cient in terms of pure message <lb/>transmission time, requiring each node to send (and receive) `bytes during each of the N ?1 steps, for <lb/>a total of (N ? 1)`bytes per node; the total time for the operation is (N ? 1)(t s + `tf ). The standard <lb/>exchange algorithm, on the other hand, requires that each node send (and receive) (N=2)`bytes during <lb/>each of n steps for a total of n(N=2)`bytes per node and a total execution time of n(t s + (N=2)`t f ). <lb/>Also, where the standard exchange algorithm must completely \shu e&quot; the local data after each <lb/>communication step, no local data permutation is required by the direct algorithm. Which algorithm <lb/>is faster will depend on the ratio of the startup latency to the network latency, as well as the message <lb/>size `. Shorter messages and larger startup-to-network latency ratios will favor standard exchange <lb/>while longer messages and smaller ratios will result in better performance for direct exchange. Also, <lb/>the time required to permute the data after each step of the standard exchange algorithm should not <lb/>be ignored. <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>26 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>In a variation of the direct algorithm for hypercubes, the direct algorithm for 2D meshes 23] <lb/>performs the complete exchange in a wormhole-routed 2D mesh through a combination of pairwise <lb/>message exchanges. Like its hypercube counterpart, the direct algorithm for 2D meshes is a simple <lb/>algorithm: the nodes of the mesh are assigned the ordinal numbers 0 through N ? 1 in a row-major <lb/>fashion. During step k, for k = 1; 2; : : : ; N?1, the node with ordinal number i, for 0 i N?1, sends <lb/>message M i;j to, and receives message M j;i from, the node whose ordinal number is j = i k. Figure 7 <lb/>shows the communication steps of the direct algorithm on a 2D (2 4) mesh using dimension-ordered <lb/>routing. Notice that in steps 2, 3, 6, and 7, messages simultaneously require common communication <lb/>channels, leading to stepwise channel contention. Nonetheless, on systems with relatively high startup <lb/>latency and, therefore, excessive communication capacity, this factor may not signi cantly degrade <lb/>performance 23]. The algorithm can be generalized to accommodate arbitrary mesh sizes 23]. <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>Step 1. No contention <lb/>Step 2. Contention <lb/>Step 3. Contention <lb/>Step 4. No contention <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>Step 5. No contention <lb/>Step 6. Contention <lb/>Step 7. Contention <lb/>message path <lb/>area of contention <lb/>Figure 26. Direct pairwise exchange on a 2D mesh 23] <lb/>Another algorithm for complete exchange on a wormhole-routed 2D mesh, the binary exchange <lb/>algorithm for 2D mesh 24, 25], is derived from the standard exchange algorithm for hypercubes. <lb/>Whereas the standard exchange recursively halves the hypercube during each phase, the binary ex-<lb/>change algorithm recursively halves the 2D mesh during each phase, alternately in the X and Y <lb/>dimensions. Thus, 2d = log 2 N communication phases are required for a 2 d 2 d mesh; these phases <lb/>are illustrated in Figure 27 for an 8 8 mesh. As with standard exchange, blocks of N=2 messages are <lb/>exchanged between pairs of nodes, and local message permutation is required after each communication <lb/>phase. During the rst phase, each node exchanges messages with the corresponding node in the other <lb/>half of the mesh, based on a vertical division of the mesh; in the second phase, communication occurs <lb/>between corresponding nodes in opposing quadrants, based on the new horizontal division of the mesh. <lb/>As Figure 27 illustrates, all communication occurs between pairs of nodes in the same row or in the <lb/>same column. Since this algorithm incurs channel contention in each phase, we have labeled each <lb/>phase with the actual number of message-passing steps required. <lb/>In contrast, the quadrant exchange algorithm 24] was designed speci cally to account for worm-<lb/>hole routing and mesh topologies. During each stage of the algorithm, quadruples of nodes exchange <lb/>messages among themselves in a series of three communication steps, as shown in Figure 28(a). Af-<lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>27 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>(a) phase 1 (8 steps) <lb/>(b) phase 2 (4 steps) <lb/>(c) phase 3 (2 steps) <lb/>Figure 27. Binary exchange algorithm on a 2D mesh 25] <lb/>ter completing these three steps, the four nodes have then completely exchanged messages among <lb/>themselves. Due to the distance-insensitive nature of wormhole routing, the four nodes shown in <lb/>Figure 28(a) need not be adjacent; they need only form the corners of a rectangle in the mesh. <lb/>(a) three basic steps <lb/>(c) first phase in 8x8 mesh <lb/>(b) solution in 4x4 mesh <lb/>Figure 28. The quadrant exchange algorithm on a 2D mesh <lb/>The quadrant exchange performs the complete exchange by \quartering&quot; the mesh; this process <lb/>involves performing all the necessary inter-quadrant exchanges in a series of stages. Figure 28(b) <lb/>shows complete exchange operation on a 4 4 mesh. In the top two steps, the data at every node is <lb/>copied to one node in each of the other three 2 2 quadrants. In the bottom step, the operation is <lb/>performed on each of the quadrants. Figure 28(c) shows the rst phase (four steps) on an 8 8 mesh; <lb/>these operations quarter the mesh. To complete the total exchange, the algorithm is then concurrently <lb/>applied to each of the four 4 4 quadrants. The selection of the quadruples within each stage is made <lb/>so that communication is stepwise contention-free. In general, 2 j?1 stages are required to quarter a <lb/>2 j 2 j mesh, thus, P j i=1 2 j?i = 2 j ? 1 = p N ? 1 stages are needed. Since each stage consists of three <lb/>communication steps, then a total of 3( <lb/>p <lb/>N ? 1) communication steps are needed for the quadrant <lb/>exchange. <lb/>The quadrant exchange algorithm falls somewhere between a strictly store-and-forward algorithm <lb/>and a direct exchange. Although messages are transmitted in blocks rather than individually, direct <lb/>communication often occurs between non-adjacent nodes. During each stage of the algorithm, a node <lb/>x involved in a quadruple must send to the other three nodes of the quadruple all messages currently <lb/>at node x that are destined for the three quadrants of the mesh not containing node x. A node in <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>28 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>one such quadrant is accessed by x during each of the three communication steps of the stage. Thus, <lb/>blocks of N=4 messages are exchanged between pairs of nodes during each communication step. <lb/>Table 2 compares the number of steps and individual messages per transmitted message block <lb/>for each of the three 2D mesh total exchange algorithms described above. The quadrant exchange <lb/>algorithm, designed to balance the extremes of the binary exchange and direct algorithms, clearly <lb/>falls between these two algorithms in both the number of communication steps and the size of the <lb/>message blocks that are exchanged during each step. The product of these two values, which represents <lb/>the total number of messages sent (and received) by a single node during the total exchange, is also <lb/>balanced by quadrant exchange between the extremes of the other two algorithms. Only quadrant <lb/>exchange generates messages that are stepwise contention-free. Thus, the number of communication <lb/>steps listed for the other two algorithms is, in a sense, optimistic, since contention for communication <lb/>channels will cause some of these communication steps to be delayed. <lb/>Algorithm <lb/>Direct Binary exchange Quadrant exchange <lb/>Comm. steps N ? 1 <lb/>log 2 N <lb/>3( <lb/>p <lb/>N ? 1) <lb/>Message size <lb/>`(N=2)`(N=4)M <lb/>sgs per node N ? 1 (N=2)(log 2 N) (3N=4)( <lb/>p <lb/>N ? 1) <lb/>Table 2. Communication steps and message block size for 2D mesh total exchange algorithms <lb/>All-to-All Broadcast. All-to-all broadcast can be regarded as a degenerate case of complete ex-<lb/>change in which each node sends the same message to every other node. Although this operation may <lb/>be implemented as multiple, separate one-to-all broadcasts, network contention could signi cantly <lb/>delay the operation. Most algorithms for store-and-forward networks use a pipelining approach, in <lb/>which nodes forward each other&apos;s messages across the dimensions of the system. Unlike other collective <lb/>operations, the availability of wormhole routing has thus far yielded relatively little improvement over <lb/>store-and-forward algorithms. In one-port hypercubes the standard exchange method described earlier <lb/>achieves the same performance on both store-and-forward and wormhole-routed systems, since it uses <lb/>only nearest-neighbor communication. Since the message size doubles with each of the n steps, the <lb/>time required is nt s + (N ? 1)`t f . In one-port meshes and tori, a generalization of this method can <lb/>be used, whereby the nodes forward the messages around a logical ring in each dimension. The one <lb/>bene t of wormhole routing is that the message can be wormhole-routed across the mesh; there is no <lb/>di erence for the torus. This approach is identical to the collect phase of the scatter-collect broadcast <lb/>algorithm described earlier. For example, the suboperations depicted in Figures 20(c) and 20(d) are <lb/>actually performing an all-to-all broadcast on the data that had been scattered about the network. <lb/>This operation requires w i steps in each dimension i of a n-dimensional mesh or torus. The message <lb/>size increases by a factor of w i with each dimension. The total time is P n?1 i=0 (w i t s + (n ? i)`t f ). <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>29 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>For all-port hypercubes, Bertsekas et al 26], developed algorithms for all-to-all broadcast, scat-<lb/>ter, and complete exchange that were shown to be optimal under both store-and-forward as well <lb/>as wormhole-routed switching. These algorithms require relatively close synchronization among the <lb/>nodes. The fundamental concept is to use multiple disjoint spanning trees and to schedule message <lb/>transmission such that broadcasts from di erent nodes avoid one another in the network. Since the <lb/>port utilization at some nodes is 100% over the entire operation, the network switching technique does <lb/>not a ect performance. <lb/>Reduction and Scan. We mentioned previously that the standard exchange algorithm (Figure 25) <lb/>can be used to implement N/N reduction in hypercubes. As described by Kumar 9], the same <lb/>approach can also be used to implement scan operations by adding an additional \result&quot; bu er at <lb/>each node 9]. The result bu er at each node is initialized with the data at that node. The message <lb/>passing procedure is the same as for N=N reduction, except for one additional operation: at the end <lb/>of each communication step, the result bu er is updated only if the message in that step arrived from <lb/>a processor with a lower label than the recipient processor. At the end of n steps, each node&apos;s result <lb/>bu er will hold the appropriate value. <lb/>In a similar manner, the same message passing pattern used in all-to-all broadcast in meshes and <lb/>tori can also be used to implement reduction and scan operations in those topologies. In the case of <lb/>scan, some care must be taken to ensure that the nal result bu er at every node is based only on data <lb/>from lower-labeled nodes. Finally, if only a subset of nodes in the network is involved in the operation, <lb/>then a dimension-ordered chain can be used to circulate reduction/scan data, in the manner shown <lb/>in Figure 21 for the collect phase of scatter-collect multicast. These approaches are straightforward <lb/>extensions of the algorithms described earlier; hence, their details are omitted. <lb/>8 Hardware Support for Collective Communication <lb/>Until now, we have considered how to implement collective communication in systems that provide <lb/>only unicast communication in hardware. Such algorithms can improve their performance by taking <lb/>into account several characteristics of the underlying architecture, such as the wormhole switching and <lb/>the port model, even though those features were intended primarily to support unicast communication. <lb/>An alternative approach is to provide more direct support for collective communication in the system. <lb/>Two main approaches have been studied. In the rst, collective communication is implemented in a <lb/>network other than the primary data network. In the second, the data network is enhanced to better <lb/>support certain collective operations. Each of these approaches is discussed in turn. <lb/>Support Outside the Data Network. Two MPCs that supplement the data network with ad-<lb/>ditional hardware for various collective operations are the TMC Connection Machine CM-5 and the <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>30 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>Cray T3D. In the following , we brie y describe these systems and how they support collective com-<lb/>munication. However, the readers are referred to the papers cited in the following text for many of <lb/>the details that we omit due to space limitations. <lb/>The CM-5 27] is a descendent of the CM-2, a ne-grained SIMD system. The CM-5 is designed <lb/>to support synchronized MIMD computing, whereby nodes are allowed to execute independently of <lb/>one another, with special hardware provided to support relatively ne-grained synchronization and <lb/>communication among nodes. The CM-5 actually has three independent networks: a data network, <lb/>a diagnostic network, and a control network. The wormhole-routed data network for unicast com-<lb/>munication is based on the fat-tree topology, which o ers several paths between every pair of nodes <lb/>and a routing function that \pseudo-randomly&quot; selects among the possible paths. Figure 8 shows <lb/>an abstract representation of the data network for a 64-node CM-5. The data network interface <lb/>is memory-mapped in order to reduce startup latency. The diagnostic network uses a binary tree <lb/>topology and permits \back door&quot; access to all system hardware. The control network, which also <lb/>uses a binary tree with processors located at the leaves, provides direct hardware support for three <lb/>classes of collective operations: broadcast, reduction, and barrier synchronization. The interfaces to <lb/>the control network are memory mapped, and the operations must be executed on small, xed-length <lb/>messages; operations on larger messages may be implemented by cascading multiple operations. <lb/>SWITCH <lb/>160 MB/sec <lb/>80 MB/sec <lb/>40 MB/sec <lb/>Processors <lb/>Figure 29. A 64-node CM-5 data network fat-tree 28] <lb/>Since the CM-5 data network supports adaptive routing, the issues associated with the design of <lb/>unicast-based collective operations for this system are much di erent than those of deterministically-<lb/>routed k-ary n-cube systems, on which this paper has focused. Ponnusamy et al 29] have experimented <lb/>with various collective operations and parallel algorithms on the CM-5, concentrating primarily on <lb/>the e ect of contention in the network. Bozkus et al 30] have compared the performance of collective <lb/>operations as implemented on the CM-5 with the Intel Touchstone Delta. The CM-5 implementations <lb/>take advantage of the control network, while the Delta implementations rely strictly on software. The <lb/>CM-5 wins easily for operations on small messages. However, for large messages, the software overhead <lb/>of the Delta is su ciently amortized over the network latencies of the messages that the operations <lb/>execute faster on the Delta than on the CM-5. Finally, Brewer and Kuszmaul 28] suggest that the <lb/>data and control networks of the CM-5 may be used in tandem to implement fast collective operations. <lb/>In particular, barriers are used within collective data operations in order to reduce contention within <lb/>and between di erent phases of the operation, greatly improving performance. <lb/>The Cray T3D is an MPC that is designed to run as a back-end processor to a Cray Y-MP or Cray <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>31 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>C90 host system. The data network of the T3D is a wormhole-routed, bidirectional 3D torus. Unlike <lb/>the other systems considered in this paper, the architecture of the T3D supports distributed shared <lb/>memory. Memory references are translated into small packets by the network interface hardware; <lb/>memory consistency is the responsibility of software. Besides the data network, the T3D provides <lb/>additional hardware support for two forms of collective communication; both involve synchronization <lb/>among the nodes. First, each processor has access to two 8-bit registers. Each bit is connected to <lb/>a separate tree-wired barrier synchronization circuit. A processor invokes a barrier by setting the <lb/>corresponding bit; when all processors involved have set their bits, the circuit will reset all the bits <lb/>to 0. A processor may either busy-wait or be informed by an interrupt when the barrier condition is <lb/>met. Second, each node contains two fetch-and-increment registers, which are globally accessible to all <lb/>other processors. Since many programs distribute work segments to processes according to data array <lb/>indices, these registers provide an e cient mechanism for a process to seize a particular piece of the <lb/>computation without using a server process. These choices of functionality appear to provide a good <lb/>balance between limiting extra hardware and providing fast support for synchronization operations <lb/>most frequently used under the shared-memory programming paradigm. <lb/>Support Within the Data Network. An alternative to implementing a separate network is to <lb/>enhance the primary data network with somewhat more generic functionality that can be used to <lb/>support several collective operations. To improve collective communication performance and reduce <lb/>software overhead, two such enhancements to network routers have been proposed and implemented: <lb/>message replication and intermediate reception. Message replication refers to the ability to duplicate <lb/>incoming messages onto more than one outgoing channel, while intermediate reception is the ability <lb/>to simultaneously deliver an incoming message to the local processor/memory and to an outgoing <lb/>channel. <lb/>Unicast-based collective operations rely on local processors to replicate and relay messages. A <lb/>seemingly natural extension of a multicast or broadcast tree is to have the router itself replicate <lb/>the message, it-by-it, and forward it onto multiple outgoing links, e ectively producing a tree-like <lb/>message worm. Figure 30(a) shows how message replication can be used to solve a multicast problem <lb/>in a 6 6 2D mesh. The routers at nodes (1,2), (3,2), (4,2), and (5,2) are required to replicate the <lb/>message; the heads of the message follow XY paths. A di culty with this approach is that when any <lb/>branch of the tree encounters a required channel that is unavailable, the entire message tree must <lb/>be blocked, rendering all channels used by the tree unavailable for other communication. Moreover, <lb/>the dependencies among branches of the tree can lead to deadlock. The nCUBE-2 hypercube includes <lb/>hardware support for message replication, which is used to implement tree-based broadcast and limited <lb/>multicast where the destinations form a subcube, but this mechanism is not deadlock-free 31]. <lb/>In order to avoid the disadvantages of the tree-like worms produced by message replication, and yet <lb/>apply hardware support to the implementation of collective communication operations, the technique <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>32 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>source node <lb/>destination node <lb/>other node <lb/>0,0 <lb/>2,0 <lb/>3,0 <lb/>4,0 <lb/>1,0 <lb/>5,0 <lb/>3,1 <lb/>0,1 <lb/>1,1 <lb/>2,1 <lb/>4,1 <lb/>5,1 <lb/>0,2 <lb/>3,2 <lb/>1,2 <lb/>2,2 <lb/>4,2 <lb/>5,2 <lb/>2,3 <lb/>1,3 <lb/>3,3 <lb/>0,3 <lb/>5,3 <lb/>0,4 <lb/>1,4 <lb/>2,4 <lb/>3,4 <lb/>4,4 <lb/>5,4 <lb/>0,5 <lb/>1,5 <lb/>2,5 <lb/>3,5 <lb/>4,5 <lb/>5,5 <lb/>4,3 <lb/>0,0 <lb/>2,0 <lb/>3,0 <lb/>4,0 <lb/>1,0 <lb/>5,0 <lb/>3,1 <lb/>0,1 <lb/>1,1 <lb/>2,1 <lb/>4,1 <lb/>5,1 <lb/>0,2 <lb/>3,2 <lb/>1,2 <lb/>2,2 <lb/>4,2 <lb/>5,2 <lb/>2,3 <lb/>1,3 <lb/>3,3 <lb/>0,3 <lb/>5,3 <lb/>0,4 <lb/>1,4 <lb/>2,4 <lb/>3,4 <lb/>4,4 <lb/>5,4 <lb/>0,5 <lb/>1,5 <lb/>2,5 <lb/>3,5 <lb/>4,5 <lb/>5,5 <lb/>4,3 <lb/>5 <lb/>6 <lb/>17 <lb/>18 <lb/>29 <lb/>30 <lb/>4 <lb/>7 <lb/>16 <lb/>19 <lb/>28 <lb/>31 <lb/>3 <lb/>8 <lb/>15 <lb/>20 <lb/>27 <lb/>32 <lb/>2 <lb/>9 <lb/>14 <lb/>21 <lb/>26 <lb/>33 <lb/>1 <lb/>10 <lb/>13 <lb/>22 <lb/>25 <lb/>34 <lb/>0 <lb/>11 <lb/>12 <lb/>23 <lb/>24 <lb/>35 <lb/>0,0 <lb/>2,0 <lb/>3,0 <lb/>4,0 <lb/>1,0 <lb/>5,0 <lb/>3,1 <lb/>0,1 <lb/>1,1 <lb/>2,1 <lb/>4,1 <lb/>5,1 <lb/>0,2 <lb/>3,2 <lb/>1,2 <lb/>2,2 <lb/>4,2 <lb/>5,2 <lb/>2,3 <lb/>1,3 <lb/>3,3 <lb/>0,3 <lb/>5,3 <lb/>0,4 <lb/>1,4 <lb/>2,4 <lb/>3,4 <lb/>4,4 <lb/>5,4 <lb/>0,5 <lb/>1,5 <lb/>2,5 <lb/>3,5 <lb/>4,5 <lb/>5,5 <lb/>4,3 <lb/>(b) dual−path <lb/>(c) multi path <lb/>(a) message replication <lb/>Figure 30. Message replication and path-based routing <lb/>of intermediate reception (IR) has been proposed. A router possessing IR capability is able to copy the <lb/>its of a message to the memory of the local processor as the message passes through the router enroute <lb/>to other destinations. In this way, a message originating at a source node can be routed as a single <lb/>worm through several destination nodes, depositing a copy of the message at each of the intermediate <lb/>destinations as it passes through. Such communication methods are termed path-based, while the <lb/>constituent messages are called multi-destination worms. Intermediate reception has been used in the <lb/>University of Michigan HARTS machine, which uses a wraparound hexagonal mesh topology, and in <lb/>a version of the Caltech Mesh-Routing Chips. <lb/>The routing rules used for path-based routing must be liberal enough to allow exibility in message <lb/>routing, so that many intermediate destination nodes can be visited by a single message. However, <lb/>these rules must still avoid deadlock. One way in which deadlock can be avoided in path-based routing <lb/>is by ensuring that there are no cycles of channel dependency allowed by the routing algorithm. En-<lb/>suring an absence of channel dependence cycles, and thus freedom from deadlock, can be accomplished <lb/>by means of a Hamiltonian Path (HP). For example, in Figure 30(b), the numbers near the upper-<lb/>right corner of each node correspond to one (of many) HPs through the network. If the routing <lb/>algorithm can be designed so that messages always visit nodes (including the destination nodes and <lb/>the intermediate nodes at which only the router is used) in the same order as those nodes appear on <lb/>a particular HP, then cyclic dependences will be impossible, and deadlock will be prevented. <lb/>Lin, et al 31] have used an HP similar to that represented in Figure 30(b) to develop a family of <lb/>path-based multicast routing algorithms for 2D mesh networks. In the most basic of these algorithms, <lb/>termed dual-path, the source node of a multicast generates at most two multi-destination worms. One <lb/>worm travels forward from the source, visiting all destination nodes that appear after the source on <lb/>the HP. Another multi-destination travels backwards on the HP, reaching those destination nodes <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>33 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>that appear prior to the source node on the HP. Figure 30(b) shows the two message worms generated <lb/>by the dual-path algorithm for an example multicast operation. Notice that the algorithm allows <lb/>messages to \skip over&quot; some of the nodes on the HP, provided that the nodes visited by any message <lb/>worm appear in the same relative order as de ned by the HP. <lb/>To take advantage of the communication parallelism available in systems with all-port architecture, <lb/>this algorithm can be extended to generate up to four concurrent multi-destination message worms in <lb/>a 2D mesh, as shown in Figure 30(c). Each worm reaches the destination nodes in the corresponding <lb/>quadrant of the mesh, with respect to the source node. Again, each worm visits nodes in an order that <lb/>is consistent with the HP represented in Figure 30(b), thereby ensuring deadlock-free communication. <lb/>Besides 2D meshes, the path-based approach can be applied to hypercubes and tori, as well. <lb/>In path-based routing for operations with arbitrary destination sets, such as multicast, the message <lb/>worm is generally pre xed with a list of destination node addresses, or o sets between destinations. <lb/>The order of these addresses corresponds to the order in which the destination nodes will be visited. <lb/>Once the message header reaches the rst destination node in the list, the router at that node strips <lb/>its own address from the head of the list, and forwards the remainder of the message worm toward the <lb/>next destination node, while simultaneously copying the message contents to the local host memory. <lb/>The port model of the system also has a large e ect on the design and performance of path-<lb/>based collective communication operations. Path-based routing typically requires multiple input ports, <lb/>also called consumption channels, else deadlock may occur when two or more path-based collective <lb/>operations are issued concurrently. Figure 31 illustrates a scenario in which two multi-destination <lb/>worms are each attempting to deliver a message to nodes x and y. However, each message is holding <lb/>the single input port at one node, while waiting to use the input port at the other node, resulting in <lb/>communication deadlock. <lb/>Local Processor <lb/>and <lb/>Memory <lb/>Router <lb/>Local Processor <lb/>and <lb/>Memory <lb/>Router <lb/>x <lb/>Node <lb/>Node y <lb/>Message a <lb/>Message b <lb/>Deadlock point <lb/>Message <lb/>Internal port <lb/>Figure 31. Port-induced communication deadlock in a one-port system <lb/>Finally, multiple path-based operations may be combined to form another collective operation. <lb/>Even broadcast may be implemented more quickly using multiple, multidestination worms than using <lb/>a single worm, due to the excessive length of the latter. In the worst case, a single worm of length m <lb/>requires time t s + (N ? 1 + m)t f , where N is the number of nodes in the network and t f is the time <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>34 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>to transmit a it between two routers. In a 2D mesh, however, if the source multicasts to all nodes in <lb/>its row, and each node in the row then multicasts to all nodes in their respective columns, then the <lb/>time required is 2(t s + ( <lb/>p <lb/>N ? 1 + m)t f ). Panda and Singal 32] have proposed just such an approach <lb/>to broadcast in a mesh, which can achieve much better performance than a single-path approach in <lb/>large meshes. Ho and Kao 33] have also proposed a path-based version of the broadcast algorithm <lb/>described in Section 4. The routing is identical to that of the algorithm in Section 4, except that <lb/>each node sends to all its children along a single path in one step. It would appear that path-based <lb/>collective communication has merits, although this area has received relatively little attention thus <lb/>far 32]. <lb/>9 Conclusions <lb/>As the supercomputing industry has migrated towards parallel architectures, many with distributed <lb/>memory, wormhole-routed switching has played a major role in reducing the cost of unicast com-<lb/>munication among nodes. As this paper has shown, many new algorithms have also been proposed <lb/>to implement the collective communication operations in wormhole-routed networks. In many cases, <lb/>these solutions di er fundamentally from their store-and-forward counterparts because they attempt <lb/>to exploit the distance insensitivity of wormhole routing. Other architectural characteristics, such as <lb/>the topology, port model, and hardware routing algorithm, must also be accounted for in order to <lb/>achieve good performance. <lb/>This paper has attempted to elucidate several key issues in the design of collective operations <lb/>in wormhole-routed networks and has highlighted some of the extensive research that has already <lb/>been conducted in this area. We have focused primarily on software solutions that take advantage <lb/>of underlying architectural characteristics. In order to keep the survey manageable, however, several <lb/>related topics were intentionally omitted, such as adaptive routing, fault-tolerance, and switch-based <lb/>interconnection networks. These are important topics in their own right, and surveys of the research <lb/>and commercial projects in these areas would likely be of great interest to the community. <lb/>Several of the topics mentioned in the paper have only recently begun to receive attention from <lb/>the research community and merit further study. First, as adaptive routing algorithms mature and <lb/>begin to appear in research prototypes and commercial systems, the need arises to study how this <lb/>capability can be exploited by collective operations. For example, it may be possible to design collec-<lb/>tive operations so that the constituent messages adapt to one another in a desired manner. Second, <lb/>the study of collective operations in multi-port systems and in systems that support intermediate <lb/>reception has only just started. Many subtopics have not yet been addressed at all, even through <lb/>analysis and simulation. In addition , research prototypes that possess these features are needed for <lb/>experimental studies. A particularly intriguing subject is multiphase collective operations based on <lb/>underlying path-based messages. Third, the mapping between higher-level programming paradigms <lb/></body>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>35 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<body>and collective operations is becoming a research area in its own right. Although a good deal of research <lb/>is presently studying the use of collective communication in numerical libraries 10], and the relation-<lb/>ship between data distribution patterns and the collective communication generated by compilers for <lb/>data parallel languages 3], the potential payo s from both areas are high, and increased research is <lb/>needed. Finally, another trend in high-performance computing is towards clusters of workstations con-<lb/>nected by high-speed switches. Many such environments o er some of the same features as wormhole <lb/>networks, including pipelining of messages, concurrent input and output, concurrent message passing, <lb/>and hardware-supported multicast communication. As such systems become more widespread, the <lb/>need arises to explore how to best support collective operations in this new class of architectures 34]. <lb/></body>

			<div type="acknowledgement">Acknowledgments <lb/>The authors would like to thank several faculty and students at Michigan State University whose work <lb/>has either directly or indirectly contributed to this paper: Lionel M. Ni, Abdol-Hossein Esfahanian, <lb/>Betty H. C. Cheng, Xiaola Lin, Hong Xu, Christian Tre tz, Chengchang Huang, Dan Judd, and <lb/>Jeremy Uniacke. We would also like to thank those many researchers who have made contributions <lb/>to this area and who have helped to promote the bene ts of e cient collective communication. This <lb/>work was supported in part by the NSF grants MIP-9204066, CDA-9121641, CDA9222901, by DOE <lb/>grant DE-FG02-93ER25167, and by an Ameritech Faculty Fellowship. <lb/></div>

			<div type="annex">Further Information <lb/>A number of related papers and technical reports of the Communications Research Group at Michigan <lb/>State University are available via anonymous ftp. The site is ftp.cps.msu.edu and the directory is <lb/>pub/crg. <lb/></div>

			<listBibl>References <lb/>1] V. S. Sunderam, \PVM: A framework for parallel distributed computing,&quot; Concurrency: Practice and <lb/>Experience, vol. 2(4), pp. 315{339, Dec. 1990. <lb/>2] D. B. Loveman, \High Performance Fortran,&quot; IEEE Parallel and Distributed Technology, vol. 1, pp. 25{42, <lb/>Feb. 1993. <lb/>3] J. Li and M. Chen, \Compiling communication-e cient programs for massively parallel machines,&quot; IEEE <lb/>Transactions on Parallel and Distributed Systems, vol. 2, pp. 361{375, July 1991. <lb/>4] S. Hiranandani, K. Kennedy, and C.-W. Tseng, \Compiling Fortran D for MIMD distributed-memory <lb/>machines,&quot; Communications of the ACM, pp. 66{80, Aug. 1992. <lb/>5] Message Passing Interface Forum, \Document for standard message-passing interface,&quot; Tech. Rep. CS-93-<lb/>214, University of Tennessee, Nov. 1993. <lb/>6] W. J. Dally and C. L. Seitz, \The torus routing chip,&quot; Journal of Distributed Computing, vol. 1, no. 3, <lb/>pp. 187{196, 1986. <lb/></listBibl>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>36 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<listBibl>7] L. M. Ni and P. K. McKinley, \A survey of wormhole routing techniques in direct networks,&quot; IEEE <lb/>Computer, vol. 26, pp. 62{76, Feb. 1993. <lb/>8] P. K. McKinley, H. Xu, A.-H. Esfahanian, and L. M. Ni, \Unicast-based multicast communication in <lb/>wormhole-routed networks,&quot; in Proc. of the 1992 International Conference on Parallel Processing, vol. II, <lb/>pp. 10{19, Aug. 1992. <lb/>9] V. Kumar, A. Grama, A. Gupta, and G. Karypis, Introduction to Parallel Computing: Design and Analysis <lb/>of Algorithms. Redwood City, California: Benjamin/Cummings, 1994. <lb/>10] J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, \ScaLAPACK: A scalable linear algebra library for <lb/>distributed memory concurrent computers,&quot; in Proceedings of the Fourth Symposium on the Frontiers of <lb/>Massively Paralllel Computation, pp. 120{127, IEEE CS Press, 1992. <lb/>11] J. Dongarra, R. van de Geijn, and R. Whaley, \Two dimensional basic linear algebra communication <lb/>subprograms,&quot; in Proceedings of the sixth SIAM conference on Parallel Processing, pp. 347{352, 1993. <lb/>12] B. Nitzberg and V. Lo, \Distributed shared memory: A survey of issues and algorithms,&quot; IEEE Computer, <lb/>vol. 24, pp. 52{60, Aug. 1991. <lb/>13] H. Sullivan and T. R. Bashkow, \A large scale, homogeneous, fully distributed parallel machine,&quot; in <lb/>Proceedings of the 4th Annu. Symp. Comput. Architecture, vol. 5, pp. 105{124, Mar. 1977. <lb/>14] S. L. Johnsson and C.-T. Ho, \Optimum broadcasting and personalized communication in hypercubes,&quot; <lb/>IEEE Transactions on Computers, vol. C-38, pp. 1249{1268, Sept. 1989. <lb/>15] P. K. McKinley and C. Tre tz, \E cient broadcast in all-port wormhole routed hypercubes,&quot; in Proc. of <lb/>the 1993 International Conference on Parallel Processing, vol. II, pp. 288{291, 1993. <lb/>16] C. Ho and M. Kao, \Optimal broadcast on hypercubes with wormhole and e-cube routings,&quot; in Proceedings <lb/>of the 1993 International Conference on Parallel and Distributed Systems, (Taipei, Taiwan), pp. 694{697, <lb/>Dec. 1993. <lb/>17] M. Barnett, D. G. Payne, and R. van de Geijn, \Optimal broadcasting in mesh-connected architectures,&quot; <lb/>Tech. Rep. TR-91-38, Department of Computer Science, The University of Texas at Austin, Dec. 1991. <lb/>18] D. F. Robinson, P. K. McKinley, and B. H. C. Cheng, \Optimal multicast communication in torus net-<lb/>works,&quot; in Proc. of the 1994 International Conference on Parallel Processing, Aug. 1994. accepted to <lb/>appear. <lb/>19] M. Barnett, D. G. Payne, R. A. van de Geijn, and J. Watts, \Broadcasting on meshes with worm-hole <lb/>routing,&quot; Tech. Rep. TR-93-24, Department of Computer Science, The University of Texas at Austin, <lb/>November 2 1993. <lb/>20] Y.-J. Tsai and P. K. McKinley, \An extended dominating node approach to collective communication in <lb/>wormhole-routed 2D meshes,&quot; in Proceedings of the Scalable High Performance Computing Conference, <lb/>pp. 199{206, 1994. <lb/>21] D. F. Robinson, D. Judd, P. K. McKinley, and B. H. C. Cheng, \E cient collective data distribution in <lb/>all-port wormhole-routed hypercubes,&quot; in Proceedings of Supercomputing&apos;93, pp. 792{803, November 1993. <lb/>22] S. R. Seidel, \Circuit switched vs. store-and-forward solutions to symmetric communication problems,&quot; <lb/>in Proceedings of the 4th Conference on Hypercube Computers and Concurrent Applications, pp. 253{255, <lb/>1989. <lb/>23] R. Thakur and A. Choudhary, \All-to-all communication on meshes with wormhole routing,&quot; in Proceedings <lb/>of the 1994 International Parallel Processing Symposium, pp. 561{565, 1994. <lb/>24] S. H. Bokhari and H. Berryman, \Complete exchange on a circuit switched mesh,&quot; in Proceedings of the <lb/>1992 Scalable High Performance Computing Conference, pp. 300{306, Apr. 1992. <lb/>25] S. Gupta, S. Hawkinson, and B. Baxter, \A binary interleaved algorithm for complete exchange on a mesh <lb/>architecture,&quot; tech. rep., Intel, Beaverton, Oregon. <lb/></listBibl>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>37 <lb/></page>

			<note place="footnote">Michigan State University <lb/></note>

			<listBibl>26] D. P. Bertsekas, C. Ozveren, G. D. Stamoulis, and J. N. Tsitsiklis, \Optimal communication algorithms <lb/>for hypercubes,&quot; Journal of Parallel and Distributed Computing, vol. 15, no. 11, pp. 263{175, 1991. <lb/>27] C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. <lb/>Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S.-W. Yang, and R. Zak, \The network <lb/>architecture of the connection machine CM-5,&quot; in Proc. 4th ACM Symposium on Parallel Algorithms and <lb/>Architectures, pp. 272{285, June 1992. <lb/>28] E. A. Brewer and B. C. Kuszmaul, \How to get good performance from the CM-5 data network,&quot; in <lb/>Proceedings of the 1994 International Parallel Processing Symposium, pp. 858{867, 1994. <lb/>29] R. Ponnusamy, R. Thakur, A. Choudhary, K. Velamakanni, Z. Bozkus, and G. Fox, \Experimental per-<lb/>formance evaluation of the CM-5,&quot; Journal of Parallel and Distributed Computing, vol. 19, pp. 192{202, <lb/>1993. <lb/>30] Z. Bozkus, S. Ranka, G. Fox, and A. Choudhary, \Performance comparison of the cm-5 and intel touchstone <lb/>delta for dataparallel operations,&quot; in Proceedings of the Fifth IEEE Symposium on Parallel and Distributed <lb/>Processing, (Dallas, Texas), pp. 307{310, December 1993. <lb/>31] X. Lin, P. K. McKinley, and L. M. Ni, \Deadlock-free multicast wormhole routing in 2D mesh multicom-<lb/>puters,&quot; IEEE Transactions on Parallel and Distributed Systems, vol. 5, pp. 793{804, Aug. 1994. <lb/>32] D. K. Panda and S. Singal, \Broadcasting in k-ary n-cube wormhole routed networks using path-based <lb/>routing,&quot; Tech. Rep. TR36., Ohio State University, Sept. 1993. <lb/>33] C.-T. Ho and M. Kao, \Optimal broadcast in all-port wormhole-routed hypercubes,&quot; in Proceedings of the <lb/>1994 International Conference on Parallel Processing, Aug. 1994. accepted to appear. <lb/>34] C. C. Huang and P. K. McKinley, \Communication issues in parallel computing across ATM networks,&quot; <lb/>IEEE Parallel and Distributed Technology, 1994. accepted to appear. <lb/></listBibl>

			<note place="footnote">Communications Research Group <lb/></note>

			<page>38 <lb/></page>

			<note place="footnote">Michigan State University </note>


	</text>
</tei>
