<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title level="a">Finding âˆ€âˆƒ Hyperbugs using Symbolic Execution</title>
        <author>
          <persName>
            <forename>Arthur</forename>
            <surname>Correnson</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Tobias</forename>
            <surname>NieÃŸen</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Bernd</forename>
            <surname>Finkbeiner</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Georg</forename>
            <surname>Weissenbacher</surname>
          </persName>
        </author>
      </titleStmt>
      <editionStmt>
        <edition>
          <date when="2025-10-30T14:58:20.194823Z">30.10.2025 14:58:20</date>
          <title>grobid.training.segmentation [default]</title>
          <idno type="fileref">10.1145$1$3689761</idno>
        </edition>
      </editionStmt>
      <publicationStmt>
        <publisher>Association for Computing Machinery (ACM)</publisher>
        <availability>
          <licence target="https://creativecommons.org/licenses/by/4.0/"/>
        </availability>
        <date type="publication">2024</date>
        <idno type="DOI">10.1145/3689761</idno>
      </publicationStmt>
      <sourceDesc>
        <bibl>Arthur Correnson, Tobias NieÃŸen, Bernd Finkbeiner, Georg Weissenbacher. (2024). Finding âˆ€âˆƒ Hyperbugs using Symbolic Execution. Proceedings of the ACM on Programming Languages, 8(OOPSLA2), 1420-1445. DOI: 10.1145/3689761</bibl>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <appInfo>
        <application version="1.0" ident="pdf-tei-editor" type="editor">
          <ref target="https://github.com/mpilhlt/pdf-tei-editor"/>
        </application>
        <application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-10-30T14:58:20.194823Z" type="extractor">
          <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
          <label type="revision">eb7768b</label>
          <label type="flavor">default</label>
          <label type="variant-id">grobid.training.segmentation</label>
          <ref target="https://github.com/kermitt2/grobid"/>
        </application>
      </appInfo>
    </encodingDesc>
    <revisionDesc>
      <change when="2025-10-30T14:58:20.194823Z" status="draft">
        <desc>Generated with createTraining API</desc>
      </change>
    </revisionDesc>
  </teiHeader>
  <text xmlns="http://www.tei-c.org/ns/1.0" xml:lang="en">
        <front>Fully Verified Instruction Scheduling <lb/>ZITENG YANG, Georgia Institute of Technology, USA <lb/>JUN SHIRAKO, Georgia Institute of Technology, USA <lb/>VIVEK SARKAR, Georgia Institute of Technology, USA <lb/>CompCert project, the state-of-the-art compiler that achieves the first end-to-end formally verified C compiler, <lb/>does not support fully verified instruction scheduling. Instead, existing research that works on such topics <lb/>only implements translation validation. This means they do not have direct formal proof that the scheduling <lb/>algorithm is correct, but only a posterior validation to check each compiling case. Using such a method, <lb/>CompCert accepts a valid C program and compiles correctly only when the untrusted scheduler generates <lb/>a correct result. However, it does not guarantee the complete correctness of the scheduler. It also causes <lb/>compile-time validation overhead in the view of runtime performance. <lb/>In this work, we present the first achievement in developing a mechanized library for fully verified <lb/>instruction scheduling while keeping the proof workload acceptably lightweight. The idea to reduce the proof <lb/>length is to exploit a simple property that the topological reordering of a topological sorted list is equal to a <lb/>sequence of swapping adjacent unordered elements. Together with the transitivity of semantic simulation <lb/>relation, the only burden will become proving the semantic preservation of a transition that only swaps <lb/>two adjacent independent instructions inside one block. After successfully proving this result, proving the <lb/>correctness of any new instruction scheduling algorithm only requires proof that it preserved the syntax-level <lb/>dependence among instructions, instead of reasoning about semantics details every time. We implemented a <lb/>mechanized library of such methods in the Coq proof assistant based on CompCert&apos;s library as a framework <lb/>and used the list scheduling algorithm as a case study to show the correctness can be formally proved using <lb/>our theory. <lb/>We show that with our method that abstracts away the semantics details, it is flexible to implement any <lb/>scheduler that reorders instructions with little extra proof burden. Our scheduler in the case study also <lb/>abstracts away the outside scheduling heuristic as a universal parameter so it is flexible to modify without <lb/>touching any correctness proof. <lb/>CCS Concepts: â€¢ Theory of computation â†’ Program verification; Operational semantics; â€¢ Software <lb/>and its engineering â†’ Formal software verification; Compilers. <lb/>Additional Key Words and Phrases: Compiler Verification, Coq Proof Assistant, CompCert, Instruction-level <lb/>Parallelism <lb/>ACM Reference Format: <lb/>Ziteng Yang, Jun Shirako, and Vivek Sarkar. 2024. Fully Verified Instruction Scheduling. Proc. ACM Program. <lb/>Lang. 8, OOPSLA2, Article 299 (October 2024), 26 pages. https://doi.org/10.1145/3689739 <lb/>Authors&apos; Contact Information: Ziteng Yang, Georgia Institute of Technology, Atlanta, USA, ziteng.yang@gatech.edu; Jun <lb/>Shirako, Georgia Institute of Technology, Atlanta, USA, shirako@gatech.edu; Vivek Sarkar, Georgia Institute of Technology, <lb/>Atlanta, USA, vsarkar@gatech.edu. <lb/>Â© 2024 Copyright held by the owner/author(s). <lb/>ACM 2475-1421/2024/10-ART299 <lb/>https://doi.org/10.1145/3689739 <lb/>Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/>This work is licensed under a Creative Commons Attribution 4.0 International License. <lb/>299:2 <lb/>Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></front>
        
        <body>1 Introduction <lb/>1.1 Compiler Correctness and Formally Verified Compiler <lb/>There has been significant progress on the topic of compiler correctness in the past decades. A key <lb/>goal for compiler correctness is to formally verify the code translations performed by a compiler. <lb/>The CompCert certified compiler [31, 32] is the first successful project to formally verify a realistic <lb/>compiler translation. In the later decade, more advanced formal theories on more optimizing passes <lb/>[35], supporting linking [26, 27, 37, 46], concurrency [26, 43], improving memory models [6, 50], etc. <lb/>have occurred. In another aspect of the actual performance, both the compiler testing experiment <lb/>of [52] and [54] shows that CompCert&apos;s verified passes have passed all the provided test cases while <lb/>GCC and Clang were found to have hundreds of test failures respectively. <lb/>We observed that most of the state-of-the-art research for certified compilation involving paral-<lb/>lelism/concurrency are compiling functions of the multi-thread program, a.k.a developing advanced <lb/>correctness theory for concurrent settings (e.g. [26, 33]). However, there&apos;s much less progress on <lb/>improving the parallelism of a program: compile-time optimization for data/instruction/thread-level <lb/>parallelism. In other words, CompCert does not support -O2 or -O3 optimizations like GCC or Clang <lb/>currently. Instruction scheduling [4, 12, 17, 19, 22, 23, 51] is one of the most important methods to <lb/>improve instruction-level parallelism. It can be conducted during two different stages: runtime or <lb/>compile-time. Instruction scheduling during runtime is known as out-of-order (OOO) execution. <lb/>Compiler-level instruction scheduling statically reorders the instruction sequence to improve <lb/>CPU throughput by removing pipeline hazards and multi-issue pipeline performance by enhancing <lb/>instruction-level parallelism. Instruction scheduling is a key optimizing pass for in-order processors, <lb/>which do not support out-of-order pipeline execution in hardware so as to achieve higher energy <lb/>efficiency, lower hardware cost, and more predictable execution time. These characteristics are <lb/>often critical for various areas, including embedded systems and mobile devices [15, 49]. <lb/>1.2 Related Work on Instruction Scheduling and its Verification <lb/>Instruction scheduling is a classical compiler optimization pass that aims to minimize the schedule <lb/>length of given code fragments on the target processor. The instruction scheduling problem can be <lb/>classified into local scheduling to reorder instructions within a basic block and global scheduling <lb/>to reorder instructions within and across basic blocks (also referred as intra-block scheduling and <lb/>inter-block scheduling). The optimal local scheduling problem for modern multi-issue pipelined <lb/>processors is known to be NP-complete [5]. Although the approach based on integer programming <lb/>can produce the optimal schedule in a reasonable time [51], heuristic approaches typified by <lb/>list scheduling [12, 23, 28] are often used in production compilers to achieve fast and efficient <lb/>scheduling. The global scheduling problem also has a long history and there is an extensive body <lb/>of literature including scheduling algorithms targeting VLIW processors [17, 20, 22, 30, 41] and <lb/>superscalar machines [4, 11, 13, 19]. <lb/>To the best of our knowledge, the most recent existing works for supporting instruction sched-<lb/>uling in a verified compiler are that of [48], which achieved translation validation [36, 39] and <lb/>a case study on list scheduling and trace scheduling [18, 22], and the more recent work on [44] <lb/>which also used translation validation for a list-scheduling instruction scheduling pass for VLIW <lb/>architecture and its follow-up papers [24, 45] that use the same approach to further support and <lb/>improve inter-block instruction scheduling. However, there&apos;s a major difference between direct <lb/>verification (the goal of this paper) and verified translation validation (past work), as explained in <lb/>the next subsection. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:3 <lb/></page>
        
        <body>1.3 Full Verification v.s. Verified Translation Validation <lb/>Full verification includes proving the correctness of the entire algorithm for a compiler pass, while <lb/>verified translation validation includes only proving the correctness of the checker of the algorithm. <lb/>In general, full verification is preferable to translation validation. However, it can be much more <lb/>challenging due to its heavy verification requirements. A representative example is verifying <lb/>register allocation compiler pass in CompCert using a graph coloring algorithm. In the work of [7], <lb/>4300 lines of Coq proof codes were used to formally verify the register allocation pass in CompCert, <lb/>with a prior lemma of 10,000 lines proof on the graph coloring algorithm. Compared with this, the <lb/>verified translation validation was also presented in the same year, with only 900 lines of proofs <lb/>[42]. <lb/>We summarize both the benefits and drawbacks of these two technical routes that aim to <lb/>guarantee software safety: <lb/>Full Verification: verifying the correctness of an algorithm for a compiler pass, <lb/>â€¢ Core Technology: correctness of an algorithm <lb/>â€¢ Pro: Full correctness guarantee: verified scheduler has no bug and any execution instance <lb/>will be correct. <lb/>â€¢ Cons: Development-time overhead: potentially heavy work and strongly related to algorithm <lb/>implementation (our work will show there exists acceptable length of proofs for verifying <lb/>instruction scheduling). <lb/>â€¢ Pro: No runtime overhead: correctness is checked during development with no overhead <lb/>incurred at runtime. <lb/>Verified Translation Validation: correctness of each result from each input of an algorithm <lb/>â€¢ Core Technology: Symbolic Execution of source/target codes <lb/>â€¢ Pro: Simpler Correctness Proof and independent of algorithm implementations <lb/>â€¢ Cons: Partial correctness guarantee: only guarantee each execution instance will reject the <lb/>wrong result by scheduler bugs. Also if the validator is incomplete, it may reject a valid <lb/>program. <lb/>â€¢ Cons: Run-time overhead: correctness requires runtime validation and usually has high <lb/>complexity <lb/>More formally, the correctness proof for the verified translation validator builds on the following <lb/>lemma: <lb/>âˆ€ğ‘ ğ‘  ğ‘ ğ‘¡ : ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘’ (ğ‘ ğ‘  ) = ğ‘‚ğ¾ ğ‘ ğ‘¡ â†’ ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ (ğ‘ ğ‘  , ğ‘ ğ‘¡ ) = ğ‘¡ğ‘Ÿğ‘¢ğ‘’ â†’ ğ‘†ğ‘–ğ‘šğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ ğ‘  , ğ‘ ğ‘¡ ) <lb/>while the result of direct verification is proved to have: <lb/>âˆ€ğ‘ ğ‘  ğ‘ ğ‘¡ : ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘’ (ğ‘ ğ‘  ) = ğ‘‚ğ¾ ğ‘ ğ‘¡ â†’ ğ‘†ğ‘–ğ‘šğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ ğ‘  , ğ‘ ğ‘¡ ) <lb/>which is a much stronger result. <lb/>Fig. 1 shows the difference between verified translation validation and full verification in terms <lb/>of instruction scheduling passes. <lb/>1.4 Contributions <lb/>In the conclusion of [48], the authors write that they believe it would be significantly more difficult <lb/>to directly prove the correctness of list scheduling. However, our experience in this work shows that <lb/>the proof burden can lead to relatively lightweight workloads for proof, compared with previous <lb/>verification work on instruction scheduling. <lb/>In this work, we introduce the first known machine-independent correctness framework that <lb/>supports a full correctness proof for intra-block scheduling, i.e. instruction scheduling within a basic <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:4 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>Scheduling Pass <lb/>Scheduler ğ‘†: ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› <lb/>Untrusted <lb/>Error <lb/>Validate(p, p&apos;) <lb/>IR program ğ‘ <lb/>ğ‘â€² <lb/>Trusted <lb/>OK <lb/>Other Passes <lb/>Compiler Bug, Revise Scheduler <lb/>(a) Verified Translation Validation [44, 48] <lb/>Scheduling Pass <lb/>Scheduler ğ‘†: ğ‘‚ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’ â†’ ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› <lb/>Untrusted <lb/>IR program ğ‘ <lb/>ğ‘â€² <lb/>Trusted <lb/>Other Passes <lb/>Any Scheduler Oracles ğ‘‚ <lb/>OK <lb/>(b) Full Verification (This work) <lb/>Fig. 1. Comparasion between two verification results of instruction scheduling <lb/>block. Our result is a once-for-all formal correctness framework with an acceptable length (around <lb/>3,000 lines of Coq code) for proving any instruction scheduling algorithm. We then implemented <lb/>a case study on its usage of proving list scheduling on Risc-V architecture using critical path <lb/>scheduling as a heuristic, which also shows optimization improvements on benchmarks PolyBench <lb/>C 4.2. The proof of this case study has around further 1,000 lines of Coq code. <lb/>We summarize the highlights of our results as multiple levels of flexibility: <lb/>â€¢ Flexible algorithm changes: Our framework guarantees that any program transformation <lb/>can be proved semantically sound by only proving that it satisfies the dependency relation <lb/>defined in syntax level of the original program, without repeatedly reasoning anything about <lb/>semantic details again. That means changing a scheduling algorithm only changes a small <lb/>part of the proofs. <lb/>â€¢ Flexible instruction scheduling heuristics: as we implemented list scheduling as a case study, <lb/>the heuristics to give priority when scheduling an instruction is an abstract parameter in our <lb/>proof. This means it can be any heuristics that vary among different machine architectures, <lb/>runtime environments, or even profiling results, etc., and the actual choice does not influence <lb/>the correctness so it does not change proof codes. This part makes our methods equally <lb/>flexible as previous translation validation methods <lb/>â€¢ Flexible Machine Architecture: combined with CompCert&apos;s original design, our implementa-<lb/>tion of correctness theorems is almost machine-independent except for only less than 100 <lb/>lines of lemmas related to machine architecture. <lb/>We built our implementation of this theory into CompCert&apos;s backend (CompCert 3.12) thus it can <lb/>directly improve the compiler effect of verified compilation of C programs and any other compiler <lb/>engineering that uses CompCert&apos;s backends. <lb/>1.5 Structure of This Paper <lb/>Section 2 introduces some fundamental concepts related to instruction scheduling and basic settings <lb/>of the language model of CompCert&apos;s low-level IR that make this paper&apos;s theory established from <lb/>the ground up. <lb/>Section 3 briefly summarizes the main logical chain of our solution and the architecture of our <lb/>implementation. Readers can refer to it for a quick overview of our approach. <lb/>Sections 4 and 5 give our complete theory following the logical chain introduced in Section 3. <lb/>Section 5 shows how to use theories in Section 4 to prove the correctness (semantics preservation) <lb/>of a concrete instruction scheduling algorithm without additional reasoning on semantics details. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:5 <lb/></page>
        
        <body>Section 6 explains critical implementation details in Coq proof assistant based on CompCert&apos;s <lb/>original framework and case implementation on list scheduling (excluding heuristics). Readers <lb/>interested in Coq definition/proof details can refer to this section. <lb/>Section 7 introduces our case implementation of scheduling heuristic towards the RISC-V machine, <lb/>together with Section 8 that presents the evaluation results in both view of proof engineering and <lb/>compiler optimization performance of the scheduler we implemented for CompCert. <lb/>2 Background <lb/>In this section, we introduce several existing basic concepts and settings as a preparation for our <lb/>work. <lb/>2.1 Compiler-level Instruction Scheduling <lb/>Pipeline stall or pipeline hazards is a common performance issue in pipelined architecture, which <lb/>decreases the overall CPU throughput due to instruction-level data/control dependencies and <lb/>hardware resource conflicts. Out-of-order execution is a key micro-architectural technique to avoid <lb/>pipeline stall by dynamically reordering the instruction sequence, at the cost of hardware and <lb/>algorithmic complexity in processor pipelines. To achieve higher energy efficiency and lower hard-<lb/>ware cost, in-order processors are often adopted in areas including mobile devices and embedded <lb/>systems [15, 49]. On in-order processors, it is the responsibility of compilers to avoid pipeline stall <lb/>and enhance instruction-level parallelism for multi-issue pipelines. Instruction scheduling serves <lb/>as the fundamental compiler pass that statically reorders the instruction sequence to improve <lb/>instruction-level parallelism and reduce pipeline stall for in-order processors. Cited as an example, <lb/>GCC implemented this pass in its O2 options. <lb/>Examples in Fig. 2b shows the effect of reordering independent instructions: it may reduce the <lb/>cycles per instruction (CPI) by enabling parallel functional unit resource usage. <lb/>ğ‘– 3 <lb/>ğ‘– 4 <lb/>ğ‘– 1 <lb/>ğ‘– 5 <lb/>ğ‘– 2 <lb/>reads ğ’“ ğŸ <lb/>writes ğ’“ ğŸ <lb/>writes ğ’“ ğŸ <lb/>writes ğ’“ ğŸ <lb/>writes ğ’“ ğŸ‘ <lb/>WAW <lb/>W A R <lb/>reads ğ’“ ğŸ‘ <lb/>writes ğ’ğ’†ğ’ <lb/>WAW(mem) <lb/>writes ğ’ğ’†ğ’ <lb/>RAW <lb/>(a) Data dependence of some basic block [ğ‘– 1 , ğ‘– 2 , ğ‘– 3 , ğ‘– 4 , ğ‘– 5 ]. <lb/>Cycle ADD SUB FLOAT <lb/>1 ğ‘– 1 <lb/>ğ‘– 1 <lb/>2 <lb/>ğ‘– 1 <lb/>3 ğ‘– 2 <lb/>ğ‘– 2 <lb/>4 <lb/>ğ‘– 2 <lb/>5 ğ‘– 3 <lb/>ğ‘– 3 <lb/>6 ğ‘– 4 ğ‘– 4 <lb/>7 ğ‘– 5 ğ‘– 5 ğ‘– 5 <lb/>Cycle ADD SUB FLOAT <lb/>1 ğ‘– 1 <lb/>ğ‘– 1 <lb/>2 <lb/>ğ‘– 4 ğ‘– 4 ğ‘– 1 <lb/>3 ğ‘– 2 <lb/>ğ‘– 2 <lb/>4 <lb/>ğ‘– 2 <lb/>5 ğ‘– 3 <lb/>ğ‘– 3 <lb/>6 ğ‘– 5 ğ‘– 5 ğ‘– 5 <lb/>7 <lb/>(b) Effect of Instruction Scheduling. An instruc-<lb/>tion was filled in some cycle if it occupies that <lb/>resource in that cycle. <lb/>Fig. 2. Example on instruction scheduling under some dependence <lb/>2.2 Dependence Relation <lb/>The execution of a program at machine&apos;s view can be considered as a sequence of instructions that <lb/>are processed under program control flow. We revisit the basic concept of data dependence, which <lb/>is the principal legality constraint of instruction scheduling. <lb/>Dependence relation is a binary relation between two program instructions that access a register <lb/>or the same memory location and at least one is write access. A dependence relation, ğ‘– 1 â†’ ğ‘– 2 <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:6 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>to denote ğ‘– 1 must be in front of ğ‘– 2 , can be classified into three groups: (i) read-after-write (RAW) <lb/>if instruction ğ‘– 1 writes an identical register or memory location ğ‘Ÿ and instruction ğ‘– 2 reads ğ‘Ÿ ; (ii) <lb/>write-after-read (WAR) if ğ‘– 1 reads ğ‘Ÿ and ğ‘– 2 writes ğ‘Ÿ ; (iii) write-after-write (WAW) if ğ‘– 1 writes ğ‘Ÿ and ğ‘– 2 <lb/>writes ğ‘Ÿ . The scheduling of example in Fig. 2b follows the dependence relation in Fig. 2a. There is <lb/>an extensive body of literature to analyze whether two memory accesses are in isolated locations, <lb/>which is orthogonal to the scope of this paper. We roughly define memory accesses in default have <lb/>data dependence on each other in this paper. <lb/>Any compiler pass that reorders instructions must obey the rules that dependence relation is not <lb/>changed. One of the major works of this paper is formally proving that as long as such rules are <lb/>obeyed, the (mechanized) semantics preservation will be preserved. <lb/>2.3 Abstract Language of Low-level IR and Semantics Model of CompCert <lb/>We introduce the abstract model of the low-level IR where we conduct scheduling algorithms. <lb/>2.3.1 IR Language Model. For simplification, we ignored most of the irrelevant structure of IR and <lb/>only kept the common three-address code that triggers the main formal proof burdens of this work. <lb/>Definition 1. (Abstract IR Model) <lb/>â€¢ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ::= ğ¼ ğ‘œğ‘ (ğ‘œğ‘, ğ‘Ÿ ğ‘ ğ‘Ÿğ‘ , ğ‘Ÿ ğ‘‘ğ‘ ğ‘¡ ) | ğ¼ ğ‘™ğ‘œğ‘ğ‘‘ (ğ‘ğ‘‘ğ‘‘ğ‘Ÿ, ğ‘‘ğ‘ ğ‘¡)| ğ¼ ğ‘ ğ‘¡ğ‘œğ‘Ÿğ‘’ (ğ‘ ğ‘Ÿğ‘, ğ‘ğ‘‘ğ‘‘ğ‘Ÿ ) | ğ¼ ğ‘ğ‘ğ‘™ğ‘™ (ğ‘“ ) | ğ¼ ğ‘™ğ‘ğ‘ğ‘’ğ‘™ | ğ¼ ğ‘”ğ‘œğ‘¡ğ‘œ <lb/>â€¢ ğ‘“ ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘“ ::= (ğ¼ğ‘‘ ğ‘“ , ğµ ğ‘“ = [ğ‘– 1 :: ğ‘– 2 :: ... :: ğ‘– ğ‘š ]) <lb/>â€¢ ğ‘ğ‘Ÿğ‘œğ‘”ğ‘Ÿğ‘ğ‘š ğ‘ ::= (ğ¼ğ‘‘ * <lb/>ğ‘ , {ğ‘“ 1 , ğ‘“ 2 , ...ğ‘“ ğ‘› }) <lb/>2.3.2 Operational Semantics. The semantics of a program in CompCert is defined as a transition <lb/>system (small step semantics) starting from some initial state and, if not entering an &quot;infinite <lb/>stuttering&quot;, ending at a final state. Readers can refer to Part IV of [2] for details. Here we give a <lb/>concrete semantics model for low-level IR same way as [48], but omit some of the semantics details. <lb/>A state ğ‘† = (ğ‘¡, ğ¼ğ‘‘ ğ‘“ , ğ‘€, ğ‘…, ğ¶) consists of state type ğ‘¡ âˆˆ {ğ¶ğ‘ğ‘™ğ‘™, ğ‘…ğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ, ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›}, current function-id <lb/>ğ¼ğ‘‘ ğ‘“ , memory states ğ‘€, register states ğ‘…, and remaining code. We also use ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  (ğ‘) to denote the <lb/>set of reachable program states of a program ğ‘. Here are some semantics rules of the IR, given the <lb/>global environment ğº: <lb/>â€¢ SEM-OP: <lb/>ğ‘– = ğ¼ ğ‘œğ‘ (ğ‘œğ‘, ğ‘Ÿ ğ‘ ğ‘Ÿğ‘ , ğ‘Ÿ ğ‘‘ğ‘ ğ‘¡ ), ğ‘£ = ğ‘’ğ‘£ğ‘ğ‘™ (ğ‘œğ‘, ğ‘Ÿ ğ‘ ğ‘Ÿğ‘ , ğ‘Ÿ ğ‘‘ğ‘ ğ‘¡ ) <lb/>ğº âŠ¢ (ğ‘…ğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ, ğ‘“ , ğ‘€, ğ‘…, ğ‘– :: ğ‘™) ğœ– (ğ‘…ğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ, ğ‘“ , ğ‘€, ğ‘… [ğ‘Ÿ ğ‘‘ğ‘ ğ‘¡ â† ğ‘£], ğ‘™) <lb/>â€¢ SEM-LOAD <lb/>ğ‘– = ğ¼ ğ‘™ğ‘œğ‘ğ‘‘ (ğ‘ğ‘‘ğ‘‘ğ‘Ÿ, ğ‘‘ğ‘ ğ‘¡), ğ‘£ = ğ‘™ğ‘œğ‘ğ‘‘ (ğ‘€, ğ‘ğ‘‘ğ‘‘ğ‘Ÿ ) <lb/>ğº âŠ¢ (ğ‘…ğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ, ğ‘“ , ğ‘€, ğ‘…, ğ‘– :: ğ‘™) ğœ– (ğ‘…ğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ, ğ‘€, ğ‘… [ğ‘Ÿ ğ‘‘ğ‘ ğ‘¡ â† ğ‘£], ğ‘™) <lb/>â€¢ SEM-STORE <lb/>ğ‘– = ğ¼ ğ‘ ğ‘¡ğ‘œğ‘Ÿğ‘’ (ğ‘ ğ‘Ÿğ‘, ğ‘ğ‘‘ğ‘‘ğ‘Ÿ ), ğ‘… [ğ‘ ğ‘Ÿğ‘] = ğ‘£, ğ‘€ â€² = ğ‘ ğ‘¡ğ‘œğ‘Ÿğ‘’ (ğ‘€, ğ‘£) <lb/>ğº âŠ¢ (ğ‘…ğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ, ğ‘“ , ğ‘€, ğ‘…, ğ‘– :: ğ‘™) ğœ– (ğ‘…ğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ, ğ‘“ , ğ‘€ â€² , ğ‘…, ğ‘™) <lb/>An initial state of a program is in the form S I <lb/>p = (ğ¶ğ‘ğ‘™ğ‘™, M I , R I , ğ¶ (ğ‘“ ğ¼ğ‘‘ * )). A state is said to be a final <lb/>state if its in the shape (ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›, _, _, ğ‘›ğ‘–ğ‘™) <lb/>2.3.3 Program Behavior. We define the behaviour of a program to be the event trace generated by <lb/>it. <lb/>Definition 2. (Program Behavior: halting) We say a program ğ‘ behaves with a finite event trace ğ‘’, <lb/>denoted by E (ğ‘, ğ‘’), if exists a final state ğ‘† ğ‘“ such that S I <lb/>p <lb/> * <lb/>ğ‘’ ğ‘† ğ‘“ <lb/>Definition 3. (Program Behavior: stuttering) We say a program ğ‘ behaves with an infinite event <lb/>trace ğ‘’ âˆ , denoted by E (ğ‘, ğ‘’ âˆ ), if exists a final state ğ‘† ğ‘“ such that S I <lb/>p <lb/> * <lb/>ğ‘’ ğ‘† ğ‘“ <lb/>Definition 4. (Program Behavior: error) We say a program ğ‘ can result in an error with a finite <lb/>event trace ğ‘’, denoted by E (ğ‘, ğ‘’), if exists a non-final states ğ‘† ğ‘’ğ‘Ÿğ‘Ÿ such that S I <lb/>p <lb/> * <lb/>ğ‘’ ğ‘† ğ‘’ğ‘Ÿğ‘Ÿ and there <lb/>doesn&apos;t exist any state ğ‘† such that ğ‘† ğ‘’ğ‘Ÿğ‘Ÿ <lb/>ğ‘†. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:7 <lb/></page>
        
        <body>We also use E (ğ‘) to denote the set of all possible program behavior of a program ğ‘. <lb/>2.4 Compiler Correctness (Semantic Preservation) <lb/>The correctness of a compiler is defined as a refinement relationship: the behavior of target program <lb/>is a &quot;subset&quot; of the behavior of source program. <lb/>Definition 5. (Refinement Relation) We say the target program ğ‘ ğ‘¡ is a refinement of the source <lb/>program ğ‘ ğ‘  if E (ğ‘ ğ‘¡ ) âŠ† E (ğ‘ ğ‘  ), denoted by ğ‘ ğ‘¡ âŠ‘ ğ‘ ğ‘  <lb/>The simulation relation that implies the refinement relation in this paper is discussed between <lb/>two programs in the same intermediate language. There are two types of semantics preservation: <lb/>forward simulation and backward simulation. The backward simulation of any source program <lb/>and its compiled target program is the final result we need to reach, since it directly implies the <lb/>refinement relation. <lb/>S 2 <lb/>ğ‘  <lb/>S 2 <lb/>ğ‘¡ <lb/>S 1 <lb/>ğ‘¡ <lb/>S 1 <lb/>ğ‘  <lb/>ğ‘¹ ğ’ <lb/>ğ‘¹ ğ’ <lb/>ğ’† <lb/>ğ’† <lb/>(a) Forward simulation <lb/>S 2 <lb/>ğ‘  <lb/>S 2 <lb/>ğ‘¡ <lb/>S 1 <lb/>ğ‘¡ <lb/>S 1 <lb/>ğ‘  <lb/>ğ‘¹ ğ’ <lb/>ğ‘¹ ğ’ <lb/>ğ’† <lb/>ğ’† <lb/>(b) Backward simulation <lb/>Fig. 3. Simulation relations between program&apos;s small-step semantics <lb/>Definition 6. (Backward Simulation, Fig. 3b) The backward simulation B (ğ‘ ğ‘  , ğ‘ ğ‘¡ ) := is satisfied <lb/>if exists a matching relation relation ğ‘… ğ‘š âˆˆ ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ Ã— ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ between states such that âˆ€ ğ‘† ğ‘¡ <lb/>1 ğ‘† ğ‘¡ <lb/>2 âˆˆ <lb/>ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  (ğ‘ ğ‘¡ ). âˆ€ğ‘† ğ‘  <lb/>1 âˆˆ ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  (ğ‘ ğ‘  ).âˆ€ğ‘’ âˆˆ ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ .ğ‘… ğ‘š (ğ‘† ğ‘  <lb/>1 , ğ‘† ğ‘¡ <lb/>1 ) â†’ ğ‘† ğ‘¡ <lb/>1 <lb/>ğ‘’ ğ‘† ğ‘¡ <lb/>2 â†’ âˆƒğ‘† ğ‘  <lb/>2 . ğ‘† ğ‘  <lb/>1 <lb/>ğ‘’ ğ‘† ğ‘  <lb/>2 âˆ§ ğ‘… ğ‘š (ğ‘† ğ‘  <lb/>2 , ğ‘† ğ‘¡ <lb/>2 ) <lb/>However, the backward property is usually hard to prove directly. All the previous proof en-<lb/>gineering used the trick that proves forward simulation first, which is usually straightforward, <lb/>then used a theorem that forward simulation implies backward simulation as long as the compiled <lb/>program is deterministic. <lb/>Definition 7. (Forward Simulation. 3a) The forward simulation F (ğ‘ ğ‘  , ğ‘ ğ‘¡ ) is defined like flipping <lb/>backward simulation as: exists a matching relation ğ‘… ğ‘š âˆˆ ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ Ã— ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ between states such that <lb/>âˆ€ ğ‘† ğ‘  <lb/>1 ğ‘† ğ‘  <lb/>2 âˆˆ ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  (ğ‘ ğ‘  ). âˆ€ğ‘† ğ‘¡ <lb/>1 âˆˆ ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  (ğ‘ ğ‘¡ ).âˆ€ğ‘’ âˆˆ ğ¸ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘ .ğ‘… ğ‘š (ğ‘† ğ‘  <lb/>1 , ğ‘† ğ‘¡ <lb/>1 ) â†’ ğ‘† ğ‘  <lb/>1 <lb/>ğ‘’ ğ‘† ğ‘  <lb/>2 â†’ âˆƒğ‘† ğ‘¡ <lb/>2 . ğ‘† ğ‘¡ <lb/>1 <lb/>ğ‘’ <lb/>ğ‘† ğ‘¡ <lb/>2 âˆ§ ğ‘… ğ‘š (ğ‘† ğ‘  <lb/>2 , ğ‘† ğ‘¡ <lb/>2 ) <lb/>Definition 8. (Determinism) A program ğ‘ is said to be deterministic if for all program state ğ‘† such <lb/>that S I <lb/>p â†’ * ğ‘†, there exists at most one program states ğ‘† â€² and event sequence ğ‘’ such that ğ‘† ğ‘’ ğ‘† â€² <lb/>The following lemmas was proved in general theory of the original work of CompCert. <lb/>Lemma 1. Forward simulation implies backward simulation if target program is deterministic: <lb/>forall program ğ‘ ğ‘  and ğ‘ ğ‘¡ , if ğ‘ ğ‘  is deterministic and F (ğ‘ ğ‘  , ğ‘ ğ‘¡ ), then B (ğ‘ ğ‘  , ğ‘ ğ‘¡ ) <lb/>Lemma 2. Forward simulation is transitive. <lb/>Lemma 3. Backward Simulation Implies Behavior Refinement: forall program p, if B (ğ‘ ğ‘  , ğ‘ ğ‘¡ ) then <lb/>E (ğ‘ ğ‘  ) âŠ† E (ğ‘ ğ‘¡ ). <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:8 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>3 Overview of Our Approach <lb/>Figure. 4 shows the structure of our system, and how it was incorporated into the CompCert project. <lb/>The boxes and arrows in black denote the compiler passes of the original CompCert project. Those <lb/>in red denote our extension of instruction scheduling pass and heuristics, accompanied by our <lb/>correctness proof in Coq. <lb/>CompCert C <lb/>Frontend <lb/>RTL <lb/>â€¦ <lb/>LTL <lb/>Optimizations <lb/>Register <lb/>Allocations <lb/>Linear <lb/>Mach <lb/>Asm <lb/>Emission of <lb/>assembly code <lb/>Laying out the <lb/>activation records <lb/>Instruction Scheduling <lb/>ğ‘ ğ‘  âˆˆ Linear <lb/>ğ‘ ğ‘¡ âˆˆ Linear <lb/>Graph <lb/>Build <lb/>Prioritizer <lb/>Dependence <lb/>Graph ğº <lb/>Scheduler <lb/>(Correct with any heuristics) <lb/>Scheduling <lb/>Heuristics <lb/>Fig. 4. Architecture of our system <lb/>Our correctness proof includes the following components, which are discussed in more detail in <lb/>the following sections: <lb/>(1) Swapping-lemma: A topological reordering of a list of partially ordered elements is equiva-<lb/>lent to a finite sequence of swaps of adjacent non-ordered elements. See Fig. 5. A restricted <lb/>version of this result was proved in [29] 1 . <lb/>(2) Rule of instruction scheduling: Any valid scheduler (or other compiler pass) that reorders <lb/>a program&apos;s instructions must obey the dependence constraints of the original program, i.e., <lb/>conduct a toplogical reordering (topo-reorder) based on the dependence relation. <lb/>(3) Decompose scheduler: Combining (1) and (2), any valid scheduler (or other compiler pass) <lb/>that reorders a program&apos;s instructions within a basic block is equivalent to a finite sequence of <lb/>compiler passes that only swap one pair of instructions not ordered by the original program <lb/>dependence (a.k.a. independent instructions). <lb/>(4) Swapping correctness: Swapping only one pair of adjacent independent instructions inside <lb/>only one basic block of a program preserves the semantics of the program. <lb/>(5) Transitivity of semantics preservation: if two program transformations preserve program <lb/>semantics, composing them also preserves it. Recursively, composing a finite sequence of <lb/>semantics-preserving transformation preserves program semantics <lb/>(6) Final result: Combining (3) (4) (5), given any instruction scheduler, as long as it preserves <lb/>the instruction dependence relations of the original program, it preserves the semantics of <lb/>the original program. See Fig. 6 <lb/></body>
        
        <note place="footnote">1 This paper proved that there exists a sequence of all permutations of a set that adjacent two permutations only differs on <lb/>one pair of adjacent elements was swapped. In our work, the conclusion has a different requirement that only unordered <lb/>elements can be swapped. <lb/></note>
		<note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:9 <lb/></page>
        
        <body>4 Main Theorem <lb/>The structure of our main theorem follows the six steps summarized in Section 3, which are <lb/>described in detail in this section. We use formal mathematical notation for lemmas and proofs in <lb/>this section that reflect the structure of the proof that we developed using the Coq proof assistant. <lb/>4.1 Topological Sort and Topological Re-ordering <lb/>Assuming ğ‘– âˆˆ ğ‘ + , we use ğ‘™ [ğ‘–] to denote the ğ‘– th element of a list ğ‘™ of elements taken from set ğ´ <lb/>(ğ‘– &lt;= size of ğ‘™ by default), and E ğ‘™ to denote the set of all elements in ğ‘™. We also assume that there <lb/>are no duplicate elements in list ğ‘™. <lb/>Definition 9. (Topo-sorted List) Given ğ‘™ and a partial order ğ‘… on E ğ‘™ , a list of elements from E ğ‘™ is <lb/>said to be an topo-sorted list by ğ‘… if âˆ€ğ‘– 1 ğ‘– 2 âˆˆ ğ‘ , ğ‘…ğ‘™ [ğ‘– 1 ]ğ‘™ [ğ‘– 2 ] â†’ ğ‘– 1 &lt; ğ‘– 2 . <lb/>Definition 10. (Generated Order by Position) Given a no-duplicate list ğ‘™ of elements from some <lb/>set ğ´, a partial order relation ğ‘… on E ğ‘™ , we define a generated order by position (GOP) of ğ‘™ using ğ‘…, <lb/>denoted by G ğ‘… <lb/>ğ‘™ , to be: âˆ€ğ‘– 1 ğ‘– 2 G ğ‘… <lb/>ğ‘™ ğ‘™ [ğ‘– 1 ] ğ‘™ [ğ‘– 2 ] iff ğ‘– 1 &lt; ğ‘– 2 âˆ§ ğ‘… ğ‘™ [ğ‘– 1 ] ğ‘™ [ğ‘– 2 ]. <lb/>Lemma 4. A list is topo-sorted by its own GOP: for any no-duplicate list ğ‘™ with length ğ‘› and <lb/>relation ğ‘… on ğ´, ğ‘™ is a topo-sorted list G ğ‘… <lb/>ğ‘™ <lb/>Proof. Immediately by definition of GOP. <lb/>â–¡ <lb/>In later sections, we will see that this abstract definition of GOP represents the data dependence <lb/>definition within a basic block. That is, ğ‘… will be instantiated by data dependence definition between <lb/>two instructions (RAW/WAR/WAW), and G ğ‘… <lb/>ğ‘™ will be the dependence relation derived from a whole <lb/>basic block (a.k.a happens-before relation inside a basic block). <lb/>4.2 Swapping Lemma <lb/>We introduce a simple but the most important mathematical property of topological orders used in <lb/>this work. It is also one of the core ideas to reduce the verification work&apos;s hardship to only one <lb/>complicated but trivial lemma. We name the property swapping lemma. We prove it in a purely <lb/>mathematical way independent from the compiler engineering. <lb/>This lemma summarizes that, given a topo-sorted list of elements following some order, any <lb/>topological reordering of this list is equal to a finite sequence of swapping adjacent unordered <lb/>elements. See Fig. 5 for an illustration: all the three swapped pair (ğ‘– 1 , ğ‘– 2 ), (ğ‘– 3 , ğ‘– 4 ), (ğ‘– 3 , ğ‘– 5 ), <lb/>ğ‘– 3 <lb/>ğ‘– 4 <lb/>ğ‘– 1 <lb/>ğ‘– 5 <lb/>ğ‘– 2 <lb/>re-order <lb/>ğ‘– 3 <lb/>ğ‘– 4 <lb/>ğ‘– 1 <lb/>ğ‘– 5 <lb/>ğ‘– 2 <lb/>(a) Some valid topo-reorder of a list of elements <lb/>ğ‘– 3 <lb/>ğ‘– 4 <lb/>ğ‘– 1 <lb/>ğ‘– 5 <lb/>ğ‘– 2 <lb/>ğ‘– 3 <lb/>ğ‘– 4 <lb/>ğ‘– 1 <lb/>ğ‘– 5 <lb/>ğ‘– 2 <lb/>ğ‘– 3 <lb/>ğ‘– 4 <lb/>ğ‘– 1 <lb/>ğ‘– 5 <lb/>ğ‘– 2 <lb/>ğ‘– 4 <lb/>ğ‘– 5 <lb/>ğ‘– 1 <lb/>ğ‘– 3 <lb/>ğ‘– 2 <lb/>a series of <lb/>swapping <lb/>attempt <lb/>(b) Equivlent transforming <lb/>Fig. 5. Illustration of Swapping Lemma <lb/>Now we formally define and prove it. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:10 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>Definition 11. (Swapping Attempt) Given a set ğ´, relation ğ‘… on ğ´, and an topo-sorted list ğ‘™ = <lb/>[ğ‘– 0 , ğ‘– 1 , ..., ğ‘– ğ‘› ] by ğ‘… containing a finite number of elements of ğ´, we say a swapping attempt of ğ‘™ <lb/>at location ğ‘˜ âˆˆ ğ‘ , denoted as ğ‘™ â€² = ğ‘†ğ´ ğ‘… (ğ‘™, ğ‘˜) is a transformation from ğ‘™ = [ğ‘– 0 , ..., ğ‘– ğ‘˜ , ğ‘– ğ‘˜+1 , ..., ğ‘– ğ‘› ] <lb/>to ğ‘™ â€² = [ğ‘– 0 , ..., ğ‘– ğ‘˜+1 , ğ‘– ğ‘˜ , ..., ğ‘– ğ‘› ] if ğ‘… ğ‘– ğ‘˜ ğ‘– ğ‘˜+1 does not hold or ğ‘™ â€² = ğ‘™ if ğ‘… ğ‘– ğ‘˜ ğ‘– ğ‘˜+1 holds. We also extend <lb/>this definition to a list of natural numbers recursively as a sequence of swapping attempts by <lb/>ğ‘†ğ´(ğ‘™, [ğ‘› 1 , ğ‘› 2 , ...]) = ğ‘†ğ´ ğ‘… (ğ‘†ğ´ ğ‘… (ğ‘™, ğ‘› 1 ), [ğ‘› 2 , ...]). <lb/>Definition 12. (Topological reorder) Given a topo-sorted list ğ‘™ of elements ğ´ by ğ‘…, another list ğ‘™ â€² is <lb/>said to be a topo-reorder of ğ‘™ iff ğ‘™ â€² contains exactly the same elements as ğ‘™ and is also topo-sorted <lb/>by ğ‘…. <lb/>Lemma 5. (Swapping Lemma) Given a relation ğ‘…, a topo-sorted list ğ‘™, for any ğ‘™&apos;s topological reorder <lb/>ğ‘™ â€² , exists a finite list of nature number ğ‘™ ğ‘› such that ğ‘™ â€² = ğ‘†ğ´ ğ‘… (ğ‘™, ğ‘™ ğ‘› ) <lb/>Proof. This lemma declares that a topological reorder of a list is equivalent to a series of <lb/>swappings of adjacent elements. We prove this by induction on the length of ğ‘™. Base case is trivial. <lb/>Suppose the conclusion holds for any ğ‘™ with 1 â‰¤ ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘™) â‰¤ ğ‘˜, we prove it also holds for any ğ‘™ * <lb/>that ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘™ * ) = ğ‘˜ + 1. Given an ğ‘™ * &apos;s topological reorder ğ‘™ â€² <lb/> * we destruct it by two cases: <lb/>â€¢ If ğ‘™ * [0] = ğ‘™ â€² <lb/> * [0], then the remaining parts of these two lists are also a pair of topological <lb/>reordering with length ğ‘˜, and the conclusion holds by induction hypothesis. <lb/>â€¢ Otherwise, writing ğ‘™ â€² <lb/> * = [ğ‘– 0 , ğ‘– 1 , ..., ğ‘– ğ‘˜+1 ], ğ‘™ * can be separated into ğ‘™ 1 + +[ğ‘– 0 ] + +ğ‘™ 2 . Since ğ‘™ â€² <lb/> * is <lb/>topo-sorted, for any ğ‘– ğ‘— in ğ‘™ 1 or ğ‘™ 2 , ğ‘…ğ‘– ğ‘— ğ‘– 0 does not hold. This means we can get ğ‘™ â€²â€² <lb/> * = [ğ‘– 0 ] ++ğ‘™ 1 ++ğ‘™ 2 <lb/>from ğ‘™ * = ğ‘™ 1 + +[ğ‘– 0 ] + +ğ‘™ 2 by swapping ğ‘– 0 with every elements of ğ‘™ 1 one by one and ğ‘™ â€²â€² <lb/> * is still <lb/>topo-sorted. Since ğ‘™ â€²â€² <lb/> * has the same head and same elements as ğ‘™ â€² <lb/> * , by induction hypothesis, <lb/>same as the previous case, ğ‘™ â€² <lb/> * can be derived from swapping a sequence of adjacent elements <lb/>of ğ‘™ â€²â€² <lb/> * . Connecting with the previous swapping sequence from ğ‘™ * to ğ‘™ â€²â€² <lb/> * , we can construct the <lb/>final swapping sequence from ğ‘™ * to ğ‘™ â€² <lb/> * <lb/>â–¡ <lb/>4.3 Rule of Valid Scheduler <lb/>We define an instruction scheduler as a transformation on a list of instructions indexed by their <lb/>original position at the list so that the list to schedule is always non-duplicate. <lb/>Definition 13. (Instruction Scheduler) Given a indexed list of instructions ğ‘™ = [ğ‘– 1 , ğ‘– 2 , ..., ğ‘– ğ‘› ], a <lb/>scheduler is a function S : ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 2 such that ğ‘™ and S(ğ‘™) have exactly <lb/>the same elements, i.e. E ğ‘™ = E S (ğ‘™ ) . We also use the same symbol S(ğ‘) for scheduler of a whole <lb/>program ğ‘ <lb/>The dependence relation between two instructions depends on their operation to registers and <lb/>memory: <lb/>Definition 14. (Dependence Relation) D = D ğ‘…ğ´ğ‘Š D ğ‘Š ğ´ğ‘… D ğ‘Š ğ´ğ‘Š D ğ‘ ğ‘œğ‘™ğ‘–ğ‘‘ where: (i) D ğ‘…ğ´ğ‘Š ğ‘– 1 ğ‘– 2 <lb/>iff ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘  (ğ‘– 1 ) = ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘  (ğ‘– 2 ), (ii) D ğ‘Š ğ´ğ‘… ğ‘– 1 ğ‘– 2 iff ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘  (ğ‘– 1 ) = ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘  (ğ‘– 2 ), (ii) D ğ‘Š ğ´ğ‘Š ğ‘– 1 ğ‘– 2 iff ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘  (ğ‘– 1 ) = <lb/>ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘  (ğ‘– 2 ), (iv) D ğ‘ ğ‘œğ‘™ğ‘–ğ‘‘ ğ‘– 1 ğ‘– 2 iff one of ğ‘– 1 and ğ‘– 2 was a function call or branch jump, or writes to a <lb/>memory. <lb/>A valid instruction scheduler should always follow this dependence relation at syntax level: <lb/>Definition 15. (Valid Instruction Scheduler) An instruction scheduler S is said to be valid if for <lb/>any list of instructions ğ‘™, S(ğ‘™) is a topo-reorder of ğ‘™ by G D <lb/>ğ‘™ . <lb/></body>
        
        <note place="footnote">2 The index was omitted from the scheduler&apos;s type for simplification. <lb/></note>
		<note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:11 <lb/></page>
        
        <body>â€¦ <lb/>ğ‘™ <lb/>ğ‘™ <lb/>Lemma 7. <lb/>Lemma 7. <lb/>âŠ‘ <lb/>â€¦ <lb/>â‡” <lb/>â€¦ <lb/>Lemma 7. <lb/>Lemma 2, <lb/>transitivity <lb/>Of <lb/>â‹ˆ <lb/>â‹ˆ : forward simulation <lb/>â‹ˆ <lb/>â‹ˆ â‹ˆ â‹ˆ <lb/>: swapping attempt <lb/>: composing scheduler <lb/>Fig. 6. Structure of our proofs <lb/>4.4 Equivalence of Valid Schedulers <lb/>Definition 16. (Single Swapper) A single swapper S ğ‘› <lb/>ğ‘  where ğ‘› âˆˆ ğ‘ is a special kind of instruction <lb/>scheduler that only tries to swap one pair of adjacent instructions with a parameter to specify the <lb/>swapping location defined inductively: (i) S ğ‘› <lb/>ğ‘  ( []) = [] for any ğ‘› âˆˆ ğ‘ , (ii)S ğ‘›+1 <lb/>ğ‘  <lb/>(ğ‘– :: ğ‘™) = ğ‘– :: S ğ‘› <lb/>ğ‘  (ğ‘™), <lb/>(iii)S 0 <lb/>ğ‘  (ğ‘– ğ‘– :: ğ‘– 2 :: ğ‘™) = ğ‘– 2 :: ğ‘– 1 :: ğ‘™) if not Dğ‘– 1 ğ‘– 2 , (iv) S 0 <lb/>ğ‘  (ğ‘– ğ‘– :: ğ‘– 2 :: ğ‘™) = ğ‘– 1 :: ğ‘– 2 :: ğ‘™ if Dğ‘– 1 ğ‘– 2 . <lb/>We also use the same symbol S ğ‘› <lb/>ğ‘  (ğ‘“ , ğ‘) to denote swapping a single pair in side code blocks of <lb/>the function ğ‘“ in a program ğ‘ <lb/>Definition 17. (Composing Scheduler) Given a list of scheduler ğ¿ S = [S 1 , S 2 , . . . , S ğ‘› ], we directly <lb/>use ğ¿ ğ‘† as the function by composing the scheduler in it one by one, i.e. ğ¿ S (ğ‘™) = S ğ‘› (S ğ‘›-1 (...S 1 (ğ‘™))) <lb/>Now we can conclude that a valid scheduler is equal to a composing of a sequence of single <lb/>swappers. <lb/>Lemma 6. (decomposing lemma) Given a valid scheduler S, there exists ğ¿ S = [S ğ‘– 1 <lb/>ğ‘  , . . . , S ğ‘– ğ‘› <lb/>ğ‘  ] such <lb/>that for any ğ‘™, S(ğ‘™) = ğ¿ S (ğ‘™). <lb/>Proof. According to the definition of valid scheduler, since both ğ‘™ and S(ğ‘™) are sorted by G D <lb/>ğ‘™ , <lb/>this property is proved by swapping lemma. <lb/>â–¡ <lb/>4.5 Swapping Correctness and the Final Theorem <lb/>We have proved in Coq that swapping a single pair of adjacent independent instructions still <lb/>preserves the forward simulation relation of the program. <lb/>Lemma 7. (correctness of single swapper) Given a single swapper S ğ‘› <lb/>ğ‘  of a program, F (ğ‘, S ğ‘› <lb/>ğ‘  (ğ‘)). <lb/>Lemma 7 is the only lemma that requires us to reason about the different cases of semantic <lb/>details of IR execution. Briefly, we have to reason case by case that executing two independent <lb/>instructions consecutively results in the same memory and register states no matter which one <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:12 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>was executed first. Till here, we have shown how breaking a scheduler down to single swappers <lb/>made the verification of a scheduler easier. <lb/>Together with Lemma 6 and 2, we can prove that any valid scheduler of a program preserves the <lb/>forward simulation relation, thus can be incorporated into the original CompCert system. <lb/>Lemma 8. For any valid instruction scheduler S of a program ğ‘, F (ğ‘, S(ğ‘)). <lb/>The proof procedure of Lemma 8 is described in Fig. 6. <lb/>5 Correctness of List Scheduling <lb/>We show how our framework works when proving a concrete instruction scheduling algorithm <lb/>in this section, i.e., list scheduling (one of the simplest intra-block scheduling algorithms). As <lb/>mentioned in Section 1.2, no previous work had ever achieved a formally verified compiler pass for <lb/>list scheduling, even for such a simple algorithm. For list scheduling, the work from [44, 48] used <lb/>verified translation validation to check the validity each time a program is compiled. <lb/>The scheduling algorithm that we focus on in this proof consists of three parts: dependence <lb/>graph, scheduling heuristics, and iterative scheduling. With our previous theorem, we only have to <lb/>prove that our algorithm is a valid scheduler. This proof will no longer involve semantics details <lb/>since the definition of a correct schedule is only based on the dependence relation, D, defined at <lb/>the syntax level. <lb/>Our algorithm was implemented and proved in Coq and can be directly used by the CompCert <lb/>project. <lb/>5.1 Dependence Graph Construction and Scheduling Heuristics <lb/>Scheduling a block of instructions requires us to construct a dependence graph that records the <lb/>dependence relation inside a basic block. This step is noted in Algorithm. 1. <lb/>Algorithm 1 Dependence Graph Generating: ğ·ğ‘…ğ‘’ğ‘™ (ğ‘™) <lb/>Require: List of instructions ğ‘™ = [ğ‘– 1 , ğ‘– 2 , ..., ğ‘– ğ‘› ] <lb/>âŠ² Non-duplicate by giving index to them <lb/>Ensure: Graph G that records G D <lb/>ğ‘™ , the generated order of ğ‘™ by D <lb/>âŠ² Proved in Section.5.3 <lb/>if ğ‘™ = ğ‘›ğ‘–ğ‘™ then <lb/>G.ğ‘›ğ‘œğ‘‘ğ‘’ğ‘  â† E ğ‘™ <lb/>G.ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  â† âˆ… <lb/>else if ğ‘™ = ğ‘– â€² :: ğ‘™ â€² then <lb/>G.ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  â† G.ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  âˆª {(ğ‘– â€² , ğ‘–)|ğ‘– âˆˆ ğ‘™ â€² âˆ§ Dğ‘– â€² ğ‘–} <lb/>G.ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  â† G.ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  âˆª ğ·ğ‘…ğ‘’ğ‘™ (ğ‘™ â€² ).ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  <lb/>end if <lb/>After building the dependence graph, our scheduling algorithm will need a heuristic (oracle) to <lb/>make choices among several available instructions to be scheduled during each step in iterative <lb/>scheduling. The heuristic (also named prioritizer in our implementation) we use is an abstract <lb/>parameter P : ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›. In the actual implementation, it takes the <lb/>original basic block as input, and returns the priority of each instruction based on analyzing <lb/>some performance aspects (e.g., specific architectures, clock time of instruction types, etc). In each <lb/>iteration, our scheduler will pick an instruction with the highest priority. <lb/>The details of heuristics do not influence the correctness of the scheduling algorithm, but only <lb/>the performance of scheduled code. That means no matter how unreasonable the priority function <lb/>is, the scheduled code should be correct in semantics. Therefore, we do not introduce a concrete <lb/>heuristic in this section but leave that to our final instantiation during experiments in Section 7. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:13 <lb/></page>
        
        <body>5.2 Scheduling Algorithm <lb/>The list scheduling algorithm we are going to prove correctness for (Algorithm. 2) is an iterative <lb/>processing algorithm on the dependence graph. It has exactly the same number of iterations as the <lb/>length of the instruction list (basic block). Each iteration identifies all nodes that do not depend on <lb/>any other nodes, and picks one according to the priority from the input heuristics. <lb/>Algorithm 2 List Scheduling S * (P, ğ‘™) <lb/>Require: A heuristic function P : ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘ <lb/>Require: List instructions ğ‘™ = [ğ‘– 1 , ğ‘– 2 , ..., ğ‘– ğ‘› ] <lb/>âŠ² Non-duplicate by giving index to them <lb/>Ensure: ğ‘™ * is a topo-reorder of ğ‘™ by G D <lb/>ğ‘™ <lb/>âŠ² Proved in Section.5.3 <lb/>G â† ğ·ğ‘…ğ‘’ğ‘™ (ğ‘™) <lb/>ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘Ÿğ‘–ğ‘¡ğ‘¦ â† P (ğ‘™) <lb/>âŠ² P (ğ‘™) (ğ‘˜) will be the priority of ğ‘– ğ‘˜ <lb/>ğ‘™ * â† [] <lb/>while G not empty do <lb/>A â† {ğ‘– ğ‘˜ âˆˆ ğ‘™ |âˆ€ğ‘– ğ‘˜ â€² âˆˆ ğ‘™ .(ğ‘– ğ‘˜ â€² , ğ‘– ğ‘˜ ) âˆ‰ G} <lb/>ğ‘– ğ‘˜ * â† ğ‘– ğ‘˜ * âˆˆ A such that ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘Ÿğ‘–ğ‘¡ğ‘¦ [ğ‘˜ * ] ğ‘–ğ‘  ğ‘šğ‘ğ‘¥ <lb/>ğ‘™ * â† ğ‘™ * + +[ğ‘– ğ‘˜ * ] <lb/>G â† remove node ğ‘– ğ‘˜ * from G <lb/>end while <lb/>return ğ‘™ * <lb/>5.3 Proving Correctness <lb/>Now we prove that for any scheduling heuristic P, our list scheduler S * (P) is correct. Based on <lb/>Lemma 8, all we need to do is to prove S * (P) is a valid scheduler, a.k.a. S * (P, ğ‘™) will generate an <lb/>ğ‘™ â€² that is a topo-reorder of ğ‘™ by the generated order from ğ‘™. To achieve this, we first prove that the <lb/>dependence graph represents G D <lb/>ğ‘™ correctly. Then we prove that an invariant was preserved during <lb/>the iterative scheduling process. <lb/>5.3.1 Graph Construction. We firstly proved that the graph we constructed from given ğ‘™ using <lb/>Algorithm. 1 correctly stores the relation of G D <lb/>ğ‘™ . <lb/>Lemma 9. Given a list of instruction ğ‘™ = [ğ‘– 1 , ğ‘– 2 , ...], âˆ€ğ‘– ğ‘— , ğ‘– ğ‘˜ âˆˆ ğ·ğ‘…ğ‘’ğ‘™ (ğ‘™).ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  we have G D <lb/>ğ‘™ ğ‘– ğ‘— ğ‘– ğ‘˜ . <lb/>5.3.2 Scheduling Invariant: In the loop inside Algorithm 2, we proposed the following invariant: <lb/>Definition 18. (Scheduling Invariant) The invariant during the iterative scheduling process consists <lb/>of the following assertions: <lb/>â€¢ LENGTH: |ğ‘™ * | + |G.ğ‘›ğ‘œğ‘‘ğ‘’ | = |ğ‘™ | <lb/>â€¢ SUBSET: G.ğ‘›ğ‘œğ‘‘ğ‘’ğ‘  âˆˆ E ğ‘™ âˆ§ E ğ‘™ * âˆˆ E ğ‘™ <lb/>â€¢ INTERSECTION: G.ğ‘›ğ‘œğ‘‘ğ‘’ âˆ© E ğ‘™ = âˆ… <lb/>â€¢ NGT: âˆ€ğ‘– ğ‘— âˆˆ E ğ‘™ , ğ‘– ğ‘˜ âˆˆ ğº, (ğ‘– ğ‘˜ , ğ‘– ğ‘— ) âˆ‰ ğ·ğ‘…ğ‘’ğ‘™ (ğ‘™).ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘  <lb/>â€¢ SORT: ğ‘™ * is sorted by G D <lb/>ğ‘™ <lb/>We prove that this invariant is preserved during scheduling: <lb/>Lemma 10. (Invariant Preservation) The invariant in Definition 18 was preserved in the loop <lb/>inside Algorithm 2. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:14 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>Proof. <lb/>â€¢ LENGTH, SUBSET, and INTERSECTION: One node was removed from G to the tail of ğ‘™ * <lb/>in each iteration <lb/>â€¢ NGT: let the new graph be ğº â€² after removing ğ‘– ğ‘˜ * from it. For any ğ‘– ğ‘— âˆˆ E ğ‘™++[ğ‘– ğ‘˜ * ] , if ğ‘– ğ‘— = ğ‘– ğ‘˜ * , since <lb/>ğ‘– ğ‘˜ * âˆˆ ğº .ğ‘›ğ‘œğ‘‘ğ‘’ was the one with no incoming edges, this invariant was still preserved since <lb/>âˆ€ğ‘– ğ‘˜ âˆˆ ğº â€² , (ğ‘– ğ‘˜ , ğ‘– ğ‘˜ * ) âˆ‰ ğ·ğ‘…ğ‘’ğ‘™ (ğ‘™).ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘ . Otherwise if ğ‘– ğ‘— âˆˆ E ğ‘™ , this invariant was also preserved <lb/>immediately. <lb/>â€¢ SORT: by combining NGT and Lemma 9. <lb/>â–¡ <lb/>Now, we establish the property that the scheduled instructions have the same elements as before <lb/>our schedule: <lb/>Lemma 11. For any scheduling heuristic P, given an ğ‘™ and ğ‘™ â€² = S * (P, ğ‘™), E ğ‘™ = E ğ‘™ â€² <lb/>Proof. In iterative scheduling, one instruction was removed from G to the tail of ğ‘™ * in each <lb/>iteration until G becomes empty. <lb/>â–¡ <lb/>5.3.3 Validity: Now we can immediately reach the conclusion that our algorithm is a valid sched-<lb/>uler. <lb/>Lemma 12. Given a list of instruction ğ‘™ and scheduling heuristic P, S * (P, ğ‘™) is a topo-reorder of ğ‘™ <lb/>by G D <lb/>ğ‘™ i.e. Algorithm 2. provides a valid scheduler. <lb/>Proof. Combining Lemma 11. and SORT of the preserved invariant after scheduling. <lb/>â–¡ <lb/>5.3.4 Final Correctness: With the conclusion from Section.4, our algorithm preserves the forward <lb/>simulation relation of a program. <lb/>Theorem 1. Scheduler in Algorithm 2 preserved the forward simulation relation of program <lb/>semantics regardless of scheduling heuristic: for any program ğ‘ and heuristic P, F (ğ‘, S * (P, ğ‘)). <lb/>Proof. Immediately, by combining Lemma 8 and Lemma 12. <lb/>â–¡ <lb/>Incorporating this theorem into original CompCert theories, our new compiler still preserves <lb/>the backward simulation between source C program and compiled Asm program. <lb/>6 Framework Implementations in Coq Proof Assistant <lb/>This section describes some essential details of the formalization of our theory in Coq. The establish-<lb/>ment of formal Coq theorems follows the same route as the paper-written mathematical theorems <lb/>Section 4 and 5. However, the detail of implementation involves much more trivial definitions or <lb/>parameters. For example, to distinguish repeated same instructions, we have to give an index to <lb/>each instruction in a list so the the list we are scheduling becomes non-duplicate. <lb/>Context {A: Type}. <lb/>Fixpoint numlistgen&apos; (l: list A) (n: positive): list (positive * A) := <lb/>match l with <lb/>| [] =&gt; [] <lb/>| x :: l&apos; =&gt; (n, x) :: numlistgen&apos; l&apos; (n + 1) <lb/>end. <lb/>Definition numlistgen (l: list A) := numlistgen&apos; l 1. <lb/>... <lb/>Lemma numlistgen _ NoDup: forall l, NoDup (numlistgen l). <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:15 <lb/></page>
        
        <body>6.1 Formalization of Topological Properties and Swapping Lemma <lb/>6.1.1 topo-sorted and topo-reorder. Given a relation ğ‘… on some type ğ´, the topo-sorted property <lb/>and topo-reorder relation on list of ğ´ is defined inductively: <lb/>Context {A: Type}. <lb/>Variable R: A -&gt; A -&gt; Prop. <lb/>( * not greater than any elements in list * ) <lb/>Inductive NoDupSame: list A -&gt; list A -&gt; Prop := <lb/>| NoDupSame _ intro: <lb/>forall l1 l2, NoDup l1 -&gt; NoDup l2 -&gt; incl l1 l2 -&gt; incl l2 l1 -&gt; NoDupSame l1 l2. <lb/>Inductive ngt (a: A): list A -&gt; Prop := <lb/>| ngt _ nil: ngt a [] <lb/>| ngt _ cons: forall x l, ngt a l -&gt; ~R x a -&gt; ngt a (x :: l). <lb/>Inductive topo _ sorted: list A -&gt; Prop := <lb/>| topo _ sorted _ nil: topo _ sorted [] <lb/>| topo _ sorted _ cons: forall x l, ngt x l -&gt; topo _ sorted l -&gt; topo _ sorted (x :: l). <lb/>Inductive topo _ reorder : list A -&gt; list A -&gt; Prop := <lb/>| topo _ reorder _ nil: topo _ reorder [] [] <lb/>| topo _ reorder _ skip x l l&apos; : ngt x l -&gt; topo _ reorder l l&apos; -&gt; topo _ reorder (x::l) (x::l&apos;) <lb/>| topo _ reorder _ swap x y l : (~R x y) -&gt; (~R y x) -&gt; topo _ reorder (y::x::l) (x::y::l) <lb/>| topo _ reorder _ trans l l&apos; l&apos;&apos; : <lb/>topo _ reorder l l&apos; -&gt; topo _ reorder l&apos; l&apos;&apos; -&gt; topo _ reorder l l&apos;&apos;. <lb/>Note that the topo-reorder we defined in Coq is an alternative but equal form of the one we <lb/>defined in Definition 12. This makes our proof of swapping lemma more convenient since the <lb/>constructor already contains the &apos;swapping&apos;. <lb/>The induction on the length of the list was conducted in the following lemmas and theorem. <lb/>Note that this proof actually corresponds to the proof of swapping lemma in Lemma 5 due to the <lb/>alternative definition of topo-reorder. <lb/>Lemma sorted _ same _ elements _ topo _ reorder _ ind: <lb/>forall n, <lb/>(forall k l1 l2, k &lt; n -&gt; length l1 = k -&gt; NoDupSame l1 l2 -&gt; <lb/>topo _ sorted l1 -&gt; topo _ sorted l2 -&gt; topo _ reorder l1 l2) -&gt; <lb/>(forall l1 l2, length l1 = n -&gt; NoDupSame l1 l2 -&gt; <lb/>topo _ sorted l1 -&gt; topo _ sorted l2 -&gt; topo _ reorder l1 l2) . <lb/>Lemma sorted _ same _ elements _ topo _ reorder&apos;: <lb/>forall n l1 l2, length l1 = n -&gt; NoDupSame l1 l2 -&gt; <lb/>topo _ sorted l1 -&gt; topo _ sorted l2 -&gt; topo _ reorder l1 l2. <lb/>Theorem sorted _ same _ elements _ topo _ reorder: <lb/>forall l1 l2, NoDupSame l1 l2 -&gt; <lb/>topo _ sorted l1 -&gt; topo _ sorted l2 -&gt; topo _ reorder l1 l2. <lb/>6.1.2 swapping lemma. Now we can show how the final theorem of the swapping lemma was <lb/>formalized in Coq. <lb/>The swapping attempt (Definition 11) was formalized by <lb/>Context {A: Type}. <lb/>Variable (rel: A -&gt; A -&gt; bool). <lb/>( * swapping attempt at location n * ) <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:16 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>Fixpoint try _ swap (n: nat) (l: list A): list A := <lb/>match n, l with <lb/>| _ , nil =&gt; nil | _ , i :: nil =&gt; l <lb/>( * None: do not swap * ) <lb/>| O, i1 :: i2 :: l&apos; =&gt; if rel i1 i2 then l <lb/>else (i2 :: i1 :: l&apos;) <lb/>| Datatypes.S n&apos;, i :: l&apos; =&gt; i :: try _ swap n&apos; l&apos; <lb/>end. <lb/>( * a sequence of swapping attempts * ) <lb/>Fixpoint try _ swap _ seq (ln: list nat) (la: list A): list A := <lb/>match ln with <lb/>| nil =&gt; la <lb/>| n :: ln&apos; =&gt; try _ swap _ seq ln&apos; (try _ swap n la) <lb/>end. <lb/>Then the final swapping lemma was formalized by <lb/>Context {A: Type}. <lb/>Variable Rb: A -&gt; A -&gt; bool. <lb/>Theorem swapping _ property: <lb/>forall l nl&apos;, (treorder R l) (numlistgen l) nl&apos; -&gt; <lb/>exists ln, nl&apos; = try _ swap _ seq Rbnum ln (numlistgen l). <lb/>6.1.3 Generated order by position. The actual order of a basic block comes from both their original <lb/>position and their dependence relation to others. In Coq, D in Definition 10 was formalized as <lb/>( * Generated order by position from a list, aux. definition for simpler proofs * ) <lb/>Inductive GenR&apos; (i: positive) (na1 na2: positive * A): Prop := <lb/>GenR _ intro: List.In na1 (numlistgen&apos; l i) -&gt; List.In na2 (numlistgen&apos; l i) -&gt; <lb/>fst na1 &lt; fst na2 -&gt; R (snd na1) (snd na2) -&gt; GenR&apos; i na1 na2. <lb/>( * Generated order by position from a list * ) <lb/>Definition GenR := GenR&apos; 1. <lb/>6.2 Forward Simulation Preservation of Abstract Scheduler <lb/>Before showing the final lemma of the correctness of single swapper, we wish to show the only <lb/>lemma that involves the semantics details of the Linear IR we work on and contributes to most of <lb/>the proof burden of this work. This is the core lemma of proving that the single swapper preserves <lb/>the forward simulation of the IR (Lemma. 7). <lb/>Lemma independent _ two _ step _ match: <lb/>forall stk stk&apos; f f&apos; sp sp&apos; c rs rs&apos; m m&apos; s3 i1 i2 t <lb/>(INDEP: i1 D~&gt; i2 = false) <lb/>(s1:= State stk f sp (i1::i2::c) rs m) <lb/>(STEP13: starN step ge 2 s1 t s3) <lb/>(s1&apos;:= State stk&apos; f&apos; sp&apos; (i2::i1::c) rs&apos; m&apos;) <lb/>(MAT: match _ states s1 s1&apos;), <lb/>exists s3&apos;, tPlus s1&apos; t s3&apos; /\ match _ states s3 s3&apos;. <lb/>The detailed definition of the program state of Linear IR and forward simulation relation can be <lb/>refered at [14]. After that, then the semantics preservation of the single swapper was formalized by <lb/>Fixpoint transf _ program _ try _ swap _ seq1 (seq: list (nat * nat) ) (prog: program):= <lb/>match seq with <lb/>| [] =&gt; OK prog <lb/>| (pos, n) :: seq&apos; =&gt; do prog&apos; &lt;-transf _ program _ try _ swap _ in _ one pos n prog; <lb/>transf _ program _ try _ swap _ seq1 seq&apos; prog&apos; <lb/>end. <lb/>Lemma transf _ program _ multi _ swap _ forward _ simulation1: <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:17 <lb/></page>
        
        <body>forall seq prog tprog, <lb/>transf _ program _ try _ swap _ seq1 seq prog = OK tprog -&gt; <lb/>forward _ simulation (Linear.semantics prog) (Linear.semantics tprog). <lb/>The abstract scheduler is defined to have a parameter of function type from a list of indexed <lb/>instruction to another list of indexed instructions, with a hypothesis that the result of it gener-<lb/>ated a topo-reorder. Here the happens_before corresponds to the data dependence relation D in <lb/>Definition 14 <lb/>Variable schedule&apos;: list (positive * instruction) -&gt; res (list (positive * instruction)). <lb/>Let HBR := fun i1 i2 =&gt; happens _ before i1 i2 = true. <lb/>Let HBnum (na1 na2: positive * instruction) := happens _ before (snd na1) (snd na2). <lb/>Let HBGenR (l: list instruction) := GenR HBR l. <lb/>Hypothesis schedule _ valid: <lb/>forall l nl&apos;, schedule&apos; (numlistgen l) = OK nl&apos; -&gt; <lb/>treorder HBR l (numlistgen l) nl&apos;. <lb/>Definition schedule _ program (p: program): res program := ... ( * based on schedule&apos; * ) <lb/>Theorem schedule _ program _ forward _ simulation: <lb/>forall prog tprog: program, schedule _ program prog = OK tprog -&gt; <lb/>forward _ simulation (Linear.semantics prog) (Linear.semantics tprog). <lb/>6.3 Case Implementation on List Scheduling and Correctness <lb/>The implementation of our scheduler takes a heuristic function as an abstract parameter and builds <lb/>a lookup table indexed by the location of each instruction based on it. <lb/>Variable prioritizer: list instruction -&gt; list positive. <lb/>Fixpoint prio _ map&apos; (cur: positive) (lp: list positive): PMap.t positive := <lb/>match lp with <lb/>| nil =&gt; PMap.init 1 <lb/>| p :: lp&apos; =&gt; PMap.set cur p (prio _ map&apos; (cur + 1) lp&apos;) <lb/>end. <lb/>Definition prio _ map (lp: list positive) := prio _ map&apos; 1 lp. <lb/>... <lb/>( * return the one to schedule and the new dependence graph after removing it * ) <lb/>Definition schedule _ 1 (prior: PMap.t positive) (original: DPMap _ t) <lb/>(scheduled: list (positive * instruction)) (remain: DPMap _ t) <lb/>: res (list (positive * instruction) * DPMap _ t) := <lb/>let available := indep _ nodes remain in <lb/>do pi &lt;-firstpick prior available; <lb/>OK (scheduled ++ [pi], remove _ node (fst pi) remain). <lb/>Fixpoint schedule _ n (prior: PMap.t positive) (L: nat) (original: DPMap _ t) <lb/>(scheduled: list (positive * instruction)) (remain: DPMap _ t) <lb/>: res (list (positive * instruction) * DPMap _ t) := <lb/>match L with <lb/>| O =&gt; OK (scheduled, remain) <lb/>| Datatypes.S L&apos; =&gt; <lb/>do (scheduled&apos;, remain&apos;) &lt;-schedule _ 1 prior original scheduled remain; <lb/>schedule _ n prior L&apos; original scheduled&apos; remain&apos; <lb/>end. <lb/>Definition schedule _ numblock (nl: list (positive * instruction)) := <lb/>let m := dep _ map _ gen nl in ( * dependence graph * ) <lb/>let prior := prio _ map (prioritizer (numlistoff nl)) in <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:18 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>do (nl&apos;, m) &lt;-schedule _ n prior (List.length nl) m [] m; <lb/>OK nl&apos;. <lb/>Definition list _ schedule&apos; := schedule _ program schedule _ numblock. <lb/>The list scheduler with arbitrary prioritizer is proven to preserve the forward simulation of <lb/>Linear IR through the invariant we used in Lemma 10. <lb/>Lemma schedule _ numblock _ correct: <lb/>forall l nl&apos;, schedule _ numblock (numlistgen l) = OK nl&apos; -&gt; <lb/>treorder HBR l (numlistgen l) nl&apos;. <lb/>Theorem abstract _ list _ schedule _ forward _ simulation: <lb/>forall prog tprog, list _ schedule&apos; prog = OK tprog -&gt; <lb/>forward _ simulation (Linear.semantics prog) (Linear.semantics tprog). <lb/>Proof. <lb/>intros. eapply schedule _ program _ forward _ simulation; eauto. <lb/>eapply schedule _ numblock _ correct. <lb/>Qed. <lb/>7 Scheduling Heuristics Implementation for Specified Architecture <lb/>In this section, we show how we implement the scheduling heuristic for our target RISC-V Machine. <lb/>Although it is feasible to completely implement everything in Coq and generate the improved <lb/>compiler, we chose to use a trick that makes it possible for developers who do not have knowledge <lb/>of Coq/OCaml systems to collaborate with us. We believe this further improves the flexibility of <lb/>our result. <lb/>The design of CompCert works in the following way. The compiler passes were both implemented <lb/>and proved in Coq. To make it an executable file that works the same way as GCC or Clang, it uses <lb/>the Extraction function of Coq to convert the composition of those passes into an Ocaml function <lb/>of type ğ¶ ğ‘ ğ‘¦ğ‘›ğ‘¡ğ‘ğ‘¥ â†’ ğ´ğ‘ ğ‘ ğ‘’ğ‘šğ‘ğ‘™ğ‘¦ ğ‘ğ‘œğ‘‘ğ‘’ (the principle of Extraction can be found at the Coq textbook <lb/>[3], chapter Extract). Note that such conversion from Coq to OCaml is not guaranteed safe, which <lb/>is the well-known trusted computing base of CompCert. <lb/>As we mentioned, the scheduling heuristic is abstracted away in algorithm implementation as <lb/>a parameter of type ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ ğ‘™ğ‘–ğ‘ ğ‘¡ ğ‘ . Therefore, instead of implementing it in Coq and <lb/>then extract to OCaml, we can safely implement it directly in OCaml. We have learned that the <lb/>OCaml language supports interfacing C with itself [34, 47]. We use this support in our OCaml <lb/>function to invoke a C function that customizes the scheduling priority towards Risc-V. The reason <lb/>we doing this instead of continuing the implementation in Coq and build the compiler in previous <lb/>way is that we believe this both reduces the learning process of compiler engineers who are not <lb/>familiar enough with Coq/OCaml development and proof engineers who are not familiar enough <lb/>with specific machine architectures. <lb/>Nevertheless, using such a method is not necessary but just an option. Future developers that use <lb/>our result to improve the performance of instruction scheduling can freely choose to modify the C <lb/>function under the same interface, modify under the OCaml function without using C interface, <lb/>or modify under the Coq scheduler without using abstract parameters but a concrete heuristic <lb/>implemented in Coq. Any of the above choices will not change any of the proof codes. <lb/>7.1 Critical Path Scheduling <lb/>As our instruction scheduling heuristics, we use the Critical Path (CP) method [12, 28], where <lb/>the instruction with the longest path length has the highest priority. Given a dependence graph <lb/>whose node represents an instruction and an edge represents inter-instruction dependence, the <lb/>path length of ğ‘›ğ‘œğ‘‘ğ‘’ ğ‘– to the exit node is computed as: <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:19 <lb/></page>
        
        <body>ğ‘ğ‘ğ‘¡â„ ğ‘– = ğ‘šğ‘ğ‘¥ ğ‘— âˆˆğ‘ ğ‘¢ğ‘ğ‘ ğ‘– (ğ‘ğ‘ğ‘¡â„ ğ‘— ) + ğ‘ğ‘œğ‘ ğ‘¡ ğ‘– <lb/>where ğ‘ ğ‘¢ğ‘ğ‘ ğ‘– is the dependence successors of ğ‘›ğ‘œğ‘‘ğ‘’ ğ‘– , ğ‘ğ‘œğ‘ ğ‘¡ ğ‘– is the instruction latency of ğ‘›ğ‘œğ‘‘ğ‘’ ğ‘– , and exit <lb/>node is the zero-cost dummy node that precedes all other nodes. Our target machine used for the <lb/>performance study in Section 8.2 is SiFive U74-MC Core Complex. We collected the instruction <lb/>latencies from their architecture manual [49] and summarized them as a cost table, i.e., a map of <lb/>operator type to corresponding latency. <lb/>The CP method is a fast and efficient heuristic scheduling algorithm, which can enhance <lb/>instruction-level parallelism and achieve near-optimal scheduling results in practice. Our cer-<lb/>tified instruction scheduling uses the above path length as the priority among available instructions <lb/>(Section 5.2). <lb/>7.2 Interaction between Coq/OCaml and C function <lb/>7.2.1 Coq-OCaml Interface. In the Coq part, we need a parameter to represent the outside world <lb/>function from OCaml: <lb/>Require Import ExtrOcamlIntConv. <lb/>Parameter prioritizer : list int -&gt; int -&gt; list (list int) -&gt; int -&gt; (list int). <lb/>... <lb/>( * definition of encoding of instruction to an integer * ) <lb/>... <lb/>Definition prioritizer&apos; (l: list instruction): list positive := <lb/>let nodes := block2ids l in <lb/>let edges := nblock2edges (numlistgen l) in <lb/>let prior&apos; := prioritizer nodes (int _ of _ nat (length nodes)) <lb/>edges (int _ of _ nat (length edges)) in <lb/>List.map pos _ of _ int prior&apos;. <lb/>The prioritize function that is manually implemented in OCaml takes the parameters of a list of <lb/>nodes encoding the instruction&apos;s operation type and edges of the same dependence graph from <lb/>which the scheduling algorithm is processing. It further passed the parameters to invoke the C <lb/>functions that implement the CP heuristic using the Ctypes 3 library (mentioned in Chapter 22 of <lb/>[34]) , and get a list of integers that represent the priority of each node. <lb/>open Ctypes <lb/>( * The prioritizer function in OCaml * ) <lb/>let prioritizer nodes n edges m: int list = <lb/>( * First, we will need to convert them to C arrays * ) <lb/>let nodes _ arr = CArray.of _ list int nodes in <lb/>let edges _ arr = <lb/>let inner = List.map (fun e -&gt; CArray.of _ list int e |&gt; CArray.start) edges in <lb/>let outer = CArray.of _ list (ptr int) inner in outer <lb/>in <lb/>( * Now, we pass arguments into prioritizer * ) <lb/>let result = <lb/>C.Functions.prioritizer (CArray.start nodes _ arr) n (CArray.start edges _ arr) m <lb/>in <lb/>CArray.from _ ptr result n |&gt; CArray.to _ list <lb/>7.2.2 OCaml-C interface. We then implemented the following C function to compute the path-<lb/>based priority discussed in Section 7.1, where nodes and edges respectively capture operator <lb/></body>
        
        <note place="footnote">3 This library of OCaml has a name conflict with a module in CompCert. We have to change the name of that module when <lb/>implementing the heuristic in this way <lb/></note>
		<note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:20 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>types and dependence edges while the return value represents the computed priorities for given <lb/>nodes: <lb/>int * prioritizer(int * nodes, int n, int ** edges, int m); <lb/>8 Evaluation <lb/>We evaluate our result in two different views: proof workload of our verification and optimization <lb/>performance of our implementation case of list scheduling with critical path scheduling heuristics. <lb/>We will show that our proofs have acceptable length and are comparably as lightweight as previous <lb/>work in lines of codes (LOC) with the same coding style as the open source code of CompCert. <lb/>We will also show that our implementation case results in an improved compiler optimization on <lb/>benchmarks. <lb/>8.1 View 1: Proof Engineering <lb/>We believe our verification work is relatively lightweight in two aspects. Firstly, we used a similar <lb/>amount of LOC on proofs as in previous work on validating intra-block scheduling. Secondly, 75% <lb/>of our work are once-for-all result. We believe future work on different scheduling algorithms <lb/>inside a basic block can directly use our result to finish proof without any reasoning on semantics <lb/>details again, i.e. we also make future work lightweight. <lb/>Table. 1 shows the proof workload of our verification work on instruction scheduling. <lb/>Table 1. LOC of program/functions and proofs in our work <lb/>Language Functions Proofs <lb/>Base theories on topo-reorder&apos;s properties (once-for-all) Coq <lb/>-<lb/>0.8k <lb/>Base theories on semantics (once-for-all) <lb/>Coq <lb/>-<lb/>2.2k <lb/>List-scheduling algorithm (excluding heuristics) <lb/>Coq <lb/>0.15k <lb/>1.0k <lb/>Scheduling heuristics <lb/>OCaml <lb/>25 <lb/>-<lb/>Scheduling heuristics <lb/>C <lb/>0.7k <lb/>-<lb/>Machine dependent code (Risc-V) <lb/>Coq <lb/>-<lb/>40 <lb/>Machine dependent code (x86) <lb/>Coq <lb/>-<lb/>35 <lb/>Table. 2 shows proof workload of both our and previous work. Note that the total goal of the <lb/>three work are not exactly the same: [48] also implemented a validator for trace scheduling and <lb/>[44] did some specific work to support VLIW instruction parallelism, while our work verified a <lb/>machine-independent scheduling algorithm with machine dependent heuristics. A remark is that <lb/>our work has a stronger result, i.e. correctness of an algorithm. Both the work of [44, 48] only <lb/>guarantees the correctness of the translation validator, a.k.a guarantees the correctness of each <lb/>compiled case that does not return an error message. With only a verified translation validator, the <lb/>compilation will be aborted if the unverified scheduler generates a wrong result. <lb/>Table 2. LOC of related work <lb/>Fully verified Scope <lb/>LOC of proof codes <lb/>This work Yes <lb/>list scheduling <lb/>4k <lb/>[48] <lb/>No <lb/>list and trace scheduling 11k <lb/>[44] <lb/>No <lb/>list scheduling (VLIW) <lb/>18k+10k(architecture) <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:21 <lb/></page>
        
        <body>In the view of flexibility, the methods of verified translation validation are naturally 100% flexible. <lb/>That means different scheduling algorithms can share the same verified validator, there&apos;s no proof <lb/>cost to change when changing a scheduler, but only the codes of the scheduler itself. Nevertheless, <lb/>our result shows that in the goal of a fully verified scheduling algorithm, 75% of the proof codes <lb/>are fixed, both algorithm-independent and machine-independent. Bringing a list scheduler with a <lb/>different algorithm only influences the remaining proofs. If we treat our list scheduling as some <lb/>new work that imports our basic theory as a library, the proof-code LOC ratio is around 1:10, <lb/>approximating the average ratio at current age of program verification technology. <lb/>8.2 View 2: Effect of Optimization <lb/>We used a SiFive U74-MC Core Complex as our experimental platform. The U74-MC Core Complex <lb/>includes four 64-bit U7 RISC-V cores, each of which has a dual-issue, in-order execution pipeline, <lb/>with a peak execution rate of two instructions per clock cycle. Each U7 core supports standard <lb/>Multiply, Single-Precision Floating Point, Double-Precision Floating Point, Atomic, Compressed, <lb/>and Bit Manipulation RISC-V extensions (RV64GCB) [49]. As our experimental benchmark, we <lb/>used PolyBench C 4.2 [40], a widely used benchmark suite for compiler evaluations. The PolyBench <lb/>has 30 numerical benchmarks, extracted from a variety of application domains including linear <lb/>algebra computations, image processing, physics simulation, dynamic programming, statistics, and <lb/>stencil computations. As the reference implementation, we used the latest version of CompCert <lb/>3.13.1 available from the official website [14]. <lb/>Improvement over CompCert 3.13.1 <lb/>0 <lb/>0.5 <lb/>1 <lb/>1.5 <lb/>2 <lb/>2 m <lb/>m <lb/>3 m <lb/>m <lb/>a d i <lb/>a ta <lb/>x <lb/>b ic <lb/>g <lb/>c h o le <lb/>s k y <lb/>c o rr e la <lb/>ti o n <lb/>c o v a ri a n c e <lb/>d e ri c h e <lb/>d o it g e n <lb/>d u rb <lb/>in <lb/>fd <lb/>td <lb/>-2 d <lb/>fl o y d -w <lb/>a rs <lb/>h a ll <lb/>g e m <lb/>m <lb/>g e m <lb/>v e r <lb/>g e s u m <lb/>m <lb/>v <lb/>g ra <lb/>m <lb/>s c h m <lb/>id <lb/>t <lb/>h e a t-3 d <lb/>ja <lb/>c o b i-1 d <lb/>ja <lb/>c o b i-2 d <lb/>lu <lb/>d c m <lb/>p <lb/>lu <lb/>m <lb/>v t <lb/>n u s s in <lb/>o v <lb/>s e id <lb/>e l-2 d <lb/>s y m <lb/>m <lb/>s y r2 <lb/>k <lb/>s y rk <lb/>tr is <lb/>o lv <lb/>tr m <lb/>m <lb/>g e o . m <lb/>e a n <lb/>1.17 <lb/>0.98 <lb/>1.06 <lb/>1.40 <lb/>1.29 <lb/>1.29 <lb/>1.01 <lb/>1.06 <lb/>1.29 <lb/>1.26 <lb/>1.09 <lb/>1.02 <lb/>1.01 <lb/>1.18 <lb/>1.09 <lb/>1.11 <lb/>1.14 <lb/>1.34 <lb/>1.12 <lb/>1.09 <lb/>1.19 <lb/>1.81 <lb/>1.03 <lb/>1.36 <lb/>1.37 <lb/>1.34 <lb/>1.15 <lb/>1.17 <lb/>1.11 <lb/>1.08 <lb/>1.11 <lb/>Fig. 7. Performance improvements by the certified instruction scheduler for PolyBench C 4.2 <lb/>Figure 7 shows the improvement factors of the PolyBench execution performance by our proposed <lb/>approach, compared to the reference implementation CompCert 3.13.1. To evaluate the impacts of <lb/>instruction scheduling combined with source-level program optimizations, we applied the PLUTO <lb/>polyhedral source-to-source compiler [8] to generate the optimized source programs that were <lb/>used for both the reference version and our version. After the loop transformations including <lb/>multi-level loop unrolling, the kernel basic blocks (i.e., the innermost loop bodies) have a huge <lb/>amount of instructions and thereby efficient instruction-level scheduling is the key to enhance <lb/>the overall execution performance. As shown in Figure 7, our certified instruction scheduler <lb/>enhanced the performance for most benchmarks, with the geometric mean of 1.17Ã— and up to 1.81Ã— <lb/>improvements. By comparing the outputs of all 30 benchmarks, we verified that the equivalent <lb/>outputs were generated: 1) between the reference version and our version; and 2) between enabling <lb/>and disabling the PLUTO source-to-source compilation. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:22 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>To see the impact of source-level program optimizations, we also collected the runtime per-<lb/>formance numbers when disabling source-to-source polyhedral optimizer. In this case, especially <lb/>without loop unrolling for parallel loops, the instruction-level parallelism is quite limited for most <lb/>benchmarks and hence there are few opportunities to reorder instructions. As expected, we could <lb/>not see notable differences in the scheduling results between the baseline CompCert and our <lb/>proposed instruction scheduler, with the geometric mean speedup of 1.04. <lb/>Normalized time to CompCert 3.13.1 <lb/>0 <lb/>5 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>2 m <lb/>m <lb/>3 m <lb/>m <lb/>a d i <lb/>a ta <lb/>x <lb/>b ic <lb/>g <lb/>c h o le <lb/>s k y <lb/>c o rr e la <lb/>ti o n <lb/>c o v a ri a n c e <lb/>d e ri c h e <lb/>d o it g e n <lb/>d u rb <lb/>in <lb/>fd <lb/>td <lb/>-2 d <lb/>fl o y d -w <lb/>a rs <lb/>h a ll <lb/>g e m <lb/>m <lb/>g e m <lb/>v e r <lb/>g e s u m <lb/>m <lb/>v <lb/>g ra <lb/>m <lb/>s c h m <lb/>id <lb/>t <lb/>h e a t-3 d <lb/>ja <lb/>c o b i-1 d ja <lb/>c o b i-2 d <lb/>lu <lb/>d c m <lb/>p lu <lb/>m <lb/>v t n u s s in <lb/>o v s e id <lb/>e l-2 d <lb/>s y m <lb/>m s y r2 <lb/>k s y rk tr is <lb/>o lv tr m <lb/>m g e o . m <lb/>e a n <lb/>2.67 <lb/>2.28 <lb/>1.78 <lb/>1.99 <lb/>1.83 <lb/>3.55 <lb/>5.09 <lb/>6.32 <lb/>1.22 <lb/>3.46 <lb/>4.40 <lb/>3.07 <lb/>2.01 <lb/>5.28 <lb/>3.02 <lb/>2.12 <lb/>1.53 <lb/>1.58 <lb/>1.39 <lb/>3.21 <lb/>1.69 <lb/>1.81 <lb/>2.73 <lb/>3.85 <lb/>5.51 <lb/>3.23 <lb/>1.95 <lb/>1.93 <lb/>3.08 <lb/>3.98 <lb/>3.34 <lb/>9.18 <lb/>9.03 <lb/>7.13 <lb/>7.56 <lb/>7.00 <lb/>12.34 <lb/>20.76 <lb/>26.58 <lb/>3.38 <lb/>12.82 <lb/>16.96 <lb/>9.28 <lb/>9.51 <lb/>7.84 <lb/>12.54 <lb/>7.32 <lb/>5.02 <lb/>5.52 <lb/>4.47 <lb/>10.87 <lb/>5.23 <lb/>6.03 <lb/>9.56 <lb/>12.65 <lb/>16.88 <lb/>13.67 <lb/>6.68 <lb/>6.70 <lb/>9.25 <lb/>12.02 <lb/>11.88 <lb/>Untrusted scheduler (kvx) <lb/>Trusted scheduler (ours) <lb/>Fig. 8. Normalized compilation times for untrusted KVX scheduler and proposed trusted scheduler, relative <lb/>to the baseline CompCert 3.13 for PolyBench C 4.2 <lb/>Figure 8 shows the normalized compilation times to CompCert 3.13.1 for PolyBench kernels using <lb/>both the proposed certified instruction scheduling pass and the CompCert with scheduler using <lb/>certified translation validation from [44] (CompCert-KVX). Compared to the original CompCert <lb/>without the proposed scheduling pass, the normalized compilation times enabling the certified <lb/>instruction scheduler are between 1.22 (mvt) and 6.32 (nussinov). Our trusted scheduler is also <lb/>around 3x -4x faster than CompCert-KVX. We also observed that the absolute compilation times <lb/>are correlated to the kernel code sizes of benchmarks while there is not strong correlation between <lb/>the normalized compilation time and kernel code size, i.e. the slowdown of compile time does not <lb/>depend on the kernel size. <lb/>A remark here is CompCert-KVX used different scheduler implementations and heuristics and <lb/>was implemented directly in OCaml. Nevertheless, in a theoretical view, the compile time overhead <lb/>of a trusted scheduler in our method has to be strictly less than the untrusted scheduler with <lb/>the same scheduling pass plus a validating process, whether implemented directly in OCaml or <lb/>implemented in Coq then extracted to OCaml. Our work just ensures that the validating process can <lb/>be deleted safely. However, if the untrusted scheduler was implemented in different OCaml codes, <lb/>it could be possibly faster by using some imperative features, even if it will have extra validating <lb/>time. <lb/>9 Conclusion <lb/>9.1 Trusted Computing Base (TCB) <lb/>As we mentioned in Section 7, the CompCert project was implemented as a Coq function from <lb/>C code to assembly code. It will be extracted to OCaml codes to be further compiled into an <lb/>executable file. The whole verification work only guarantees the correctness of the Coq function. <lb/>The TCB of the CompCert project trusts the automatic conversion from the Coq functions to the <lb/>OCaml functions. The TCB of our base theory is the same as the original CompCert. However, in <lb/>our implementation case using an engineering trick (not mandatory), the implementation of list <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:23 <lb/></page>
        
        <body>scheduling and experiments is slightly different and increased the TCB since the abstract function <lb/>in Coq was instantiated directly in the OCaml function instead of directly coding it in Coq and <lb/>it uses OCaml&apos;s interface with C language. Doing this adds two more components to the trusted <lb/>computing base: the reliability of this interface and the convention between Coq natural number <lb/>and OCaml integers. <lb/>As we explained in Section 7 this is not unavoidable but just a better option. We only do it in <lb/>this way to show how flexible our results can be. It is completely feasible to implement everything <lb/>of the scheduling heuristics in our case study and experiments in Coq and generate OCaml codes <lb/>like the original CompCert did, though it requires much more workload for both compiler-backend <lb/>engineers and proof engineers. Our method makes it possible for compiler engineers to improve <lb/>the CompCert project without knowing the detailed technology of Coq development. <lb/>9.2 Inspiration on Reducing Verification Burden <lb/>As a common sense in program verification, the price of a bug-free program is heavy work on <lb/>proof engineering. In semi-automatic verification methods like Coq proof assistant, the usual idea <lb/>to reduce the proof workload is using some proof automation tricks. One example are Ltac tactic <lb/>language embedded in Coq [16, 38]. <lb/>In the verified translation validation work of [44, 48], the whole scheduling algorithm to be <lb/>validated can be treated as an uninterpreted parameter since the verified validator will check the <lb/>correctness result of each translation itself. Similarly, an inspiration we can get from our work is to <lb/>abstract away components of an algorithm that do not influence correctness but only influence <lb/>the performance, by abstracting it into a parameter in proof to reduce unnecessary proof <lb/>One example can be the scheduling heuristics during scheduling choice. Another example is the <lb/>machine architecture: since we implemented the algorithm on intermediate IR that abstracts away <lb/>machine architecture and only customizes the optimization methods towards architecture in the <lb/>heuristics, machine details were barely involved in our verification work (only around 40 lines of <lb/>proof codes which are reusable for different architecture). A similar idea is described in the PhD <lb/>thesis of [9] <lb/>9.3 Future Work <lb/>9.3.1 Improved Scheduling Algorithms or Heuristics. The first possible future work is to apply our <lb/>framework to some advanced efficient (intra-block) instruction scheduling techniques like using <lb/>profiling [10] and integer programming [51]. Using our framework, those improvements will have <lb/>little influence on the proof code to further improve optimization performance. <lb/>9.3.2 Verified Inter-block Scheduling. Our framework currently only supports optimizations inside <lb/>a basic block (intra-block scheduling). We believe that similar idea used in our methods can be <lb/>applied to the correctness of inter-block scheduling. To achieve this, we need to use the concept <lb/>of program dependence relation that consists of both data dependence and control dependence <lb/>relations, i.e. constructing the program dependence graph [21, 25]. <lb/>9.3.3 Verification of Parallelizing Compiler. We wish to stress that this work is not only the first <lb/>work on fully verified instruction scheduling, but also one of the first step on verification of compiler <lb/>optimization for multi-level parallelism, a.k.a the first step to bring CompCert to fully verified <lb/>-O2 and -O3 optimization. Instruction scheduling improves instruction-level parallelism. Besides <lb/>this, we should also consider the possibility of optimizing a program at data-level and thread-level <lb/>during compiler time. For example, transforming independent parts of a single-thread program <lb/>into a multi-thread program like loop parallelization [1]. <lb/></body>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:24 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <body>We believe our result in this work gave a hint on how to improve the parallelism technique of a <lb/>certified compiler. Current CompCert barely supports optimizations towards parallelly executing a <lb/>program in either instruction-or task-level. <lb/></body>
        
        <div type="availability">10 Data Availability <lb/>The data and implementation referenced in this paper have been persistently archived [53]. <lb/></div>
        
        <div type="acknowledgement">Acknowledgments <lb/>We thank Elton Pinto from the University of Pennsylvania for helping us with knowledge of the <lb/>C interface in OCaml language in our implementation of the list scheduling We thank <lb/>Prithayan Barua from SiFive and Jeffrey Young from Georgia Tech for helping with knowledge and <lb/>resources related to our experiments on the Risc-V machine. We thank Xiwei Wu from Shanghai <lb/>Jiao Tong University for inspiration on several mathematical problems related to our formal proofs <lb/>in Coq. <lb/>This material is based upon work supported by the U.S. Department of Energy, Office of Science, <lb/>Office of Advanced Scientific Computing Research under Award Number DE-FOA-0002460: X-Stack: <lb/>Programming Environments for Scientific Computing. <lb/></div>
        
        <listBibl>References <lb/>[1] Alexander Aiken and Alexandru Nicolau. 1988. Optimal loop parallelization. ACM SIGPLAN Notices 23, 7 (1988), <lb/>308-317. <lb/>[2] Andrew W Appel. 2014. Program logics for certified compilers. Cambridge University Press. <lb/>[3] Andrew W. Appel. 2023. Verified Functional Algorithms. Software Foundations, Vol. 3. Electronic textbook. http: <lb/>//softwarefoundations.cis.upenn.edu <lb/>[4] David Bernstein and Michael Rodeh. 1991. Global instruction scheduling for superscalar machines. In Proceedings of <lb/>the ACM SIGPLAN 1991 conference on Programming language design and implementation. 241-255. <lb/>[5] D. Bernstein, M. Rodeh, and I. Gertner. 1989. On the complexity of scheduling problems for parallel/pipelined machines. <lb/>IEEE Trans. Comput. 38, 9 (1989), 1308-1313. https://doi.org/10.1109/12.29469 <lb/>[6] FrÃ©dÃ©ric Besson, Sandrine Blazy, and Pierre Wilke. 2019. CompCertS: a memory-aware verified C compiler using a <lb/>pointer as integer semantics. Journal of Automated Reasoning 63 (2019), 369-392. <lb/>[7] Sandrine Blazy, BenoÃ®t Robillard, and Andrew W. Appel. 2010. Formal Verification of Coalescing Graph-Coloring <lb/>Register Allocation. In Programming Languages and Systems, Andrew D. Gordon (Ed.). Springer Berlin Heidelberg, <lb/>Berlin, Heidelberg, 145-164. <lb/>[8] Uday Bondhugula, Aravind Acharya, and Albert Cohen. 2016. The Pluto+ Algorithm: A Practical Approach for <lb/>Parallelization and Locality Optimization of Affine Loop Nests. ACM Trans. Program. Lang. Syst. 38, 3, Article 12 (April <lb/>2016), 32 pages. https://doi.org/10.1145/2896389 <lb/>[9] Sylvain BoulmÃ©. 2021. Formally Verified Defensive Programming (efficient Coq-verified computations from untrusted ML <lb/>oracles). Habilitation Ã  diriger des recherches. UniversitÃ© Grenoble-Alpes. https://hal.science/tel-03356701 See also <lb/>http://www-verimag.imag.fr/ boulme/hdr.html. <lb/>[10] William Y. Chen, Scott A. Mahlke, Nancy J. Warter, Sadun Anik, and Wen-Mei W. Hwu. 1994. Profile-assisted instruction <lb/>scheduling. International Journal of Parallel Programming (1994). https://doi.org/10.1007/BF02577873 <lb/>[11] Hong-Chich Chou and Chung-Ping Chung. 1995. An optimal instruction scheduler for superscalar processor. IEEE <lb/>Transactions on Parallel and Distributed Systems 6, 3 (1995), 303-313. https://doi.org/10.1109/71.372778 <lb/>[12] Edward G. Coffman and John Bruno. 1976. Computer and job-shop scheduling theory. https://api.semanticscholar. <lb/>org/CorpusID:60396080 <lb/>[13] R. Collins and G.B. Steven. 1996. Instruction scheduling for a superscalar architecture. In Proceedings of EUROMICRO <lb/>96. 22nd Euromicro Conference. Beyond 2000: Hardware and Software Design Strategies. 643-650. https://doi.org/10. <lb/>1109/EURMIC.1996.546492 <lb/>[14] CompCert web 2023. COMPCERT: COMPILERS YOU CAN FORMALLY TRUST. https://compcert.org. <lb/>[15] Cortex-A53 2012. Cortex-A53. https://developer.arm.com/Processors/Cortex-A53. <lb/>[16] David Delahaye. 2000. A tactic language for the system Coq. In Logic for Programming and Automated Reasoning: 7th <lb/>International Conference, LPAR 2000 Reunion Island, France, November 6-10, 2000 Proceedings 7. Springer, 85-95. <lb/>[17] J R Ellis. 1985. Bulldog: a compiler for VLIW architectures. (1 1985). https://www.osti.gov/biblio/5724953 <lb/></listBibl>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <note place="headnote">Fully Verified Instruction Scheduling <lb/></note>
        
        <page>299:25 <lb/></page>
        
        <listBibl>[18] John R Ellis. 1986. Bulldog: a compiler for VLSI architectures. Mit Press. <lb/>[19] Paolo Faraboschi, Joseph A Fisher, and Cliff Young. 2001. Instruction scheduling for instruction level parallel processors. <lb/>Proc. IEEE 89, 11 (2001), 1638-1659. <lb/>[20] Paul Feautrier. 1991. Dataflow analysis of array and scalar references. International Journal of Programming <lb/>(1991). https://doi.org/10.1007/BF01407931 <lb/>[21] Jeanne Ferrante, Karl J Ottenstein, and Joe D Warren. 1987. The program dependence graph and its use in optimization. <lb/>ACM Transactions on Programming Languages and Systems (TOPLAS) 9, 3 (1987), 319-349. <lb/>[22] Fisher. 1981. Trace Scheduling: A Technique for Global Microcode Compaction. IEEE Trans. Comput. C-30, 7 (1981), <lb/>478-490. https://doi.org/10.1109/TC.1981.1675827 <lb/>[23] Philip B Gibbons and Steven S Muchnick. 1986. Efficient instruction scheduling for a pipelined architecture. In <lb/>Proceedings of the 1986 SIGPLAN symposium on Compiler construction. 11-16. <lb/>[24] LÃ©o Gourdin, Benjamin Bonneau, Sylvain BoulmÃ©, David Monniaux, and Alexandre BÃ©rard. 2023. Formally Verifying <lb/>Optimizations with Block Simulations. Proceedings of the ACM on Programming Languages 7, OOPSLA2 (2023), 59-88. <lb/>[25] Mary Jean Harrold, Brian Malloy, and Gregg Rothermel. 1993. Efficient construction of program dependence graphs. <lb/>ACM SIGSOFT Software Engineering Notes 18, 3 (1993), 160-170. <lb/>[26] Hanru Jiang, Hongjin Liang, Siyang Xiao, Junpeng Zha, and Xinyu Feng. 2019. Towards Certified Separate Compilation <lb/>for Concurrent Programs. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and <lb/>Implementation (Phoenix, AZ, USA) (PLDI 2019). Association for Computing Machinery, New York, NY, USA, 111-125. <lb/>https://doi.org/10.1145/3314221.3314595 <lb/>[27] Jeehoon Kang, Yoonseung Kim, Chung-Kil Hur, Derek Dreyer, and Viktor Vafeiadis. 2016. Lightweight verification of <lb/>separate compilation. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming <lb/>Languages. 178-190. <lb/>[28] Hironori Kasahara and Seinosuke Narita. 1984. Practical Multiprocessor Scheduling Algorithms for Efficient Parallel <lb/>Processing. IEEE Trans. Comput. C-33, 11 (1984), 1023-1029. https://doi.org/10.1109/TC.1984.1676376 <lb/>[29] V. L. Kompel&apos;makher and V. A. Liskovets. 1975. Sequential generation of arrangements by means of a basis of <lb/>transpositions. Cybernetics (1975). https://doi.org/10.1007/BF01069459 <lb/>[30] M. Lam. 1988. Software pipelining: an effective scheduling technique for VLIW machines. SIGPLAN Not. 23, 7 (jun <lb/>1988), 318-328. https://doi.org/10.1145/960116.54022 <lb/>[31] Xavier Leroy. 2009. Formal verification of a realistic compiler. Commun. ACM 52, 7 (2009), 107-115. <lb/>[32] Xavier Leroy. 2009. A formally verified compiler back-end. Journal of Automated Reasoning 43 (2009), 363-446. <lb/>[33] Hongjin Liang and Xinyu Feng. 2016. A program logic for concurrent objects under fair scheduling. In Proceedings of <lb/>the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. 385-399. <lb/>[34] Anil Madhavapeddy and Yaron Minsky. 2022. Real World OCaml: Functional Programming for the Masses. Cambridge <lb/>University Press. <lb/>[35] Eric Mullen, Daryl Zuniga, Zachary Tatlock, and Dan Grossman. 2016. Verified peephole optimizations for CompCert. <lb/>In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation. 448-461. <lb/>[36] George C Necula. 2000. Translation validation for an optimizing compiler. In Proceedings of the ACM SIGPLAN 2000 <lb/>conference on Programming language design and implementation. 83-94. <lb/>[37] Daniel Patterson and Amal Ahmed. 2019. The next 700 compiler correctness theorems (functional pearl). Proceedings <lb/>of the ACM on Programming Languages 3, ICFP (2019), 1-29. <lb/>[38] Pierre-Marie PÃ©drot. 2019. Ltac2: tactical warfare. In The Fifth International Workshop on Coq for Programming <lb/>Languages, CoqPL, Vol. 2019. <lb/>[39] Amir Pnueli, Michael Siegel, and Eli Singerman. 1998. Translation validation. In Tools and Algorithms for the Construction <lb/>and Analysis of Systems: 4th International Conference, TACAS&apos;98 Held as Part of the Joint European Conferences on Theory <lb/>and Practice of Software, ETAPS&apos;98 Lisbon, Portugal, March 28-April 4, 1998 Proceedings 4. Springer, 151-166. <lb/>[40] PolyBench. 2011. The Polyhedral Benchmark Suite. http://www.cse.ohio-state.edu/~pouchet/software/polybench/. <lb/>[41] B. Ramakrishna Rau, Christopher D. Glaeser, and Raymond L. Picard. 1982. Efficient code generation for horizontal <lb/>architectures: Compiler techniques and architectural support. In Proceedings of the 9th Annual Symposium on Computer <lb/>Architecture (Austin, Texas, USA) (ISCA &apos;82). IEEE Computer Society Press, Washington, DC, USA, 131-139. <lb/>[42] Silvain Rideau and Xavier Leroy. 2010. Validating Register Allocation and Spilling. CC 6011 (2010), 224-243. <lb/>[43] Jaroslav Å evÄÃ­k, Viktor Vafeiadis, Francesco Zappa Nardelli, Suresh Jagannathan, and Peter Sewell. 2013. CompCertTSO: <lb/>A verified compiler for relaxed-memory concurrency. Journal of the ACM (JACM) 60, 3 (2013), 1-50. <lb/>[44] Cyril Six, Sylvain BoulmÃ©, and David Monniaux. 2020. Certified and Efficient Instruction Scheduling: Application <lb/>to Interlocked VLIW Processors. Proc. ACM Program. Lang. 4, OOPSLA, Article 129 (nov 2020), 29 pages. https: <lb/>//doi.org/10.1145/3428197 <lb/>[45] Cyril Six, LÃ©o Gourdin, Sylvain BoulmÃ©, David Monniaux, Justus Fasse, and Nicolas Nardino. 2022. Formally verified <lb/>superblock scheduling. In Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs <lb/></listBibl>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. <lb/></note>
        
        <page>299:26 <lb/></page>
        
        <note place="headnote">Ziteng Yang, Jun Shirako, and Vivek Sarkar <lb/></note>
        
        <listBibl>and Proofs (Philadelphia, PA, USA) (CPP 2022). Association for Computing Machinery, New York, NY, USA, 40-54. <lb/>https://doi.org/10.1145/3497775.3503679 <lb/>[46] Gordon Stewart, Lennart Beringer, Santiago Cuellar, and Andrew W Appel. 2015. Compositional compcert. In <lb/>Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. 275-287. <lb/>[47] OCaml Manual, chapter 22 2023. https://v2.ocaml.org/manual/intfc.html#c%3Aintf-c <lb/>[48] Jean-Baptiste Tristan and Xavier Leroy. 2008. Formal Verification of Translation Validators: A Case Study on Instruction <lb/>Scheduling Optimizations. In Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of <lb/>Programming Languages (San Francisco, California, USA) (POPL &apos;08). Association for Computing Machinery, New York, <lb/>NY, USA, 17-27. https://doi.org/10.1145/1328438.1328444 <lb/>[49] U74MC 2021. SiFive U74-MC Core Complex Manual 21G1.01.00. https://starfivetech.com/uploads/u74mc_core_ <lb/>complex_manual_21G1.pdf. <lb/>[50] Yuting Wang, Ling Zhang, Zhong Shao, and JÃ©rÃ©mie Koenig. 2022. Verified Compilation of C Programs with a Nominal <lb/>Memory Model. Proc. ACM Program. Lang. 6, POPL, Article 25 (jan 2022), 31 pages. https://doi.org/10.1145/3498686 <lb/>[51] Kent Wilken, Jack Liu, and Mark Heffernan. 2000. Optimal instruction scheduling using integer programming. Acm <lb/>sigplan notices 35, 5 (2000), 121-133. <lb/>[52] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and understanding bugs in C compilers. In <lb/>Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and implementation. 283-294. <lb/>[53] Ziteng Yang, Jun Shirako, and Vivek Sarkar. 2024. Artifact for paper &quot;Fully Verified Instruction Scheduling&quot;. https: <lb/>//doi.org/10.5281/zenodo.13625830. <lb/>[54] Qirun Zhang, Chengnian Sun, and Zhendong Su. 2017. Skeletal Program Enumeration for Rigorous Compiler Testing. <lb/>SIGPLAN Not. 52, 6 (jun 2017), 347-361. https://doi.org/10.1145/3140587.3062379 <lb/></listBibl>
        
        <front>Received 2024-04-06; accepted 2024-08-18 <lb/></front>
        
        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 299. Publication date: October 2024. </note>
        

	</text>

</TEI>