<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_P98-2207"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Keyword Extraction using Term-Domain Interdependence for <lb/>Dictation of Radio News <lb/> Yoshimi Suzuki <lb/>Fumiyo Fukumoto <lb/>Yoshihiro Sekiguchi <lb/>Dept. of Computer Science and Media Engineering <lb/>Yamanashi University <lb/>4-3-11 Takeda, Kofu 400 Japan <lb/> {ysuzuki@suwa, fukumot o@skyo, sokiguti©saiko}, osi. yamanashi, ac. jp <lb/> Abstract <lb/>In this paper, we propose keyword extraction <lb/>method for dictation of radio news which con-<lb/>sists of several domains. In our method, news-<lb/>paper articles which are automatically classified <lb/>into suitable domains are used in order to calcu-<lb/>late feature vectors. The feature vectors shows <lb/>term-domain interdependence and are used for <lb/>selecting a suitable domain of each part of ra-<lb/>dio news. Keywords are extracted by using the <lb/>selected domain. The results of keyword extrac-<lb/>tion experiments showed that our methods are <lb/>robust and effective for dictation of radio news. <lb/></front>

			<body>1 Introduction <lb/>Recently, many speech recognition systems <lb/>are designed for various tasks. However, most <lb/>of them are restricted to certain tasks, for ex-<lb/>ample, a tourist information and a hamburger <lb/>shop. Speech recognition systems for the task <lb/>which consists of various domains seems to be <lb/>required for some tasks, e.g. a closed caption <lb/>system for TV and a transcription system of <lb/>public proceedings. In order to recognize spoken <lb/>discourse which has several domains, the speech <lb/>recognition system has to have large vocabu-<lb/>lary. Therefore, it is necessary to limit word <lb/>search space using linguistic restricts, e.g. do-<lb/>main identification. <lb/>There have been many studies of do-<lb/>main identification which used term weight-<lb/>ing (J.McDonough et al., 1994; Yokoi et al., <lb/>1997). McDonough proposed a topic identifi-<lb/>cation method on switch board corpus. He re-<lb/>ported that the result was best when the num-<lb/>ber of words in keyword dictionary was about <lb/>800. In his method, duration of discourses of <lb/>switch board corpora is rather long and there <lb/>are many keywords in the discourse. However, <lb/>for a short discourse, there are few keywords <lb/>in a short discourse. Yokoi also proposed a <lb/>topic identification method using co-occurrence <lb/>of words for topic identification (Yokoi et al., <lb/>1997). He classified each dictated sentence of <lb/>news into 8 topics. In TV or Radio news, how-<lb/>ever, it is difficult to segment each sentence au-<lb/>tomatically. Sekine proposed a method for se-<lb/>lecting a suitable sentence from sentences which <lb/>were extracted by a speech recognition system <lb/>using statistical language model (Sekine, 1996). <lb/>However, if the statistical model is used for ex-<lb/>traction of sentence candidates, we will obtain <lb/>higher recognition accuracy. <lb/>Some initial studies of transcription of broad-<lb/>cast news proceed (Bakis et al., 1997). However <lb/>there are some remaining problems, e.g. speak-<lb/>ing styles and domain identification. <lb/>We conducted domain identification and key-<lb/>word extraction experiment (Suzuki et al., <lb/>1997) for radio news. <lb/>In the experiment, <lb/>we classified radio news into 5 domains (i.e. <lb/>accident, economy, international, politics and <lb/>sports). The problems which we faced with are; <lb/>1. Classification of newspaper articles into <lb/>suitable domains could not be performed <lb/>automatically. <lb/>2. Many incorrect keywords are extracted, be-<lb/>cause the number of domains was few. <lb/>In this paper, we propose a method for key-<lb/>word extraction using term-domain interdepen-<lb/>dence in order to cope with these two problems. <lb/>The results of the experiments demonstrated <lb/>the effectiveness of our method. <lb/>2 An overview of our method <lb/>Figure 1 shows an overview of our method. <lb/>Our method consists of two procedures. In the <lb/>procedure of term-domain interdependence cal-<lb/>culation, the system calculates feature vectors <lb/>

			<page>1272 <lb/></page>

			of term-domain interdependence using an ency-<lb/>clopedia of current term and newspaper articles. <lb/>In the procedure of keyword extraction in radio <lb/>news, firstly, the system divides radio news into <lb/>segments according to the length of pauses. We <lb/>call the segments units. The domain which has <lb/>the largest similarity between the unit of news <lb/>and the feature vector of each domain is selected <lb/>as domain of the unit. Finally, the system ex-<lb/>tracts keywords in each unit using the feature <lb/>vector of selected domain which is selected by <lb/>domain identification. <lb/> Explanations of~ ~ <lb/>Radio News <lb/> n en,~pediaJ Lar~icle~=i&quot;~&apos; j <lb/> Q::::::::::::::l <lb/> Feature vectors <lb/> caVe) <lb/> D1 <lb/>D7 <lb/> ... <lb/> [~ <lb/> D141 <lb/> Feature vectors <lb/>(FeaVa) <lb/>,~ <lb/>...D1 <lb/>[~ <lb/>Domain identification <lb/> D7 <lb/> &quot;0&quot; <lb/> ... <lb/>ID3 ID7 <lb/>D18 1 <lb/> © <lb/> [~ <lb/>Keyword Extraction <lb/>D141 <lb/>*:~ <lb/>I President <lb/> ] <lb/> I  [ <lb/>{,  Democratic partyJl <lb/>Keyword extraction <lb/>Calculation of term-domain <lb/> interdependence <lb/> Figure 1: An overview of our method <lb/>3 Calculating feature vectors <lb/>In the procedure of term-domain interdepen-<lb/>dence calculation, We calculate likelihood of ap-<lb/>pearance of each noun in each domain. Figure 2 <lb/>shows how to calculate feature vectors of term-<lb/>domain interdependence. <lb/>In our previous experiments, we used 5 do-<lb/>mains which were sorted manually and calcu-<lb/>lated 5 feature vectors for classifying domains of <lb/>each unit of radio news and for extracting key-<lb/>words. Our previous system could not extract <lb/>some keywords because of many noisy keywords. <lb/>In our method, newspaper articles and units of <lb/>radio news are classified into many domains. At <lb/>each domain, a feature vector is calculated by <lb/>an encyclopedia of current terms and newspaper <lb/>articles. <lb/>3.1&quot; Sorting newspaper articles <lb/>according to their domains <lb/>Firstly, all sentences in the encyclopedia are <lb/>analyzed morpheme by Chasen (Matsumoto et <lb/> An encyclopedia of current terms 1 <lb/>41domains 10,236 explanations) <lb/> © <lb/> ISorting  explanations  ] <lb/>Q  Newspaper articles <lb/> about 110,000 articles.,/ <lb/> © <lb/> [Separa~articles I <lb/>[ Extra~:~nouns I <lb/> IE~rac~&apos; ~ <lb/>n°unsl i Calculating frequ~ vectors (FreqVa) I <lb/> ICalculating frequency vectors (FreqVe)l <lb/>._1 Calculating similarity <lb/>I <lb/>. <lb/>~&apos;~ between FeaVe and FreqVa I <lb/>Calculating X values of <lb/>| <lb/>&apos; <lb/>_r-L <lb/>J each noun on domains <lb/>J <lb/>X,,7 <lb/>I <lb/>I Sorting articles into domains <lb/>. <lb/>V <lb/>. I <lb/>laccording to simitarity <lb/>I <lb/>~41 feature vectors (FeaVe)~ -.-I <lb/>.:~ <lb/>Calculating x:values of <lb/>each noun on doma ns <lb/> © <lb/> 041 feature vectors (FeaVa)~ <lb/> Figure  2:  Calculating feature vectors <lb/>al., 1997) and nouns which frequently appear <lb/>are extracted. A feature vector is calculated by <lb/>frequency of each noun at each domain. We <lb/>call the feature vector  FeaVe.  Each element <lb/>of  FeaVe  is a X 2 value (Suzuki et al., 1997). <lb/>Then, nouns are extracted from newspaper ar-<lb/>ticles by a morphological analysis system (Mat-<lb/>sumoto et al., 1997), and frequency of each noun <lb/>are counted. Next, similarity between  FeaVe  of <lb/>each domain and each newspaper article are cal-<lb/>culated by using formula (1). Finally, a suitable <lb/>domain of each newspaper article are selected by <lb/>using formula (2). <lb/> Sirn(i,j) = FeaVej. FreqVai <lb/> (1) <lb/> Dornainl  = arg max  Sim(i,j) <lb/> (2) <lb/> I~j~N <lb/> where i means a newspaper article and j means <lb/>a domain. (.) means operation of inner vector. <lb/>3.2 Term-domain interdependence <lb/>represented by feature vectors <lb/>Firstly, at each newspaper articles, less than <lb/>5 domains whose similarities between each ar-<lb/>ticle and each domain are large are selected. <lb/>Then, at each selected domain, the frequency <lb/>vector is modified according to similarity value <lb/>and frequency of each noun in the article. For <lb/>example, If an article whose selected domains <lb/>are &quot;political party&quot; and &quot;election&quot;, and simi-<lb/>larity between the article and &quot;political party&quot; <lb/>

			<page> 1273 <lb/></page>

			and similarity between the article and &quot;elec-<lb/>tion&quot; are 100 and 60 respectively, each fre-<lb/>quency vector is calculated by formula (3) and <lb/>formula (4). <lb/>100 <lb/> FreqVm = FreqV~ + FreqVal  x 1-~ (3) <lb/>60 <lb/> freqV~l = FreqV~z + freqVai  x 1-~ (4) <lb/>where i means a newspaper article. <lb/>Then, we calculate feature vectors  FeaVa  us-<lb/>ing  FreqV  using the method mentioned in our <lb/>previous paper (Suzuki et al., 1997). Each el-<lb/>ement of feature vectors shows X 2 value of the <lb/>domain and wordk. All wordk (1 &lt; k &lt; M :M <lb/>means the number of elements of a feature vec-<lb/>tor) are put into the keyword dictionary. <lb/>4 Keyword extraction <lb/>Input news stories are represented by <lb/>phoneme lattice. There are no marks for word <lb/>boundaries in input news stories. Phoneme lat-<lb/>tices are segmented by pauses which are longer <lb/>than 0.5 second in recorded radio news. The <lb/>system selects a domain of each unit which is <lb/>a segmented phoneme lattice. At each frame of <lb/>phoneme lattice, the system selects maximum <lb/>20 words from keyword dictionary. <lb/>4.1 Similarity between a domain and <lb/>an unit <lb/>We define the words whose X 2 values in <lb/>the feature vector of domainj are large as key-<lb/>words of the domainj. In an unit of radio <lb/>news about &quot;political party&quot;, there are many <lb/>keywords of &quot;political party&quot; and the X 2 value <lb/>of keywords in the feature vector of &quot;political <lb/> 2 <lb/> party&quot; is large. Therefore, sum of Xw,pollticalparty <lb/> tends to be large (w : a word in the unit). In our <lb/>method, the system selects a word path whose <lb/>2 is maximized in the word lattice <lb/>sum of  Xkj <lb/> at domaini. The similarity between unit/ and <lb/>domainj is calculated by formula (5). <lb/> Sim(i, j) = max Sim&apos;(i, j) <lb/> all paths <lb/> = <lb/>max <lb/>np(wordk) x Xk,15) <lb/> all paths <lb/> In formula (5), wordk is a word in the <lb/>word lattice, and each selected word does not <lb/>share any frames with any other selected words. <lb/>np(wordk) is the number of phonemes of wordk. <lb/> 2 <lb/> Xk,j is x2value of wordk for domainj. <lb/>The system selects a word path whose <lb/> Siml(i,j)  is the largest among all word paths <lb/>for domainj. <lb/>Figure 3 shows the method of calculating sim-<lb/>ilarity between unit/ and domainD1. The sys-<lb/>tem selects a word path whose  Sim~(uniti,  D1) <lb/>is larger than those of any other word paths. <lb/> phoneme lattice of  uni~ <lb/> andidates <lb/> i-<lb/> Si~unit.  DI ) =max(3.2x3+ 0.5x6,3,2x3+ 4.3x4+ 0.7× 2, <lb/>3.2x3+ 4.3x4+ 4.3x3, <lb/>1.2 x 3+ 0.3 x 4,--.) <lb/> Figure 3: Calculating similarity between unit/ <lb/>and D1 <lb/>4.2 Domain identification and keyword <lb/> extraction <lb/> In the domain identification process, the sys-<lb/>tem identifies each unit to a domain by formula <lb/>(5). If  Sim(i,j)  is larger than similarities be-<lb/>tween an unit and any other domains, domainj <lb/>seems to be the domain of unit~. The system se-<lb/>lects the domain which is the largest of all sim-<lb/>ilarities in N of domains as the domain of the <lb/>unit (formula (6)) . The words in the selected <lb/>word path for selected domain are selected as <lb/>keywords of the unit. <lb/> Domaini  = arg max  Sim(i,j) <lb/> (6) <lb/>X&lt;j&lt;N <lb/>&quot; <lb/>5 Experiments <lb/>5.1 Test data <lb/>The test data we have used is a radio news <lb/>which is selected from NHK 6 o&apos;clock radio news <lb/>in August and September of 1995. Some news <lb/>stories are hard to be classified into one do-<lb/>main in radio news by human. For evalua-<lb/>tion of domain identification experiments, we <lb/>

			<page>1274 <lb/></page>

			selected news stories which two persons classi-<lb/>fied into the same domains are selected. The <lb/>units which were used as test data are seg-<lb/>mented by pauses which are longer than 0.5 <lb/>second. We selected 50 units of radio news for <lb/>the experiments. The 50 units consisted of 10 <lb/>units of each domain. We used two kinds of test <lb/>data. One is described with correct phoneme <lb/>sequence. The other is written in phoneme lat-<lb/>tice which is obtained by a phoneme recognition <lb/>system (Suzuki et al., 1993). In each frame of <lb/>phoneme lattice, the number of phoneme candi-<lb/>dates did not exceed 3. The following equations <lb/>show the results of phoneme recognition. <lb/> the  number of correct phonemes in <lb/>phoneme lattice <lb/>the number of uttered phonemes <lb/>the number of correct phonemes in <lb/>phoneme lattice <lb/>phoneme segments in phoneme lattice <lb/>= 95.6% <lb/>= 81.2% <lb/>5.2 Training  data <lb/> In order to classify newspaper articles into <lb/>small domain, we used an encyclopedia of cur-<lb/>rent terms &quot;Chiezo&quot;(Yamamoto, 1995). In the <lb/>encyclopedia, there are 141 domains in 9 large <lb/>domains. There are 10,236 head-words and <lb/>those explanations in the encyclopedia. In or-<lb/>der to calculate feature vectors of domains, all <lb/>explanations in the encyclopedia are performed <lb/>morphological analysis by Chasen (Matsumoto <lb/>et al., 1997). 9,805 nouns which appeared more <lb/>than 5 times in the same domains were selected <lb/>and a feature vector of each domain was cal-<lb/>culated. Using 141 feature vectors which were <lb/>calculated in the encyclopedia, we identified do-<lb/>mains of newspaper articles. We identified do-<lb/>mains of 110,000 articles of newspaper for cal-<lb/>culating feature vectors automatically. We se-<lb/>lected 61,727 nouns which appeared at least 5 <lb/>times in the newspaper articles of same domains <lb/>and calculated 141 feature vectors. <lb/>5.3 Domain identification experiment <lb/>The system selects suitable domain of each <lb/>unit for keyword extraction. Table I shows <lb/>the results of domain identification. We con-<lb/>ducted domain identification experiments using <lb/>two kinds of input data, i.e. correct phoneme <lb/>sequence and phoneme lattice and two kinds of <lb/>domains, i.e. 141 domains and 9 large domains. <lb/>We also compared the results and the result us-<lb/>ing previous method (Suzuki et al., 1997). For <lb/>comparison, we selected 5 domains which are <lb/>used by previous method in our method. In <lb/>previous method, we used a keyword dictionary <lb/>which has 4,212 words. <lb/>Table 1: The result of domain identification <lb/>number of <lb/>Correct Phoneme <lb/>method <lb/>domains phoneme <lb/>lattice <lb/>our <lb/>141 <lb/>62% <lb/>40% <lb/>method <lb/>9 <lb/>78% <lb/>54% <lb/>5 <lb/>90% <lb/>82% <lb/>previous <lb/>5 <lb/>86% <lb/>78% <lb/>method <lb/>5.4 Keyword extraction experiment <lb/>We have conducted keyword extraction ex-<lb/>periment using the method with 141 feature <lb/>vectors (our method), 5 feature vectors (pre-<lb/>vious method) and without domain identifica-<lb/>tion. Table 2 shows recall and precision which <lb/>are shown in formula (7), and formula (8), re-<lb/>spectively, when the input data was phoneme <lb/>lattice. <lb/>the number of correct words in <lb/>recall = MSKP <lb/>the number of selected words in (7) <lb/>MSKP <lb/>the number of correct words <lb/>precision = in MSKP <lb/>the number of correct nouns (8) <lb/>in the unit <lb/>MSKP : the most suitable keyword path for se-<lb/>lected domain <lb/>6 Discussion <lb/> 6.1 <lb/> Sorting newspaper  articles <lb/> according to their domains <lb/>For using X 2 values in feature vectors, we <lb/>have good result of domain identification of <lb/>newspaper articles. Even if the newspaper ar-<lb/>ticles which are classified into several domains, <lb/>the suitable domains are selected correctly. <lb/> 6.2 <lb/>Domain identification of radio news <lb/> Table I shows that when we used 141 kinds of <lb/>domains and phoneme lattice, 40% of units were <lb/>identified as the most suitable domains by our <lb/>

			<page>1275 <lb/></page>

			Table 2: Recall and precision of keyword extrac-<lb/>tion <lb/>Method <lb/>R/P <lb/>our method <lb/>R <lb/>(141 domains) <lb/>P <lb/>previous method <lb/>R <lb/>(5 domains) <lb/>P <lb/>without DI <lb/>R <lb/>. (1 domain) <lb/>P <lb/>Correct <lb/>phoneme <lb/>Phoneme <lb/>lattice <lb/>88.5% <lb/>48.9% <lb/> 69.0% <lb/> 38.1% <lb/> 80.0% <lb/> 63.1% <lb/> 77.0% <lb/> 60.1% <lb/> 24.0% <lb/>33.0% <lb/> 12.2% <lb/> 9.5% <lb/> R: recall P: precision Dh domain identification <lb/>method and shows that when we used 9 kinds <lb/>of domains and phoneme lattice, 54% of units <lb/>are identified as the most suitable domains by <lb/>our method. When the number of domains was <lb/>5, the results using our method are better than <lb/>our previous experiment. The reason is that we <lb/>use small domains. Using small domains, the <lb/>number of words whose X 2 values of a certain <lb/>domain are high is smaller than when large do-<lb/>mains are used. <lb/>For further improvement of domain identifi-<lb/>cation, it is necessary to use larger newspaper <lb/>corpus in order to calculate feature vectors pre-<lb/>cisely and have to improve phoneme recogni-<lb/>tion. <lb/>6.3 Keyword extraction of radio news <lb/>When we used our method to phoneme lat-<lb/>tice, recall was 48.9% and precision was 38.1%. <lb/>We compared the result with the result of our <lb/>previous experiment (Suzuki et al., 1997). The <lb/>result of our method is better than the our pre-<lb/>vious result. The reason is that we used do-<lb/>mains which are precisely classified, and we can <lb/>limit keyword search space. However recall was <lb/>48.9% using our method. It shows that about <lb/>50% of selected keywords were incorrect words, <lb/>because the system tries to find keywords for <lb/>all parts of the units. In order to raise recall <lb/>value, the system has to use co-occurrence be-<lb/>tween keywords in the most suitable keyword <lb/>path. <lb/> 7 Conclusions <lb/> In this paper, we proposed keyword extrac-<lb/>tion in radio news using term-domain interde-<lb/>pendence. In our method, we could obtain <lb/>sorted large corpus according to domains for <lb/>keyword extraction automatically. Using our <lb/>method, the number of incorrect keywords in <lb/>extracted words was smaller than the previous <lb/>method. <lb/>In future, we will study how to select correct <lb/>words from extracted keywords in order to ap-<lb/>ply our method for dictation of radio news. <lb/>8 <lb/>
			
		</body>
		
		<back>
			
			<div type="acknowledgement">Acknowledgments <lb/>The authors would like to thank Mainichi <lb/>Shimbun for permission to use newspaper arti-<lb/>cles on CD-Mainichi Shimbun 1994 and 1995, <lb/>Asahi Shimbun for permission to use the data <lb/>of the encyclopedia of current terms &quot;Chiezo <lb/>1996&quot; and Japan Broadcasting Corporation <lb/>(NHK) for permission to use radio news. The <lb/>authors would also like to thank the anonymous <lb/>reviewers for their valuable comments. <lb/></div>

			<listBibl>References <lb/>Baimo Bakis, Scott Chen, Ponani Gopalakrishnan, <lb/>Ramesh Gopinath, Stephane Maes, and Lazaros <lb/>Pllymenakos. 1997. Transcription of broadcast <lb/>news -system robustness issues and adaptation <lb/>techniques. In Proc. ICASSP&apos;97, pages 711-714. <lb/>J.McDonough, K.Ng, P.Jeanrenaud, H.Gish, and <lb/>J.R.Rohlicek. 1994. Approaches to topic identifi-<lb/>cation on the switchboard corpus. In Proc. IEEE <lb/>ICASSP&apos;94, volume 1, pages 385-388. <lb/>Yuji Matsumoto, Akira Kitauchi, Tatuo Yamashita, <lb/>Osamu Imaichi, and Tomoaki Imamura, 1997. <lb/> Japanese Morphological Analysis System ChaSen <lb/>Manual. Matsumoto Lab. Nara Institute of Sci-<lb/>ence and Technology. <lb/>Satoshi. Sekine. 1996. Modeling topic coherence for <lb/>speech recognition. In Proc. COLING 96, pages <lb/>913-918. <lb/>Yoshimi Suzuki, Chieko Furuichi, and Satoshi Imai. <lb/>1993. Spoken japanese sentence recognition us-<lb/>ing dependency relationship with systematical <lb/>semantic category. Trans. of IEICE J76 D-II, <lb/> 11:2264-2273. (in Japanese). <lb/>Yoshimi Suzuki, Fumiyo Fukumoto, and Yoshihiro <lb/>Sekiguchi. 1997. Keyword extraction of radio <lb/>news using term weighting for speech recognition. <lb/>In NLPRS97, pages 301-306. <lb/>Shin Yamamoto, editor. 1995. The Asahi Encyclo-<lb/>pedia of Current Terms &apos;Chiezo&apos;. Asahi Shimbun. <lb/>Kentaro Yokoi, Tatsuya Kawahara, and Shuji <lb/>Doshita. 1997. Topic identification of news <lb/>speech using word cooccurrence statistics. In <lb/> Technical Report of IEICE SP96-I05, pages 71-<lb/>78. (in Japanese). <lb/></listBibl>

			<page>1276 </page>

		</back>
	</text>
</tei>
