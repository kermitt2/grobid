<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Journal of <lb/>Information Systems Engineering <lb/>and Business Intelligence <lb/>Vol.10, No.2, June 2024 <lb/>Available online at: http://e-journal.unair.ac.id/index.php/JISEBI <lb/>ISSN 2443-2555 (online) 2598-6333 (print) © 2024 The Authors. Published by Universitas Airlangga. <lb/>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/) <lb/>doi: http://dx.doi.org/10.20473/jisebi.10.2.206-216 <lb/>Ground Coverage Classification in UAV Image Using a <lb/>Convolutional Neural Network Feature Map <lb/>Erika Maulidiya 1) , Chastine Fatichah 2)* , Nanik Suciati 3) , Yuslena Sari 4) <lb/>1 2) 3) Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia <lb/>1) erikamaulidiya18@gmail.com, 2) chastine@if.its.ac.id, 3) nanik@if.its.ac.id <lb/>4) Department of Information Technology, Lambung Mangkurat University, Banjarmasin, Indonesia <lb/>4) yuzlena@ulm.ac.id <lb/>Abstract <lb/>Background: To understand land transformation at the local level, there is a need to develop new strategies appropriate for land <lb/>management policies and practices. In various geographical research, ground coverage plays an important role particularly in <lb/>planning, physical geography explorations, environmental analysis, and sustainable planning. <lb/>Objective: The research aimed to analyze land cover using vegetation density data collected through remote sensing. <lb/>Specifically, the data assisted in land processing and land cover classification based on vegetation density. <lb/>Methods: Before classification, image was preprocessed using Convolutional Neural Network (CNN) architecture&apos;s ResNet 50 <lb/>and DenseNet 121 feature extraction methods. Furthermore, several algorithm were used, namely Decision Tree, Naïve Bayes, <lb/>K-Nearest Neighbor, Random Forest, Support Vector Machine (SVM), and eXtreme Gradient Boosting (XGBoost). <lb/>Results: Classification comparison between methods showed that using CNN method obtained better results than machine <lb/>learning. By using CNN architecture for feature extraction, SVM method, which adopted ResNet-50 for feature extraction, <lb/>achieved an impressive accuracy of 85%. Similarly using SVM method with DenseNet121 feature extraction led to a <lb/>performance of 81%. <lb/>Conclusion: Based on results comparing CNN and machine learning, ResNet 50 architecture performed the best, achieving a <lb/>result of 92%. Meanwhile, SVM performed better than other machine learning method, achieving an 84% accuracy rate with <lb/>ResNet-50 feature extraction. XGBoost came next, with an 82% accuracy rate using the same ResNet-50 feature extraction. <lb/>Finally, SVM and XGBoost produced the best results for feature extraction using DenseNet-121, with an accuracy rate of 81%. <lb/>Keywords: Classification, CNN Architecture, Feature Extraction, Ground Coverage, Vegetation Density. <lb/>Article history: Received 14 November 2023, first decision 9 January 2024, accepted 27 May 2024, available online 28 June 2024 <lb/></front>

			<body>I. INTRODUCTION <lb/>Indonesia is home to 149,056 km² of peatlands, distributed across three islands: Kalimantan, Sumatra, and Papua. <lb/>[1]. Previous research has shown that these peatlands are the second most abundant in the world [2]. Typically, <lb/>peatlands offer numerous essential services to nearby communities, including maintaining air and water quality and <lb/>supporting fish populations. However, the intentional or accidental human activities often cause fire outbreaks, <lb/>disrupting many operations in the country and neighboring countries [3], [4]. Peatlands conditions are strongly <lb/>influenced by drought characteristics, particularly during El-Nino [5], [6]. The reductions in peatlands are also caused <lb/>by factors such as Drought Index, Evapotranspiration, and Interception Loss. Analysis of this area requires ground <lb/>coverage data based on vegetation density [7]. Conceptually, the term &quot;vegetation&quot; refers to all plants covering ground <lb/>in a given area. To determine drought level, the drought index is obtained using Keetch-Byram Dryness Index (KBDI) <lb/>method. Moreover, density of vegetation cover on KBDI peat is classified into three categories namely, bare, medium, <lb/>and high [8]. Vegetation analysis is a method for exploring the arrangement and composition of vegetation in terms <lb/>of its structure. Remote sensing technology can be used to access data related to vegetation density effectively. <lb/>Remote sensing uses two types of data, namely satellite and Unmanned Aerial Vehicle (UAV). Specifically, remote <lb/>sensing with satellite data operates at high altitudes and is easily affected by external factors such as weather and <lb/>clouds. Satellites have limitations such as low spectral and temporal resolution, relatively long review times, <lb/>difficulties in data extraction and image interpretation, and climatic conditions affecting image capture. These <lb/></body>

			<front>* Corresponding author <lb/>Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></front>

			<page>207 <lb/></page>

			<body>limitations, along with high costs, lack of flexibility, and operational complexity, make satellite remote sensing <lb/>impractical and not recommended [9]. On the other hand, UAV produce high-resolution image and are widely used <lb/>in agricultural research, successfully bridging the gap between ground data and larger-scale observations but require <lb/>more preprocessing. UAV drone offer great potential for developing remote sensing technology such as area <lb/>classification by creating spatial data from aerial image. However, optimal results can be achieved using a UAV or <lb/>drone for classification based on the object and the appropriate processing method. Convolutional Neural Network <lb/>(CNN) in deep learning methods revolutionizes image classification by learning basic shapes in the first layer and <lb/>deeper layers into learned image features, leading to more accurate classification [10]. <lb/>CNN profound learning strategy produced the most significant results in image recognition among developing deep <lb/>learning methods [11]. CNN has been extensively used, improving classification and enabling the use of its feature in <lb/>the current algorithms. Furthermore, this procedure has been especially applied to tasks related to remote sensing <lb/>image analysis, such as segmentation, object-based image analysis, recording image, grouping scenes, objects <lb/>detection, and ordering ground inclusion and land use classification. CNN can automatically learn image feature <lb/>representations and outperform many feature extraction methods [12]. Additionally, previous explorers have used <lb/>CNN method for feature extraction. For example, Simon and Uma [13] used deep learning-based feature extraction <lb/>for texture classification, using Support Vector Machine (SVM) as the classification method. The explorers used <lb/>architecture such as DenseNet 201, ResNet 50, ResNet 101, Inception v3, and AlexNet. The accuracy results obtained <lb/>after using CNN feature are significantly improved, achieving a 95% accuracy rate [13]. Suganya Athisayamani et al <lb/>[14] conducted research on ResNet 152 feature extraction and optimal feature dimension reduction for MRI brain <lb/>tumor classification. The research found that using CNN architecture for image processing, especially for feature <lb/>extraction, achieved an accuracy of 98.85% [14]. Several other explorations have also applied CNN method for feature <lb/>extraction in processing image data before classification. However, there is a lack of application of CNN architecture <lb/>in processing vegetation density image data, as identifying and characterizing plants requires space and time [16]. The <lb/>research proposes a method for Ground Coverage Classification based on vegetation density. The exploration uses <lb/>CNN feature extraction to analyze image data more efficiently, accurately, and flexibly [15]. When CNN is used as <lb/>feature extractor, an additional classification method is needed. The exploration uses machine learning methods to <lb/>classify UAV image. <lb/>The application of machine learning algorithms to remote sensing image for ground coverage classification has <lb/>been widely used in previous research [16]. Recent advancements in land use and ground coverage classification <lb/>methodology have shown significant progress, particularly with the implementation of machine learning methods <lb/>[17]. Furthermore, these machine learning models are leading analytical tools used to monitor, map, and measure land <lb/>and ground coverage changes over time. Based on literature explorations, widely used methods for ground coverage <lb/>classification based on vegetation density include Decision Tree, Naïve Bayes, K-Nearest Neighbor (KNN), Random <lb/>Forest, SVM, and eXtreme Gradient Boosting (XGBoost) [16], [17], [18]. This research will apply machine learning <lb/>classification methods to achieve its objectives. The use of these methods has acquired significant interest among <lb/>explorers, especially with satellite image data [19]. However, machine learning methods have generally been limited <lb/>to satellite data and have not been widely applied to UAV data [20]. <lb/>The research aims to combine architectural principles with machine learning methods. In this context, CNN is used <lb/>for feature extraction, which is further combined with machine learning methods for classification. Previous <lb/>exploration primarily focused on comparing results between methods without combining different methods. However, <lb/>the current research combined different architectures for land cover classification to achieve better results. CNN <lb/>should be considered the best component extractor because it offers several advantages over other strategies. For <lb/>instance, it can capture better and stronger elements from a large amount of image data and significantly improve <lb/>precision more quickly [12]. Furthermore, CNN learns image feature representations automatically and outperforms <lb/>many feature extraction methods [13]. This research uses ResNet-50 and DenseNet-121 architecture for feature <lb/>extraction. ResNet 50 has a simple design strategy with a single identity shortcut, while DenseNet 121 has higher <lb/>capacity by combining feature multiple layer, which helps in training deep network more effectively. Additionally, <lb/>DenseNet 121 uses feature more efficiently with fewer parameters [21]. Consequently, this research examines feature <lb/>extraction using architecture and classification using machine learning. The analysis determines results based on <lb/>precision, precision, recall, and F1 score. <lb/>II. LITERATURE REVIEW <lb/>Research on Ground Coverage Classification based on vegetation density has been conducted by several previous <lb/>explorers. Consequently, the explorers have processed complex image data before classification, signifying that <lb/></body>

			<note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>208 <lb/></page>

			<body>achieving good accuracy depends on both the method and the quality of image data processing. Several members <lb/>recommend using deep learning methods to process complex image data effectively. <lb/>Camile Sothe et al. processed UAV dataset using CNN method and feature extraction in 2020 [22]. This research <lb/>examines classification of 14 different tree species in Southern Brazil&apos;s subtropical forests. The performance of CNN <lb/>method is compared to other traditional machine learning methods, specifically SVM and Random Forest (RF), using <lb/>the original training data. Furthermore, the process of extending and adjusting the sample set (data expansion) for tree <lb/>species classification is associated with CNN method. Execution of SVM and RF classifiers combines data expansion <lb/>and spatial elements extracted by CNN. The outcomes show that CNN classifier outperforms traditional SVM and RF <lb/>classifiers, achieving a total accuracy (TA) of 84.37% and a Kappa coefficient of 0.82. Despite the initial poor accuracy <lb/>with original spectral groups (TA 62.67% and 59.24%), SVM and RF classifiers showed significant improvement in <lb/>TA by 14% to 21% when combined with data expansion and spatial feature extracted by CNN [22]. <lb/>Tanmay Kumar Behera and colleagues presented a dense module-influenced deep learning model in 2022 [20]. <lb/>This model, known as UDD tackles the vanishing gradient problem commonly observed in mechanisms and improves <lb/>the feedforward properties of networks. The proposed end-to-end CNN architecture accurately extracts global feature <lb/>to segment vegetation classes from aerial image through symmetric downscaling and upscaling passes. Furthermore, <lb/>two UAV image datasets were used to evaluate the proposed architecture which includes the NITRDrone dataset and <lb/>the UDD. Compared to the current methods, this method achieved intersection over union (IoU) values of 74% on <lb/>UDD dataset and 84% on NITRDrone dataset [20]. <lb/>Guilia Cecilia et al. used UAV image classification to monitor ground coverage in 2023. The research examined <lb/>and compared prediction models built with CNN, VGG16, DenseNet 121, and ResNet 50 using multitemporal and <lb/>single-date Sentinel-2 satellite data. Specifically, VGG16 model achieved a TA of 71% when applied to single-dated <lb/>and multi-temporal image, showing improved performance [23]. Following this result, Darwin Alexis Arrechea-<lb/>Castillo et al. conducted research in 2023 using UAV to classify ground coverage by applying CNN architecture, <lb/>specifically LeNet to sentinel-2 image. Analysis of the validation data showed that the proposed CNN achieved a <lb/>kappa coefficient and TA of 0.962 and 96.51%, respectively [24]. <lb/>Several previous explorations have applied CNN feature extraction to datasets beyond UAV data. For example, <lb/>Suganya Athisayamani et al. (2023) used Residual Deep Convolutional Neural Network (ResNet-152) to extract <lb/>feature and minimize ideal feature dimensions for tumor classification. Feature classification procedure was then <lb/>completed using the softmax classifier and ResNet-152. It should be acknowledge that this method was applied to the <lb/>Figshare dataset using Python. Various criteria, including sensitivity, specificity, and accuracy, were used to evaluate <lb/>the total effectiveness of the proposed cancer classification system. Consequently, the proposed method outperformed <lb/>others, achieving a final evaluation accuracy of 98.85%. However, Enhanced Chimpanzee Optimization Algorithm <lb/>(EChOA) was used to select feature by reducing the dimensions of the retrieved feature [14]. <lb/>Research has also conducted ground coverage classification trials using CNN method in data processing or <lb/>classification fields. However, very few have compared deep learning and machine learning methods or combined the <lb/>two for ground coverage classification based on vegetation density. To classify ground coverage based on vegetation <lb/>density, this research used CNN for both data processing and classification. Classification results were then compared <lb/>with those obtained using machine learning methods. <lb/>III. METHODS <lb/>A. Research Flow <lb/>The flowchart in Fig. 1 showed the steps of the research process and the methodologies used in this research. The <lb/>process started with capturing image from a standard camera or UAV. This image was then passed through <lb/>preprocessing, which included tasks such as resizing and augmentation to facilitate easier handling during the training. <lb/>After preprocessing, the data was divided into training and examining sets with 80:20 ratio. Following this division, <lb/>feature extraction was performed using CNN architecture, specifically ResNet 50 and DenseNet 121. The extracted <lb/>feature was subsequently used for classification through various machine learning methods, including Decision Tree, <lb/>Naive Bayer, KNN, Random Forest, SVM, and XGBoost. After classification, the results for each class were obtained. <lb/>B. Dataset <lb/>The dataset was collected in Liang Anggang Protected Forest, South Kalimantan using DJI Mavic Pro drone, and a <lb/>total of 10,000 vegetation image was obtained. Classification classes in this research were divided into 3 categories <lb/>namely, bare, medium, and high, with each class comprising 1000 image. In addition, this classification was based on <lb/>the state of the Liang Anggang Protected Forest, which had an abundance of plants suitable for data collection on <lb/>ground coverage characteristics and vegetation density types. Certain areas of Liang Anggang protected forest were <lb/></body>

			<note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>209 <lb/></page>

			<body>inaccessible to drone due to proximity to Syamsudin Noor airport, which was in a restricted fly zone limiting drone <lb/>flight to a maximum altitude of 60 meters. Consequently, the capture area was limited to a maximum of 60 meters in <lb/>height, with drone operating at an altitude of 20 meters to ensure adequate area coverage. The dataset was accessible <lb/>through Mendeley at this link: https://data.mendeley.com/datasets/tb26zy2jst/1. <lb/>Fig. 1 Research flow <lb/>The dataset consisted of three classes, each containing 1000 image of vegetation density categorized as high, <lb/>medium, and bare. Image data acquired by the drone had a high resolution of 4000 x 3000 pixels. To make the <lb/>classification process easier, image was cropped, which reduced the space and memory required during classification. <lb/>For training, image was resized to 256 x 256 pixels to improve learning efficiency, and the next step included labeling. <lb/>Moreover, pre-processing was crucial because the obtained dataset was used to design the distribution of training and <lb/>examined data at this stage. [25], [26]. Image cropping and labeling process was shown in Fig. 2. <lb/>(a) <lb/>Image <lb/>Class <lb/>Bare <lb/>Medium <lb/>High <lb/>(b) <lb/>Fig. 2 Dataset processing (a) image cropping; (b) labelling process <lb/>C. Feature Extraction from CNN Architecture <lb/>Before conducting the classification process, features were extracted to improve prediction accuracy, increase <lb/>confidence in calibration, and make interpretation easier. [27]. Various research developed different methods for <lb/>feature extraction, each based on distinct principles, but none was perfect. Additionally, numerous explorations <lb/>showed the effectiveness of deep learning in feature extraction [12]. An example of deep learning used to extract <lb/>visual features was CNN [28], which was used alongside Transfer Learning for both feature extraction and <lb/>classification purposes [29]. <lb/>CNN feature was extracted from the resulting feature map and the first convolution layer. In the context of this <lb/>research, cross-entropy was computed as a loss hyperparameter. The exploration predicted feature map from the first <lb/>convolution layer, which provided discriminative feature to expedite the classification process. In addition, a remnant <lb/>network known as ResNet 50 was good at capturing distinctive feature. Moreover, DenseNet 121 was used because <lb/></body>

			<note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>210 <lb/></page>

			<body>the algorithm could analyze millions of image from ImageNet database [13]. The model also resized input image to <lb/>224x224 pixels and used relatively few parameters, improving interconnections throughout the network. This research <lb/>used ResNet 50 and DenseNet 121 feature extraction from CNN architecture. <lb/>1) ResNet-50 Architecture <lb/>ResNet, which was a complex CNN based on residual blocks, tackled the problem of gradient degradation in <lb/>extremely deep networks [12]. ResNet introduced residual blocks by combining shortcut connections between layers, <lb/>which improved accuracy without increasing network depth and prevented corruption as the process became more <lb/>complex. Furthermore, ResNet had over eleven million parameters and 152 layers. The model used a 3x3 convolution <lb/>filter, batch normalization, residual blocking, global average pooling, and classification layer (SoftMax). For this <lb/>research, ResNet 50 was used to propose multiple variations with different numbers of layers. An overview of the <lb/>ResNet-50 architecture was shown in Fig. 3 [30]. <lb/>Fig. 3 ResNet-50 architecture [30] <lb/>2) DenseNet-121 Architecture <lb/>DenseNet 121 architecture connected all layers gradually compared to the conventional CNN architecture [26]. <lb/>This architecture provided various methods for handling image data, using dense blocks with multiple layers. Each <lb/>layer in DenseNet 121 had feature map connected from the first layer until a new layer was created. Additionally, <lb/>architecture introduced benefits such as feature reuse and reduction in bursting or gradient loss. Changes were made <lb/>to DenseNet 121 structure to enable feature extraction, including downsampling feature map to facilitate combining. <lb/>A solid block concept was proposed to facilitate downsampling, with transitional layer containing convolution and <lb/>batch normalization operation in between the solid blocks. Furthermore, DenseNet 121 also took advantage of shortcut <lb/>connections, comprising transition layer connecting dense group of blocks [12]. As feature map from the previous <lb/>layer were transferred to the next layer, the network became denser and thinner. Architecture included classification <lb/>layer, a convolution and pooling layer, a transition layer, and multiple dense blocks arranged in series. An overview <lb/>of DenseNet-121 architecture was shown in Fig. 4 [31]. <lb/>Fig. 4 DenseNet-121 architecture [31] <lb/>D. Classification Methods Classification Methods <lb/>The research proposed decision trees and machine learning methods for classification. This classification included <lb/>Nave Bayes, KNN, Random Forest, SVM, and XGBoost. <lb/>1) Decision Tree <lb/>A decision tree was a graph showing a sequence of actions and different outcomes by using the branching method. <lb/>[12]. Decision trees were effective for both numerical and categorical variables because assumptions were not made <lb/>about data distribution or the structure of classification. Large data sets were efficiently and accurately classified using <lb/>decision trees [12]. <lb/></body>

            <note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>211 <lb/></page>

			<body>2) Naïve Bayes <lb/>Bayes&apos;s classification methods relied on Bayes&apos; theorem, which calculated the probability of an event based on prior <lb/>knowledge of associated conditions. When all of the class probabilities for the aimed feature were calculated, the naïve <lb/>Bayes classifier identified the class with the highest probability. Additionally, naïve Bayes assumed that each class <lb/>value for each feature followed a Gaussian distribution [12]. <lb/>3) KNN <lb/>KNN excelled as a favored method for guided classification in multivariate scenarios, known for its simplicity and <lb/>efficacy [12]. The method operated as a non-parametric algorithm, leveraging training data to construct a model that <lb/>retained the classifier&apos;s memory [10]. Using KNN, unlabeled data was sorted into categories based on the nearest and <lb/>most analogous labeled data points. The parameter K integral to KNN dictated the number of nearest neighbors to <lb/>consider. Typically, K value ranging from 3 to 10 was selected to mitigate overfitting and underfitting [12]. By <lb/>comparing all training image and transferring labels from K most similar prior training examples, the method assigned <lb/>classification to each examined image [12]. <lb/>4) Random Forest <lb/>Random Forest identified an ensemble by combining several hierarchical tree structure predictors. The Random <lb/>Forest was based on the idea that when a single decision tree makes a mistake and is unable to correlate, a collection <lb/>of tree models can outperform it. [32]. This ensemble classifier known as Random Forest comprised numerous random <lb/>decision trees. Each decision tree produced individual classification output, and the values were combined to generate <lb/>final classification outcome [32]. <lb/>5) Support Vector Machine (SVM) <lb/>SVMs were used in both kernel-based and non-linear classification, where input data was implicitly mapped into <lb/>high-dimensional feature spaces. [32]. To classify data, SVM typically built a hyperplane or straight line that divided <lb/>space into two homogeneous zones. The kernel&apos;s role was to determine high-dimensional space to which data should <lb/>be mapped. Following the context, data points had to be linearly separable. By using the delta margin feature, SVM <lb/>ensured that the learning model assigned higher score to the correct class than the incorrect one. As a result, the model <lb/>produced more accurate results [10]. <lb/>6) eXtreme Gradient Boosting (XGBoost) <lb/>XGBoost used a Gradient Boosting Machine (GBM), which combined gradient descent and boosting method [33]. <lb/>Boosting, as an ensemble learning algorithm, adjusted the weight of the training data distribution for each iteration. <lb/>Furthermore, the method increased the weight of misclassified error samples and decreased the weight of correctly <lb/>classified samples in each method iteration, effectively modifying the training data distribution [33]. <lb/>(∅) ∑ 1( , ) + ∑ Ω( ) <lb/>(1) <lb/>ℎ <lb/>Ω( ) = <lb/>+ <lb/>|| || <lb/>(2) <lb/>As a descending tree-based algorithm, GBM found split points that were not trivialized large data sets. <lb/>Consequently, Chen and Guestrin in 2016 developed a new distributed quantile sketching algorithm that handled <lb/>weighted data with provable theoretical guarantees of GBM derivatives, leading to a new scalable and efficient <lb/>algorithm called XGBoost [32]. <lb/>E. Model Evaluation <lb/>The accuracy of the training from each experiment was examined using the training dataset in the final stage. <lb/>Subsequently, confusion matrix was used to calculate the accuracy of the available data. To evaluate the effectiveness <lb/>of accurate classification using 3000 datasets, training and examining data was divided into 80:20 ratio for analysis. <lb/>The four terms in Table 1 showed the result of classification process when a confusion matrix was used to measure <lb/>performance. <lb/>TABLE 1 <lb/>CLASSIFICATION CONFUSION MATRIX <lb/>Multiclass <lb/>Confusion Matrix <lb/>Predicted <lb/>Bare <lb/>Medium <lb/>High <lb/>Actual <lb/>Bare <lb/>TP <lb/>FP <lb/>FN <lb/>Medium <lb/>FP <lb/>TN <lb/>TN <lb/>High <lb/>FP <lb/>TN <lb/>TN <lb/>*TP=true positive; FP=false positive; TN=true negative; FN=false negative <lb/></body>

			<note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

            <page>212 <lb/></page>

			<div type="annex">The terms true positive, true negative, false positive, and false negative were used to calculate accuracy, precision, <lb/>recall, and F1 score. The predictive power of a model was a measure of how well it predicted the future [26]. Based <lb/>on the confusion matrix results, The formulation provided in the equation was used to determine classification model <lb/>prediction&apos;s accuracy, sensitivity, and specificity. <lb/>= <lb/>(3) <lb/>= <lb/>(4) <lb/>= <lb/>(5) <lb/>1 <lb/>= 2 <lb/>(6) <lb/>When the false positive and false negative rates were the same, accuracy worked optimally. Precision and recall <lb/>were considered when there was a significant difference between false positive and false negative rates. Additionally, <lb/>this research found accuracy, precision, recall, and f1 score values. <lb/>IV. RESULTS <lb/>A. Preprocessing <lb/>The acquired image data were 4.92MB resolution of 4000x3000. However, because image from the drone were too <lb/>large, it was cropped and resized to create learning data. Image data was manually cropped to a size of 256x256 pixels <lb/>using Photoshop, reducing the file size to 159KB. The cropped image was later categorized into 3 classes, namely <lb/>bare, medium, and high. Additionally, each pre-trained model was trained with 100 epochs, using the same parameters. <lb/>Fig. 5 showed the result of image data cropping process. After cropping, the three class categories were labeled. There <lb/>were 1000 data in each class labeling to facilitate future use of the method with ease of operation. <lb/>Fig. 5 Image cropping <lb/>B. Model Performance Evaluation <lb/>(a) <lb/>(b) <lb/>Fig. 6 Feature extraction results from (a) ResNet 50 and (b) DenseNet 121 <lb/></div>

            <note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>213 <lb/></page>

			<body>The research included trials on land cover classification based on vegetation density using UAV datasets and Google <lb/>Colabs Pro (online) tools, primarily using Tensorflow and Keras libraries in Python. This research evaluated the <lb/>accuracy of feature extraction using CNN architecture and classification through machine learning methods. The <lb/>results of feature extraction from CNN method, presented as a heatmap were shown in Fig. 6. <lb/>Fig. 6 showed feature extraction using ResNet 50 and DenseNet 121. The left image showed feature extraction from <lb/>ResNet 50 at layer conv 2 block 1, while the right image showed extraction from DenseNet 121 at the same layer. The <lb/>difference between the two was evident when DenseNet 121 produced more textured feature extraction results <lb/>compared to ResNet 50. <lb/>V. DISCUSSION <lb/>The results obtained in this research compared the performance of different CNN architecture in feature extraction. <lb/>ResNet-50 combined with SVM produced an accuracy of 84%, precision of 91%, recall of 77%, and f1 score of 84%. <lb/>Following the context, the performance was a better result compared to other models. The second-best result was <lb/>achieved by DenseNet 121 with SVM and XGBoost, which yielded an accuracy of 81% and the ShuffleNet V2 <lb/>achieved an accuracy of 80% [34]. Meanwhile, other explorations using the same case and similar dataset reported <lb/>the following results. ResNet 18 with an accuracy of 81.90%, MobileNet V2 with 82.30%, Xception with 83.40%, <lb/>InceptionResNet v2 with results of 84.10%, and VGG16 with results of 83.33% [35], [36]. The following results of <lb/>the research were shown in Table 2. <lb/>TABLE 2 <lb/>CLASSIFICATION COMPARISON RESULTS <lb/>Model <lb/>Accuracy Precision Recall F1 Score Time <lb/>Resnet-50 + Decision Tree <lb/>58% <lb/>71% <lb/>53% <lb/>58% <lb/>4mnt 28s <lb/>ResNet-50 + Naïve Bayes <lb/>61% <lb/>84% <lb/>62% <lb/>62% <lb/>2mnt 6s <lb/>ResNet-50 + KNN <lb/>70% <lb/>90% <lb/>70% <lb/>70% <lb/>5mnt 6s <lb/>ResNet-50 + RF <lb/>77% <lb/>92% <lb/>66% <lb/>77% <lb/>5mnt 32s <lb/>ResNet-50 + SVM <lb/>84% <lb/>91% <lb/>77% <lb/>84% <lb/>8mnt 58s <lb/>ResNet-50 + XGBoost <lb/>82% <lb/>93% <lb/>71% <lb/>81% <lb/>1h 32mnt <lb/>DenseNet-121 + Decision Tree <lb/>65% <lb/>76% <lb/>58% <lb/>65% <lb/>2mnt 55s <lb/>DenseNet-121 + Naïve Bayes <lb/>68% <lb/>83% <lb/>66% <lb/>68% <lb/>2mnt 3s <lb/>DenseNet-121 + KNN <lb/>77% <lb/>93% <lb/>65% <lb/>77% <lb/>4mnt 5s <lb/>DenseNet-121 + RF <lb/>78% <lb/>91% <lb/>68% <lb/>78% <lb/>4mnt 32s <lb/>DenseNet-121 + SVM <lb/>81% <lb/>85% <lb/>74% <lb/>81% <lb/>5mnt 19s <lb/>DenseNet-121 + XGBoost <lb/>81% <lb/>90% <lb/>72% <lb/>81% <lb/>59mnt 36s <lb/>Back Propagation Neural Network [8] 85.67% <lb/>-<lb/>-<lb/>-<lb/>-<lb/>ResNet18 [35] <lb/>81.90% <lb/>-<lb/>-<lb/>-<lb/>-<lb/>MobileNet V2 [35] <lb/>82.30% <lb/>-<lb/>-<lb/>-<lb/>-<lb/>Xception [35] <lb/>83.40% <lb/>-<lb/>-<lb/>-<lb/>-<lb/>InceptionResNetV2 [35] <lb/>84.10% <lb/>-<lb/>-<lb/>-<lb/>-<lb/>VGG16 [36] <lb/>83.33% <lb/>-<lb/>-<lb/>-<lb/>-<lb/>Table 2 showed the results for CNN and the combination of it with machine learning methods. CNN architecture <lb/>was used for extraction methods while machine learning was applied for classification. By combining ResNet-50 with <lb/>SVM, the explorers achieved an accuracy of 84%, precision of 91%, recall of 77%, and f1 score was 84%. Following <lb/>this context, the performance outperformed other models. The second-best result, 81%, was achieved by using <lb/>DenseNet 121 with SVM and XGBoost. Regarding the processing time, XGBoost method required significantly more <lb/>time for classification. Graphical representation comparing all models in classifying vegetation density was shown in <lb/>Fig. 7. <lb/>Classification process using CNN combined with machine learning methods still faced several obstacles, <lb/>particularly in encountering misclassifications. These inaccuracies resulted from the challenge of precisely classifying <lb/>image. According to the explorer&apos;s analysis, the issue arose from the characteristics of the land cover dataset. The <lb/>dataset consisted of predominantly green-colored and textured areas, with variations primarily seen in the foliage. <lb/>However, upon initial observation, distinguishing between the medium and high classes of land cover proved difficult <lb/>due to similar color and texture. Misclassification often occurred in these two classes because image was almost <lb/>similar and significantly different from the bare class. A similar dataset and the frequent occurrence of errors in <lb/>classification were shown in Table 3. <lb/></body>

			<note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>214 <lb/></page>

			<body>Fig. 7 Classification results <lb/>TABLE 3 <lb/>RESULTS MISS CLASSIFICATION <lb/>Class <lb/>Medium High <lb/>VI. CONCLUSIONS <lb/>In conclusion, the research discussed the advantages of using feature extraction from CNN architecture. Given this <lb/>scenario, CNN was combined with several machine learning methods to evaluate accuracy. Additionally, the research <lb/>compared the classification performance of CNN architecture, namely ResNet-50 and DenseNet-121. To improve the <lb/>classification accuracy, machine learning methods were used with the architecture for feature extraction. The machine <lb/>learning methods proposed included Decision Tree, Naïve Bayes, KNN, Random Forest, SVM, and XGBoost. Based <lb/>on the analysis, the comparison between machine learning methods showed that SVM was superior to others. This <lb/>superiority was evidenced by 84% accuracy through feature extraction using ResNet-50. The second-best performance <lb/>was observed with XGBoost, achieving 82% accuracy using ResNet 50 feature extraction. Finally, for feature <lb/>extraction with DenseNet-121, the best results were also obtained with SVM and XGBoost, producing an accuracy of <lb/>81%. <lb/></body>

            <div type="contribution">Author Contributions: Erika Maulidiya: Conceptualization, Approach, and writing-first draft, writing-review and <lb/>editing, and writing-supervision. Chastine Fatichah: Software, investigation, data curation, test results, and writing <lb/>(first draft). Nanik Suciati: Investigation, Data Curation, Pilot Testing, and Software Testing are all part of the process. <lb/>Yuslena Sari: Data Provision, Data Collection and Data Processing <lb/>All authors have read and agreed to the published version of the manuscript. <lb/></div>

            <div type="acknowledgement">Acknowledgments: The author gratefully acknowledges financial support for this work from Institut Teknologi <lb/>Sepuluh Nopember (ITS). <lb/></div>

            <div type="availability">Data Availability: The data used is open to the public and can be found on Mendeley. The dataset is available at the <lb/>following address: https://data.mendeley.com/datasets/tb26zy2jst/1. <lb/></div>

            <div type="annex">Institutional Review Board Statement: Not applicable. <lb/>Informed Consent: There were no human subjects. <lb/>Animal Subjects: There were no animal subjects. <lb/></div>

			<body>0 <lb/>0,5 <lb/>1 <lb/>Accuracy <lb/>Precision <lb/>Recall <lb/>F1 Score <lb/>ResNet-50 + Decesion Tree2 <lb/>ResNet-50 + Naïve Bayes <lb/>ResNet-50 + KNN <lb/>ResNet-50 + RF <lb/>ResNet-50 + SVM <lb/>ResNet-50 + XGBoost <lb/>DenseNet-121 + Decision Tree <lb/>DenseNet-121 + Naïve Bayes <lb/>DenseNet-121 + KNN <lb/>DenseNet-121 + RF <lb/>DenseNet-121 + SVM <lb/>DenseNet-121 + XGBoost <lb/></body>

			<note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>215 <lb/></page>

			<front>ORCID: <lb/>Erika Maulidiya: https://orcid.org/0009-0001-8640-6735 <lb/>Chastine Fatichah: https://orcid.org/0000-0002-7348-9762 <lb/>Nanik Suciati: https://orcid.org/0000-0002-1991-0464 <lb/>Yuslena Sari: https://orcid.org/0000-0002-0272-5764 <lb/></front>

			<listBibl>REFERENCES <lb/>[1] M. Warren, K. Hergoualc&apos;h, J. B. Kauffman, D. Murdiyarso, and R. Kolka, &quot;An Appraisal of Indonesia&apos;s Immense Peat Carbon Stock Using <lb/>National Peatland Maps: Uncertainties and Potential Losses from Conversion,&quot; Carbon Balance Manag., vol. 12, no. 1, p. 12, Dec. 2017, doi: <lb/>10.1186/s13021-017-0080-2. <lb/>[2] T. Gumbricht et al., &quot;Tropical and Subtropical Wetlands Distribution version 2 -CIFOR Knowledge,&quot; CIFOR. [Online]. Available: <lb/>https://www.cifor-icraf.org/knowledge/dataset/0058/ <lb/>[3] G. Hope, U. Chokkalingam, and S. Anwar, &quot;The Stratigraphy and Fire History of the Kutai Peatlands, Kalimantan, Indonesia,&quot; Quat. Res., <lb/>vol. 64, no. 3, pp. 407-417, 2005, doi: 10.1016/j.yqres.2005.08.009. <lb/>[4] L. Tacconi, &quot;Preventing fires and haze in Southeast Asia,&quot; Nat. Clim. Chang., vol. 6, no. 7, pp. 640-643, 2016, doi: 10.1038/nclimate3008. <lb/>[5] R. Kovats, M. Bouma, and a. Haines, &quot;El Niño and Health Protection of the Human Environment,&quot; World Heal. Organ., p. 54, 1999. <lb/>[6] N. Novitasari, J. Sujono, S. Harto, A. Maas, and R. Jayadi, &quot;Drought index for peatland wildfire management in central kalimantan, indonesia <lb/>during el niño phenomenon,&quot; J. Disaster Res., vol. 14, no. 7, pp. 939-948, 2019, doi: 10.20965/jdr.2019.p0939. <lb/>[7] Z. Xu, K. Guan, N. Casler, B. Peng, and S. Wang, &quot;A 3D Convolutional Neural Network Method for Land Cover Classification Using LiDAR <lb/>and Multi-Temporal Landsat Imagery,&quot; ISPRS J. Photogramm. Remote Sens., vol. 144, pp. 423-434, 2018, doi: <lb/>10.1016/j.isprsjprs.2018.08.005. <lb/>[8] Y. Sari, Y. F. Arifin, N. Novitasari, and M. R. Faisal, &quot;Effect of Feature Engineering Technique for Determining Vegetation Density,&quot; Int. J. <lb/>Adv. Comput. Sci. Appl., vol. 13, no. 7, pp. 655-661, 2022, doi: 10.14569/IJACSA.2022.0130776. <lb/>[9] B. Bansod, R. Singh, R. Thakur, and G. Singhal, &quot;A comparision between satellite based and drone based remote sensing technology to <lb/>achieve sustainable development: A review,&quot; J. Agric. Environ. Int. Dev., vol. 111, no. 2, pp. 383-407, 2017, doi: 10.12895/jaeid.20172.690. <lb/>[10] M. Jogin, Mohana, M. S. Madhulika, G. D. Divya, R. K. Meghana, and S. Apoorva, &quot;Feature Extraction Using Convolution Neural Networks <lb/>(CNN) and Deep Learning,&quot; 2018 3rd IEEE Int. Conf. Recent Trends Electron. Inf. Commun. Technol. RTEICT 2018 -Proc., no. November, <lb/>pp. 2319-2323, 2018, doi: 10.1109/RTEICT42901.2018.9012507. <lb/>[11] Y. Heryadi and E. Miranda, Land Cover Classification Based on Sentinel-2 Satellite Imagery Using Convolutional Neural Network Model: <lb/>A Case Study in Semarang Area, Indonesia, vol. 830, no. January. Springer International Publishing, 2020. doi: 10.1007/978-3-030-14132-<lb/>5_15. <lb/>[12] S. Benyahia, B. Meftah, and O. Lézoray, &quot;Multi-features extraction based on deep learning for skin lesion classification,&quot; Tissue Cell, vol. <lb/>74, p. 101701, Feb. 2022, doi: 10.1016/j.tice.2021.101701. <lb/>[13] P. Simon and V. Uma, &quot;Deep Learning based Feature Extraction for Texture Classification,&quot; Procedia Comput. Sci., vol. 171, no. 2019, pp. <lb/>1680-1687, 2020, doi: 10.1016/j.procs.2020.04.180. <lb/>[14] S. Athisayamani, R. S. Antonyswamy, V. Sarveshwaran, M. Almeshari, Y. Alzamil, and V. Ravi, &quot;Feature Extraction Using a Residual Deep <lb/>Convolutional Neural Network (ResNet-152) and Optimized Feature Dimension Reduction for MRI Brain Tumor Classification,&quot; <lb/>Diagnostics, vol. 13, no. 4, p. 668, Feb. 2023, doi: 10.3390/diagnostics13040668. <lb/>[15] T. Kattenborn, J. Leitloff, F. Schiefer, and S. Hinz, &quot;Review on Convolutional Neural Networks (CNN) in Vegetation Remote Sensing,&quot; <lb/>ISPRS J. Photogramm. Remote Sens., vol. 173, no. March, pp. 24-49, 2021, doi: 10.1016/j.isprsjprs.2020.12.010. <lb/>[16] A. Przybyś-Małaczek, I. Antoniuk, K. Szymanowski, M. Kruk, and J. Kurek, &quot;Application of Machine Learning Algorithms for Tool <lb/>Condition Monitoring in Milling Chipboard Process,&quot; Sensors, vol. 23, no. 13, p. 5850, Jun. 2023, doi: 10.3390/s23135850. <lb/>[17] P. Thanh Noi and M. Kappas, &quot;Comparison of Random Forest, k-Nearest Neighbor, and Support Vector Machine Classifiers for Land Cover <lb/>Classification Using Sentinel-2 Imagery,&quot; Sensors, vol. 18, no. 1, p. 18, Dec. 2017, doi: 10.3390/s18010018. <lb/>[18] E. Maulidiya, C. Fatichah, N. Suciati, and F. Baskoro, &quot;Klasifikasi Tutupan Lahan UAV Menggunakan Convolutional Neural Network <lb/>Feature Map dengan Kombinasi Machine Learning,&quot; J. Ilm. Teknol. Inf., vol. 22, no. 1, pp. 45-55, 2024, doi: <lb/>http://dx.doi.org/10.12962/j24068535.v22i1.a1214. <lb/>[19] R. Mahmoud, M. Hassanin, H. Al Feel, and R. M. Badry, &quot;Machine Learning-Based Land Use and Land Cover Mapping Using Multi-Spectral <lb/>Satellite Imagery: A Case Study in Egypt,&quot; Sustain., vol. 15, no. 12, pp. 1-21, 2023, doi: 10.3390/su15129467. <lb/>[20] T. K. Behera, S. Bakshi, and P. K. Sa, &quot;Vegetation Extraction from UAV-based Aerial Images through Deep Learning,&quot; Comput. Electron. <lb/>Agric., vol. 198, p. 107094, Jul. 2022, doi: 10.1016/j.compag.2022.107094. <lb/>[21] P. Benz, C. Zhang, D. M. Argaw, S. Lee, and J. Kim, &quot;Introducing Dense Shortcuts to ResNet,&quot; Korea Advanced Institute of Science and <lb/>Technology, vol. 388. pp. 539-547, 2020. <lb/>[22] C. Sothe et al., &quot;Evaluating a Convolutional Neural Network for Feature Extraction and Tree Species Classification Using UAV-Hyperspectral <lb/>Images,&quot; ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci., vol. 5, no. 3, pp. 193-199, 2020, doi: 10.5194/isprs-Annals-V-3-2020-193-<lb/>2020. <lb/>[23] G. Cecili, P. De Fioravante, P. Dichicco, L. Congedo, M. Marchetti, and M. Munafò, &quot;Land Cover Mapping with Convolutional Neural <lb/>Networks Using Sentinel-2 Images: Case Study of Rome,&quot; Land, vol. 12, no. 4, p. 879, Apr. 2023, doi: 10.3390/land12040879. <lb/></listBibl>

            <note place="headnote">Maulidiya, Fatichah, Suciati, &amp; Sari <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 206-216 <lb/></note>

			<page>216 <lb/></page>

			<listBibl>[24] D. A. Arrechea-Castillo, Y. T. Solano-Correa, J. F. Muñoz-Ordóñez, E. L. Pencue-Fierro, and A. Figueroa-Casas, &quot;Multiclass Land Use and <lb/>Land Cover Classification of Andean Sub-Basins in Colombia with Sentinel-2 and Deep Learning,&quot; Remote Sens., vol. 15, no. 10, pp. 1-20, <lb/>2023, doi: 10.3390/rs15102521. <lb/>[25] S. Supangat, M. Z. Bin Saringat, and M. Y. F. Rochman, &quot;Predicting Handling Covid-19 Opinion using Naive Bayes and TF-IDF for Polarity <lb/>Detection,&quot; MATRIK J. Manajemen, Tek. Inform. dan Rekayasa Komput., vol. 22, no. 2, pp. 173-184, 2023, doi: 10.30812/matrik.v22i2.2227. <lb/>[26] A. N. A. Thohari, L. Triyono, I. Hestiningsih, B. Suyanto, and A. Yobioktobera, &quot;Performance Evaluation of Pre-Trained Convolutional <lb/>Neural Network Model for Skin Disease Classification,&quot; JUITA J. Inform., vol. 10, no. 1, p. 9, 2022, doi: 10.30595/juita.v10i1.12041. <lb/>[27] Y. Xu, W. Yang, X. Wu, Y. Wang, and J. Zhang, &quot;ResNet Model Automatically Extracts and Identifies FT-NIR Features for Geographical <lb/>Traceability of Polygonatum kingianum,&quot; Foods, vol. 11, no. 22, p. 3568, Nov. 2022, doi: 10.3390/foods11223568. <lb/>[28] J. H. Jonnadula Harikiran, D. B. S. Rao, D. R. B, and D. S. B, &quot;An Effective Ensemble Deep Learning Approach for COVID-19 Detection <lb/>Using InceptionV3 and Optimized Squeeze Net,&quot; SSRN Electron. J., 2022, doi: 10.2139/ssrn.4192619. <lb/>[29] M. A. Al-Malla, A. Jafar, and N. Ghneim, &quot;Pre-trained CNNs as Feature-Extraction Modules for Image Captioning: An Experimental Study,&quot; <lb/>Electron. Lett. Comput. Vis. Image Anal., vol. 21, no. 1, pp. 1-16, 2022, doi: 10.5565/rev/elcvia.1436. <lb/>[30] K. E. E. Rani and S. Baulkani, &quot;Construction of Deep Learning Model using RESNET 50 for Schizophrenia Prediction from rsFMRI Images,&quot; <lb/>2022. doi: doi.org/10.21203/rs.3.rs-2106170/v1. <lb/>[31] F. Salim, F. Saeed, S. Basurra, S. N. Qasem, and T. Al-Hadhrami, &quot;DenseNet-201 and Xception Pre-Trained Deep Learning Models for Fruit <lb/>Recognition,&quot; Electronics, vol. 12, no. 14, p. 3132, Jul. 2023, doi: 10.3390/electronics12143132. <lb/>[32] M. Hassaballah, Y. M. Wazery, I. E. Ibrahim, and A. Farag, &quot;ECG Heartbeat Classification Using Machine Learning and Metaheuristic <lb/>Optimization for Smart Healthcare Systems,&quot; Bioengineering, vol. 10, no. 4, pp. 1-16, 2023, doi: 10.3390/bioengineering10040429. <lb/>[33] I. Hanif, &quot;Implementing Extreme Gradient Boosting (XGBoost) Classifier to Improve Customer Churn Prediction,&quot; in Proceedings of the <lb/>Proceedings of the 1st International Conference on Statistics and Analytics, ICSA 2019, 2-3 August 2019, Bogor, Indonesia, EAI, 2020. doi: <lb/>10.4108/eai.2-8-2019.2290338. <lb/>[34] N. Novitasari, Y. Sari, Y. F. Arifin, N. F. Mustamin, and E. Maulidiya, &quot;Use of UAV images for Peatland Cover Classification Using the <lb/>Convolutional Neural Network Method,&quot; J. Southwest Jiaotong Univ., vol. 58, no. 3, 2023, doi: 10.35741/issn.0258-2724.58.3.63. <lb/>[35] Y. Sari, Y. Arifin, Novitasari, and M. Faisal, &quot;Implementation of Deep Learning Based Semantic Segmentation Method To Determine <lb/>Vegetation Density,&quot; Eastern-European J. Enterp. Technol., vol. 5, no. 2-119, pp. 42-54, 2022, doi: 10.15587/1729-4061.2022.265807. <lb/>[36] Y. Sari, A. R. Baskara, F. Pratama, and M. Faidhorrahman, &quot;Penerapan Arsitektur VGG untuk Klasifikasi Hutan,&quot; J. Teknol. Inf. Univ. <lb/>Lambung Mangkurat, vol. 6, no. 2, pp. 85-92, Oct. 2021, doi: 10.20527/jtiulm.v6i2.99. <lb/></listBibl>

			<div type="annex">Publisher&apos;s Note: Publisher stays neutral with regard to jurisdictional claims in published maps and institutional <lb/>affiliations. </div>


	</text>
</tei>
