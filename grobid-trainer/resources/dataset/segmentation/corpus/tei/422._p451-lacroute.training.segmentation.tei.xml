<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>COMPUTER GRAPHICS Proceedings, Annual Conference Series, 1994 <lb/>9 4 <lb/>Fast Volume Rendering Using a Shear-Warp Factorization <lb/>of the Viewing Transformation <lb/>Philippe Lacroute <lb/>Computer Systems Laboratory <lb/>Stanford University <lb/>Marc Levoy <lb/>Computer Science Department <lb/>Stanford University <lb/>/&quot; <lb/>Abstract <lb/>Several existing volume rendering algorithms operate by factor-<lb/>ing the viewing transformation into a 3D shear parallel to the data <lb/>slices, a projection to form an intermediate but distorted image, <lb/>and a 2D warp to form an undistorted final image. We extend <lb/>this class of algorithms in three ways. First, we describe a new <lb/>object-order rendering algorithm based on the factorization that is <lb/>significantly faster than published algorithms with minimal loss <lb/>of image quality. Shear-warp factorizations have the property that <lb/>rows of voxels in the volume are aligned with rows of pixels in the <lb/>intermediate image. We use this fact to construct a scanline-based <lb/>algorithm that traverses the volume and the intermediate image in <lb/>synchrony, taking advantage of the spatial coherence present in <lb/>both. We use spatial data structures based on run-length encoding <lb/>for both the volume and the intermediate image. Our implemen-<lb/>tation running on an SGI Indigo workstation renders a 2563 voxel <lb/>medical data set in one second. Our second extension is a shear-<lb/>warp factorization for perspective viewing transformations, and <lb/>we show how our rendering algorithm can support this extension. <lb/>Third, we introduce a data structure for encoding spatial coherence <lb/>in unclassified volumes (i.e. scalar fields with no precomputed <lb/>opacity). When combined with our shear-warp rendering algo-<lb/>rithm this data structure allows us to classify and render a 2563 <lb/>voxel volume in three seconds. The method extends to support <lb/>mixed volumes and geometry and is parallelizable. <lb/>CR Categories: 1.3.7 [Computer Graphics]: Three-Dimensional <lb/>Graphics and Realism; 1.3.3 [Computer Graphics]: Picture/Image <lb/>Generation--Display Algorithms. <lb/>Additional Keywords: Volume rendering, Coherence, Scientific <lb/>visualization, Medical imaging. <lb/></front>

			<body>1 Introduction <lb/>Volume rendering is a flexible technique for visualizing scalar <lb/>fields with widespread applicability in medical imaging and sci-<lb/>entific visualization, but its use has been limited because it is <lb/></body>

			<front>Authors&apos; Address: Center for Integrated Systems, Stanford University, <lb/>Stanford, CA 94305-4070 <lb/>E-mail: lacroute@weevil.stanford.edu, levoy@cs.stanford.edu <lb/>Word Wide Web: http://www-graphics.stanford.edu/ <lb/>Permission to copy without fee all or part of this material is granted <lb/>provided that the copies are not made or distributed for direct <lb/>commercial advantage, the ACM copyright notice and the title of the <lb/>publication and its date appear, and notice is given that copying is by <lb/>permission of the Association for Computing Machinery. To copy <lb/>otherwise, or to republish, requires a fee and/or specific permission. <lb/></front>

			<body>computationally expensive. Interactive rendering rates have been <lb/>reported using large parallel processors [17] [19] and using algo-<lb/>rithms that trade off image quality for speed [10] [8], but high-<lb/>quality images take tens of seconds or minutes to generate on <lb/>current workstations. In this paper we present a new algorithm <lb/>which achieves near-interactive rendering rates on a workstation <lb/>without significantly sacrificing quality. <lb/>Many researchers have proposed methods that reduce render-<lb/>ing cost without affecting image quality by exploiting coherence <lb/>in the data set. These methods rely on spatial data structures that <lb/>encode the presence or absence of high-opacity voxels so that <lb/>computation can be omitted in transparent regions of the volume. <lb/>These data structures are built during a preprocessing step from a <lb/>classified volume: a volume to which an opacity transfer function <lb/>has been applied. Such spatial data structures include octrees and <lb/>pyramids [13] [12] [8] [3], k-d trees [18] and distance transforms <lb/>[23]. Although this type of optimization is data-dependent, re-<lb/>searchers have reported that in typical classified volumes 70-95% <lb/>of the voxels are transparent [12] [18]. <lb/>Algorithms that use spatial data structures can be divided into <lb/>two categories according to the order in which the data structures <lb/>are traversed: image-order or object-order. Image-order algo-<lb/>rithms operate by casting rays from each image pixel and pro-<lb/>cessing the voxels along each ray [9]. This processing order has <lb/>the disadvantage that the spatial data structure must be traversed <lb/>once for every ray, resulting in redundant computation (e.g. mul-<lb/>tiple descents of an octree). In contrast, object-order algorithms <lb/>operate by splatting voxels into the image while streaming through <lb/>the volume data in storage order [20] [8]. However, this process-<lb/>ing order makes it difficult to implement early ray termination, an <lb/>effective optimization in ray-casting algorithms [12]. <lb/>In this paper we describe a new algorithm which combines <lb/>the advantages of image-order and object-order algorithms. The <lb/>method is based on a factorization of the viewing matrix into a 3D <lb/>shear parallel to the slices of the volume data, a projection to form <lb/>a distorted intermediate image, and a 2D warp to produce the final <lb/>image. Shear-warp factorizations are not new. They have been <lb/>used to simplify data communication patterns in volume rendering <lb/>algorithms for SIMD parallel processors [1] [17] and to simplify <lb/>the generation of paths through a volume in a serial image-order <lb/>algorithm [22]. The advantage of shear-warp factorizations is that <lb/>scanlines of tlae volume data and scanlines of the intermediate im-<lb/>age are always aligned. In previous efforts this property has been <lb/>used to develop SIMD volume rendering algorithms. We exploit <lb/>the property for a different reason: it allows efficient, synchro-<lb/>nized access to data structures that separately encode coherence <lb/>in the volume and the image. <lb/>The factorization also makes efficient, high-quality resampling <lb/>possible in an object-order algorithm. In our algorithm the re-<lb/></body>

			<front>© 1994 <lb/>ACM-0-89791-667-0/94/007/0451 <lb/>$01,50 <lb/></front>

			<page>451 <lb/></page>

			<note place="headnote">SIGGRAPH 94, Orlando,Florida, July 24-29, 1994 <lb/></note>

			<body>viewing rays <lb/>shear <lb/>volume <lb/>slices <lb/>roject <lb/>ima <lb/>plane <lb/>varp <lb/>Figure 1: A volume is transformed to sheared object space for <lb/>a parallel projection by translating each slice. The projection in <lb/>sheared object space is simple and efficient. <lb/>viewing rays <lb/>)lume <lb/>ices <lb/>shear and scale <lb/>~t <lb/>~roject <lb/>warp <lb/>ir <lb/>lter of <lb/>P <lb/>projection <lb/>Figure 2: A volume is transformed to sheared object space for a <lb/>perspective projection by translating and scaling each slice. The <lb/>projection in sheared object space is again simple and efficient. <lb/>sampling filter footprint is not view dependent, so the resampling <lb/>complications of splatting algorithms [20] are avoided. Several <lb/>other algorithms also use multipass resampling [4] [7] [19], but <lb/>these methods require three or more resampling steps. Our al-<lb/>gorithm requires only two resampling steps for an arbitrary per-<lb/>spective viewing transformation, and the second resampling is an <lb/>inexpensive 2D warp. The 3D volume is traversed only once. <lb/>Our implementation running on an SGI Indigo workstation can <lb/>render a 2563 voxel medical data set in one second, a factor of at <lb/>least five faster than previous algorithms running on comparable <lb/>hardware. Other than a slight loss due to the two-pass resampling, <lb/>our algorithm does not trade off quality for speed. This is in <lb/>contrast to algorithms that subsample the data set and can therefore <lb/>miss small features [10] [3]. <lb/>Section 2 of this paper describes the shear-warp factoriza-<lb/>tion and its important mathematical properties. We also describe <lb/>a new extension of the factorization for perspective projections. <lb/>Section 3 describes three variants of our volume rendering algo-<lb/>rithm. The first algorithm renders classified volumes with a paral-<lb/>lel projection using our new coherence optimizations. The second <lb/>algorithm supports perspective projections. The third algorithm is <lb/>a fast classification algorithm for rendering unclassified volumes. <lb/>Previous algorithms that employ spatial data structures require an <lb/>expensive preprocessing step when the opacity transfer function <lb/>changes. Our third algorithm uses a classification-independent <lb/>rain-max octree data structure to avoid this step. Section 4 con-<lb/>tains our performance results and a discussion of image quality. <lb/>Finally we conclude and discuss some extensions to the algorithm <lb/>in Section 5. <lb/>2 The Shear-Warp Factorization <lb/>The arbitrary nature of the transformation from object space to <lb/>image space complicates efficient, high-quality filtering and pro-<lb/>jection in object-order volume rendering algorithms. This problem <lb/>can be solved by transforming the volume to an intermediate co-<lb/>ordinate system for which there is a very simple mapping from the <lb/>object coordinate system and which allows efficient projection. <lb/>We call the intermediate coordinate system &quot;sheared object <lb/>space&quot; and define it as follows: <lb/>Definition 1: By construction, in sheared object space <lb/>all viewing rays are parallel to the third coordinate <lb/>axis. <lb/>Figure 1 illustrates the transformation from object space to sheared <lb/>object space for a parallel projection. We assume the volume is <lb/>sampled on a rectilinear grid. The horizontal lines in the figure <lb/>represent slices of the volume data viewed in cross-section. After <lb/>transformation the volume data has been sheared parallel to the set <lb/>of slices that is most perpendicular to the viewing direction and <lb/>the viewing rays are perpendicular to the slices. For a perspective <lb/>transformation the definition implies that each slice must be scaled <lb/>as well as sheared as shown schematically in Figure 2. <lb/>Definition 1 can be formalized as a set of equations that trans-<lb/>form object coordinates into sheared object coordinates. These <lb/>equations can be written as a factorization of the view transfor-<lb/>marion matrix Mview as follows: <lb/>Mview = P &apos; S -Mwaw <lb/>P is a permutation matrix which transposes the coordinate system <lb/>in order to make the z-axis the principal viewing axis. S trans-<lb/>forms the volume into sheared object space, and M w ~ transforms <lb/>sheared object coordinates into image coordinates. Cameron and <lb/>Undrill [1] and SchrSder and Stoll [17] describe this factorization <lb/>for the case of rotation matrices. For a general parallel projection <lb/>S has the form of a shear perpendicular to the z-axis: <lb/>(1000) <lb/>Spar = <lb/>0 <lb/>1 0 0 <lb/>Sx sv 1 0 <lb/>0 <lb/>0 0 1 <lb/>where s= and s u can be computed from the elements of Mview. <lb/>For perspective projections the transformation to sheared object <lb/>space is of the form: <lb/>1 0 0 0 / <lb/>Sper~p <lb/>0 <lb/>1 0 0 <lb/>-~ <lb/>l <lb/>i <lb/>1 <lb/>i <lb/>&apos;-qx 8y <lb/>s w <lb/>0 <lb/>0 0 <lb/>1 <lb/>This matrix specifies that to transform a particular slice z0 of <lb/>voxel data from object space to sheared object space the slice <lb/>must be translated by (zos~, zos~) and then scaled uniformly by <lb/>1/(1 + z0s~). The final term of the factorization is a matrix <lb/>which warps sheared object space into image space: <lb/>Mwarp = S-1 . p -1 . Mview <lb/>A simple volume rendering algorithm based on the shear-warp <lb/>factorization operates as follows (see Figure 3): <lb/>1. Transform the volume data to sheared object space by trans-<lb/>lating and resampling each slice according to S. For per-<lb/>spective transformations, also scale each slice. P specifies <lb/>which of the three possible slicing directions to use. <lb/>2. Composite the resampled slices together in front-to-back <lb/>order using the &quot;over&quot; operator [15]. This step projects <lb/>the volume into a 2D intermediate image in sheared object <lb/>space. <lb/></body>

			<page>452 <lb/></page>

			<note place="headnote">COMPUTER GRAPHICS Proceedings, Annual Conference Series, 1994 <lb/></note>

			<body>1. shear &amp; ~ <lb/>resample/ r4.... ~ <lb/>L / <lb/>intermediate <lb/>voxel <lb/>l~~llJ¢~ql[/imagescanline <lb/>scanline ~ <lb/>~&quot;&quot;&quot;&quot;&apos;,,,,,,,,,,,,~....~ I 3. warp &amp; <lb/>voxel slice <lb/>intermediate image <lb/>final image <lb/>Figure 3: The shear-warp algorithm includes three conceptual <lb/>steps: shear and resample the volume slices, project resampled <lb/>voxel scanlines onto intermediate image scanlines, and warp the <lb/>intermediate image into the final image. <lb/>3. Transform the intermediate image to image space by warp-<lb/>ing it according to Mw~. This second resampling step <lb/>produces the correct final image. <lb/>The parallel-projection version of this algorithm was first de-<lb/>scribed by Cameron and Undrill [l]. Our new optimizations are <lb/>described in the next section. <lb/>The projection in sheared object space has several geometric <lb/>properties that simplify the compositing step of the algorithm: <lb/>Property 1: Scanlines of pixels in the intermediate <lb/>image are parallel to scanlines of voxels in the volume <lb/>data. <lb/>Property 2: All voxels in a given voxel slice are <lb/>scaled by the same factor. <lb/>Property 3 (parallel projections only): Every voxel <lb/>slice has the same scale factor, and this factor can <lb/>be chosen arbitrarily. In particular, we can choose a <lb/>unity scale factor so that for a given voxel scanline <lb/>there is a one-to-one mapping between voxels and <lb/>intermediate-image pixels. <lb/>In the next section we make use of these properties. <lb/>3 Shear-Warp Algorithms <lb/>We have developed three volume rendering algorithms based on <lb/>the shear-warp factorization. The first algorithm is optimized for <lb/>parallel projections and assumes that the opacity transfer function <lb/>does not change between renderings, but the viewing and shad-<lb/>ing parameters can be modified. The second algorithm supports <lb/>perspective projections. The third algorithm allows the opacity <lb/>transfer function to be modified as well as the viewing and shad-<lb/>ing parameters, with a moderate performance penalty. <lb/>3.1 Parallel Projection Rendering Algorithm <lb/>Property 1 of the previous section states that voxel scanlines in the <lb/>sheared volume are aligned with pixel scanlines in the intermediate <lb/>image, which means that the volume and image data structures can <lb/>II opaque <lb/>pixel <lb/>D <lb/>non-opaque <lb/>pixel <lb/>Figure 4: Offsets stored with opaque pixels in the intermediate <lb/>image allow occluded voxels to be skipped efficiently. <lb/>both be traversed in scanline order. Scanline-based coherence data <lb/>structures are therefore a natural choice. The first data structure we <lb/>use is a run-length encoding of the voxel scanlines which allows us <lb/>to take advantage of coherence in the volume by skipping runs of <lb/>transparent voxels. The encoded scanlines consist of two types of <lb/>runs, transparent and non-transparent, defined by a user-specified <lb/>opacity threshold. Next, to take advantage of coherence in the <lb/>image, we store with each opaque intermediate image pixel an <lb/>offset to the next non-opaque pixel in the same scanline (Figure 4). <lb/>An image pixel is defined to be opaque when its opacity exceeds <lb/>a user-specified threshold, in which case the corresponding voxels <lb/>in yet-to-be-processed slices are occluded. The offsets associated <lb/>with the image pixels are used to skip runs of opaque pixels <lb/>without examining every pixel. The pixel array and the offsets <lb/>form a run-length encoding of the intermediate image which is <lb/>computed on-the-fly during rendering. <lb/>These two data structures and Property 1 lead to a fast scanline-<lb/>based rendering algorithm (Figure 5). By marching through the <lb/>volume and the image simultaneously in scanline order we reduce <lb/>addressing arithmetic. By using the run-length encoding of the <lb/>voxel data to skip voxels which are transparent and the run-length <lb/>encoding of the image to skip voxels which are occluded, we per-<lb/>form work only for voxels which are both non-transparent and <lb/>visible. <lb/>For voxel runs that are not skipped we use a tightly-coded <lb/>loop that performs shading, resampling and compositing. Prop-<lb/>erties 2 and 3 allow us to simplify the resampling step in this <lb/>loop. Since the transformation applied to each slice of volume <lb/>data before projection consists only of a translation (no scaling or <lb/>rotation), the resampling weights are the same for every voxel in <lb/>a slice (Figure 6). Algorithms which do not use the shear-warp <lb/>factorization must recompute new weights for every voxel. We <lb/>use a bilinear interpolation filter and a gather-type convolution <lb/>(backward projection): two voxel scanlines are traversed simulta-<lb/>neously to compute a single intermediate image scanline at a time. <lb/>Scatter-type convolution (forward projection) is also possible. We <lb/>use a lookup-table based system for shading [6]. We also use a <lb/>lookup table to correct voxel opacity for the current viewing angle <lb/>voxel scanline: ! <lb/>[ <lb/>] <lb/>[ <lb/>I <lb/>I <lb/>&quot; <lb/>| resample and <lb/>&apos; <lb/>I&apos; composite <lb/>image !ntermediate : <lb/>i <lb/>! <lb/>: <lb/>scanline: <lb/>~ <lb/>~! <lb/>~! <lb/>=! ~: <lb/>=.. <lb/>skip i work ! skip <lb/>i work I skip <lb/>[] transparent voxel run <lb/>• opaque image pixel run <lb/>[] non-transparent voxel run <lb/>[] non-opaque image pixel run <lb/>Figure 5: Resampling and compositing are performed by stream-<lb/>ing through both the voxels and the intermediate image in scanline <lb/>order, skipping over voxels which are transparent and pixels which <lb/>are opaque. <lb/></body>

			<page>453 <lb/></page>

			<note place="headnote">SlGGRAPH 94, Orlando, Florida, July 24-29, 1994 <lb/></note>

			<body>• original voxel <lb/>© resampled voxel <lb/>Figure 6: Since each slice of the volume is only translated, every <lb/>voxel in the slice has the same resampling weights. <lb/>since the apparent thickness of a slice of voxels depends on the <lb/>viewing angle with respect to the orientation of the slice. <lb/>The opaque pixel links achieve the same effect as early ray <lb/>termination in ray-casting algorithms [12]. However, the effec-<lb/>tiveness of this optimization depends on coherence of the opaque <lb/>regions of the image. The runs of opaque pixels are typically <lb/>large so that many pixels can be skipped at once, minimizing the <lb/>number of pixels that are examined. The cost of computing the <lb/>pixel offsets is low because a pixel&apos;s offset is updated only when <lb/>the pixel first becomes opaque. <lb/>After the volume has been composited the intermediate image <lb/>must be warped into the final image. Since the 2D image is small <lb/>compared to the size of the volume this part of the computation <lb/>is relatively inexpensive. We use a general-purpose affine image <lb/>warper with a bilinear filter. <lb/>The rendering algorithm described in this section requires a <lb/>run-length encoded volume which must be constructed in a pre-<lb/>processing step, but the data structure is view-independent so the <lb/>cost to compute it can be amortized over many renderings. Three <lb/>encodings are computed, one for each possible principal viewing <lb/>direction, so that transposing the volume is never necessary. Dur-<lb/>ing rendering one of the three encodings is chosen depending upon <lb/>the value of the permutation matrix P in the shear-warp factoriza-<lb/>tion. Transparent voxels are not stored, so even with three-fold <lb/>redundancy the encoded volume is typically much smaller than <lb/>the original volume (see Section 4.1). Fast computation of the <lb/>run-length encoded data structure is discussed further at the end <lb/>of Section 3.3. <lb/>In this section we have shown how the shear-warp factoriza-<lb/>tion allows us to combine optimizations based on object coherence <lb/>and image coherence with very low overhead and simple, high-<lb/>quality resampling. In the next section we extend these advantages <lb/>to a perspective volume rendering algorithm. <lb/>3.2 Perspective Projection Rendering Algorithm <lb/>Most of the work in volume rendering has focused on parallel pro-<lb/>jections. However, perspective projections provide additional cues <lb/>for resolving depth ambiguities [14] and are essential to correctly <lb/>compute occlusions in such applications as a beam&apos;s eye view <lb/>for radiation treatment planning. Perspective projections present <lb/>a problem because the viewing rays diverge so it is difficult to <lb/>sample the volume uniformly. Two types of solutions have been <lb/>proposed for perspective volume rendering using ray-casters: as <lb/>the distance along a ray increases the ray can be split into multi-<lb/>ple rays [14], or each sample point can sample a larger portion of <lb/>the volume using a mip-map [11] [16]. The object-order splatting <lb/>algorithm can also handle perspective, but the resampling filter <lb/>footprint must be recomputed for every voxel [20]. <lb/>The shear-warp factorization provides a simple and efficient <lb/>solution to the sampling problem for perspective projections. Each <lb/>slice of the volume is transformed to sheared object space by a <lb/>translation and a uniform scale, and the slices are then resampled <lb/>and composited together. These steps are equivalent to a ray-<lb/>casting algorithm in which rays are cast to uniformly sample the <lb/>first slice of volume data, and as each ray hits subsequent (more <lb/>distant) slices a larger portion of the slice is sampled (Figure 2). <lb/>The key point is that within each slice the sampling rate is uniform <lb/>(Property 2 of the factofization), so there is no need to implement <lb/>a complicated multirate filter. <lb/>The perspective algorithm is nearly identical to the parallel <lb/>projection algorithm. The only difference is that each voxel must <lb/>be scaled as well as translated during resampling, so more than <lb/>two voxel scanlines may be traversed simultaneously to produce a <lb/>given intermediate image scanline and the voxel scanlines may not <lb/>be traversed at the same rate as the image scanlines. We always <lb/>choose a factorization of the viewing transformation in which the <lb/>slice closest to the viewer is scaled by a factor of one so that no <lb/>slice is ever! enlarged. To resample we use a box reconstruction <lb/>filter and a box low-pass filter, an appropriate combination for <lb/>both decimation and unity scaling. In the case of unity scaling <lb/>the two filter widths are identical and their convolution reduces <lb/>to the bilinear interpolation filter used in the parallel projection <lb/>algorithm. <lb/>The perspective algorithm is more expensive than the parallel <lb/>projection algorithm because extra time is required to compute <lb/>resampling weights and because the many-to-one mapping from <lb/>voxels to pixels complicates the flow of control. Nevertheless, the <lb/>algorithm is efficient because of the properties of the shear-warp <lb/>factorization: the volume and the intermediate image are both <lb/>traversed scanline by scanline, and resampling is accomplished via <lb/>two simple resampling steps despite the diverging ray problem. <lb/>3.3 Fast Classification Algorithm <lb/>The previous two algorithms require a preprocessing step to run-<lb/>length encode the volume based on the opacity transfer function. <lb/>The preprocessing time is insignificant if the user wishes to gen-<lb/>erate many images from a single classified volume, but if the user <lb/>wishes to experiment interactively with the transfer function then <lb/>the preprocessing step is unacceptably slow. In this section we <lb/>present a third variation of the shear-warp algorithm that eval-<lb/>uates the opacity transfer function during rendering and is only <lb/>moderately slower than the previous algorithms. <lb/>A run-length encoding of the volume based upon opacity is <lb/>not an appropriate data structure when the opacity transfer func-<lb/>tion is not fixed. Instead we apply the algorithms described in <lb/>Sections 3.1-3.2 to unencoded voxel scanlines, but with a new <lb/>method to determine which portions of each scanline are non-<lb/>transparent. We allow the opacity transfer function to be any <lb/>scalar function of a multi-dimensional scalar domain: <lb/>= f(p, q .... ) <lb/>For example, the opacity might be a function of the scalar field <lb/>and its gradient magnitude [9]: <lb/>o~ = f(d, 1~7dl) <lb/>The function f essentially partitions a multi-dimensional feature <lb/>space into transparent and non-transparent regions, and our goal <lb/>is to decide quickly which portions of a given scanline contain <lb/>voxels in the non-transparent regions of the feature space. <lb/>We solve this problem with the following recursive algorithm <lb/>which takes advantage of coherence in both the opacity transfer <lb/>function and the volume data: <lb/>Step 1: For some block of the volume that contains the current <lb/>scanline, find the extrema of the parameters of the opac-<lb/>ity transfer function (min(p),max(p),min(q), max(q) .... ). <lb/>These extrema bound a rectangular region of the feature <lb/>space. <lb/>Step 2: Determine if the region is transparent, i.e. f evaluated for <lb/>all parameter points in the region yields only transparent <lb/></body>

			<page>454 <lb/></page>

			<note place="headnote">COMPUTER GRAPHI CS Proceedings, Annual Conference Series, 1994 <lb/></note>

			<body>~ <lb/>summed <lb/>area table <lb/>--~ R --~f ( p , q ) <lb/>min-max octree <lb/>Pmin Pma~-<lb/>(a) <lb/>(b) <lb/>(c) <lb/>9 <lb/>-0 <lb/>Figure 7: A min-max octree (a) is used to determine the range <lb/>of the parameters p, q of the opacity transfer function f(p, q) in <lb/>a subcube of the volume. A summed area table (b) is used to <lb/>integrate f over that range of p, q. If the integral is zero (c) then <lb/>the subcube contains only transparent voxels. <lb/>opacities. If so, then discard the scanline since it must be <lb/>transparent. <lb/>Step 3: Subdivide the scanline and repeat this algorithm recur-<lb/>sively. If the size of the current scanline portion is below a <lb/>threshold then render it instead of subdividing. <lb/>This algorithm relies on two data structures for efficiency (Fig-<lb/>ure 7). First, Step 1 uses a precomputed min-max octree [21]. <lb/>Each octree node contains the extrema of the parameter values <lb/>for a subcube of the volume. Second, to implement Step 2 of the <lb/>algorithm we need to integrate the function f over the region of <lb/>the feature space found in Step 1. If the integral is zero then all <lb/>voxels must be transparent.* This integration can be performed <lb/>in constant time using a multi-dimensional summed-area table [2] <lb/>[5]. The voxels themselves are stored in a third data structure, a <lb/>simple 3D array. <lb/>The overall algorithm for rendering unclassified data sets pro-<lb/>ceeds as follows. The rain-max octree is computed at the time the <lb/>volume is first loaded since the octree is independent of the opac-<lb/>ity transfer function and the viewing parameters. Next, just before <lb/>rendering begins the opacity transfer function is used to compute <lb/>the summed area table. This computation is inexpensive provided <lb/>that the domain of the opacity transfer function is not too large. <lb/>We then use either the parallel projection or the perspective pro-<lb/>jection rendering algorithm to render voxels from an unencoded <lb/>3D voxel array. The array is traversed scanline by scanline. For <lb/>each scanline we use the octree and the summed area table to de-<lb/>termine which portions of the scanline are non-transparent. Voxels <lb/>in the non-transparent portions are individually classified using a <lb/>lookup table and rendered as in the previous algorithms. Opaque <lb/>regions of the image are skipped just as before. Note that voxels <lb/>that are either transparent or occluded are never classified, which <lb/>reduces the amount of computation. <lb/>The octree traversal and summed area table lookups add over-<lb/>head to the algorithm which were not present in the previous <lb/>algorithms. In order to reduce this overhead we save as much <lb/>computed data as possible for later reuse: an octree node is tested <lb/>for transparency using the summed area table only the first time <lb/>it is visited and the result is saved for subsequent traversals, and <lb/>if two adjacent scanlines intersect the same set of octree nodes <lb/>then we record this fact and reuse information instead of making <lb/>multiple traversals. <lb/>This rendering algorithm places two restrictions on the opacity <lb/>transfer function: the parameters of the function must be precom-<lb/>putable for each voxel so that the octree may be precomputed, <lb/>and the total number of possible argument tuples to the function <lb/>(the cardinality of the domain) must not be too large since the <lb/>*The user may choose a non-zero opacity threshold for transparent voxels, in <lb/>which case a thresholded version of f must be integrated: let ] &apos; = f whenever f <lb/>exceeds the threshold, and f J = 0 otherwise. <lb/>summed area table must contain one entry for each possible tu-<lb/>pie. Context-sensitive segmentation (classification based upon the <lb/>position and surroundings of a voxel) does not meet these criteria <lb/>unless the segmentation is entirely precomputed. <lb/>The fast-classification algorithm presented here also suffers <lb/>from a problem common to many object-order algorithms: if the <lb/>major viewing axis changes then the volume data must be ac-<lb/>cessed against the stride and performance degrades. Alternatively <lb/>the 3D array of voxels can be transposed, resulting in a delay <lb/>during interactive viewing. Unlike the algorithms based on a run-<lb/>length encoded volume, it is typically not practical to maintain <lb/>three copies of the unencoded volume since it is much larger than <lb/>a run-length encoding. It is better to use a small range of view-<lb/>points while modifying the classification function, and then to <lb/>switch to one of the previous two rendering methods for render-<lb/>ing animation sequences. In fact, the octree and the summed-area <lb/>table can be used to convert the 3D voxel array into a run-length <lb/>encoded volume without accessing transparent voxels, leading to <lb/>a significant time savings (see the &quot;Switch Modes&quot; arrow in Fig-<lb/>ure 12). Thus the three algorithms fit together well to yield an <lb/>interactive tool for classifying and viewing volumes. <lb/>4 Results <lb/>4.1 <lb/>S p e e d a n d Memory <lb/>Our performance results for the three algorithms are summarized <lb/>in Table 1. The &quot;Fast Classification&quot; timings are for the algorithm <lb/>in Section 3.3 with a parallel projection. The timings were mea-<lb/>sured on an SGI Indigo R4000 without hardware graphics accel-<lb/>erators. Rendering times include all steps required to render from <lb/>a new viewpoint, including computation of the shading lookup <lb/>table, compositing and wmping, hut the preprocessing step is not <lb/>included. The &quot;Avg.&quot; field in the table is the average time in sec-<lb/>onds for rendering 360 frames at one degree angle increments, and <lb/>the &quot;Min/Max&quot; times are for the best and worst case angles. The <lb/>&quot;Mem.&quot; field gives the size in megabytes of all data structures. <lb/>For the first two algorithms the size includes the three run-length <lb/>encodings of the volume, the image data structures and all lookup <lb/>tables. For the third algorithm the size includes the unencoded <lb/>volume, the octree, the summed-area table, the image data struc-<lb/>tures, and the lookup tables. The &quot;brain&quot; data set is an MRI scan <lb/>of a human head (Figure 8) and the &quot;head&quot; data set is a CT scan <lb/>of a human head (Figure 9). The &quot;brainsmall&quot; and &quot;headsmall&quot; <lb/>data sets are decimated versions of the larger volumes. <lb/>The timings are nearly independent of image size because this <lb/>factor affects only the final wa R which is relatively insignificant. <lb/>Rendering time is dependent on viewing angle (Figure 11) because <lb/>the effectiveness of the coherence optimizations varies with view-<lb/>point and because the size of the intermediate image increases as <lb/>the rotation angle approaches 45 degrees, so more compositing <lb/>operations must be performed. For the algorithms described in <lb/>Sections 3.1-3.2 there is no jump in rendering time when the ma-<lb/>jor viewing axis changes, provided the three run-length encoded <lb/>copies of the volume fit into real memory simultaneously. Each <lb/>copy contains four bytes per non-transparent voxel and one byte <lb/>per run. For the 256x256x226 voxel head data set the three run-<lb/>length encodings total only 9.8 Mbytes. All of the images were <lb/>rendered on a workstation with 64 Mbytes of memory. To test the <lb/>fast classification algorithm (Section 3.3) on the 256 ~ data sets we <lb/>used a workstation with 96 Mbytes of memory. <lb/>Figure 12 gives a breakdown of the time required to render the <lb/>brain data set with a parallel projection using the fast classification <lb/>algorithm (left branch) and the parallel projection algorithm (right <lb/>branch). The time required to warp the intermediate image into <lb/>the final image is typically 10-20% of the total rendering time <lb/>for the parallel projection algorithm. The &quot;Switch Modes&quot; arrow <lb/></body>

			<page>455 <lb/></page>

			<note place="headnote">SIGGRAPH 94, Orlando, Florida, July 24-29, 1994 <lb/></note>

			<body>Data set <lb/>brainsmall <lb/>headsmall <lb/>brain <lb/>head <lb/>Size (voxels) <lb/>128x128x109 <lb/>128x128x113 <lb/>256x256x167 <lb/>256x256x225 <lb/>Parallel projection ( § 3.1) <lb/>Avg. <lb/>Min/Max <lb/>Mere. <lb/>0.4 s. 0.37-0.48 s. <lb/>4 Mb. <lb/>0.4 <lb/>0.35-0.43 <lb/>2 <lb/>1.1 <lb/>0.91-1.39 <lb/>19 <lb/>1.2 <lb/>1.04-1.33 <lb/>13 <lb/>Perspective projection ( §3.2) <lb/>Fast classification ( §3.3) <lb/>Avg. <lb/>Min/Max <lb/>Mem. <lb/>Avg. <lb/>Min/Max <lb/>Mem. <lb/>1.0 s. 0.84-1.13 s. <lb/>4 Mb. 0.7 s. 0.61-0.84 s. <lb/>8 Mb. <lb/>0.9 <lb/>0.82-1.00 <lb/>2 <lb/>0.8 <lb/>0.72-0.87 <lb/>8 <lb/>3.0 <lb/>2.44-2.98 <lb/>19 <lb/>2.4 <lb/>1.91-2.91 <lb/>46 <lb/>3.3 <lb/>2.99-3.68 <lb/>13 <lb/>2.8 <lb/>2.43-3.23 <lb/>61 <lb/>Table 1: Rendering time and memory usage on an SGI Indigo workstation. Times are in seconds and include shading, resampling, <lb/>projection and warping. The fast classification times include rendering with a parallel projection. The &quot;Mem.&quot; field is the total size of <lb/>the data structures used by each algorithm. <lb/>1500 -<lb/>d <lb/>&quot;~&apos;1000 <lb/>I-<lb/>~g <lb/>&quot;E <lb/>~-500 -<lb/>E <lb/>Max: 1330 msec. <lb/>Min: 1039 msec. <lb/>Avg: 1166 msec. <lb/>0 <lb/>I <lb/>I <lb/>I <lb/>I <lb/>0 <lb/>90 <lb/>180 <lb/>270 <lb/>360 <lb/>Rotation Angle (Degrees) <lb/>Figure 11 : Rendering time for a parallel projection of the head <lb/>data set as the viewing angle changes. <lb/>shows the time required for all three copies of the run-length <lb/>encoded volume to be computed from the unencoded volume and <lb/>the min-max octree once the user has settled on an opacity transfer <lb/>function. <lb/>The timings above are for grayscale renderings. Color ren-<lb/>derings take roughly twice as long for parallel projections and <lb/>1.3x longer for perspective because of the additional resampling <lb/>required for the two extra color channels. Figure 13 is a color <lb/>rendering of the head data set classified with semitransparent skin <lb/>which took 3.0 sec. to render. Figure 14 is a rendering of a <lb/>256x256x 110 voxel engine block, classified with semi-transparent <lb/>and opaque surfaces; it took 2.3 sec. to render. Figure 15 is a ren-<lb/>dering of a 256x256x159 CT scan of a human abdomen, rendered <lb/>in 2.2 sec. The blood vessels of the subject contain a radio-opaque <lb/>dye, and the data set was classified to reveal both the dye and bone <lb/>surfaces. Figure 16 is a perspective color rendering of the engine <lb/>data set which took 3.8 sec. to compute. <lb/>For comparison purposes we rendered the head data set with <lb/>a ray-caster that uses early ray termination and a pyramid to ex-<lb/>ploit object coherence [12]. Because of its lower computational <lb/>overhead the shear-warp algorithm is more than five times faster <lb/>for the 1283 data sets and more than ten times faster for the 2563 <lb/>data sets. Our algorithm running on a workstation is competitive <lb/>with algorithms for massively parallel processors ([17], [19] and <lb/>others), although the parallel implementations do not rely on co-<lb/>herence optimizations and therefore their performance results are <lb/>not data dependent as ours are. <lb/>Our experiments show that the running time of the algorithms <lb/>in Sections 3.1-3.2 is proportional to the number of voxels which <lb/>are resampled and composited. This number is small either if a <lb/>significant fraction of the voxels are transparent or if the aver-<lb/>age voxel opacity is high. In the latter case the image quickly <lb/>becomes opaque and the remaining voxels are skipped. For the <lb/>data sets and classification functions we have tried roughly n 2 <lb/>voxels are both non-transparent and visible, so we observe O(n 2) <lb/>performance as shown in Table 1: an eight-fold increase in the <lb/>~Preprocess Dataset <lb/>........ t 77sec.____ Switc <lb/>I v°lume + l <lb/>octree <lb/>• 2280 msec. <lb/>i nt er medi at ei mage I <lb/>.~0 <lb/>msec. <lb/>New Classification-i~:3-.:3) <lb/>Switch <lb/>Modes <lb/>8.5 sec. <lb/>,I run-length <lb/>I encoding I <lb/>980 msec. <lb/>i nt er medi at ei mage I <lb/>el <lb/>0 msec. ; <lb/>New viewpoint ( §3.1) <lb/>Figure 12: Performance results for each stage of rendering the <lb/>brain data set with a parallel projection. The left side uses the <lb/>fast classification algorithm and the right side uses the parallel <lb/>projection algorithm. <lb/>number of voxels leads to only a four-fold increase in time for <lb/>the compositing stage and just under a four-fold increase in over-<lb/>all rendering time. For our rendering of the head data set 5% of <lb/>the voxels are non-transparent, and for the brain data set 11% of <lb/>the voxels are non-transparent. Degraded performance can be ex-<lb/>pected if a substantial fraction of the classified volume has low but <lb/>non-transparent opacity, but in our experience such classification <lb/>functions are less useful. <lb/>4.2 Image Quality <lb/>Figure 10 is a volume rendering of the same data set as in Figure 9, <lb/>but produced by a ray-caster using tfilinear interpolation [12]. The <lb/>two images are virtually identical. <lb/>Nevertheless, there are two potential quality problems associ-<lb/>ated with the shear-warp algorithm. First, the algorithm involves <lb/>two resampling steps: each slice is resampled during composit-<lb/>ing, and the intermediate image is resampled during the final warp. <lb/>Multiple resampling steps can potentially cause blurring and loss <lb/>of detail. However even in the high-detail regions of Figure 9 this <lb/>effect is not noticeable. <lb/>The second potential problem is that the shear-warp algorithm <lb/>uses a 2D rather than a 3D reconstruction filter to resample the <lb/>volume data. The bilinear filter used for resampling is a first-order <lb/>filter in the plane of a voxel slice, but it is a zero-order (nearest-<lb/>neighbor) filter in the direction orthogonal to the slice. Artifacts <lb/>are likely to appear if the opacity or color attributes of the volume <lb/>contain very high frequencies (although if the frequencies exceed <lb/>the Nyquist rate then perfect reconstruction is impossible). <lb/></body>

			<page>456 <lb/></page>

			<note place="headnote">COMPUTER GRAPHICS Proceedings, Annual Conference Series, 1994 <lb/></note>

			<body>Figure 17 shows a case where a trilinear interpolation filter <lb/>outperforms a bilinear filter. The left-most image is a rendering <lb/>by the shear-warp algorithm of a portion of the engine data set <lb/>which has been classified with extremely sharp ramps to produce <lb/>high frequencies in the volume&apos;s opacity. The viewing angle is <lb/>set to 45 degrees relative to the slices of the data set--the worst <lb/>case--and aliasing is apparent. For comparison, the middle image <lb/>is a rendering produced with a ray-caster using trilinear interpo-<lb/>lation and otherwise identical rendering parameters; here there is <lb/>virtually no aliasing. However, by using a smoother opacity trans-<lb/>fer function these reconstruction artifacts can be reduced. The <lb/>fight-most image is a rendering using the shear-warp algorithm <lb/>and a less-extreme opacity transfer function. Here the aliasing is <lb/>barely noticeable because the high frequencies in the scalar field <lb/>have effectively been low-pass filtered by the transfer function. <lb/>In practice, as long as the opacity transfer function is not a binary <lb/>classification the bilinear filter produces good results. <lb/>5 Conclusion <lb/>The shear-warp factorization allows us to implement coherence <lb/>optimizations for both the volume data and the image with low <lb/>computational overhead because both data structures can be tra-<lb/>versed simultaneously in scanline order. The algorithm is flexible <lb/>enough to accommodate a wide range of user-defined shading <lb/>models and can handle perspective projections. We have also <lb/>presented a variant of the algorithm that does not assume a fixed <lb/>opacity transfer function. The result is an algorithm which pro-<lb/>duces high-quality renderings of a 2563 volume in roughly one <lb/>second on a workstation with no specialized hardware. <lb/>We are currently extending our rendering algorithm to support <lb/>data sets containing both geometry and volume data. We have <lb/>also found that the shear-warp algorithms parallelize naturally for <lb/>MIMD shared-memory multiprocessors. We parallelized the re-<lb/>sampling and compositing steps by distributing scanlines of the <lb/>intermediate image to the processors. On a 16 processor SGI <lb/>Challenge multiprocessor the 256x256x223 voxel head data set <lb/>can be rendered at a sustained rate of 10 frames/sec. <lb/></body>

			<div type="acknowledgement">Acknowledgements <lb/>We thank Pat Hanrahan, Sandy Napel and North Carolina Memo-<lb/>rial Hospital for the data sets, and Maneesh Agrawala, Mark <lb/>Horowitz, Jason Nieh, Dave Ofelt, and Jaswinder Pal Singh for <lb/>their help. This research was supported by Software Publish-<lb/>ing Corporation, ARPA/ONR under contract N00039-91-C-0138, <lb/>NSF under contract CCR-9157767 and the sponsoring companies <lb/>of the Stanford Center for Integrated Systems. <lb/></div>

			<listBibl>References <lb/>[1] Cameron, G. G. and P. E. Undrill. Rendering volumetric <lb/>medical image data on a SIMD-architecture computer. In <lb/>Proceedings of the Third Eurographics Workshop on Ren-<lb/>dering, 135-145, Bristol, UK, May 1992. <lb/>[2] Crow, Franklin C. Summed-area tables for texture map-<lb/>ping. Proceedings of SIGGRAPH &apos;84. Computer Graphics, <lb/>18(3):207-212, July 1984. <lb/>[3] Danskin, John and Pat Hanrahan. Fast algorithms for volume <lb/>ray tracing. In 1992 Workshop on Volume Visualization, 91-<lb/>98, Boston, MA, October 1992. <lb/>[4] Drebin, Robert A., Loren Carpenter and Pat Hanrahan. Vol-<lb/>ume rendering. Proceedings of SIGGRAPH &apos;88. Computer <lb/>Graphics, 22(4):65-74, August 1988. <lb/>[5] Glassner, Andrew S. Multidimensional sum tables. In <lb/>Graphics Gems, 376-381. Academic Press, New York, 1990. <lb/>[6] Glassner, Andrew S. Normal coding. In Graphics Gems, <lb/>257-264. Academic Press, New York, 1990. <lb/>[7] Hanrahan, Pat. Three-pass affine transforms for volume ren-<lb/>dering. Computer Graphics, 24(5):71-77, November 1990. <lb/>[8] Laur, David and Pat Hanrahan. Hierarchical splatting: <lb/>A progressive refinement algorithm for volume render-<lb/>ing. Proceedings of SIGGRAPH &apos;91. Computer Graphics, <lb/>25(4):285-288, July 1991. <lb/>[9] Levoy, Marc. Display of surfaces from volume data. IEEE <lb/>Computer Graphics &amp; Applications, 8(3):29-37, May 1988. <lb/>[10] Levoy, Marc. Volume rendering by adaptive refinement. The <lb/>Visual Computer, 6(1):2-7, February 1990. <lb/>[11] Levoy, Marc and Ross Whitaker. Gaze-directed volume ren-<lb/>dering. Computer Graphics, 24(2):217-223, March 1990. <lb/>[12] Levoy, Marc. Efficient ray tracing of volume data. ACM <lb/>Transactions on Graphics, 9(3):245-261, July 1990. <lb/>[13] Meagher, Donald J. Efficient synthetic image generation of <lb/>arbitrary 3-D objects. In Proceeding of the IEEE Confer-<lb/>ence on Pattern Recognition and Image Processing, 473-<lb/>478, 1982. <lb/>[14] Novins, Kevin L., Francois X. Sillion, and Donald P. Green-<lb/>berg. An efficient method for volume rendering using <lb/>perspective projection. Computer Graphics, 24(5):95-102, <lb/>November 1990. <lb/>[15] Porter, Thomas and Tom Duff. Compositing digital im-<lb/>ages. Proceedings of SIGGRAPH &apos;84. Computer Graphics, <lb/>18(3):253-259, July 1984. <lb/>[16] Sakas, Georgios and Matthias Gerth. Sampling and anti-<lb/>aliasing of discrete 3-D volume density textures. In Proceed-<lb/>ings of Eurographics &apos;91, 87-102, Vienna, Austria, Septem-<lb/>ber 1991. <lb/>[17] Schr~der, Peter and Gordon Stoll. Data parallel volume ren-<lb/>dering as line drawing. In Proceedings of the 1992 Workshop <lb/>on Volume Visualization, 25-32, Boston, October 1992. <lb/>[18] Subramanian, K. R. and Donald S. Fussell. Applying space <lb/>subdivision techniques to volume rendering. In Proceedings <lb/>of Visualization &apos;90, 150-159, San Francisco, California, Oc-<lb/>tober 1990. <lb/>[19] V6zina, Guy, Peter A. Fletcher, and Philip K. Robertson. <lb/>Volume rendering on the MasPar MP-1. In 1992 Workshop <lb/>on Volume Visualization, 3-8, Boston, October 1992. <lb/>[20] Westover, Lee. Footprint evaluation for volume render-<lb/>ing. Proceedings of SIGGRAPH &apos;90. Computer Graphics, <lb/>24(4):367-376, August 1990. <lb/>[21] Wilhelms, Jane and Allen Van Gelder. Octrees for faster <lb/>isosurface generation. Computer Graphics, 24(5):57-62, <lb/>November 1990. <lb/>[22] Yagel, Roni and Arie Kaufman. Template-based volume <lb/>viewing. In Eurographics 92, C-153-167, Cambridge, UK, <lb/>September 1992. <lb/>[23] Zuiderveld, Karel J., Anton H.J. Koning, and Max A. <lb/>Viergever. Acceleration of ray-casting using 3D distance <lb/>transforms. In Proceedings of Visualization in Biomedical <lb/>Computing 1992, 324-335, Chapel Hill, North Carolina, Oc-<lb/>tober 1992. <lb/></listBibl>

			<page>457 <lb/></page>

			<note place="headnote">SIGGRAPH 94, Orlando, Florida, July 24-29, 1994 <lb/></note>

			<body>Figure 8: Volume rendering with a par-<lb/>allel projection of an MRI scan of a hu-<lb/>man brain using the shear-warp algo-<lb/>rithm (1.1 sec.). <lb/>Figure 9: Volume rendering with a par-<lb/>allel projection of a CT scan of a human <lb/>head oriented at 45 degrees relative to <lb/>the axes of the volume (1.2 sec.). <lb/>Figure 10: Volume rendering of the <lb/>same data set as in Figure 9 using a <lb/>ray-caster [12] for quality comparison <lb/>(13.8 sec.). <lb/>Figure 13: Volume rendering with a <lb/>parallel projection of the human head <lb/>data set classified with semitransparent <lb/>skin (3.0 sec.). <lb/>Figure 14: Volume rendering with a <lb/>parallel projection of an engine block <lb/>with semitransparent and opaque sur-<lb/>faces (2.3 sec.). <lb/>Figure 15: Volume rendering with a <lb/>parallel projection of a CT scan of a <lb/>human abdomen (2.2 sec.). The blood <lb/>vessels contain a radio-opaque dye. <lb/>Figure 16: Volume rendering with a perspective projection of the <lb/>engine data set (3.8 sec.). <lb/>(a) <lb/>(b) <lb/>(c) <lb/>Figure 17: Comparison of image quality with bilinear and trilinear <lb/>filters for a portion of the engine data set. The images have been <lb/>enlarged. (a) Bilinear filter with binary classification. (b) Trilinear <lb/>filter with binary classification. (c) Bilinear filter with smooth <lb/>classification. <lb/></body>

			<page>458 </page>


	</text>
</tei>
