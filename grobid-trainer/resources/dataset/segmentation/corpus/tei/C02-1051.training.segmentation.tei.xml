<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_C02-1051"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Automatic Linguistic Analysis for Language Teachers: <lb/>The Case of Zeros <lb/> MITSUKO YAMURA-TAKEI <lb/>Graduate School of Information Sciences <lb/>Hiroshima City University <lb/>3-4-1 Ozuka-higashi, Asaminami-ku, <lb/>Hiroshima, JAPAN 731-3194 <lb/>yamuram@nlp.its.hiroshima-cu.ac.jp <lb/>MIHO FUJIWARA <lb/>Department of Japanese and Chinese <lb/>Willamette University <lb/>900 State Street, Salem, <lb/>OR. USA 97301 <lb/>mfujiwar@willamette.edu <lb/>MAKOTO YOSHIE <lb/>Graduate School of Information Sciences <lb/>Hiroshima City University <lb/>yoshie@nlp.its.hiroshima-cu.ac.jp <lb/>TERUAKI AIZAWA <lb/>Faculty of Information Sciences <lb/>Hiroshima City University <lb/>aizawa@its.hiroshima-cu.ac.jp <lb/> Abstract <lb/> This paper presents the Natural Language <lb/> Processing-based linguistic analysis tool that we have developed for Japanese as a Second <lb/>Language teachers. This program, Zero De-<lb/>tector (ZD), aims to promote effective instruc-<lb/>tion of zero anaphora, on the basis of a hy-<lb/>pothesis about ideal conditions for second <lb/>language acquisition, by making invisible <lb/>zeros visible. ZD takes Japanese written <lb/>narrative discourse as input and provides the <lb/>zero-specified texts and their underlying <lb/>structures as output. We evaluated ZD&apos;s <lb/>performance in terms of its zero detecting <lb/>accuracy. We also present an experimental <lb/>report of its validity for practical use. As a <lb/>result, ZD has proven to be pedagogically <lb/>feasible in terms of its accuracy and its impact <lb/>on effective instruction. <lb/></front>

			<body> Introduction <lb/> Natural Language Processing (NLP) is an <lb/>emerging technology with a variety of real-world <lb/>applications. <lb/>Computer-Assisted Language <lb/>Learning/Teaching (CALL/CALT) is one area <lb/>that NLP techniques can contribute to. Such <lb/>techniques range from indexing and concor-<lb/>dancing to morphological processing with <lb/>on-demand dictionary look-ups and syntactic <lb/>processing with diagnostic error analysis, to <lb/>name a few. But little work has been done on <lb/>discourse-level phenomena, including anaphora. <lb/>Zero anaphora or zero pronouns (henceforth <lb/>zeros) are referential noun phrases (NPs) that are <lb/>not overtly expressed in Japanese discourse. <lb/>These NPs can be omitted if they are recoverable <lb/>from a given context or relevant knowledge. <lb/>The use of zeros is common in Japanese and this <lb/>poses a challenge for Japanese as a Second Lan-<lb/>guage (JSL) learners for their accurate compre-<lb/>hension and natural-sounding production of <lb/>Japanese discourse with zeros. Some learners <lb/>fail to understand a passage correctly because of <lb/>the difficulty of identifying zeros and/or their <lb/>antecedents. Other learners produce grammati-<lb/>cally correct but still unnatural-sounding Japa-<lb/>nese due to overuse or underuse of zeros. <lb/>Yet, very few textbooks provide systematic <lb/>instruction or intensive exercises to overcome <lb/>these difficulties with zeros. <lb/>Consequently <lb/>many Japanese language teachers rely on their <lb/>intuitions when explaining zeros. Intuition is a <lb/>conventional tool in teaching one&apos;s native lan-<lb/>guage, but from a student&apos;s perspective, a <lb/>well-developed systematic method of instruction <lb/>can be more convincing. Also from a teacher&apos;s <lb/>standpoint, such analysis will be helpful in pre-<lb/>paring teaching materials and evaluating stu-<lb/>dents&apos; performance. <lb/>Analysis of zeros can be divided into three <lb/>phases: zero identification, zero interpretation <lb/>and zero production. This paper focuses on the <lb/>first phase and proposes a method of systemati-<lb/>cally identifying the presence of zeros in order <lb/>that teachers might provide effective instruction <lb/>of zeros, based on some pedagogical principles <lb/>from relevant second language acquisition (SLA) <lb/>theory. We regard teachers as primary users of <lb/>the program and aim to help them enhance their <lb/>instruction. We implemented the program and <lb/>evaluated its potential benefits for language <lb/>teachers. <lb/>In Sections 1 and 2 we discuss the peda-<lb/>gogical assumptions from SLA theory that moti-<lb/>vate our program design, and present the linguis-<lb/>tic assumptions from which our heuristics were <lb/>drawn. Section 3 provides an overview of our <lb/>system implementation. In Section 4, we pre-<lb/>sent the results of evaluation from the viewpoints <lb/>of both the accuracy and the empirical validity of <lb/>the program. We conclude with a discussion of <lb/>possible future work. <lb/> 1 <lb/> Pedagogical Assumptions <lb/> There have been many studies about how people <lb/>learn foreign languages and what is responsible <lb/>for successful language learning. <lb/>Recent SLA theory progresses beyond <lb/>Krashen (e.g., 1982)&apos;s emphasis on automatic <lb/>processes of acquisition. Empirical research <lb/>has shown that learners&apos; consciousness-raising <lb/>through explicit instruction does contribute to <lb/>successful second language learning (see Norris <lb/>&amp; Ortega, 2000 for comprehensive review). <lb/>Chapelle (1998) reviewed seven hypotheses <lb/>about ideal SLA conditions that are relevant for <lb/>CALL program design. At the top of her list is <lb/>that &quot; the linguistic characteristics of target lan-<lb/>guage input need to be made salient &quot; (p. 23). <lb/>Effective input enhancement, by prompting <lb/>learners to notice particular learning items, with <lb/>highlighting for example, plays a significant role <lb/>in facilitating acquisition. We conjecture that <lb/>this salience effect can also be realized by mak-<lb/>ing zeros visible. <lb/>

			<page> 2 <lb/></page>

			Linguistic Assumptions <lb/> Japanese is a head-final language. A sentence <lb/>or a clause is headed by a predicate, which takes <lb/>a set of arguments and adjuncts. Predicates in <lb/>Japanese include verbs, adjectives, nominal ad-<lb/>jectives and copula, and usually consist of a core <lb/>predicate and some auxiliary elements. Argu-<lb/>ments are classified into three types: Topic <lb/>Phrase (TP), headed by a topic marker wa, Focus <lb/>Phrase (FP), headed by focus particles mo, koso, <lb/>dake, sae, shika, etc., and Kase Phrase (KP), <lb/>headed by case particles ga, wo, ni, e, to, yori, de, <lb/>kara, and made. <lb/> We regard adjuncts as <lb/>non-particle-headed phrases. <lb/>We define zeros as unexpressed obligatory <lb/>arguments of a core predicate. <lb/>What is <lb/> &quot; obligatory &quot; is the next question to arise. <lb/>Obligatoriness is a controversial issue, and there <lb/>is no set agreement among linguists on its <lb/>definition. Somers (1984) proposed a six-level <lb/>scale of valency binding that reflects the degree <lb/>of closeness of an element to the predicate. The <lb/>levels are (i) integral complements, (ii) <lb/>obligatory <lb/>complements, <lb/>(iii) <lb/>optional <lb/>complements, (iv) middles, (v) adjuncts and (vi) <lb/>extraperipherals. Ishiwata (1999) suggests that <lb/>in Japanese group (i) is often treated as part of <lb/>idioms and is not omissible, and Japanese <lb/>nominative –ga and accusative –wo fall into the <lb/>category (ii), while dative –ni belongs to (iii). <lb/>In light of this, we assume that obligatory <lb/>arguments that can be zero-pronominalized are <lb/>phrases headed by nominative-case particle ga <lb/> and accusative wo, and ni, excluding dative ni in <lb/>an indirect object position. <lb/> 3 <lb/>Zero Detector <lb/> Zero Detector (henceforth ZD) is an automatic <lb/>zero identifying tool, which takes Japanese writ-<lb/>ten narrative texts as input and provides the <lb/>zero-specified texts and their underlying struc-<lb/>tures as output. This aims to draw learners&apos; and <lb/>teachers&apos; attention to zeros, by making these <lb/>invisible elements visible in effectively enhanced <lb/>formats. <lb/> 3.1 System Overview <lb/> ZD employs a rule-based approach, with theo-<lb/>retically sound heuristics. Our heuristics are <lb/>drawn from the linguistic assumptions described <lb/>in Section 2. <lb/>ZD reuses and integrates two existing natu-<lb/>ral language analysis tools and an electronic dic-<lb/>tionary, none of which were intended for a lan-<lb/>guage learning purpose, into its architecture, <lb/>attempting to make the best possible use of their <lb/>capabilities for our purpose. Morphological <lb/>analysis is done by ChaSen 2.2.8 (NAIST, Ma-<lb/>tsumoto, Y. et al., 2001), and dependency struc-<lb/>ture analysis by CaboCha 0.21 (NAIST, Kudo, <lb/>K., 2001). The Goi-Taikei Valency Dictionary <lb/>(hereafter GTVD; Ikehara et al., 1997) serves as <lb/>a source for valency pattern search. <lb/>The flow of the system is illustrated in Fig-<lb/>ure 1. <lb/> Clause Splitter <lb/> Morphological Analysis <lb/> Clause Splitting <lb/> (Manual Correction) <lb/> Revised Split Clauses <lb/> Zero Detector <lb/> Dependency Structure Analysis <lb/> Zero Detection <lb/> Valency <lb/>Dictionary <lb/> Zero Insertion <lb/> OUTPUT(B): <lb/> Clause Structure Frames <lb/> OUTPUT(C): <lb/> Predicate-Argument <lb/>Structures with Zeros <lb/> OUTPUT(D): <lb/> Zero-inserted Text <lb/>Morphological Analysis <lb/> OUTPUT(A): <lb/> Split Clauses <lb/> INPUT: Text <lb/> Figure 1: Flow diagram of zero detecting processes <lb/> 3.2 ZD Output <lb/> As shown in Figure 1, ZD produces four differ-<lb/>ent types of output: (A) split clauses, (B) clause <lb/>structure frames, (C) predicate-argument struc-<lb/>tures with zeros, and (D) zero-inserted texts. <lb/>We will show how these outputs are structured <lb/>using the example text in Figure 2. <lb/>komatta <lb/>Satsuki-wa <lb/>sassoku <lb/>in trouble Satsuki-TOP <lb/>immediately <lb/>gennin-wo shirabe-sase-ta. <lb/>cause-ACC investigate-CAUSATIVE-PAST <lb/> &quot; Satsuki, who was in trouble, immediately had <lb/>(someone) investigate its cause. &quot; <lb/>Figure 2: An example input text <lb/>First, output (A) provides a text divided into <lb/>clauses, each consisting of one and only one <lb/>predicate and its arguments. Some predicates <lb/>are simplex, while others are complex, consisting <lb/>of more than one core predicate (i.e., verb, adjec-<lb/>tive). Several complex predicates (e.g., ta-<lb/>beta-koto-ga-aru ate-experience-subject marker-<lb/>have, &quot; have eaten &quot; ) are predefined as simplex to <lb/>avoid excessive clause splitting. The clauses <lb/>are labelled with their clause types: independent <lb/>(main), dependent (coordinated/subordinated) or <lb/>embedded (relative/nominal/quoted). A clause <lb/>serves as the basic unit for the zero detecting <lb/>operation. In this study, embedded clauses are <lb/>excluded from this operation and are left within <lb/>their superordinate clauses. An example output <lb/>(A) is given in Figure 3 (next page). <lb/>komatta EC(RC)] Satsuki-wa sassoku <lb/>gennin-wo shirabe-sase-mashita. IC] <lb/>Figure 3: Split clauses 1 <lb/> Once the text is split into clauses, each <lb/>clause is analysed for its dependency structure <lb/>and then converted into its clause structure frame. <lb/>The noun phrases which depend on the predicate <lb/>are extracted, and then classified into phrase <lb/>types (TP, FP and KP) according to their accom-<lb/>panying particles. An example of this frame, <lb/>i.e., output (B), is given in Figure 4. <lb/>Input: komatta Satsuki-wa sassoku gennin-wo <lb/>shirabe-sase-ta. <lb/>Paragraph#: 2 <lb/>Sentence#: 4 <lb/> Clause#: 5 <lb/>Clause Type: Independent with EC(RC) <lb/>-----------------------------------------------------<lb/> [Predicate] : shirabe-sase-ta. <lb/>Core: <lb/>shiraberu <lb/>verb <lb/>Auxiliary: saseru <lb/>verb <lb/>ta <lb/>auxiliary verb <lb/>. <lb/>Voice: causative <lb/>Empathy: <lb/>Conjunction: <lb/>-----------------------------------------------------<lb/> [Argument] : <lb/>Topic Phrase: komatta Satsuki-wa <lb/>Topic-Case: N1-ga <lb/>Focus Phrase: &lt;none&gt; <lb/>Focus-Case: &lt;none&gt; <lb/>Kase Phrase: gennin-wo <lb/>Pre-copula: &lt;none&gt; <lb/> [Adjunct] : sassoku <lb/>Figure 4: A clause structure frame <lb/>This frame also includes the result of <lb/>valency checking, as in Figure 5, and zero iden-<lb/>tifying processes, as in Figure 6, at the bottom. <lb/> 1 Here, we use the acronyms: IC for Independent <lb/>
			
			Clause, EC for Embedded Clause, and RC for <lb/>Relative Clause. <lb/> Valency Selected: N1 ga N2 wo <lb/> Valency Obligatory: N1 ga N2 wo <lb/> Valency Changed: N1 ga <lb/>N2 wo N3 ni <lb/>Figure 5: Valency checking <lb/>A core predicate is checked against GTVD <lb/>to search for its syntactic valency pattern. <lb/>GTVD is a semantic valency dictionary, origi-<lb/>nally designed for transfer-based Japa-<lb/>nese-to-English machine translation, so it in-<lb/>cludes as many valency pattern entries for each <lb/>predicate as are necessary for effective transfer. <lb/>The entries are ordered according to expected <lb/>frequency of occurrence. We took the naïve <lb/>approach of selecting the first-ranking entry from <lb/>the listing for each core predicate (i.e.,&apos;Valency <lb/>Selected&apos; in Figure 5). <lb/>The next step is to apply the definition of <lb/>&apos;obligatoriness&apos; described in Section 2 to refine <lb/>the selected valency pattern (&apos;Valency Obliga-<lb/>tory&apos; in Figure 5). If non-ga, wo, or ni cases are <lb/>within the first three case slots of the selected <lb/>valency pattern, they are excluded. If a ni-case <lb/> still remains in the third case slot, it is also de-<lb/>leted. These operations leave us two valency <lb/>patterns: (i) N1-ga N2-wo, and (ii) N1-ga N2-ni, <lb/>in most cases. <lb/>Then, a valency changing operation is done <lb/>in the case of causatives or passives. When an <lb/>auxiliary verb is added to the core predicate in <lb/>the causative or passive construction, the verb <lb/>then requires three arguments. In the causative <lb/>case, these are a ga-marked causer, a wo-marked <lb/> object and a ni-marked causee. The valency <lb/>changing operation adds the boxed valent, N3 ni, <lb/> in Figure 5 (Valency Changed) because the voice <lb/>slot is marked as causative in Figure 4. <lb/> Valency Selected: N1 ga N2 wo <lb/> Valency Obligatory: N1 ga N2 wo <lb/> Valency Changed: N1 ga N2 wo N3 ni <lb/> Zero: N3 ni <lb/>Figure 6: Zero identifying <lb/>Now that the valency pattern for the given <lb/>predicate is assigned, it is checked against overt <lb/>arguments listed in the frame. The valent N2 is <lb/>matched with the overt argument gennin-wo and <lb/>removed from the zero candidates, as shown in <lb/>Figure 6. <lb/>Case-less elements, such as TP and FP, also <lb/>need to have their canonical case markers re-<lb/>stored. This is done by assigning the first re-<lb/>maining valent to TP and/or FP. This is based <lb/>on the linguistic fact that subjects are more likely <lb/>to be topicalized or focused than objects. In the <lb/>example, TP, Satsuki-wa, is assigned ga case. <lb/>The assigned case slot N1-ga is then matched <lb/>with Satsuki-wa (ga) and is also deleted. <lb/>Finally, the remaining valent, if any, is as-<lb/>sumed to be a zero (i.e., N3 ni in Figure 6). <lb/>Once zeros are identified, ZD decides where <lb/>to insert the identified zeros in the original text, <lb/>by keeping canonical ordering as listed in the <lb/>valency pattern. An example of the predicate-<lb/>(obligatory) argument structure from Figure 6, <lb/>with the identified zero, is presented in Figure 7. <lb/>This is output (C). Here, the restored case <lb/>marking particle is presented in parentheses. <lb/>*komatta Satsuki-wa (ga) <lb/>*gennin-wo <lb/>*[ <lb/> ni] <lb/> *shirabe-sase-ta. <lb/>Figure 7: Predicate-argument structure with zeros <lb/>Finally, ZD outputs the original series of <lb/>clauses with zeros inserted in the most plausible <lb/>positions, along with adjuncts, output (D), as in <lb/>Figure 8. <lb/>komatta Satsuki-wa sassoku gennin-wo [ <lb/>ni] <lb/> shirabe-sase-ta. <lb/>Figure 8: Zero-specified text <lb/>These outputs can later be converted into <lb/>the form of a slide presentation or hard-copy <lb/>handouts, etc., depending on how they are used <lb/>by teachers. <lb/> 4 <lb/>Evaluation <lb/> The purpose of the evaluation was to assess the <lb/>validity of ZD output for practical use in a lan-<lb/>guage learning/teaching setting. In the follow-<lb/>ing subsections, we evaluate ZD&apos;s performance <lb/>in terms of its accuracy and then present an ex-<lb/>perimental report of its validity for educational <lb/>use. <lb/> 
			
			4.1 Performance <lb/> First, we compared the ZD output with human <lb/>judgements. The test corpus consisted of two <lb/>reading selections from a JSL textbook and one <lb/>student written narrative monologue, all of which <lb/>were representative samples for lower intermedi-<lb/>ate level Japanese. Five subjects (native speak-<lb/>ers of Japanese and trained natural language re-<lb/>searchers) served as our human zero detectors. <lb/>They were asked to intuitively identify missing <lb/>arguments in each clause. We used average <lb/>human performance as a baseline against which <lb/>to evaluate ZD output. Here, zeros detected by <lb/>three or more, out of five, subjects were regarded <lb/>as average human performance. <lb/>As Table 1 shows, ZD achieved a 73% <lb/>per-clause matching rate with human output. <lb/>That number represents the ratio of the number <lb/>of exact matches between the two outputs over <lb/>the total number of clauses. <lb/>Table 1: Per-clause matching rates <lb/> # of clauses # of matched <lb/> Reading (1) <lb/>30 <lb/>22 (73%) <lb/>Reading (2) <lb/>25 <lb/>18 (72%) <lb/>Writing <lb/>23 <lb/>17 (74%) <lb/>Total <lb/>78 <lb/>57 (73%) <lb/>A closer examination of each case element <lb/>(ga, wo, ni) is given in Table 2 (next page). The <lb/>level &apos;matched&apos; includes both cases where ZD <lb/>and human detect a zero and cases where neither <lb/>detects it. The accuracy (89% average) is high <lb/>enough for the ZD output to be put into practical <lb/>use as a learning aid, without an excessive load <lb/>on teachers for post-editing output errors. Re-<lb/>leasing teachers from having to spend enormous <lb/> amount of time on the tedious work of analysing <lb/>educational materials is one of the biggest ad-<lb/>vantages of computerization of linguistic analy-<lb/>sis. <lb/>
			
			Table 2: Per-case element matching rates <lb/>‚ ª <lb/> ga <lb/> ‚ ð <lb/>wo <lb/>‚ É <lb/>ni <lb/> Human <lb/>ZD <lb/>Human <lb/>ZD <lb/>Huma <lb/>n <lb/>ZD <lb/>Detected <lb/>35 <lb/>32 <lb/>5 <lb/>4 <lb/>5 <lb/>2 <lb/>Not Detected <lb/>43 <lb/>39 <lb/>73 <lb/>68 <lb/>73 <lb/>63 <lb/> Matched <lb/> Total <lb/>78 <lb/> 71 (91%) <lb/> 78 <lb/> 72 (92%) <lb/> 78 <lb/> 65 (83%) <lb/> Under-detected <lb/>3 <lb/>1 <lb/>3 <lb/>Over-detected <lb/>4 <lb/>5 <lb/>10 <lb/> Not <lb/>Matched Total <lb/> 7 (9%) <lb/>6 (8%) <lb/>13 (17%) <lb/> Also, we analysed &apos;not matched&apos; cases to <lb/>improve future performance. There were 26 <lb/>cases of both underproduction and overproduc-<lb/>tion of zeros. Nearly half of them, 12 out of 26, <lb/>were caused by our naïve valency selection algo-<lb/>rithm, which selects the first entry from the <lb/>GTVD valency pattern listing for each predicate. <lb/>Three were caused by our canonical-case-<lb/>marker-restoring heuristics, which assign a first <lb/>available case marker from ga and wo in its <lb/>preference order. <lb/>They sometimes do not <lb/>function properly when accusatives or adjuncts <lb/>are topicalized (or focused). These are two <lb/>major areas for future enhancement. Four cases <lb/>were affected by morphological/sysntactic <lb/>analyses. Also, our definition of obligatory <lb/>arguments, which excludes dative –ni, produced <lb/>three &apos;not matched&apos; cases. This definition is <lb/>also an issue for further consideration. <lb/>What should be noted here, on the other <lb/>hand, is that there were six ZD produced zeros <lb/>which did not match our human zero detectors&apos; <lb/>decision but whose validity was later confirmed <lb/>by a JSL teacher who carefully examined the <lb/>result from an instructional point of view. This <lb/>implies that human-recognized zeros and <lb/>linguistically/pedagogically plausible zeros do <lb/>not always match, and demonstrates the potential <lb/>of ZD to fill this gap. <lb/> 4.2 Experiment <lb/> In order to verify the pedagogical effectiveness <lb/>of ZD, the output files were experimentally used <lb/>in a university-level intermediate JSL classroom, <lb/>through digital presentation. The aim of this <lb/>lesson was to familiarize the students with zeros <lb/>by making these invisible elements visible in <lb/>texts and presenting their underlying structures. <lb/>In their post-lesson feedback, the students <lb/>showed a positive reaction to this analytic in-<lb/>struction. They described this approach as &quot; in-<lb/>novative &quot; , &quot; effective &quot; , &quot; clear &quot; and &quot; easy &quot; for <lb/>understanding zeros, in contrast to their past &quot; just <lb/>guessing or being lost &quot; experiences. <lb/>
			
			The teacher who conducted this experimen-<lb/>tal lesson also acknowledged the impact of ZD <lb/>on effective instruction. She pointed out the <lb/>following benefits for students: <lb/>(i) <lb/>The valency checking segment of <lb/>output (B) helps students realize that <lb/>each predicate has its own valency <lb/>pattern, and as a consequence, clarifies <lb/>when to use what particles, <lb/>(ii) <lb/>the predicate-argument structures with <lb/>zeros, output (C), help students realize <lb/>that locating zeros is not a random op-<lb/>eration, but a canonical designation, <lb/>and <lb/>(iii) <lb/>the clause-by-clause parallel arrange-<lb/>ment in output (D) facilitates realizing <lb/>zero distributions in discourse and <lb/>tracking down antecedents for each <lb/>zero. <lb/>These include positive side effects that we ini-<lb/> tially did not foresee. <lb/>From a teaching point of view, ZD helps <lb/>teachers predict the difficulties with zeros that <lb/>students might encounter, by analysing text in <lb/>advance. This leads to the careful selection of <lb/>teaching materials and the well-thought-out crea-<lb/>tion of reading comprehension questions and <lb/>tests. Also, ZD output will be helpful in ex-<lb/>plaining the illegal use of zeros and particles <lb/>found in students&apos; writing. <lb/> Conclusions and Future Work <lb/> We have developed an automatic zero detecting <lb/>program that is intended mainly to serve as <lb/>teacher support. The program has proven to be <lb/>pedagogically feasible in terms of its accuracy <lb/>and its impact on effective instruction. The <lb/>great contribution of ZD is to introduce consis-<lb/>tency and systematic analysis into an area where <lb/>human intuitions play a dominant, but not always <lb/>accurate and effective, role. <lb/>ZD is currently a purely syntactic-based tool <lb/>that utilizes only surface-level heuristics, ex-<lb/>cluding any semantic cues. As our error analy-<lb/>sis in Section 4 indicates, more accuracy can be <lb/>achieved in a semantically enhanced version, <lb/>which in fact is our next project goal. <lb/>Valency-pattern-selecting (from GTVD) and <lb/>canonical-case-marker-restoring (from TP and <lb/>FP) algorithms are two major areas to which <lb/>semantic information can greatly contribute. <lb/>Also, ZD has been designed as a teaching <lb/>aid in a teacher-controlled class instruction mode. <lb/>To extend its use to a self-study mode, as some <lb/>students suggested, clear guidance and a <lb/>user-friendly interface will be required to replace <lb/>teachers&apos; explanation. <lb/>ZD is a part of the CALL program for JSL <lb/>learners, Zero Checker, which supports reading <lb/>comprehension and writing revision process with <lb/>a focus on zeros. Thus, ZD will also serve as a <lb/>pre-processing module for the models of resolv-<lb/>ing and generating zeros, created within the cen-<lb/>tering framework (e.g., Grosz et al., 1995). <lb/> </body>

		<back>	
			<listBibl>References <lb/> Chapelle, Carol A. (1998). Multimedia CALL: <lb/>Lessons to be learned from research on instructed <lb/>SLA. Language Learning and Technology, vol.2, <lb/>no.1, pp.22-34. <lb/>Grosz, B. J., A. K. Joshi and S. Weinstein. (1995). <lb/>Centering: A framework for modelling the local <lb/>coherence of discourse. Computational Linguis-<lb/>tics, 21/2, pp. 203-225. <lb/>Ikehara, S., M. Miyazaki, S. Shirai, A. Yokoo, H. <lb/>Nakaiwa, K. Ogura and Y. Hayashi (1997). <lb/> Goi-Taikei – A Japanese Lexicon, 5 volumes, <lb/>Iwanami Shoten, Tokyo. <lb/>Krashen, S. (1982). Principles and Practice in <lb/>Second Language Acquisition. Pergamon, Ox-<lb/>ford. <lb/>NAIST, Kudo, K. (2001). CaboCha 0.21. <lb/>http://cl.aist-nara.ac.jp/~taku-ku/software/caboch <lb/>a/ <lb/>NAIST, Matsumoto, Y. et al. (2001). ChaSen 2.2.8. <lb/>http://chasen.aist-nara.ac.jp/ <lb/>Ishiwata, T. (1999). Gendai GengoRiron to Kaku, <lb/> Hituzi Shobo, Tokyo. <lb/>Norris, J. M. and L. Ortega (2000). Effectiveness of <lb/>L2 instruction: A research synthesis and quantita-<lb/>tive meta-analysis. Language Learning 50 (3), <lb/>pp.417-528. <lb/> Somers, H. L. (1984). On the validity of the comple-<lb/>ment-adjunct distinction in valency grammar. Lin-<lb/>guistics 22, pp. 507-53. </listBibl>

		</back>
	</text>
</tei>
