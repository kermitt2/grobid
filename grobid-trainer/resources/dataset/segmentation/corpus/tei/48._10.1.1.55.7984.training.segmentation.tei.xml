<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>A SIMPLE APPROACH TO DISTRIBUTED POOLS <lb/>BENEDICT M. RAFANELLO &amp; THEODORE JOHNSON <lb/>University of Florida, Department of Computer and information Sciences <lb/>January 1994 <lb/>Computer networks hold the potential to coordinate the activities of multiple machines so that their combined com-<lb/>putational abilities can be applied to solving a single problem. Several methods have been developed over the years to <lb/>harness the power of networked systems for solving certain classes of problems. One such class of problems is the distrib-<lb/>uted producer/consumer problem, in which a set of producer processes supply items to a set of consumer processes. Each <lb/>of the processes involved resides on a different machine, with the machines being connected by a network and the proc-<lb/>esses communicating via message passing. The problem, then, is how to coordinate the activities of the producers and con-<lb/>sumers so that an acceptable level of throughput can be maintained with a minimal amount of overhead. This paper <lb/>presents a simple solution to the distributed producer/consumer problem. This solution, which is based upon the notion of <lb/>a distributed pool, is described in detail and its performance analyzed. As the analysis shows, the distributed pools algo-<lb/>rithm presented herein is a simple, efficient solution to the distributed producer/consumer problem, and is capable of bet-<lb/>ter than 90% efficiency under common conditions. Its major failing is that it needs the production rates of the producers <lb/>to be reasonably similar. <lb/>Key words: distributed computing, distributed queue, performance modeling, producer/consumer, simulation. <lb/></front>

			<body>INTRODUCTION <lb/>The distributed producer/consumer problem consists of a set of producer processes that supply items to a set of <lb/>consumer processes. Each of the processes involved resides on a different machine, with the machines being connected by <lb/>a network. Communication between the producers and consumers is accomplished via message passing. <lb/>One example of a distributed producer/consumer problem comes from the High Energy Physics community. High <lb/>Energy Physics research typically involves particle collision experiments. These experiments generate huge volumes of <lb/>data which are stored on a series of tapes 1 . The data itself consists of hundreds of millions of independent events, where <lb/>each event describes a collision between two or more particles 2 . Since the events are independent, they can be processed <lb/></body>

			<page>1 <lb/></page>

			<listBibl>1 F. J. Rinaldo and M. R. Fausey, &quot;Event Reconstruction in High-Energy Physics,&quot; Computer, June 1993, <lb/>p. 68. <lb/>2 Rinaldo and Fausey, p. 68. <lb/></listBibl>

			<body>in parallel. A reasonably cost effective way to do this would be to employ a series of workstations connected by a network. <lb/>Several workstations could be reading the data from the tapes and thereby acting as producers, while other workstations <lb/>would be used to process the events, thereby acting as consumers. Thus, we have a distributed producer/consumer prob-<lb/>lem. As it turns out, the High Energy Physics community has been moving in this very direction for several years now, as <lb/>is demonstrated by the work on UFMULTI II 3 at the University of Florida and Cooperative Processes Software 4 at Fermi-<lb/>lab. This work is motivated by UFMULTI II. <lb/>Existing Solutions <lb/>Two existing solutions to the distributed producer/consumer problem are the bag of tasks 5 and the concurrent <lb/>pool 6 . The bag of tasks solves a special case and the concurrent pool is designed for a concurrent, shared memory environ-<lb/>ment 7 . <lb/>Bag of Tasks <lb/>The bag of tasks approach uses one producer and multiple consumers. The producer does not actually produce <lb/>anything, but instead manages a bag. While the bag of tasks approach is not a general solution to the distributed pro-<lb/>ducer/consumer problem, distributed producer/consumer problems can be altered so that the bag of tasks approach can be <lb/>used. This is accomplished by having the actual producers place the items they produce into the bag. The obvious disad-<lb/>vantage to this approach is that the bag becomes a bottleneck. A variation on this idea has been successfully employed in <lb/>the implementation of the NetQueue mechanism used in UFMULTI II 8 . <lb/>Concurrent Pools <lb/>The concurrent pools approach is based on the notion of a pool. A pool is a collection to which items can be <lb/>added and removed 9 . A concurrent pool divides the pool into segments, where each segment resides on the same proces-<lb/></body>

			<listBibl>2 <lb/>3 Paul Avery et al., &quot;A New Approach to Distributed Computing in High Energy Physics,&quot; Proceedings of <lb/>the 26th International Conference on High Energy Physics, 1992, p. 5. <lb/>4 Rinaldo and Fausey, p. 70. <lb/>5 G. R. Andrews, &quot;Paradigms for Process Interaction in Distributed Programs,&quot; ACM Computing Surveys, <lb/>23, No. 1(1991), 85. <lb/>6 U. Manber. &quot;On Maintaining Dynamic Information in a Concurrent Environment,&quot; SIAM Journal on <lb/>Computing, 15, No. 4 (1986), 1130. <lb/>7 Manber, p. 1130. <lb/>8 Aric F. Zion, &quot;NetQueues: A Distributed Processing System for High-Energy Physics,&quot; Thesis, <lb/>University of Florida, 1992. <lb/>9 D. Kotz and C. S. Ellis, &quot;Evaluation of Concurrent Pools,&quot; In Proceedings of the Third Annual Parallel <lb/>Processing Symposium, p. 378. <lb/></listBibl>

			<body>sor as one of the processes which utilizes the concurrent pool. The items in the pool are distributed among the segments <lb/>comprising the pool, and the processes using the pool work primarily with the local segment of the pool . When a local <lb/>segment of the pool can not meet a request from the process(es) it serves, it finds another segment of the concurrent pool <lb/>which is not empty, and copies over half of the items. 10 . <lb/>Distributed implementations of the concurrent pool concept have been used successfully in various applications, <lb/>such as DIB -A Distributed Implementation of Backtracking 11 . However, when used in a distributed producer/consumer <lb/>situation, concurrent pools have some drawbacks. One such drawback is the bunching phenomenon, which can signifi-<lb/>cantly decrease performance 12 . A second drawback is that, due to the way the concurrent pools algorithm redistributes <lb/>items stored in the pool, items may cross the network multiple times. If the items are large, then having the items cross the <lb/>network multiple times drives up the load on the network and may result in a performance penalty. <lb/>The distributed pool algorithm presented in this paper solves the distributed producer/consumer problem while <lb/>avoiding the pitfalls of the existing solutions. The algorithm is presented in detail in Section 2, as are the results of a per-<lb/>formance study of the algorithm. Section 3 presents the conclusions drawn from the results of the performance study pre-<lb/>sented in Section 2, and presents guidelines for the use of the distributed pool algorithm. <lb/>Algorithm Description <lb/>The distributed pool algorithm was designed specifically to solve the distributed producer/consumer problem. As <lb/>such, all of the assumptions underlying the distributed producer/consumer problem apply to the distributed pool algorithm. <lb/>However, in addition to those assumptions, we add one more: the size of an item produced by a producer is large enough <lb/>that the cost of transferring an item from one process to another is significant. This assumption is important since it leads <lb/>to algorithms designed to minimize the number of times that an item must be transferred. This is in contrast to the concur-<lb/>rent pools algorithm, where items may be transferred multiple times before finally being consumed. <lb/>Given these assumptions, the distributed pool algorithm operates as follows: <lb/>• Each producer has a pool associated with it. The pool resides on the same machine as the producer. <lb/>• Whenever a producer produces an item, it places the item into its associated pool. If the pool is full, <lb/>the producer blocks until the pool is no longer full. <lb/>• Whenever a consumer needs an item to consume, it can make up to X attempts to get one, where X is a <lb/>parameter of the algorithm. Each attempt consists of randomly choosing a pool, requesting an item <lb/></body>

			<page>3 <lb/></page>

			<listBibl>10 Kotz and Ellis, pp. 378-379. <lb/>11 R. Finkel and U. Manber, &quot;DIB -A Distributed Implementation of Backtracking,&quot; ACM Transactions <lb/>on Programming Languages and Systems, 9, No. 2 (1987), 236. <lb/>12 Kotz and Ellis, p. 382. <lb/></listBibl>

			<body>PROCEDURE Producer <lb/>LOOP <lb/>WHILE NOT PoolFull DO <lb/>Produce Item <lb/>Send Message Containing Item to Pool <lb/>END WHILE <lb/>END LOOP <lb/>END PROCEDUR E <lb/>Figure 1: Pseudocode for a Producer <lb/>PROCEDURE Pool <lb/>LOOP <lb/>Get Message <lb/>IF message is from producer THEN <lb/>IF ConsumerQueue is empty THEN <lb/>Add item to pool <lb/>IF pool is full THEN <lb/>PoolFull = TRUE <lb/>END IF <lb/>ELSE <lb/>Remove message from Consumer Queue <lb/>Send item in message from producer to consumer <lb/>END IF <lb/>ELSE <lb/>IF NOT Pool Empty THEN <lb/>Remove item from pool <lb/>Send item to consumer <lb/>PoolFull = FALSE <lb/>ELSE <lb/>IF message type is &quot;block&quot; THEN <lb/>Put message in Consumer Queue <lb/>ELSE <lb/>Send pool empty message to consumer <lb/>END IF <lb/>END IF <lb/>END IF <lb/>END LOOP <lb/>END PROCEDUR E <lb/>Figure 2: Pseudocode for the Pool <lb/>PROCEDURE Consumer <lb/>LOOP <lb/>X = # of attempts allowed <lb/>REPEAT <lb/>X = X -1 <lb/>Choose pool <lb/>IF X &gt; 0 THEN <lb/>Send &quot;no block&quot; request to pool <lb/>ELSE <lb/>Send &quot;block&quot; request to pool <lb/>END IF <lb/>Wait for response from pool <lb/>UNTIL Response &lt;&gt; Pool Empty <lb/>Consume Item <lb/>END LOOP <lb/>END PROCEDUR E <lb/>Figure 3: Pseudocode for the Consumer <lb/></body>

			<page>4 <lb/></page>

			<body>from that pool, and getting either an item from the pool or a response indicating that the pool is empty. <lb/>If the consumer has made X attempts and still has not obtained an item to process, it blocks on the last <lb/>pool contacted and waits for that pool to provide it with an item to process. <lb/>For purposes of clarification, Figures 1, 2, and 3 present pseudocode which describes the operation of the pro-<lb/>ducer, the pool, and the consumer, respectively. The pseudocode assumes that, when a message is sent to a pool, the <lb/>sender blocks until it receives a reply. The pool,however, never blocks after sending a message. All communication is as-<lb/>sumed to be reliable. <lb/>The pseudocode given in Figures 1 and 2 assume the existence of a flag, PoolFull, which is available to both the <lb/>producer and the pool. Since both the producer and the pool reside on the same machine, the PoolFull flag can be imple-<lb/>mented using standard IPC mechanisms. <lb/>Figures 2 and 3 assume that the consumers can issue two different types of requests. The first type of request is a <lb/>&quot;no block&quot; request, which indicates to the pool that, if the pool is empty, it should return a pool empty message. The sec-<lb/>ond type of request is a &quot;block&quot; request, which indicates to the pool that the consumer will block until the pool returns an <lb/>item. Since the consumer will only block after X unsuccessful attempts to get an item, the consumer will send &quot;no block&quot; <lb/>requests for its first (X -1) attempts. For its final attempt, the consumer will use a &quot;block&quot; request, which will cause the <lb/>consumer to block until the chosen pool returns an item. Using two types of request is just one method of implementing <lb/>the consumers prescribed behavior. <lb/>The distributed pool algorithm solves the distributed producer/consumer problem, but the question is how well? <lb/>The algorithm is randomized, so it should distribute the request load evenly over the available pools. The pools at the pro-<lb/>ducers should handle variations in the request load. In order to determine how well the algorithm works, and under what <lb/>circumstances, we made a performance study, which is the subject of the next section. <lb/>Performance Analysis <lb/>Algorithm Simulation <lb/>In order to study the properties of the distributed pool algorithm, a discrete event simulator was built. Each execu-<lb/>tion simulated the operation of the algorithm for 6 hours. The inter-production and inter-consumption times were modeled <lb/>using exponential distributions. The time consumed by a failed attempt to get an item to process (retry interval) was set to <lb/>a constant 0.1 seconds. Each producer had a pool with 10 entries. Confidence Intervals for the resulting data, calculated <lb/>with a 95% level of confidence, were within 2%. <lb/>Measuring Performance <lb/>The performance measures used were actual throughput, achieved throughput, and the average number of requests <lb/>per item consumed (ANRIC). <lb/></body>

			<page>5 <lb/></page>

			<body>Absolute throughput. Absolute throughput is the total number of items consumed by a (the) consumer(s) during the <lb/>simulation, and is reported directly by the simulator. <lb/>Achieved Throughput. Achieved throughput is the absolute throughput expressed as a percentage of the maximum pos-<lb/>sible throughput. Achieved throughput was chosen as a performance measure because it is a measure of efficiency, i.e., <lb/>how well the producer/consumer/system being measured is living up to its potential. <lb/>Average number of requests per item consumed (ANRIC). ANRIC is the number of messages required for the coor-<lb/>dination of the producers and consumers. It is our measure of overhead. <lb/>Simulation Experiments <lb/>Given the wide array of variables offered by the simulator, examination of each variable&apos;s effect on the distributed <lb/>pool algorithm was beyond the scope of this paper. Therefore, it was decided to direct the research towards answering <lb/>questions about the use of the distributed pool algorithm in practical situations. As such, the examination of variables was <lb/>limited to those involved in answering the following questions: <lb/>• What kind of throughput can be expected from this algorithm? <lb/>• How much overhead does the algorithm require? <lb/>• For a given level of consumption/production, how many consumers/producers should be used? <lb/>• Given an existing producer-consumer situation, what happens if we add another producer or consumer? <lb/>• How many attempts should consumers have to obtain items? <lb/>• What happens if the producers produce at different rates? <lb/>In order to answer these questions, several series of simulations were set up. Each was designed to provide an-<lb/>swers to one or more of the questions being asked. Additionally, several of the series were run multiple times in order to <lb/>provide confidence intervals. <lb/>Simulation Results <lb/>For a given level of consumption, how many consumers should be used? <lb/>The first simulation series, which was designed to answer this question, held both consumption and production <lb/>constant. This means that the consumption rates of the consumers were altered as consumers were added or removed from <lb/>a simulation so that the total level of consumption remained constant from one simulation to the next. Since the level of <lb/>consumption was the same in all of the simulations, intuition would indicate that the throughput would remain the same <lb/>from one simulation to the next. This, however, was not the case. As it turns out, dividing the level of consumption <lb/>among many slow consumers results in better throughput than dividing it among just a few fast consumers. This holds <lb/>true regardless of the number of producers or the number of attempts that a consumer has to obtain an item. We can see <lb/>this effect in Figures 4 through 8. <lb/></body>

			<page>6 <lb/></page>

			<body>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>50 <lb/>75 <lb/>99 <lb/>Consumers <lb/>1 <lb/>3 <lb/>5 <lb/>7 <lb/>9 <lb/>15 <lb/>25 <lb/>75 <lb/>Producers <lb/>50 <lb/>55 <lb/>60 <lb/>65 <lb/>70 <lb/>75 <lb/>80 <lb/>85 <lb/>90 <lb/>95 <lb/>100 <lb/>Achieved Throughput <lb/>Figure 4: Achieved Throughput vs. consumers and producers for the case where consumption equals production and <lb/>consumers have 1 attempt to obtain items to consume <lb/>1 <lb/>4 <lb/>7 <lb/>10 <lb/>25 <lb/>99 <lb/>Consumers <lb/>1 <lb/>3 <lb/>5 <lb/>7 <lb/>9 <lb/>15 <lb/>25 <lb/>75 <lb/>Producers <lb/>91 <lb/>92 <lb/>93 <lb/>94 <lb/>95 <lb/>96 <lb/>97 <lb/>98 <lb/>99 <lb/>100 <lb/>Achieved Throughput <lb/>Figure 5: Achieved Throughput vs. consumers and producers for the case where consumption equals production and <lb/>consumers have 5 attempts to obtain items to consume <lb/></body>

			<page>7 <lb/></page>

			<body>Figures 4, 5, and 6 demonstrate that using many slow consumers results in better throughput regardless of the num-<lb/>ber of producers used. Figure 4 shows the results of the simulations where each consumer had just one attempt to get an <lb/>item to process. Since the maximum throughput for these simulations varied by less than one percent, the changes in <lb/>achieved throughput seen here are due to increases in actual throughput. <lb/>Figures 5 and 6 depict the results of the same simulations as Figure 4, but with the consumers having an increased <lb/>number of attempts to get an item to process. Figure 5 represents the case where the consumers had 5 attempts to obtain <lb/>an item to process, and Figure 6 depicts the case where the consumers had an unlimited number of attempts to obtain an <lb/>item to process. As with Figure 4, achieved throughput increases as the number of consumers used to achieve the level of <lb/>consumption is increased. This increase occurs regardless of the number of producers used. <lb/>Figures 7 and 8 demonstrate that increasing the number of consumers increases throughput regardless of how <lb/>many attempts consumers have to obtain items to process. Figure 7 shows the results from simulations with one producer, <lb/>and Figure 8 depicts the results from simulations with 25 producers. What these two figures show is that throughput is al-<lb/>ways increased as the number of consumers is increased. They also show that increasing the number of consumers gener-<lb/>ates the largest gain when consumers have fewer than three attempts to get items to process. Furthermore, once consumers <lb/>have five or more attempts to get items to process, the gains from increasing the number of consumers are minimized. <lb/>Last, but not least, we have the issue of overhead. Since the average number of requests per item consumed (AN-<lb/>RIC) is a measure of the amount of overhead required by the algorithm, how is the ANRIC affected by the use of many <lb/>1 <lb/>4 <lb/>7 <lb/>10 <lb/>25 <lb/>99 <lb/>Consumers <lb/>1 <lb/>3 <lb/>5 <lb/>7 <lb/>9 <lb/>15 <lb/>25 <lb/>75 <lb/>Producers <lb/>91 <lb/>92 <lb/>93 <lb/>94 <lb/>95 <lb/>96 <lb/>97 <lb/>98 <lb/>99 <lb/>100 <lb/>Achieved Throughput <lb/>Figure 6: Achieved Throughput vs. consumers and producers for the case where consumption equals production and <lb/>consumers have unlimited attempts to obtain items to consume <lb/></body>

			<page>8 <lb/></page>

			<body>1 <lb/>4 <lb/>7 <lb/>10 <lb/>25 <lb/>99 <lb/>Consumers <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>50 <lb/>75 <lb/>99 <lb/>infinite <lb/>Attempts Allowed <lb/>89 <lb/>90 <lb/>91 <lb/>92 <lb/>93 <lb/>94 <lb/>95 <lb/>96 <lb/>97 <lb/>Achieved Throughput <lb/>Figure 7: Achieved Throughput vs. consumers and attempts for the case where consumption equals production and there <lb/>is only 1 producer <lb/>1 <lb/>4 <lb/>7 <lb/>10 <lb/>25 <lb/>99 <lb/>Consumers <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>50 <lb/>75 <lb/>99 <lb/>infinite <lb/>Attempts Allowed <lb/>60 <lb/>65 <lb/>70 <lb/>75 <lb/>80 <lb/>85 <lb/>90 <lb/>95 <lb/>100 <lb/>Achieved Throughput <lb/>Figure 8: Achieved Throughput vs. consumers and attempts for the case where consumption equals production and there <lb/>are only 25 producers <lb/></body>

			<page>9 <lb/></page>

			<body>slow consumers instead of just a few fast consumers? Intuitively, one would expect the overhead involved in coordinating <lb/>the activities of the consumers to increase in some fashion as the number of consumers is increased, except in the case <lb/>where all consumers have only one attempt to get an item to process. Since the case where consumers have one attempt to <lb/>obtain an item is a special case with a trivial solution, we will ignore it and address the general case only. As Figure 9 <lb/>shows, the ANRIC does increase as the number of consumers is increased. Since Figure 9 shows the results of the case <lb/>where the consumers had an unlimited number of attempts to obtain an item to process, it represents the worst case, the <lb/>limit to which ANRIC can rise for the situation being simulated. For cases where the consumers had a fixed number of at-<lb/>tempts in which to obtain an item, the peak of the graph will be below the fixed limit, as in Figure 10, which depicts the <lb/>case where consumers had five attempts to obtain an item to process. Thus, while increasing the number of consumers <lb/>used to achieve a given level of consumption will result in greater throughput, the extra throughput comes at a price: in-<lb/>creased overhead. It is interesting to note, though, that the increase in overhead is also dependent upon the number of pro-<lb/>ducers and the number of attempts that consumers have to obtain an item to process. While the number of consumers is <lb/>less than the number of producers, increasing the number of consumers has very little effect upon ANRIC. However, once <lb/>the number of consumers is equal to or greater than the number of producers, increasing the number of consumers has a <lb/>larger impact upon ANRIC. <lb/>Now that we have determined that it is better to use many slow consumers to achieve a given level of consumption, <lb/>we must consider why this is so. As it turns out, the reason for the better performance obtained by using many slow con-<lb/>sumers is that fewer items are being lost to waiting. The number of items lost to waiting by a consumer is the consump-<lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>50 <lb/>75 <lb/>99 <lb/>Consumers <lb/>1 <lb/>5 <lb/>9 <lb/>25 <lb/>Producers <lb/>1.00 <lb/>6.00 <lb/>11.00 <lb/>16.00 <lb/>21.00 <lb/>26.00 <lb/>ANRIC <lb/>Figure 9: ANRIC vs. consumers and producers for the case where consumption equals production and consumers have <lb/>unlimited attempts to obtain items to consume <lb/></body>

			<page>10 <lb/></page>

			<body>tion rate of the consumer multiplied by the total time that consumer spent waiting. The total number of items lost to wait-<lb/>ing is then the sum of the items lost to waiting by each consumer. Lets assume that the total number of items lost to wait-<lb/>ing is constant. Given this, then, if we were to double the number of consumers in a simulation, each consumer would <lb/>have to lose half as many items as before. Since the level of consumption is constant here, when we double the number of <lb/>consumers we must cut their consumption rates in half. Cutting the consumption rate in half automatically causes each <lb/>consumer to lose half as many items as before, which means that the time spent waiting by each consumer must remain <lb/>constant. Since we found that the total number of items lost to waiting decreases, and since the decrease in consumption <lb/>rates offsets the increase in the number of consumers in our calculation, then the time spent waiting by each consumer <lb/>must decrease as the number of consumers is increased. <lb/>For a given level of production, how many producers should be used? <lb/>The first simulation series, which was designed to answer this question as well, held consumption and production <lb/>constant. This means that, as producers were added or removed from simulations, the production rates of the producers <lb/>were altered to maintain a constant level of production. Given this, intuition suggests that throughput would decline as the <lb/>number of producers was increased; the producers are slower so blocking times are longer. <lb/>The results from the simulations for this case show that our intuition is both right and wrong. For those simula-<lb/>tions where the consumers had only one or two attempts to get items to process, the intuitive explanation is right on the <lb/>mark. Figure 4 shows, as expected, that throughput decreases as the number of producers increases. But, as the number of <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>50 <lb/>75 <lb/>99 <lb/>Consumers <lb/>1 <lb/>5 <lb/>9 <lb/>25 <lb/>Producers <lb/>1.00 <lb/>1.50 <lb/>2.00 <lb/>2.50 <lb/>3.00 <lb/>3.50 <lb/>ANRIC <lb/>Figure 10: ANRIC vs. consumers and producers for the case where consumption equals production and consumers have 5 <lb/>attempts to obtain items to consume <lb/></body>

			<page>11 <lb/></page>

			<body>attempts to obtain an item is increased, the intuitive explanation begins to falter. Figure 5 shows that, when attempts to get <lb/>an item reaches 5, increasing the number of producers improves throughput until some limit is reached, after which <lb/>throughput declines as expected. This behavior was observed until the number of attempts to obtain an item reached 8, at <lb/>which point the limit exceeded the range of values tested and therefore no longer appeared in the data. <lb/>Turning now to the issue of overhead, Figure 10 shows the average number of requests issued by consumers (AN-<lb/>RIC) when attempting to get an item to consume. While Figure 10 shows the case where the consumers had 5 attempts to <lb/>get an item to process, the shape of the surface is virtually identical for any case where the number of attempts is greater <lb/>than two. The only difference between the cases is in the scaling of the Z axis. The greater the number of attempts that <lb/>consumers have, the higher the value associated with the peak in the graph. What Figure 10 shows is that increasing the <lb/>number of producers used to achieve a given level of production reduces the ANRIC. It is interesting to note that the re-<lb/>duction in ANRIC is greatest when the ratio of consumers to producers is greatest. As this ratio decreases, the reduction <lb/>in ANRIC due to increasing the number of producers decreases also. Furthermore, once the number of producers equals <lb/>or exceeds the number of consumers, the effect of increasing the number of producers is minimal. <lb/>Given an existing producer-consumer situation, what happens if we add another consumer? <lb/>The second series of simulations was designed to answer this question. In this simulation series, the level of con-<lb/>sumption was allowed to vary with the number of consumers in the simulation while the level of production was held con-<lb/>stant. Given this arrangement, we can break existing producer/consumer situations into two groups: those where the level <lb/>of consumption is less than the level of production, and those where the level of consumption is greater than or equal to <lb/>the level of production. For existing situations where the level of consumption is less than the level of production, intui-<lb/>tion says that adding a consumer should increase throughput with little effect upon overhead. For existing situations where <lb/>the level of consumption is greater than or equal to the level of production, adding another consumer should have no effect <lb/>upon throughput but it should increase overhead as the extra consumer competes with the existing consumers for the lim-<lb/>ited number of items available. As it turns out, intuition is indeed correct. <lb/>Breaking the existing situations into two cases was done to answer the question as asked. However, the results pre-<lb/>dicted for these two cases can be combined into a single set of results which is more general and easier to discuss. In gen-<lb/>eral, then, adding a consumer to an existing situation should increase the throughput with only a minimal increase in <lb/>overhead until the level of consumption exceeds the level of production, at which point adding another consumer only <lb/>serves to drive up overhead, not throughput. <lb/>Figure 11 shows the results of simulations in which the consumers had an unlimited number of attempts to obtain <lb/>an item to process. This figure demonstrates that actual throughput for the simulation increases as the number of consum-<lb/>ers increases, at least until some limit is reached. After that point, throughput is unaffected by further increases in the level <lb/>of consumption. <lb/>Figure 12 shows the average achieved throughput for the producers in the same simulations used to produce Fig-<lb/>ure 11. As we can see, Figure 12 looks almost identical to Figure 11. As consumers are added, the achieved throughput of <lb/>the producers increases until it reaches 100%. At this point, the producers have reached their maximum ability to produce <lb/></body>

			<page>12 <lb/></page>

			<body>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>11 <lb/>12 <lb/>13 <lb/>14 <lb/>15 <lb/>16 <lb/>Consumers <lb/>1 <lb/>4 <lb/>7 <lb/>10 <lb/>25 <lb/>99 <lb/>Producers <lb/>0 <lb/>5000 <lb/>10000 <lb/>15000 <lb/>20000 <lb/>25000 <lb/>30000 <lb/>35000 <lb/>40000 <lb/>45000 <lb/>Actual Throughput <lb/>Figure 11: Actual Throughput vs. consumers and producers for the case where consumption varies with the number of <lb/>consumers and consumers have unlimited attempts to obtain items to consume <lb/>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 <lb/>Consumers <lb/>1 <lb/>5 <lb/>9 <lb/>25 <lb/>Producers <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>90 <lb/>100 <lb/>Achieved Throughput <lb/>Figure 12: Average Achieved Throughput for Producers vs. consumers and producers for the case where consumption <lb/>varies with the number of consumers and consumers have unlimited attempts <lb/></body>

			<page>13 <lb/></page>

			<body>and everything produced is being consumed. Thus, actual throughput has reached its limit. Increasing consumption be-<lb/>yond this point can not result in any additional items being consumed. <lb/>Figure 13 shows the average number of attempts that consumers had to use to obtain an item to consume (AN-<lb/>RIC). Figure 13 demonstrates that, while the level of consumption is well below the level of production, there is very little <lb/>overhead. But, as the level of consumption begins to approach and then exceed the level of production, the overhead rises. <lb/>When the level of consumption exceeds the level of production, the overhead rises almost linearly with the increase in con-<lb/>sumption. <lb/>Given an existing producer/consumer situation, what happens if we add another producer? <lb/>We found this situation to be very similar to that of adding a consumer. As long as the level of production is less <lb/>than the level of consumption, adding a producer increases throughput and decreases overhead. But when the level of pro-<lb/>duction exceeds the level of consumption, only overhead is affected, and it continues to decrease. <lb/>What happens if the producers produce at different rates? <lb/>The fourth simulation series was designed to answer this question. In this simulation series, the level of produc-<lb/>tion was equal to the level of consumption, and the levels of consumption and production were the same in all simulations. <lb/>The number of consumers used to achieve the level of consumption was varied while the number of producers was held <lb/>constant at 7. Within a simulation, all of the consumers consumed at the same rate while the production rates of the pro-<lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>11 <lb/>12 <lb/>13 <lb/>14 <lb/>15 <lb/>16 <lb/>Consumers <lb/>1 <lb/>4 <lb/>7 <lb/>10 <lb/>25 <lb/>99 <lb/>Producers <lb/>1.00 <lb/>6.00 <lb/>11.00 <lb/>16.00 <lb/>21.00 <lb/>26.00 <lb/>31.00 <lb/>36.00 <lb/>ANRIC <lb/>Figure 13: ANRIC vs. consumers and producers for the case where consumption varies with the number of consumers <lb/>and consumers have unlimited attempts to obtain items to consume <lb/></body>

			<page>14 <lb/></page>

			<body>ducers were allowed to be different. Given this, intuition would suggest that throughput declines as the variance in the pro-<lb/>duction rates of the producers increases. Since the load imposed by the consumers is uniformly distributed over the pro-<lb/>ducers, the slower producers are not going to be able to keep up with their portion of the load. This results in the pools <lb/>associated with these slow producers being empty more often, which causes more waits. The additional waits cause the <lb/>consumers to lose more items to waiting, thereby reducing throughput. As far as overhead is concerned, one would expect <lb/>that increasing the number of attempts that consumers have to obtain items should reduce the impact of the variation in <lb/>production rates by allowing consumers to avoid long waits. Thus, as the variation in production rates increases, the con-<lb/>sumers will use a greater percentage of their allowed attempts in order to avoid long waits. This should result in an in-<lb/>crease in overhead. As it turns out, the intuitive explanation is generally correct. <lb/>Figure 14 represents the results of simulations where the consumers had only 1 attempt to obtain items to process. <lb/>Figure 14 shows that, as predicted, the throughput declines as the variation in production rates increases. Figure 15 depicts <lb/>the case where consumers had an unlimited number of attempts to obtain items to consume. Figure 15 shows that, even <lb/>with an unlimited number of attempts to obtain items, throughput still declines dramatically. However, since the through-<lb/>put does not decline as rapidly or to the same extent as in Figure 14, Figure 15 indicates that providing consumers with ex-<lb/>tra attempts reduces the decline in throughput. Figure 16 shows how the achieved throughput is affected by both the <lb/>variation in production rates and the number of attempts that consumers have to obtain items. This figure, which repre-<lb/>sents the case where there are 25 consumers, is representative of all the cases observed. It clearly shows that increasing the <lb/>0.050 <lb/>0.068 <lb/>0.120 <lb/>0.151 <lb/>0.188 <lb/>0.302 <lb/>0.659 <lb/>1.191 <lb/>1.681 <lb/>2.206 <lb/>2.560 <lb/>3.178 <lb/>4.043 <lb/>5.227 <lb/>7.806 <lb/>10.866 <lb/>Standard Deviation in Production Rates <lb/>1 <lb/>5 <lb/>9 <lb/>25 <lb/>Consumers <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>90 <lb/>Achieved Throughput <lb/>Figure 14: Achieved Throughput vs. consumers and standard deviation in production rates for the case where consumers <lb/>had only 1 attempt to obtain items to consume <lb/></body>

			<page>15 <lb/></page>

			<body>0.036 <lb/>0.055 <lb/>0.113 <lb/>0.145 <lb/>0.182 <lb/>0.292 <lb/>0.651 <lb/>1.191 <lb/>1.673 <lb/>2.197 <lb/>2.559 <lb/>3.192 <lb/>4.049 <lb/>5.240 <lb/>7.805 <lb/>10.806 <lb/>Standard Deviation in Production Rates <lb/>1 <lb/>6 <lb/>15 <lb/>99 <lb/>Consumers <lb/>60 <lb/>65 <lb/>70 <lb/>75 <lb/>80 <lb/>85 <lb/>90 <lb/>95 <lb/>100 <lb/>Achieved Throughput <lb/>Figure 15: Achieved Throughput vs. consumers and standard deviation in production rates for the case where consumers <lb/>had unlimited attempts to obtain items to consume <lb/>0.050 <lb/>0.068 <lb/>0.120 <lb/>0.151 <lb/>0.188 <lb/>0.302 <lb/>0.659 <lb/>1.191 <lb/>1.681 <lb/>2.206 <lb/>2.560 <lb/>3.178 <lb/>4.043 <lb/>5.227 <lb/>7.806 <lb/>10.866 <lb/>Standard Deviation in Production Rates <lb/>1 <lb/>4 <lb/>7 <lb/>infinite <lb/>Attempts <lb/>Allowed <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>90 <lb/>100 <lb/>Achieved Throughput <lb/>Figure 16: Achieved Throughput vs. attempts and standard deviation in production rates for the case where there are 25 <lb/>consumers <lb/></body>

			<page>16 <lb/></page>

			<body>number of attempts that consumers have can reduce the decline in throughput due to increases in the variation of the pro-<lb/>duction rates of the producers. <lb/>Figure 17 represents the average number of requests per item consumed (ANRIC) for the case where consumers <lb/>had an infinite number of attempts to obtain an item to consume. As expected, ANRIC increases as the variation in pro-<lb/>duction rates increases. <lb/>How many attempts should consumers have to obtain items? <lb/>Setting the number of attempts provides a mechanism for balancing the overhead of the distributed pool algorithm <lb/>against the throughput achieved. In general, we can improve throughput, but at the cost of higher overhead. <lb/>We found that setting the maximum number of attempts to 3 usually provided a good level of throughput, while <lb/>sharply limiting overhead. Setting the maximum number of attempts to 6 provides better throughput, and increasing the <lb/>number of attempts beyond 6 usually has almost no benefit. Furthermore, ANRIC is usually low even with a maximum <lb/>of 6 attempts. <lb/>We do not have space to show all of the data that we use to base our conclusions on, but we can show some exam-<lb/>ples. Figure 8 shows the typical case when the production rate equals the consumption rate. Figure 18 shows the typical <lb/>case when the production and consumption rates are different. Figure 16 shows the typical case when the production rates <lb/>of the individual producers is varied. In Figures 8 and 18, the achieved throughput is close to 100% for both 3 and 6 at-<lb/>0.036 <lb/>0.055 <lb/>0.113 <lb/>0.145 <lb/>0.182 <lb/>0.292 <lb/>0.651 <lb/>1.191 <lb/>1.673 <lb/>2.197 <lb/>2.559 <lb/>3.192 <lb/>4.049 <lb/>5.240 <lb/>7.805 <lb/>10.806 <lb/>Standard Deviation in Production Rates <lb/>1 <lb/>5 <lb/>9 <lb/>25 <lb/>Consumers <lb/>1 <lb/>6 <lb/>11 <lb/>16 <lb/>21 <lb/>26 <lb/>ANRIC <lb/>Figure 17: ANRIC vs. consumers and standard deviation in production rates for the case where consumers have unlimited <lb/>attempts to obtain items to consume <lb/></body>

			<page>17 <lb/></page>

			<body>tempts for most settings. In Figure 16, 6 attempts are required to achieve a good level of throughput when the variance in <lb/>the individual production rates becomes large. <lb/>What kind of throughput can be expected from this algorithm? <lb/>The answer to this question has already been presented, albeit in bits and pieces, in the answers to the previous <lb/>questions. Given this, only a brief summary will be provided here. <lb/>Under ideal conditions, the distributed pool algorithm is capable of near 100% achieved throughput performance. <lb/>Under less ideal conditions, 90% or greater achieved throughput can easily be obtained if the consumers have 6 or more at-<lb/>tempts to obtain items to consume and the variance in production rates of the producers is low. As the variance in the pro-<lb/>duction rates becomes high, performance of the algorithm deteriorates rapidly. While performance declines as the number <lb/>of attempts consumers have to obtain items drops below 6, it deteriorates markedly when consumers have less than 3 at-<lb/>tempts. Overall, then, as long as the consumers have 6 or more attempts to obtain items and the variance in the production <lb/>rates of the producers is low, excellent performance can be expected from this algorithm. <lb/>How much overhead does the algorithm require? <lb/>As with the previous question, the answer to this question has already been presented, in bits and pieces, as part of <lb/>the answers to earlier questions. To summarize briefly, the overhead required by the algorithm can be controlled by setting <lb/>the number of attempts that consumers have to obtain items to process. The number of attempts that consumers have is an <lb/>1 <lb/>4 <lb/>7 <lb/>10 <lb/>13 <lb/>16 <lb/>Consumers <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>50 <lb/>75 <lb/>99 <lb/>infinite <lb/>Attempts Allowed <lb/>60 <lb/>5060 <lb/>10060 <lb/>15060 <lb/>20060 <lb/>25060 <lb/>30060 <lb/>35060 <lb/>40060 <lb/>45060 <lb/>Actual Throughput <lb/>Figure 18: Actual Throughput vs. attempts and consumers for the case where consumption varies with the number of <lb/>consumers and there are only 25 producers <lb/></body>

			<page>18 <lb/></page>

			<body>absolute limit on how high the overhead can be, and the actual overhead will almost always be less. For most cases, giving <lb/>consumers 6 attempts to obtain items should yield excellent throughput with a worst case overhead of six requests per item <lb/>processed. Should this be too much overhead, the number of attempts that consumers have can be reduced, which will re-<lb/>duce the overhead, although there may be a reduction in throughput as well. In any case, though, the overhead of the algo-<lb/>rithm can easily be controlled. <lb/>Guidelines for the usage of the distributed pools algorithm <lb/>Based upon the simulation results presented in this paper, when using the distributed pool algorithm, the following <lb/>guidelines should be followed in order to obtain the best results: <lb/>• In order to avoid wasted capacity, the level of consumption should be approximately equal to the level <lb/>of production. <lb/>• For a given level of consumption, use as many consumers as possible. <lb/>• For a given level of production, the total number of producers used should be greater than 1 but less <lb/>than or equal to the total number of consumers used. <lb/>• If the variance in the production rates of the producers is small, then giving consumers 6 attempts to ob-<lb/>tain items to consume should result in good performance. If the variance in production rates is large, <lb/>then the number of attempts that consumers have must be increased. <lb/>If the aforementioned guidelines are followed, and if the variance in the production rates of the producers is low, <lb/>then the distributed pool algorithm should attain better than 90% achieved throughput. <lb/>CONCLUSIONS <lb/>The distributed pools algorithm is a simple, efficient solution to the distributed producer/consumer problem. Un-<lb/>der ideal conditions, it can achieve near 100% efficiency while keeping overhead low. Under more common conditions, it <lb/>can easily achieve more than 90% efficiency with low overhead. Its major failing is that it needs the production rates of <lb/>the producers to be reasonably similar. If the variance in the production rates of the producers becomes large, then per-<lb/>formance begins to decline quickly. <lb/>Directions for future work include: <lb/>• Evaluation of the effects of pool size, retry interval, and the variance in consumption rates of the con-<lb/>sumers on the algorithm&apos;s throughput, overhead, and fairness. <lb/>• Evaluation of assigning producers selection probabilities based upon their production rates. <lb/></body>

			<page>19 <lb/></page>

			<listBibl>REFERENCES <lb/>Andrews, G.R. &quot;Paradigms for Process Interaction in Distributed Programs.&quot; ACM Computing Surveys, 23, No. 1 (1991), 49-90. <lb/>Avery, P., C. Chegireddy, J. Brothers, T. Johnson, and A. Zion, &quot;A New Approach to Distributed Computing in High Energy <lb/>Physics.&quot; Proceedings of the 26th International Conference on High Energy Physics, 1992. <lb/>Finkel, R. and U. Manber. &quot;DIB -A Distributed Implementation of Backtracking.&quot; ACM Transactions on Programming <lb/>Languages and Systems. 9, No. 2 (1987), pp. 235-256. <lb/>Kotz, D. and C. S. Ellis. &quot;Evaluation of Concurrent Pools.&quot; Proceedings of the Third Annual Parallel Processing Symposium, pp. <lb/>378-385. <lb/>Lynch, N. A., and M. J. Fischer. &quot;On Describing the Behavior and Implementation of Distributed Systems.&quot; Theoretical <lb/>Computer Science, 13 (1981), pp. 17-43. <lb/>Manber, U. &quot;On Maintaining Dynamic Information in a Concurrent Environment.&quot; SIAM Journal on Computing, 15, No. 4 <lb/>(1986), pp. 1130-1142. <lb/>Rinaldo, F. J., and M. R. Fausey. &quot;Event Reconstruction in High-Energy Physics.&quot; Computer, June 1993, pp. 68-77 . <lb/>Zion, A. F. &quot;NetQueues: A Distributed Processing System for High-Energy Physics.&quot; Thesis, University of Florida, 1992. <lb/></listBibl>

			<page>20 </page>


	</text>
</tei>
