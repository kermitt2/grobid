<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Heliyon 10 (2024) e30798 <lb/>Available online 11 May 2024 <lb/>2405-8440/© 2024 The Authors. <lb/>Published by Elsevier Ltd. <lb/>This is an open access article under the CC BY-NC license <lb/>(http://creativecommons.org/licenses/by-nc/4.0/). <lb/>Research article <lb/>Image fusion using Y-net-based extractor and <lb/>global-local discriminator <lb/>Danqing Yang a , Naibo Zhu b, * , Xiaorui Wang a,** , Shuang Li b <lb/>a School of Optoelectronic Engineering, Xidian University, Xi&apos;an, 710071, China <lb/>b Research Institute of System Engineering, PLA Academy of Military Science, Beijing, 100091, China <lb/>A R T I C L E I N F O <lb/>Keywords: <lb/>Infrared and visible image fusion <lb/>Y-net <lb/>Multi-scale representation <lb/>Global-to-local detection <lb/>Contextual attention (CoA) <lb/>Generative adversarial network (GAN) <lb/>A B S T R A C T <lb/>Although some deep learning-based image fusion approaches have realized promising results, <lb/>how to extract information-rich features from different source images while preserving them in <lb/>the fused image with less distortions remains challenging issue that needs to be addressed. Here, <lb/>we propose a well worked-out GAN-based scheme with multi-scale feature extractor and global-<lb/>local discriminator for infrared and visible image fusion. We use Y-Net as the backbone archi-<lb/>tecture to design the generator network, and introduce the residual dense block (RDblock) to <lb/>yield more realistic fused images for infrared and visible images by learning discriminative multi-<lb/>scale representations that are closer to the essence of different modal images. During feature <lb/>reconstruction, the cross-modality shortcuts with contextual attention (CMSCA) are employed to <lb/>selectively aggregate features at different scales and different levels to construct information-rich <lb/>fused images with better visual effect. To ameliorate the information content of the fused image, <lb/>we not only constrain the structure and contrast information using structural similarity index, but <lb/>also evaluate the intensity and gradient similarities at both feature and image levels. Two global-<lb/>local discriminators that combine global GAN with PatchGAN as a unified architecture help to dig <lb/>for finer differences between the generated image and reference images, which force the gener-<lb/>ator to learn both the local radiation information and pervasive global details in two source <lb/>images. It is worth mentioning that image fusion is achieved during confrontation without fusion <lb/>rules. Lots of assessment tests demonstrate that the reported fusion scheme achieves superior <lb/>performance against state-of-the-art works in meaningful information preservation. <lb/></front>

			<body>1. Introduction <lb/>Practical applications such as video surveillance [1], vehicle night navigation [2] and fire rescue [3] often require a combination of <lb/>infrared and visible sensors to adequately express scene information to enhance human and robotic visual understanding. Never-<lb/>theless, information redundancy is also a ubiquitous problem in multi-source data. Proverbially, infrared images captured by sensors <lb/>that receive infrared wavelength information from object emission contain thermal radiation information characterized by significant <lb/>intensities, which can avoid visual identification obstacles caused by illumination changes or camouflage. However, infrared images <lb/>exhibit low-resolution and poor details, and are generally not convenient for human visual observation. In contrast, visible images are <lb/></body>

			<front>* Corresponding author. <lb/>** Corresponding author. <lb/>E-mail address: 1922550996@qq.com (D. Yang). <lb/>Contents lists available at ScienceDirect <lb/>Heliyon <lb/>journal homepage: www.cell.com/heliyon <lb/>https://doi.org/10.1016/j.heliyon.2024.e30798 <lb/>Received 9 February 2024; Received in revised form 27 April 2024; Accepted 6 May 2024 <lb/>i An update to this article is included at the end <lb/></front>

			<note place="headnote">Heliyon 10 (2024) e30798 <lb/></note>

			<page>2 <lb/></page>

			<body>presented by receiving visible light band information reflected by objects, so these high-definition images display rich texture details of <lb/>the object appearance represented by gradients, but they are susceptible to obstacles and environmental factors. Image fusion tech-<lb/>niques can integrate vital information from different modalities to reduce redundancy and are widely used in multifarious imaging <lb/>devices. <lb/>The taxing point of infrared and visible image fusion lies in extracting information-rich features at different scales from the different <lb/>modalities and generating a fused image that merges them to improve image aesthetics and understanding without introducing any <lb/>distortions/artifacts. Over the years, numerous solutions have been proposed to solve the above crucial aspect, including traditional <lb/>approaches and data-driven methods based on deep learning (DL) [4,5]. <lb/>According to different image processing methods, traditional approaches mainly include multi-scale transformation (MST), sparse <lb/>representation (SR), saliency-based methods, subspace projecting (SP), hybrid models, and others. MST is one of the common practices <lb/>in most traditional algorithms due to its flexibility and good visualization. Existing multi-scale techniques methods (including two-<lb/>scale) can preserve the details of the source images and circumvent spectral degradation to some extent. For example, image pyra-<lb/>mid transform [6], nonsubsampled contourlet transform [7], shearlet transform [8], guidance filter [9] and multiscale decomposition <lb/>[10] are commonly-used MST techniques in image fusion task. Nonetheless, fusion algorithms based on MST typically have limited <lb/>fusion performance for three reasons. First, this type of method, which blindly assembles transformations or representations of the <lb/>source images with some dedicated rules instead of learning them from source images, is bearing heavy computation burden. Second, <lb/>asymmetric feature information overlapping at multiple scales often results in halos and blurred edges (high-redundancy). Third, <lb/>detail loss is inevitable in the multiscale transform, manual fusing, and inverse transform processes. <lb/>Recently, DL has become a mainstream method in image fusion tasks. On the one hand, DL methods can produce more filters for <lb/>image feature extraction than that of traditional MST techniques. On the other hand, the parameters of filters in DL approaches can be <lb/>learned adaptively to achieve various image fusion subtasks. Feature extraction and fusion at different scales based on DL have <lb/>demonstrated their superiority in improving fusion performance. For example, off-the-shelf fusion models based on CNN back-<lb/>bonewhich show excellent fusion performance utilize multiple scales at the convolution kernels or feature levels to capture the <lb/>meaningful information of the different modal images. UNIFusion [11] utilized Ghost module instead of the classical convolution layer <lb/>to generate more feature maps. Fu [12] extracted the different level features via d dense connection operations. Zheng [13] achieved <lb/>feature extraction at different scales and levels using HINBlock. Reference [14-17] employed convolution kernels of different sizes to <lb/>extract common and unique features of source images. Reference [18-20] captured the multilevel features of the source images via <lb/>residual learning. Moreover, modern GAN-based approaches [21-30] exploit multi-granularity convolution kernels of the same feature <lb/>level, yielding different receptive fields and in turn improving fusion performance. For example, each network layer of the feature <lb/>extractor in Refs. [21-24] utilized convolution kernels of different sizes to extract useful information from source images. Li [25,26] <lb/>introduced multi-grained attention network to enable the fusion model to perceive the target region or detail information of the source <lb/>image from multiple scales. TC-GAN [27] built the generator using convolutional layers with different scale filters to generate the <lb/>combined texture maps in greater detail. Liu [28] proposed a contextualized dilated feature extraction module to obtain coarse-to-fine <lb/>features. In order to balance the feature extraction capability and the number of parameters of the fusion model, reference [29,30] <lb/>firstly utilized large-scale filters to expand the receptive field of the shallow network, and then used small-scale filters to further extract <lb/>the deep features. <lb/>Although the above successful cases have witnessed a great improvement in the fusion effect, there are still some drawbacks that <lb/>deserve to be emphasized. First of all, some fusion approaches do not fully extract the finer features and rich semantic information in <lb/>the source images due to lack of downsampling operation. Fused results are often contaminated by blurring effects. In addition, during <lb/>the reconstruction phase, CNN kernel-based multi-scale representation methods only utilize deep features and unreasonable fusion <lb/>strategies to reconstruct the fused images, which fails to render the resulting images photorealistic. The third item is that some loss <lb/>functions constrain the similarity between the generated image and reference images only at pixel domain, while neglecting the <lb/>improvement in perceptual quality of the fusion image. Last but not least, the discriminator has limited ability to guide and facilitate <lb/>the optimization of the generator during the adversarial process, as some GAN-based methods fail to simultaneously capture different <lb/>scale features from the multimodal images in a global-to-local manner. <lb/>Based on the above weak points, we focus on improving the information richness of fused images from two aspects: network <lb/>structure construction and loss function design. On the one hand, based on Y-Net, a generator network is constructed to directly down-<lb/>sample feature maps that have the same size as the inputs to obtain distinctive features at different scales, instead of using multiple <lb/>scales at the convolution kernels or feature levels. Moreover, to preserve finer complementary features, we design two discriminator <lb/>networks that combine global GAN with PatchGAN as a unified architecture to capture both local thermal radiation information and <lb/>holistic features in the source images. On the other hand, since similarity constraints based on objective evaluation index have a <lb/>comparatively limited ability to capture perception-related differences, we design a hybrid loss function based on both pixel and <lb/>feature domains to achieve the retention of vital information in the source images and the improvement of perceptual quality of the <lb/>fusion results. <lb/>In a nutshell, the main novelty of the work consists of the following four-fold. <lb/>(1) With Y-Net as the backbone, the generator capitalizes on residual dense block (RDblock) to extract shallow texture details and <lb/>deep target structures at different scales from the source images. <lb/>(2) The cross-modality shortcut with contextual attention (CMSCA) is devised to strengthen the discriminative encoding features at <lb/>different scales. By doing so, both shallow and deep enhancement features are used to maintain the saliency of the infrared <lb/>targets and preserve rich details. <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>3 <lb/></page>

			<body>(3) We innovatively combine global GAN with PatchGAN to construct dual discriminator, so as to fully consider the information <lb/>levels of the source images and enhance discriminative ability. <lb/>(4) A hybrid constraint is designed to guide the learning process of the proposed end-to-end fusion model from image and deep <lb/>feature domains, respectively. As a result, the proposed method improves the information richness of the fusion images. <lb/>2. Technical backgrounds <lb/>2.1. Deep learning-based image fusion methods <lb/>Recently, it is a tendency to build performance-efficient deep neural networks for various image fusion tasks due to their strong <lb/>nonlinear learning abilities. Learning-based fusion architectures, such as autoencoder (AE) [13,14,16,19], convolutional neural <lb/>network (CNN) [15,18,20] and generative adversarial network (GAN) [21,22,24,27,29] have witnessed obvious improvements in <lb/>fusion performance, but their single-scale frameworks can hardly capture the full-scale features of the real-world targets and fail to <lb/>make the fused images photorealistic. More importantly, most methods directly capitalize on the features extracted in the last layer to <lb/>reconstruct fused images, whereas earlier features do not. Consequently, some useful multi-layer information is lost in the deep <lb/>cascaded network, resulting in unfriendly visual perception. In addition, some non-end-to-end methods [11,15-17,27,28] generate <lb/>unsatisfied fusion results due to unreasonable fusion rules. To this end, in this work, we focus on developing more effective GAN <lb/>frameworks that explicitly deal with the scale-space problems faced by visible and infrared image fusion task in an end-to-end fashion. <lb/>2.2. U-net framework <lb/>U-Net is originally proposed for image segmentation tasks [31]. With the powerful multi-scale representation advantage, more and <lb/>more computer vision tasks are realized by using U-Net as the backbone network, such as image dehazing [32], salient object detection <lb/>[33], facial emotion recognition [34], image denoising [35], image fusion [13,36-39]. U-Net architecture adopts a symmetric <lb/>encoder-decoder manner that overcomes the disadvantages of local and global features loss in fully convolutional networks. In the <lb/>contraction path (encoder), the features at different scales are extracted from the source images through the downsampling operations, <lb/>and the resolution of the feature maps is gradually lessened. In the expand path (decoder), the image details are repaired by the <lb/>up-sampling operations and the reconstructed image is restored to the input size. Furthermore, the skip connections in the U-Net <lb/>architecture largely compensate for the information loss caused by the downsampling operation during fusion image restoration. <lb/>Although deep models based on U-Net have achieved remarkable performance in various application fields, there are inherent <lb/>limitations in several aspects. In image fusion, dual convolution operation in U-Net has limited feature extraction capabilities, making <lb/>it difficult to mine the intrinsic features of images with different modalities. Besides, existing U-Net often utilize simple skip con-<lb/>nections to transfer features from convolutional blocks to their corresponding deconvolutional blocks in an elementwise max/ <lb/>concatenation way. They aggregate features of different scales without considering their discriminative contextual information that <lb/>are crucial for infrared and visible image fusion task. <lb/>As an improved version of U-Net, Y-Net with two encoders and one decoder can capture deep discriminative features from different <lb/>source inputs. Therefore, we build the generator network based on the idea of Y-Net. Unlike previous architectures, the improvements <lb/>to our generator lie in: 1) since simply stacking convolutional blocks to build deep networks is difficult to obtain good fusion per-<lb/>formance, we introduce the dense-residual blocks (RDblocks) into Y-Net to enhance the abilities of feature extraction; 2) all the <lb/>features extracted by the scaled-down layers are aggregated to their corresponding deconvolutional layers via cross-modality shortcuts <lb/>with contextual attention (CMSCA), which can keep vital information in a fine-to-coarse manner for fusion image reconstruction and <lb/>make the proposed model easier to be trained. <lb/>Fig. 1. The blueprint of the proposed method for infrared and visible image fusion. <lb/>D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/>4 <lb/>3. Approach <lb/>3.1. Our motivation <lb/>Since no ground-truth image can serve as the optimization objective in infrared and visible image fusion task, the key point is to <lb/>design deeper networks to fully mine the meaningful information within the source images and selectively retain it in the fusion result. <lb/>Generally, the intensities of the dual source images at the same position often change significantly due to different imaging mecha-<lb/>nisms. So, many deep methods pursue visually better fusion results in a multi-scale manner. Unfortunately, they have limited fusion <lb/>performance for the following reasons. Above all, they all operate at the kernel level, such as convolution kernels of different sizes or <lb/>dilated convolutions, to obtain multi-resolution features regardless of the sampling operations. It leads to the failure to capture high-<lb/>level semantic features of the source images. Besides, existing GAN-based approaches classify features from the view of the overall <lb/>image, but neglect the local information across different patches, resulting in the appearance of artifacts in their fusion results. <lb/>For multimodal image fusion problems, it is inefficient to achieve salient performance gains by simply stacking more convolution <lb/>layers or building wider network layers. It is of great significance to design customized networks for fusion problems. Therefore, in this <lb/>paper, the GAN architecture is used to characterize the features of the original images from the multi-resolution perspective. Fig. 1 <lb/>shows the blueprint of the proposed method. Our entire model is composed of two functional modules, one generator based on Y-Net is <lb/>to learn a powerful feature extractor that can generate realistic fusion images guided by both image and feature-level loss functions, <lb/>and two discriminators aim to keep implicit details and enhanced radiative information of the source images from coarse to fine for <lb/>visual performance. Lots of assessment tests demonstrate that our method not only extracts information-rich multi-scale features with <lb/>low-redundancy from the two source images, but also ensures that they can be transferred to the fused images without loss of fidelity. <lb/>Simplistically, some symbols that appear frequently in this work are stated in advance as follows. IR represents the source infrared <lb/>images, and VI represents the source visible images. G stands for generator. © denotes the concatenation operation. ⊕ denotes the <lb/>element-wise add. ⊗ denotes the element-wise multiplication. EN_ir stands for the encoding path of the IR images, and EN_vi stands for <lb/>the encoding path of the VI images. RDblock represents the residual dense block. CMSCA indicates the cross-modality skip-connection <lb/>with the contextual attention, CoA indicates contextual attention, GL-Dvi stands for a discriminator that is used to capture more subtle <lb/>differences between the generated and VI images from global to local level. Similarly, GL-Dir stands for a discriminator that can <lb/>capture more subtle differences between the generated and IR images from global to local level. <lb/>3.2. Network architecture <lb/>3.2.1. Generator architecture <lb/>Unlike other image fusion subtasks, the fusion results for IR and VI can retain salient features of the input images only through the <lb/>deeper network to extract features containing more intrinsic information. In addition, for IR and VI with different modalities, there <lb/>may be differences in pixel intensities in certain regions such that information from different image scales cannot be ignored. Hence, it <lb/>is natural to think of employing Y-Net to achieve this fusion task. The Y-shaped structure fully takes into account the modal differences <lb/>and completely addresses the problem that features at different scales have no interaction in the previous multi-resolution repre-<lb/>sentation methods. Fig. 2 shows the architecture of the well-designed generator (G), and the architecture of the generator is shown in <lb/>Table 1. Clearly, G builds a simple Y-Net structure for multi-scale representation of the original images, which mainly consists of three <lb/>parts: encoder (EN_IR and EN_VI), cross-modality shortcuts with contextual attention (CMSCA) and decoder, three of which are <lb/>elaborated below. In general, the preservation of shallow features improves the image quality, while deeper features help maintain the <lb/>saliency of the thermal targets in the fused images. Therefore, the well-designed G not only extracts features at shallow and deep levels, <lb/>but also uses them to restore the fused image. <lb/>(1) Scale-down path <lb/>Fig. 2. The architecture of generator. The symbol EN represents the encoding module, DE represents the decoding module, Stem block represents <lb/>the convolution operation for extracting coarse features from source images. <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>5 <lb/></page>

			<body>Table 1 <lb/>The parameter settings for all layers of the generator. CB denotes convolution layer and activation function. IR_Pooling denotes average pooling <lb/>operation, VI_Pooling denotes max pooling operation. RDblock denotes the residual dense block. Deconv denotes the deconvolution operation. <lb/>ResBlock denotes the fusion image restore block. <lb/>Subnetwork <lb/>Layer <lb/>Input <lb/>Channel <lb/>Output Channel <lb/>Filter Size <lb/>Stride <lb/>Padding <lb/>Activation Function <lb/>Encoder_IR <lb/>CB1 <lb/>1 <lb/>16 <lb/>5 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>IR_Pooling1 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB2 <lb/>16 <lb/>32 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock1 <lb/>32 <lb/>32 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>IR_Pooling2 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB3 <lb/>32 <lb/>64 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock2 <lb/>64 <lb/>64 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>IR_Pooling3 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB4 <lb/>64 <lb/>128 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock3 <lb/>128 <lb/>128 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>IR_Pooling4 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB5 <lb/>128 <lb/>256 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock4 <lb/>256 <lb/>256 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>Encoder_VI <lb/>CB1 <lb/>1 <lb/>16 <lb/>5 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>VI_Pooling1 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB2 <lb/>16 <lb/>32 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock1 <lb/>32 <lb/>32 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>VI_Pooling2 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB3 <lb/>32 <lb/>64 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock2 <lb/>64 <lb/>64 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>VI_Pooling3 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB4 <lb/>64 <lb/>128 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock3 <lb/>128 <lb/>128 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>VI_Pooling4 <lb/>-<lb/>-<lb/>2 <lb/>2 <lb/>VALID <lb/>-<lb/>CB5 <lb/>128 <lb/>256 <lb/>3 <lb/>1 <lb/>SAME <lb/>ReLU <lb/>RDblock4 <lb/>256 <lb/>256 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>Decoder <lb/>Deconv1 <lb/>256 <lb/>128 <lb/>3 <lb/>2 <lb/>SAME <lb/>-<lb/>RDblock1 <lb/>128 <lb/>128 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>Deconv2 <lb/>128 <lb/>64 <lb/>3 <lb/>2 <lb/>SAME <lb/>-<lb/>RDblock2 <lb/>64 <lb/>64 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>Deconv3 <lb/>64 <lb/>32 <lb/>3 <lb/>2 <lb/>SAME <lb/>-<lb/>RDblock3 <lb/>32 <lb/>32 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>Deconv4 <lb/>32 <lb/>16 <lb/>3 <lb/>2 <lb/>SAME <lb/>-<lb/>RDblock4 <lb/>16 <lb/>16 <lb/>-<lb/>-<lb/>SAME <lb/>ReLU <lb/>ResBlock <lb/>16 <lb/>1 <lb/>1 <lb/>1 <lb/>SAME <lb/>Tanh <lb/>Fig. 3. Illustration of the encoding block in Fig. 2 a) the structure of encoding block; b) the structure of RDblock. <lb/>D. Yang et al. <lb/></body>

			<note place="headnote">Heliyon 10 (2024) e30798 <lb/></note>

			<page>6 <lb/></page>

			<body>Given that features from images with different modalities encode discriminative information for feature multi-scale representation, <lb/>it can help to mine more useful information in the original images. Thus, we use a two-stream network to encode IR and VI images, <lb/>respectively. Fig. 3 (a) shows the structure of the encoding block. In the encoding paths, the stem block (i.e., 5 × 5 convolution layer) of <lb/>the two encoders operates on a fine scale to extract features that have the same size as the source images, while later blocks transition <lb/>(through pooling) to coarse scales to extract high-level semantic information. Both scale features are required, but occur at different <lb/>positions in the EN_ir and EN_vi. Considering the computational complexity and time efficiency comprehensively, we consecutively <lb/>downsample the two source images for four times (the resolution of the feature map is halved at each downsampling) to achieve <lb/>feature extraction at different scales and levels. It is public knowledge that average pooling can preserve low-frequency information, <lb/>while max pooling helps keep high-frequency information. Therefore, we utilize max pooling operation for downsampling in EN_vi and <lb/>average pooling for downsampling in EN_ir. In addition, a convolution block (convolution layer and ReLU activation function) is <lb/>adopted to guarantee that the number of channels is doubled with the increasement of the hierarchy. The problem of the original <lb/>continuous convolutional encoding network is that the early texture details are lost. In addition, the batch normalization (BN) is <lb/>abandoned, resulting in unstable training and low convergence efficiency. Residual connections and dense networks are the basic ideas <lb/>of many deep frameworks to enhance the abilities of feature extraction by increasing the depth and width of the network. Therefore, <lb/>the residual dense blocks (RDblocks) are introduced into the encoding blocks to fully extract more shallow details and deep semantic <lb/>information, so as to obtain richer image representations. Fig. 3 (b) shows the structure of the RDblock. It is pretty easy to note that the <lb/>dense network consists of a 5 × 5 convolution block (5*5Conv + BN + ReLU) that is used to increase the receptive field, and two 3× 3 <lb/>convolution blocks (3*3Conv + BN + ReLU) that are adopted to extract deep features. Finally, a 1 × 1 convolution block (1*1Conv + <lb/>BN + ReLU) is used to adjust the output of the dense network to satisfy the residual connection. These RDblocks make the proposed <lb/>model easier to train and accelerate convergence. <lb/>(2) Scale-up path <lb/>During decoding, if the high-level features can be fully employed and transferred to the low-level features, it will help to produce <lb/>satisfactory fusion results. On the one hand, the high-level features of both encoders provide rich semantic structure information that <lb/>can be used to highlight the targets. On the other hand, the low-level features of both encoders represent rich texture details that help <lb/>to improve the aesthetics of the fused images. Better fused images can be obtained by combining global semantic and detail infor-<lb/>mation. However, if they are combined directly without considering their differences and global contextual information, the two <lb/>features will lack interaction, making it difficult to restore the desired fusion results in the decoder. To this end, the decoder directly <lb/>deconvolve the previously extracted high-level feature maps which combine different modal features with the output of the previous <lb/>decoded block via cross-modality shortcuts with contextual attention (CMSCA) until the input resolution is restored. Fig. 4 (a) shows <lb/>the structure of the decoding block, which enables feature integration across different scales and levels to reconstruct the fusion results. <lb/>One can see that features from the two encoding blocks at the corresponding scale are added after enhancing by contextual attention <lb/>(CoA) and then cascaded with the output of the previous decoding blocks to obtain the aggregated features. Subsequently, decon-<lb/>volution operation is used to upsample the aggregated features. After four times of upsampling, the fused images are reconstructed <lb/>through the restore block as shown in Fig. 4 (b). <lb/>(3) cross-modality shortcuts with contextual attention (CMSCA) <lb/>During feature extraction, details are inevitably lost after multiple downsampling operations of the encoders. Additionally, <lb/>deconvolution can only recover the structural details of the source images from the encoded features. In other words, the outputs of the <lb/>Fig. 4. Illustration of the decoding path in Fig. 2 a) the structure of decoding block; b) the structure of restore block. <lb/>D. Yang et al. <lb/></body>

			<note place="headnote">Heliyon 10 (2024) e30798 <lb/></note>

			<page>7 <lb/></page>

			<body>decoder are the combined results of the source images, which leads to degraded fusion performance. More importantly, if the extracted <lb/>multi-scale features are exploited in an incomplete or redundant way, the reconstruction of the information-rich fused image will be <lb/>interfered. Therefore, we capitalize on skip connections to aggregate the same-scale features of both encoders into their corresponding <lb/>upsampling blocks in an attentionally enhanced manner, thus reusing the earlier encoded feature maps and improving the recon-<lb/>struction ability of the decoder. Reference [40], we employ contextual attention (CoA) that unifies both contextual information mining <lb/>among features and self-attention learning over feature maps in a single architecture with favorable parameter budget to enhance the <lb/>representative capacity of the discriminative features extracted from the both encoders. The structure of the CoA is shown in Fig. 5. <lb/>CoA first captures the static contextual information among features via a 3 × 3 convolution operation. Then, two consecutive 1× 1 <lb/>convolution operations are applied to conduct self-attention learning based on the input and contextualized features, yielding dynamic <lb/>contextual information. On the other branch, a 1 × 1 convolution is used to capture global information of the input features, which is <lb/>multiplied with the dynamic contextual information to obtain the global dynamic contextual information. Finally, the static and global <lb/>dynamic contextual information are fused as the outputs. <lb/>CMSCA reinforces the feature interactions between the up-and down-sampled blocks in the corresponding phase by performing <lb/>pixel/region adaptive selection and learning based on feature-level attention. Compared to manual rules, learnable feature aggregation <lb/>strategies not only enable complex multi-resolution feature extraction networks have human-like attention perception, but also avoid <lb/>the information loss caused by successive convolutions and sampling, so that the fused results show competitive brightness and <lb/>contrast. <lb/>Summarily, G is constructed based on Y-Net that combines the superiorities of attention mechanism and residual dense network. <lb/>Extensive ablation studies demonstrate that our designed fusion image generation network achieves a good balance in terms of <lb/>computational load, training speed, and feature extraction. <lb/>3.2.2. Discriminator architecture <lb/>Considering the types of dominant and secondary information contained in the source images, we construct two discriminators (GL-<lb/>Dvi and GL-Dir) based on VGG16 to make the generated images more realistic with the gambling of the G and the GL-Dvi/GL-Dir. It is <lb/>worth emphasizing that the newly designed single discriminator is able to mine both the holistic details and local radiative information <lb/>within the source images and classify features by integrating global GAN and PatchGAN into a unified architecture. The branch of the <lb/>global discriminator (i.e., global GAN) forces the fused image to learn the holistic distribution and feature of the source images, while <lb/>the branch of the local discriminator (i.e., PatchGAN) focuses more on the degree of local information preservation. GL-Dvi and GL-Dir <lb/>have the identical network structure but do not share training parameters. Fig. 6 shows the network structure of the designed <lb/>discriminator, and the architecture of the global-local discriminator is shown in Table 2. The whole network consists of five convo-<lb/>lution blocks, and each block is composed of two convolution operations, two batch normalization (BN) operations, two Leak ReLU <lb/>functions, and one pooling operation. The latter part of the discriminator utilizes a split path with two separate confronted games to <lb/>capture both holistic and local features in inputs. Concretely, the global path ends up with a fully connected layer to distinguish the <lb/>whole fused image from the reference images, similar to global GAN. The local path, composed of a 1 × 1 convolution operation and a <lb/>Tanh activation function, maps the input image to a matrix representing the probability of each true patch, similar to the PatchGAN. <lb/>The final output of the local path is obtained by averaging over all probability values. <lb/>3.3. Loss functions <lb/>3.3.1. Discriminator&apos; loss functions <lb/>GL-Dvi and GL-Dir are trained to distinguish fused image from the reference images, and their loss functions are formulated based <lb/>on least squares GAN [21] as follows: <lb/>L GP-Dvi = <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D vi (VI local ) -a <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D vi <lb/>( <lb/>VI global <lb/>) <lb/>-a <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D vi (F local ) -b <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D vi <lb/>( <lb/>F global <lb/>) <lb/>-b <lb/>) 2 <lb/>(1) <lb/>Fig. 5. Illustration of the CoA in Fig. 4. <lb/>D. Yang et al. <lb/></body>

			<note place="headnote">Heliyon 10 (2024) e30798 <lb/></note>

			<page>8 <lb/></page>

			<body>L GP-Dir = <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D ir (IR local ) -a <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D ir <lb/>( <lb/>IR global <lb/>) <lb/>-a <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D ir (F local ) -b <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D ir <lb/>( <lb/>F global <lb/>) <lb/>-b <lb/>) 2 <lb/>(2) <lb/>where L GP-Dvi stands for the loss function of the visible global-local discriminator, and L GP-Dir stands for the loss function of the visible <lb/>global-local discriminator. N represents the number of the training data, GP D vi ( •) and GP D ir ( •) denote the discriminant results, <lb/>VI local and IR local indicate the patches of VI and IR images, VI global and IR global indicate the whole VI and IR images, F local indicates the <lb/>patches of generated image, and F global indicates the whole generated image. The symbols a and b are soft labels. <lb/>3.3.2. Generator&apos; loss functions <lb/>Previous fusion methods focus too much on the richness of the information, resulting in fused images that look like neutral results of <lb/>IR and VI images, or distorted VI images, or sharpened IR images. In view of that, we design a hybrid loss function based on feature and <lb/>pixel domains, which aims to improve both the information content and the perceptual quality of the fused images. <lb/>In order to keep more image content, the optimization of G considers both the guidance of deep feature and original image domains <lb/>[18]. So, we define the similarities of the gradients and intensities at the image level as follows: <lb/>L pixel <lb/>content = <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>(Int(F) -Int(IR)) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>(Grad(F) -Grad(VI)) 2 <lb/>(3) <lb/>where Int( •) represents the intensity operation, which is calculated using the mean filter. Grad( •) represents the gradient operation <lb/>performed using the Sobel operator. F, IR, and VI indicate the generated, IR, and VI images, respectively. L pixel <lb/>content helps fused image <lb/>preserves the intrinsic properties of images with different modalities. <lb/>Additionally, in the deep feature domain, the similarity constraint between the generated image and source images is formulated as <lb/>follows: <lb/>Fig. 6. The architecture of the global-local discriminator. Discriminator includes five convolution blocks, and each convolution block contains 2 <lb/>convolution layers, 2 batch normalization layers, 2 leaky ReLU activation layers, and 1 pooling layer. At the end of the discriminator, a branch <lb/>containing 1 convolution layer and Tanh activation layer outputs the local discriminant results. And the other branch outputs the global <lb/>discriminant results via fully connected layer. <lb/>Table 2 <lb/>The parameter settings for all layers of the global-local discriminator. Conv denotes the convolution block (convolution layer + batch normalization <lb/>+ activation function). <lb/>Layer <lb/>Input Channel <lb/>Output Channel <lb/>Filter Size <lb/>Stride <lb/>Padding <lb/>Activation Function <lb/>Conv1 <lb/>1 <lb/>16 <lb/>3 <lb/>1 <lb/>SAME <lb/>LeakyReLU <lb/>Conv2 <lb/>16 <lb/>16 <lb/>3 <lb/>2 <lb/>SAME <lb/>LeakyReLU <lb/>Conv3 <lb/>16 <lb/>32 <lb/>3 <lb/>1 <lb/>SAME <lb/>LeakyReLU <lb/>Conv4 <lb/>32 <lb/>32 <lb/>3 <lb/>2 <lb/>SAME <lb/>LeakyReLU <lb/>Conv5 <lb/>32 <lb/>64 <lb/>3 <lb/>1 <lb/>SAME <lb/>LeakyReLU <lb/>Conv6 <lb/>64 <lb/>64 <lb/>3 <lb/>2 <lb/>SAME <lb/>LeakyReLU <lb/>Conv7 <lb/>64 <lb/>128 <lb/>3 <lb/>1 <lb/>SAME <lb/>LeakyReLU <lb/>Conv8 <lb/>128 <lb/>128 <lb/>3 <lb/>2 <lb/>SAME <lb/>LeakyReLU <lb/>Conv9 <lb/>128 <lb/>256 <lb/>3 <lb/>1 <lb/>SAME <lb/>LeakyReLU <lb/>Conv10 <lb/>256 <lb/>256 <lb/>3 <lb/>2 <lb/>SAME <lb/>LeakyReLU <lb/>Branch1 <lb/>256 <lb/>1 <lb/>1 <lb/>1 <lb/>VALID <lb/>Sigmoid <lb/>Branch2 <lb/>2 × 2 × 256 <lb/>1 <lb/>-<lb/>-<lb/>-<lb/>-<lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>9 <lb/></page>

			<body>L feature <lb/>content = λ • <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>(φ(F) -φ(IR)) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>(Grad(φ(F)) -Grad(φ(VI))) 2 <lb/>(4) <lb/>where φ( •) stands for the feature maps extracted from the 9th, 21st and 27th convolution layers of the GL-Dvi and GL-Dir. L <lb/>feature <lb/>content can <lb/>represent the intrinsic information by combining shallow and deep features. λ is used to balance between the two terms and is fixed to <lb/>20 according to the experimental effects. <lb/>Image histogram [12] is also a common image comparison tool that reflects the statistical characteristics of image pixel values. In <lb/>general, two images can be considered to be somehow identical if their histograms are extremely similar. The histogram similarity <lb/>constraint for two images is expressed as follows: <lb/>L hist = <lb/>1 <lb/>255 <lb/>( <lb/>‖hist(F) -hist(I vi )‖ 2 <lb/>2 + ‖hist(F) -hist(I ir )‖ 2 <lb/>2 <lb/>) <lb/>(5) <lb/>where hist( •) represents the histogram of the input images. <lb/>We also expect the generated image to share more structural similarities with the two source images. From this, the structural <lb/>similarity constraint [29] is introduced and formulated as follows: <lb/>L SSIM = 1 -<lb/>SSIM(F, VI) + SSIM(F, IR) <lb/>2 <lb/>(6) <lb/>where SSIM( •) stands for the structural similarity measure between the fused image and the two source images. <lb/>During adversarial process, G expects the GL-Dvi and GL-Dir to judge the generated image as the true data [21]. Hence, the <lb/>adversarial loss can provide additional information complement and is defined as follows: <lb/>L adv = <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D ir (F local ) -a <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP Dir (Fglobal) -a <lb/>) 2 <lb/>+ <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D vi (F local ) -a <lb/>) 2 + <lb/>1 <lb/>N <lb/>∑ N <lb/>i=1 <lb/>( <lb/>GP D vi <lb/>( <lb/>F global <lb/>) <lb/>-a <lb/>) 2 <lb/>(7) <lb/>Summarily, the total loss of G is expressed as follows: <lb/>L G = α • L pixel <lb/>content + β • L feature <lb/>content + L SSIM + γ • L adv + L hist <lb/>(8) <lb/>where α, β and γ are all hyperparameters, which are experimentally fixed as 18, 19, and 10, respectively. <lb/>4. Experimental verification <lb/>4.1. Experimental details <lb/>(1) Datasets <lb/>We selected 55 pairs of IR and VI images from the TNO dataset as the samples to train our model. However, this a little bit of data is <lb/>insufficient to train a deep network model well. We adopt the measures of the non-overlapping cropping to cut each original pair of <lb/>images into 88 × 88 patches with step size of 14 to extend the training dataset. <lb/>During testing, the TNO and RoadScene datasets are used to validate the fusion effect of the proposed method. It is worth to note <lb/>that all image pairs used in this paper are aligned and grayed with high quality. <lb/>(2) Training details <lb/>During training stage, the batch size is 30, the training epoch is 20, and the learning rate is fixed at 1e-4. We initially train the two <lb/>discriminators using Adam three times before alternating the training of G and the dual discriminator once per batch. The detailed <lb/>training procedure is shown in Algorithm 1 below. At the test stage, only the generator remains. The complete pairs of test images are <lb/>fed sequentially into the well-trained generator to produce satisfactory fusion results. All experiments, including ours, are programmed <lb/>on TensorFlow and implemented on a computer configured with GPU NVIDIA GeForce RTX 3070 for fair comparison. <lb/>Algorithm 1: our model&apos;s training details <lb/>Inputs: infrared image and visible image <lb/>Output: fused image <lb/>1 for i in range maximum epoch do <lb/>2 <lb/>for t times do <lb/>3 <lb/>Select m visible and infrared image patches from training dataset; <lb/>4 <lb/>Select m fused image patches from generated set; <lb/>5 <lb/>Update global-local discriminator1 using the Adam according to Eq. (1); <lb/>6 <lb/>Update global-local discriminator2 using the Adam according to Eq. (2); <lb/>(continued on next page) <lb/>D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/>10 <lb/>(continued ) <lb/>7 <lb/>End <lb/>8 <lb/>Select m visible and infrared from training dataset; <lb/>9 <lb/>Update generator using the according to Eq. (8); <lb/>10 end <lb/>(3) Comparison methods <lb/>The proposed method adopts the idea of multi-scale representation in GAN architecture to fully extract the key information <lb/>contained in the source images and selectively preserve it in the fusion images. Herein, nine representative fusion methods based on <lb/>decomposition strategy are chosen as the baseline methods for comparison with our algorithm, including LP [41], RP [42], CVT [43], <lb/>MSVD [44], DCHWT [45], MDLatLRR [46], Dualbranch [12], CUFD [14], GANFM [47]. U2Fusion [48] inspires us to assess the <lb/>similarities between the resultant image and the two source images in deep feature domain. CSF [49] preserves valuable features by <lb/>assessing the importance of features in a deep learning way, while our approach directly leverages relatively simple contextual <lb/>attention to selectively aggregate important features for fused image reconstruction. It is therefore worth comparing the fusion effects <lb/>of the two approaches. PMGI [50] is a unified image fusion method, which has been referred to by many works. FusionGAN [21] and <lb/>GANMcC [24] are the typical GAN-based comparison methods. <lb/>(4) Objective assessment metrics <lb/>As we all know, it is a reasonable option to utilize a multi-metric evaluation system to comprehensively assess the fusion results. <lb/>Given that the original intention of our method tends to improve both the information richness and visual perceptual of the resultant <lb/>images, the following five metrics are selected to assess the fusion results. (1) Mutual information (MI) [51]: MI is used to assess the <lb/>amount of information transferred from the two source images to the resultant image. The larger the MI, the more information the <lb/>resultant image contains about the source images. (2) Visual information fidelity (VIF) [52]: VIF conforms to the human visual system <lb/>and is used to evaluate the fidelity of information of the fused images. The higher the VIF is, the better performance the fusion method <lb/>has. (3) Standard deviation (SD) [53]: SD reflects the contrast and distribution characteristics of the fused images. The larger the SD, <lb/>the better the visual quality of the fused images. (4) Petrovic metric parameter (Nabf) [54]: Nabf represents the ratio of noise to <lb/>Fig. 7. Visual examples of different methods on image pairs in the TNO dataset. From top to bottom: infrared image, visible image, results of LP, RP, <lb/>CVT, MSVD, DCHWT, MDLatLRR, CSF, Dualbranch, CUFD, PMGI, U2Fusion, FusionGAN, GANMcC, GANFM and our method. For clear comparison, <lb/>we select two small regions (i.e., the red and blue boxes) in each image, and then zoom in it and put it in the bottom corner. <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>11 <lb/></page>

			<body>artifacts in the resultant results. The smaller the Nabf, the less noise and artifacts occur in the resultant images. (5) FMI_dct [55]: <lb/>FMI_dct computes mutual information for the discrete cosine features. The larger the FMI_dct is, the better the fusion method per-<lb/>formance has. <lb/>4.2. Comparison experiments <lb/>(1) Comparison results on the TNO dataset <lb/>Fig. 7 shows the fused results of men_in_front_of_house image in the TNO dataset. IR image provides thermal targets (such as two <lb/>pedestrians) and clear structural features (such as protuberant branches and wall in the background), while texture details and better <lb/>visual effect are reflected by VI image. Therefore, the combined results of the two source images will be biased towards the IR image <lb/>and well fit the visual perception simultaneously. In each image, the thermal target is highlighted by a red rectangular region, while a <lb/>blue box is used to frame out the background area. They were subsequently scaled for easier observation. Obviously, LP, RP, CVT, <lb/>MSVD, DCHWT, CSF, and Dualbranch produce visually unfriendly blur effects and background noise/artifacts due to severe infor-<lb/>mation loss during fusion. PMGI and U2Fusion methods, which incline to VI images, have lower brightness due to detail information <lb/>loss and noise interference. MDLatLRR suffers from distortions and noise when restoring fused image, resulting in unnatural visual <lb/>effects. FusionGAN overly inclined to IR image generate fused images with blurry visual perception because of the presence of noise/ <lb/>artifacts. GANMcC has higher contrast than other approaches, including ours, but the thermal targets are still somewhat blurred due to <lb/>information loss. Although CUFD and GANFM perform best in maintaining the image contrast and thermal target saliency, rich levels <lb/>of structural details are diluted by the VI images. Our method maintains both the saliency of the hot targets and the naturalness of the <lb/>fusion image without losing the hierarchy of the background trees and introducing any unnecessary artifacts, while other comparison <lb/>algorithms restore blurred targets, or introduce coarse edges along the wall, or make the branches present as though they are on a flat <lb/>surface. Overall, our method not only maintains high brightness for objects that are skewed toward the IR images, but also conforms to <lb/>human perceptual with less distortions. In other words, the proposed approach performs best than other competitors in preserving the <lb/>targets saliency and realistic texture details. <lb/>The above intuitive analysis from the perspective of information richness of the fused images makes it difficult to say the best or <lb/>worst fusion performance in a direct manner. Therefore, it is necessary to adopt some objective image evaluation metrics for further <lb/>quantitative analysis. Table 3 shows the objective evaluation results of the above methods over the five metrics. Obviously, our metric <lb/>ranks first in terms of MI, VIF, Nabf, and FMI_dct. The presence of multi-scale representation of source images allows the MI and <lb/>FMI_dct metrics of the proposed method to surpass other competitors. In other words, our method extracts the maximum useful in-<lb/>formation (i.e., intensity feature of infrared image and gradient feature of visible image) from the different source images and transfers <lb/>it to the resultant images. The largest VIF metric shows that our results suffer from less distortions due to the cross-scale feature <lb/>aggregation submodule with contextual attention. This also means that the proposed method can enhance the visual perception <lb/>features of visible image. The smallest Nabf indicates that our fused images suffer from less noise and artifacts. Our method assumes <lb/>that intensity information exists in IR images while VI images convey gradient information. Therefore, a relatively simple content <lb/>constraint is adopted to preserve the crucial information in the two source images and improve the image aesthetics rather than the <lb/>maximum or complementary information constraints, which leads to a reduction in the contrast features of the resultant images. Thus, <lb/>it is forgivable that the SD index is in the middle. In conclusion, our method can fully excavate meaningful information in source <lb/>images and integrate it into the fusion images with the help of Y-Net-based generator and the image-feature domain-based loss function <lb/>for cooperative guidance. As a result, the fusion results generated by our method improve the information content of the fused image. <lb/>(2) Comparison results on the RoadScene dataset <lb/>Table 3 <lb/>The average of the five metrics among all algorithms on the TNO dataset (Bold: optimal). <lb/>Method <lb/>MI <lb/>VIF <lb/>SD <lb/>Nabf <lb/>FMI_dct <lb/>LP <lb/>1.4820 <lb/>0.6871 <lb/>8.3885 <lb/>0.1165 <lb/>0.2846 <lb/>RP <lb/>1.3782 <lb/>0.6181 <lb/>8.3375 <lb/>0.1812 <lb/>0.2419 <lb/>CVT <lb/>1.3905 <lb/>0.5220 <lb/>8.2301 <lb/>0.1446 <lb/>0.3755 <lb/>MSVD <lb/>1.5213 <lb/>0.5389 <lb/>7.9573 <lb/>0.1121 <lb/>0.2311 <lb/>DCHWT <lb/>1.4478 <lb/>0.5111 <lb/>7.9025 <lb/>0.0619 <lb/>0.3461 <lb/>MDLatLRR <lb/>1.3199 <lb/>0.5929 <lb/>8.8746 <lb/>0.5069 <lb/>0.3553 <lb/>CSF <lb/>1.6309 <lb/>0.5952 <lb/>8.3559 <lb/>0.0729 <lb/>0.2491 <lb/>Dualbranch <lb/>1.6374 <lb/>0.5490 <lb/>7.9859 <lb/>0.1921 <lb/>0.2961 <lb/>CUFD <lb/>3.0039 <lb/>0.8156 <lb/>9.3869 <lb/>0.1650 <lb/>0.1822 <lb/>PMGI <lb/>2.0623 <lb/>0.6769 <lb/>9.3248 <lb/>0.1021 <lb/>0.3672 <lb/>U2Fusion <lb/>1.5206 <lb/>0.5945 <lb/>8.8094 <lb/>0.2700 <lb/>0.3276 <lb/>FusionGAN <lb/>1.7728 <lb/>0.5643 <lb/>9.0638 <lb/>0.0675 <lb/>0.3815 <lb/>GANMcC <lb/>1.9488 <lb/>0.6247 <lb/>9.5547 <lb/>0.0773 <lb/>0.3363 <lb/>GANFM <lb/>2.5939 <lb/>0.8335 <lb/>9.5035 <lb/>0.2294 <lb/>0.3275 <lb/>Ours <lb/>3.4250 <lb/>1.0317 <lb/>8.6964 <lb/>0.0576 <lb/>0.3834 <lb/>D. Yang et al. <lb/></body>

			<note place="headnote">Heliyon 10 (2024) e30798 <lb/></note>

			<page>12 <lb/></page>

			<body>Fig. 8 shows the visualization results of the proposed approach compared to other alternatives on the RoadScene benchmark <lb/>dataset. Pedestrians are bright thermal targets in the IR image, and signal poles, houses, trees are the background details in the VI <lb/>image. Some distinct regions (pedestrians and signal poles) in the image are labeled with a red box and enlarged to clearly observe. On <lb/>can see that only MDLatLRR and our method highlight the pedestrian targets, while the others reduce the brightness. In terms of <lb/>texture detail recovery, almost all the fusion images produced by the comparison algorithms produce an unnatural visual perception <lb/>due to the introduction of noise/artifacts (LP, RP, MDLatLRR, U2Fusion), or display similarly distorted VI images (CVT, MSVD, <lb/>DCHWT, Dualbranch, FusionGAN), or look like neither VI nor IR images because of the pixel intensity changes so much (CSF, CUFD, <lb/>PMGI, GANMcC, GANFM). On the contrary, our fusion image not only maintains the brightness of the thermal objects, but also restores <lb/>most of the information with less distortions. <lb/>The calculation results of the five metrics of fusion results with different methods are shown in Table 4. One can notice that our <lb/>method ranks first in MI, VIFF, and FMI_dct metrics and third in SD metric. The largest MI and FMI_dct demonstrate that the proposed <lb/>method transfers the most amount of information from the two source images (i.e., intensity feature of infrared image and gradient <lb/>feature of visible image) to the fusion image. That is, our fusion results contain the richest amount of information. The optimal VIF <lb/>reflects that our results are more in line with human visual perception compared to other methods. Although the proposed method <lb/>trails CUFD and GANFM by a narrow margin in the SD measurement, our method does not lose out to them in the overall effect. This is <lb/>because the proposed method transfers and preserves more effective information from the two source images into the fusion image and <lb/>the appearance is less distorted than that of other methods. For the Nabf metric, the proposed method ranks in the middle place, which <lb/>is justifiable. The subtle differences in fusion performance between the TNO and RoadScene datasets can be attributed to their large <lb/>discrepancy in scenarios and image contents. <lb/>4.3. Ablation study <lb/>In this section, we conduct ablation studies on six closely-related modules for image generation tasks to validate the superiority of <lb/>the well-designed fusion model. <lb/>(1) Study on the number of DenseNet layers <lb/>To balance the computational burden and fusion performance, we first study the number of DenseNet layers in RDblock. The <lb/>optimal DenseNet layers can be confirmed by observing the fusion effect against the layer number both in qualitative (the second row <lb/>of Fig. 9) and quantitative results (Table 5). Obviously, the dense net with three convolution layers can preserves finer structural <lb/>details of the bench in the scene. <lb/>Fig. 8. Visual examples of different methods on image pairs in the RoadScene dataset. From top to bottom: infrared image, visible image, results of <lb/>LP, RP, CVT, MSVD, DCHWT, MDLatLRR, CSF, Dualbranch, CUFD, PMGI, U2Fusion, FusionGAN, GANMcC, GANFM and our method. For clear <lb/>comparison, we select two small regions (i.e., the red and yellow boxes) in each image, and then zoom in it. <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>13 <lb/></page>

			<body>(2) Study on the number of RDblocks <lb/>In general, the deeper the network, the easier it is to extract features that are close to the essence of the source images. Meanwhile, <lb/>the training will be longer. Therefore, we also investigate the impact of the number of RDblocks on the fusion performance to <lb/>determine the optimal number of RDblocks, so as to equilibrate the training efficiency and fusion performance. The third row of Fig. 9 <lb/>shows visual examples against the RDblock number. Clearly, the use of two RDblocks for each scale feature extraction module leads to <lb/>blurred fused image compared to our method that employs one RDblock. Worse still, fused image fails to be generated when three <lb/>RDblocks are employed in each sampling module. The evaluation results in Table 5 further prove the above statement objectively. <lb/>(3) Study on the cross-modality shortcuts with contextual attention (CMSCA) <lb/>Since the skip connections designed in the previous fusion model integrate the features of different scales without considering their <lb/>discriminative context information, we elaborate the cross-modality shortcuts with contextual attention (CMSCA) to transfer features <lb/>from convolutional blocks to their corresponding deconvolution blocks. To demonstrate the effectiveness of CMSCA, we drop <lb/>contextual attention (i.e., using only the concatenation aggregation rule), but all other settings remain unchanged. According to visual <lb/>examples shown in the fourth row of Fig. 9, the concatenation operation slightly weakens the brightness of the IR targets and leads to a <lb/>reduction in perceptual quality because of insufficient feature integration. Objective assessment results in Table 5 give further <lb/>convincing results. This indicates that it is efficient to aggregate features at different scales via CMSCA. <lb/>(4) Study on the number of downsampling operations <lb/>It is a well-known phenomenon that downsampling IR images introduces noise, while downsampling VI images loses texture in-<lb/>formation. To strike the balanced tradeoff between computation/fusion performance, we further study the number of downsampling <lb/>operations. Concretely, we downsample two source images 2, 3, 4, 5 times, so as to determine the optimal number of downsampling <lb/>based on the qualitative and quantitative evaluation results of the fusion performance. Clearly, the best fusion performance can be <lb/>achieved by downsampling two source images by a factor of 4. <lb/>(5) Study on the global-local discriminator (GL-Dir and GL-Dvi) <lb/>To demonstrate the effectiveness of applying global-local discriminators, we implement five sets of ablation studies, including 1) <lb/>remove IR global-local discriminator; 2) remove VI global-local discriminator; 3) apply two global discriminators instead of two <lb/>global-local discriminators; 4) apply two local discriminators instead of two global-local discriminators; 5) apply two global-local <lb/>discriminators (ours). For the sake of simplicity, let&apos;s denote the above ablation experiments as follows: with-GL-Dir, with-GL-Dvi, <lb/>with-Dgir and Dgvi, with-Dlir and Dlvi, and with-GL-Dir and GL-Dvi (ours). Evaluation of fusion performance from both subjective and <lb/>objective aspects, there are two distinct phenomena: 1) The single adversarial game generates the fused image that overly inclines to IR <lb/>or VI images. 2) The two-adversarial model established between two global/local discriminators and generator have a similar fusion <lb/>effect in maintaining the brightness of IR targets and preserving the details of the two source images, which are slightly blurrier than <lb/>our complete model. In contrast, the dual discriminator that combines global GNA and PatchGAN as a unified architecture can fully <lb/>capture the local radiative information and global texture details in the source images. <lb/>(6) Study on the perceptual loss <lb/>In generative networks, regularization is often used at the pixel level to make the fused images have the same characteristics as the <lb/>Table 4 <lb/>The average of the five metrics among all algorithms on the RoadScene dataset (Bold: optimal). <lb/>Method <lb/>MI <lb/>VIF <lb/>SD <lb/>Nabf <lb/>FMI_dct <lb/>LP <lb/>2.4781 <lb/>0.8117 <lb/>9.7681 <lb/>0.1481 <lb/>0.3203 <lb/>RP <lb/>2.3815 <lb/>0.7232 <lb/>9.9558 <lb/>0.2107 <lb/>0.2671 <lb/>CVT <lb/>2.2378 <lb/>0.6104 <lb/>9.8411 <lb/>0.2013 <lb/>0.3592 <lb/>MSVD <lb/>2.6566 <lb/>0.6604 <lb/>9.8026 <lb/>0.0289 <lb/>0.2318 <lb/>DCHWT <lb/>2.7426 <lb/>0.6392 <lb/>10.0968 <lb/>0.1259 <lb/>0.3366 <lb/>MDLatLRR <lb/>1.9127 <lb/>0.7125 <lb/>10.0564 <lb/>0.5539 <lb/>0.3265 <lb/>CSF <lb/>2.8683 <lb/>0.7560 <lb/>10.2582 <lb/>0.1054 <lb/>0.2551 <lb/>Dualbranch <lb/>3.0517 <lb/>0.6828 <lb/>9.8994 <lb/>0.0448 <lb/>0.2577 <lb/>CUFD <lb/>3.7967 <lb/>0.8404 <lb/>10.3422 <lb/>0.1927 <lb/>0.1995 <lb/>PMGI <lb/>3.3732 <lb/>0.8078 <lb/>10.1556 <lb/>0.0860 <lb/>0.3573 <lb/>U2Fusion <lb/>2.7289 <lb/>0.7216 <lb/>10.2790 <lb/>0.2329 <lb/>0.3177 <lb/>FusionGAN <lb/>2.9472 <lb/>0.6686 <lb/>10.1548 <lb/>0.0911 <lb/>0.3486 <lb/>GANMcC <lb/>3.1935 <lb/>0.7241 <lb/>10.2469 <lb/>0.0699 <lb/>0.3560 <lb/>GANFM <lb/>3.6028 <lb/>0.8803 <lb/>10.6870 <lb/>0.1786 <lb/>0.3258 <lb/>Ours <lb/>4.5508 <lb/>1.0572 <lb/>10.6142 <lb/>0.1047 <lb/>0.3669 <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>14 <lb/></page>

			<body>source images. However, there are many limitations to the pixel-wise constraint. For example, given two identical images, if the pixels <lb/>of one image are slightly shifted, the difference between the pixels of the two images becomes large, but the actual content remains the <lb/>same. Therefore, perceptual loss is adopted to constrain the original images and the generated image in the deep feature space, so that <lb/>the generated images can retain the high-level semantic information in the source images and improve the visual quality of the <lb/>generated images. To verify the enhancement effect of the perceptual loss on the fusion performance, we remove it from the gener-<lb/>ator&apos;s loss for ablation studying and leave the others unchanged. In terms of subjective perception, fusion images degrade the visual <lb/>effects regardless of the feature domain constraints. So, the ablation experiments demonstrate that the designed perceptual loss is <lb/>effective in constraining the intensity and gradient between the fused image and source images in the feature domain through sub-<lb/>jective and objective evaluations. <lb/>Fig. 9. Ablation examples on the TNO dataset. From top to bottom: DenseNet layers ablation study, RDblocks ablation study, CMSCA ablation <lb/>study, downsampling times ablation study, global-local discriminator ablation study and perceptual loss ablation study. <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>15 <lb/></page>

            <body>4.4. Comparison of running times and parameters <lb/>Since traditional algorithms run on CPU, we only compare the complexity among various learning-based methods in Table 6. On <lb/>the one hand, we evaluate the time complexity by computing the mean and standard deviation of the running times of different al-<lb/>gorithms on the TNO dataset. On the other hand, we count the parameters of the different deep learning models to reveal the spatial <lb/>complexity. It is worth mentioning that we only count the number of parameters in the generator network in Table 6, since only the <lb/>generator is at work during image generation in the GAN-based methods. One can notice that PMGI realizes the minimum running <lb/>time, while Dualbranch contains the smallest number of parameters. This is because PMGI and Dualbranch construct the simplest <lb/>structures in the testing phase. Due to the introduction of RDblocks and contextual attention in our model, the proposed method is <lb/>relatively time-consuming and has a large number of parameters. <lb/>5. Conclusion and discussion <lb/>In this work, we report a novel GAN-based solution for IR and VI image fusion that fully considers the multi-scale extraction and <lb/>transfer of source image information and achieves realistic fusion results with less distortions. Specifically, the generator is designed <lb/>based on Y-Net and RDblock is introduced to enhance the learning ability of discriminative multi-scale features of source images. <lb/>Moreover, CMSCAs are employed to selectively aggregate features at different scales and different levels, which enhance the recon-<lb/>struction capability of the generator. We design two discriminators that combine the structural advantages of global GAN and <lb/>PatchGAN to study the distributions of the two source images from the global and local ranges, thus forcing the generator to produce <lb/>fused images with richer information and less distortions. A hybrid loss function based on the intensity and gradient constraints in both <lb/>feature and image domains is designed to guide the proposed model optimization. Upon lots of comparisons of our method with 14 <lb/>other mainstream algorithms, our method far outperforms them in terms of source information extraction and transfer and the <lb/>perceptual quality of the fusion images. <lb/>However, there is still a lot of potential that deserves further study or excavation. On the one hand, the loss function and network <lb/>structure can be further improved to achieve the efficient fusion performance for IR and VI images in extreme environments such as <lb/>illumination variations. The above test datasets usually contain image pairs that are captured under normal exposure settings. To <lb/>validate the robustness of the proposed method to artifacts and illumination variations, 20 nighttime image pairs from the MSRS <lb/>dataset are selected to conduct testing experiments. Fig. 10 provides a failure case to visually show the limitations of the proposed <lb/>method. Obviously, all fusion methods except GANFM fail to eliminate the illumination degradation in nighttime images, but our <lb/>method exhibits a more natural overall visual perception. Table 7 also gives credible evidence. As we can see, the five metrics of the <lb/>proposed method is relatively low, compared with that of the TNO and RoadScene datasets. On the other hand, the improved solution <lb/>can also be extended to other applications, such as medical image fusion and multiple exposure image fusion. Last but not least, the <lb/>proposed method achieves numerous progresses in pre-registered multi-modality data. Recently, misaligned image fusion also a hot <lb/>spot. In the future, we will continue to improve and optimize the proposed model to solve the misalignments and calibration dis-<lb/>crepancies between the infrared and visible modalities in multi-modality image fusion task. <lb/></body>

            <div type="availability">Data availability statement <lb/>Since the data used in this work relate to our future work, the data associated with my study has not been deposited into a publicly <lb/></div>

            <body>Table 5 <lb/>Objective ablation results on the TNO dataset (Bold: Our results). <lb/>Ablation modules <lb/>Method <lb/>MI <lb/>VIF <lb/>SD <lb/>Nabf <lb/>FMI_dct <lb/>Study DenseNet layers <lb/>Two layers <lb/>3.2506 <lb/>1.0183 <lb/>8.6590 <lb/>0.0741 <lb/>0.3735 <lb/>Three layers (Ours) <lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>Four layers <lb/>3.2863 <lb/>1.0140 <lb/>8.6873 <lb/>0.0682 <lb/>0.3777 <lb/>Study RDblocks <lb/>One RDblock (Ours) <lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>Two RDblocks <lb/>3.2962 <lb/>1.0230 <lb/>8.6346 <lb/>0.0709 <lb/>0.3812 <lb/>three RDblocks <lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>Study CMSCA <lb/>Without-CMSCA <lb/>3.2957 <lb/>1.0136 <lb/>8.6106 <lb/>0.0679 <lb/>0.3790 <lb/>With-CMSCA(Ours) <lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>Study Scale-down times <lb/>Two times <lb/>3.2844 <lb/>1.0231 <lb/>8.6553 <lb/>0.0723 <lb/>0.3817 <lb/>Three times <lb/>3.2891 <lb/>1.0220 <lb/>8.6507 <lb/>0.0724 <lb/>0.3823 <lb/>Four times (Ours) <lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>Five times <lb/>3.2549 <lb/>1.0176 <lb/>8.6331 <lb/>0.0746 <lb/>0.3746 <lb/>Study GL-D <lb/>With-GL-Dir <lb/>4.0616 <lb/>1.0303 <lb/>8.6352 <lb/>0.0584 <lb/>0.3635 <lb/>With-GL-Dvi <lb/>2.7684 <lb/>0.9138 <lb/>8.6957 <lb/>0.1070 <lb/>0.4104 <lb/>With-Dgir and Dgvi <lb/>3.2947 <lb/>1.0136 <lb/>8.6634 <lb/>0.0685 <lb/>0.3822 <lb/>With-Dlir and Dlvi <lb/>3.2874 <lb/>1.0187 <lb/>8.6777 <lb/>0.0706 <lb/>0.3832 <lb/>With-GL-Dir and GL-Dvi (Ours) <lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>Study perceptual loss <lb/>Without-perceptual loss <lb/>3.2475 <lb/>1.0168 <lb/>8.6003 <lb/>0.0755 <lb/>0.3605 <lb/>With-perceptual loss (Ours) <lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>-<lb/>Ours <lb/>3.4250 <lb/>1.0317 <lb/>8.6964 <lb/>0.0576 <lb/>0.3834 <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>16 <lb/></page>

            <div type="availability">available repository. However, data will be made available on request. <lb/></div>

            <div type="annex">CRediT authorship contribution statement <lb/>Danqing Yang: Writing -original draft. Naibo Zhu: Resources. Xiaorui Wang: Writing -review &amp; editing. Shuang Li: <lb/>Methodology. <lb/></div>

            <body>Table 6 <lb/>Time and space complexity of different learning-based methods on the TNO dataset. <lb/>Method <lb/>Running time/s <lb/>parameters/K <lb/>CSF <lb/>5.06 ± 2.19 <lb/>185.4 <lb/>Dualbranch <lb/>1.06 ± 0.09 <lb/>89.5 <lb/>CUFD <lb/>21.42 ± 1.78 <lb/>955 <lb/>PMGI <lb/>0.093 ± 0.340 <lb/>161 <lb/>U2Fusion <lb/>0.469 ± 0.749 <lb/>659 <lb/>FusionGAN <lb/>0.14 ± 0.62 <lb/>925.6 <lb/>GANMcC <lb/>0.30 ± 0.76 <lb/>1867 <lb/>GANFM <lb/>0.96 ± 1.67 <lb/>10210 <lb/>Ours <lb/>0.35 ± 1.12 <lb/>15214 <lb/>Fig. 10. A failure case. Fusion results of some methods on pairs of nighttime images. From top to bottom: infrared image, visible image, results of <lb/>CSF, Dualbranch, CUFD, PMGI, U2Fusion, FusionGAN, GANMcC, GANFM and our method. <lb/>Table 7 <lb/>The average of the five metrics among some algorithms on pairs of nighttime images (Bold: optimal). <lb/>Method <lb/>MI <lb/>VIF <lb/>SD <lb/>Nabf <lb/>FMI_dct <lb/>CSF <lb/>2.4110 <lb/>0.7296 <lb/>7.5557 <lb/>0.0389 <lb/>0.2339 <lb/>Dualbranch <lb/>2.2848 <lb/>0.6950 <lb/>7.1422 <lb/>0.0128 <lb/>0.2545 <lb/>CUFD <lb/>3.0436 <lb/>0.6998 <lb/>7.8705 <lb/>0.0953 <lb/>0.2062 <lb/>PMGI <lb/>2.1371 <lb/>0.7438 <lb/>7.9849 <lb/>0.1200 <lb/>0.3368 <lb/>U2Fusion <lb/>2.0144 <lb/>0.6034 <lb/>6.6207 <lb/>0.0734 <lb/>0.2608 <lb/>FusionGAN <lb/>2.8405 <lb/>0.4289 <lb/>7.4037 <lb/>0.0364 <lb/>0.3213 <lb/>GANMcC <lb/>2.1267 <lb/>0.4193 <lb/>5.7556 <lb/>0.0099 <lb/>0.2239 <lb/>GANFM <lb/>3.2195 <lb/>0.9421 <lb/>8.9923 <lb/>0.2512 <lb/>0.2801 <lb/>Ours <lb/>2.4523 <lb/>0.7793 <lb/>6.1525 <lb/>0.0626 <lb/>0.3283 <lb/></body>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>17 <lb/></page>

            <div type="annex">Declaration of competing interest <lb/>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to <lb/>influence the work reported in this paper. <lb/></div>

            <listBibl>References <lb/>[1] J. Ma, Y. Ma, C. Li, Infrared and visible image fusion methods and applications: a survey, Inf. Fusion 45 (1) (2019) 153-178. <lb/>[2] X. Jin, Q. Jiang, S.W. Yao, A survey of infrared and visual image fusion methods, Infrared Phys. Technol. (2017) 478-501. <lb/>[3] H.M. Hu, J.W. Wu, B. Li, An adaptive fusion algorithm for visible and infrared videos based on entropy and the cumulative distribution of gray levels, IEEE <lb/>Trans. Multimed. 19 (12) (2017) 2706-2719. <lb/>[4] W.H. Ma, K. Wang, J.W. Li, Infrared and visible image fusion technology and application: a review, Sensors 23 (2) (2023) 599. <lb/>[5] Gaurav Choudhary, Dinesh Sethi, From conventional approach to machine learning and deep learning approach: an experimental and comprehensive review of <lb/>image fusion techniques, Arch. Comput. Methods Eng. 30 (2) (2023) 1267-1304. <lb/>[6] D. He, Y. Meng, C. Wang, Contrast pyramid-based image fusion scheme for infrared image and visible image. Proceedings of the IEEE International Symposium <lb/>on Geoscience and Remote Sensing, 2011, pp. 597-600. <lb/>[7] H. Li, Y. Chai, Z. Li, Multi-focus image fusion based on nonsubsampled contourlet transform and focused regions detection, Optik 124 (2013) 40-51. <lb/>[8] X. Liu, Y. Zhou, J. Wang, Image fusion based on shearlet transform and regional features, AEU-Int. J. Electron. Commun. 68 (2014) 471-477. <lb/>[9] L. Jian, X. Yang, Z. Zhou, Multi-scale image fusion through rolling guidance filter, Future Generat. Comput. Syst. 83 (2018) 310-325. <lb/>[10] D.P. Zou, B. Yang, Infrared and low-light visible image fusion based on hybrid multiscale decomposition and adaptive light adjustment, Opt Laser. Eng. 160 <lb/>(2023) 107268. <lb/>[11] C.Y. Cheng, X.J. Wu, T.Y. Xu Unifusion, A lightweight unified image fusion network, IEEE Trans. Instrum. Meas. 70 (2021) 1-14. <lb/>[12] Fu Y., Wu X.J., A dual-branch network for infrared and visible image fusion, ICPR (2021),10675-10680. <lb/>[13] X. Zheng, Q.Y. Yang, P.B. Si, A multi-stage visible and infrared image fusion network based on attention mechanism, Sensors 22 (0) (2022) 3651. <lb/>[14] H. Xu, M.Q. Gong, X. Tian, CUFD: an encoder-decoder network for visible and infrared image fusion based on common and unique feature decomposition, <lb/>Comput. Vis. Image Underst. 218 (2022) 103407. <lb/>[15] K. Ram Prabhakar, V. Sai Srikar, R. Venkatesh Babu, DeepFuse: A Deep Unsupervised Approach for Exposure Fusion with Extreme Exposure Image Pairs, CVPR, <lb/>2017, pp. 4724-4732. <lb/>[16] J.Y. Ma, L.F. Tang, M.L. Xu StdfusionNet, An infrared and visible image fusion network based on salient target detection, IEEE Trans. Instrum. Meas. 70 (2021) <lb/>1-13. <lb/>[17] J.W. Liu, J.Y. Liu, S.H. Zhou, Learning a coordinated network for detail-refinement multi-exposure image fusion, IEEE Trans. Circ. Syst. Video Technol. 33 (2) <lb/>(2023) 713-727. <lb/>[18] Y.Z. Long, H.T. Jia, Y.D. Zhong Rxdnfuse, A aggregated residual dense network for infrared and visible image fusion, Inf. Fusion 69 (2021) 128-141. <lb/>[19] J.X. Wang, X.L. Xi, D.M. Li, Fusion GRAM: an infrared and visible image fusion framework based on gradient residual and attention mechanism, IEEE Trans. <lb/>Instrum. Meas. 72 (2023) 1-12. <lb/>[20] J.W. Li, J.Y. Liu, S.H. Zhou, Infrared and visible image fusion based on residual dense network and gradient loss, Infrared Phys. Technol. 128 (2023) 104486. <lb/>[21] J.Y. Ma, W. Yu, P. Liang FusionGAN, A generative adversarial network for infrared and visible image fusion, Inf. Fusion 48 (2019) 11-26. <lb/>[22] J.Y. Ma, P.W. Lang, W. Yu, Infrared and visible image fusion via detail preserving adversarial learning, Inf. Fusion 54 (2020) 85-98. <lb/>[23] Z.L. Le, J. Huang, H. Xu Uifgan, An unsupervised continual-learning generative adversarial network for unified image fusion, Inf. Fusion 88 (C) (2022) 305-318. <lb/>[24] J. Ma, H. Zhang, Z. Shao, GANMcC: a generative adversarial network with multiclassification constraints for infrared and visible image fusion, IEEE Trans. <lb/>Instrum. Meas. 70 (2020) 1-14. <lb/>[25] J. Li, H. Huo, C. Li, Multi-grained attention network for infrared and visible image fusion, IEEE Trans. Instrum. Meas. 70 (2020) 1-12. <lb/>[26] J. Li, H. Huo, C. Li, Attention FGAN: infrared and visible image fusion using attention-based generative adversarial networks, IEEE Trans. Multimed. 23 (2020) <lb/>1383-1396. <lb/>[27] Y. Yang, J.X. Liu, S.Y. Huang, TC-GAN: infrared and visible image fusion via texture conditional generative adversarial network, IEEE Trans. Circ. Syst. Video <lb/>Technol. 31 (12) (2021) 4771-4783. <lb/>[28] J.Y. Liu, X. Fan, J. Jiang, Learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion, IEEE Trans. Circ. Syst. Video Technol. <lb/>32 (1) (2022) 105-119. <lb/>[29] Y. Fu, X.J. Wu, Tariq Durrani, Image fusion based on generative adversarial network consistent with perception, Inf. Fusion 72 (0) (2021) 110-125. <lb/>[30] S. Yi, X. Liu, L. Li Cheng Wang, Infrared and visible image fusion based on blur suppression generative adversarial network, Chin. J. Electron. 32 (1) (2023) <lb/>177-188. <lb/>[31] Nabil Ibtehaz, M. Sohel Rahman, MultiResUNet: rethinking the U-Net architecture for multimodal biomedical image segmentation, Neural Network. 121 (2020) <lb/>74-87. <lb/>[32] H. Dong, J.S. Pan, L. Xiang, Multi-Scale Boosted Dehazing Network with Dense Feature Fusion, CVPR, 2020, pp. 2154-2164. <lb/>[33] S.P. Zhou, J.J. Wang, J.Y. Zhang, Hierarchical U-shape attention network for salient object detection, IEEE Trans. Image Process. 29 (2020) 8417-8428. <lb/>[34] X.H. Wang, J.Q. Gong, M. Hu, LAUN: improved StarGAN for facial emotion recognition, IEEE Access 8 (2020) 161509-161518. <lb/>[35] Y.C. Li, X.H. Zeng, Q. Dong, RED-MAM: a residual encoder-decoder network based on multi-attention fusion for ultrasound image denoising, Biomed. Signal <lb/>Process Control 79 (2023) 104062. <lb/>[36] B. Xiao, B.C. Xu, X.L. Bi, Global-feature encoding U-Net (GEU-Net) for multi-focus image fusion, IEEE Trans. Image Process. 30 (0) (2021) 163-175. <lb/>[37] L.H. Jian, X.M. Yang, Z. Liu Sedrfuse, A symmetric encoder-decoder with residual block network for infrared and visible image fusion, IEEE Trans. Instrum. <lb/>Meas. 70 (2021) 1-15. <lb/>[38] D. Han, L. Li, X.J. Guo, DPE-MEF: multi-exposure image fusion via deep perceptual enhancement, Inf. Fusion 79 (C) (2022) 248-262. <lb/>[39] Jun-Hyung Kim, Youngbae Hwang, Infrared and visible image fusion using a guiding network to leverage perceptual similarity, Comput. Vis. Image Und 227 (C) <lb/>(2022) 103598. <lb/>[40] Y.H. Li, T. Yao, Y.W. Pan, Contextual transformer networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell. 45 (2) (2023) 1489-1500. <lb/>[41] P. Burt, E. Adelson, The Laplacian pyramid as a compact image code, IEEE Trans. Commun. 31 (4) (1983) 532-540. <lb/>[42] A. Toet, Image fusion by a ratio of low-pass pyramid, Pattern Recogn. Lett. 9 (4) (1989) 245-253. <lb/>[43] F. Nencini, A. Garzelli, S. Baronti, Remote sensing image fusion using the curvelet transform, Inf. Fusion 8 (2) (2007) 143-156. <lb/>[44] V. Naidu, Image fusion technique using multi-resolution singular value decomposition, Defence Sci. J. 61 (5) (2011) 479-484. <lb/>[45] B.K. Shreyamsha Kumar, Multifocus and multi-spectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform, Signal image <lb/>video process 7 (6) (2013) 1125-1143. <lb/>[46] H. Li, X.J. Wu, J. Kittler, MDLatLRR: A Novel Decomposition Method for Infrared and Visible Image fusion, IEEE Trans. Image Process. 29 (2020) 4733-4746. <lb/>[47] H. Zhang, J.T. Yuan, X. Tian, GAN-FM: infrared and visible image fusion using GAN with full-scale skip connection and dual Markovian discriminators, IEEE <lb/>Trans Comput Imag 21 (7) (2021) 1134-1147. <lb/>[48] H. Xu, J.Y. Ma, J. Jiang, U2fusion: a unified unsupervised image fusion network, IEEE Trans. Pattern Anal. Mach. Intell. 44 (1) (2020) 502-518. <lb/>[49] H. Xu, H. Zhang, J.Y. Ma, CSF: classification saliency-based rule for visible and infrared image fusion, IEEE Trans. Comput. Imaging 7 (2021) 824-836. <lb/></listBibl>

			<note place="headnote">D. Yang et al. <lb/>Heliyon 10 (2024) e30798 <lb/></note>

			<page>18 <lb/></page>

			<listBibl>[50] H. Zhang, H. Xu, Y. Xiao, Rethinking the image fusion: a fast unified image fusion network based on proportional maintenance of gradient and intensity, AAAI <lb/>34 (7) (2020) 12797-12804. <lb/>[51] G. Qu, D. Zhang, P. Yan. Information measure for performance of image fusion, Electron. Lett.38 (7)(2002)313-315. <lb/>[52] H.R. Sheikh, Image information and visual quality, IEEE Trans. Image Process 15 (2006) 430-444. <lb/>[53] Y.J. Rao, In-fibre Bragg grating sensors, Meas. Sci. Technol. 8 (4) (1997) 355. <lb/>[54] B.S. Kumar, Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform, Signal, Image and Video <lb/>Processing 7 (6) (2013) 1125-1143. <lb/>[55] M. Haghighat, M.A. Razian, Fast-FMI: non-reference image fusion metric, IEEE 8th Int. Conf. Appl. Inf. Commun. Technol. (AICT) (2014) 1-3. <lb/>D. Yang et al. <lb/></listBibl>

			<div type="annex">Update <lb/>Heliyon <lb/>Volume , Issue , , Page <lb/>https://doi.org/10.1016/j.heliyon.2025.e43815 <lb/>DOI: <lb/>Expression of concern: &quot;Image fusion using Y-net-based extractor and global-local discriminator&quot; <lb/>[Heliyon 10 (2024) e30798] <lb/>There are concerns that the authors appear to have used a Generative AI source in the writing process of the paper without <lb/>disclosure, which is a breach of journal policy. <lb/>DOI of original article: https://doi.org/10.1016/j.heliyon.2024.e30798. <lb/>Contents lists available at ScienceDirect <lb/>Heliyon <lb/>journal homepage: www.cell.com/heliyon <lb/>https://doi.org/10.1016/j.heliyon.2025.e43815 <lb/>Heliyon xxx (xxxx) xxx <lb/>2405-8440/© 2025 The Author(s). Published by Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and <lb/>similar technologies. <lb/>Please cite this article as: , Heliyon, https://doi.org/10.1016/j.heliyon.2025.e43815 </div>


	</text>
</tei>
