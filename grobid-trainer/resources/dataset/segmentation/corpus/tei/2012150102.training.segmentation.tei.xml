<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front> IADIS International Journal on Computer Science and Information Systems <lb/>Vol. 7, No.2, pp. 18-31 <lb/>ISSN: 1646-3692 <lb/> 18 <lb/> EMPIRICAL EVALUATION OF CRF-BASED <lb/>BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/> Manabu Ohta.  Okayama University, Japan <lb/> Ryohei Inoue.  Shikoku Hitachi Systems, Ltd., Japan <lb/> Atsuhiro Takasu.  National Institute of Informatics, Japan <lb/> ABSTRACT <lb/> We proposed an automatic bibliography extraction method for research papers scanned with OCR <lb/>markup. The method uses conditional random fields (CRFs) to label serially OCRed text lines in the <lb/>article title page as appropriate bibliographic element names. Although we achieved good extraction <lb/>accuracies for some Japanese academic journals, extraction errors are inevitable. Therefore, this paper <lb/>proposes three confidence measures for bibliography labeling to detect such extraction errors. This paper <lb/>also reports an empirical evaluation of CRF-based page analysis for research papers on the basis not only <lb/>of labeling accuracy but also of labeling error detection. We applied the three confidence measures to <lb/>detecting errors of labeling articles selected from three academic journals published in Japan. The <lb/>experiments showed that the proposed confidence measures reasonably indicated the labeling accuracies <lb/>and could be used for error detection. This paper also discusses the tradeoff between the quality of <lb/>bibliographic data assured by human post-editing of detected errors and its cost. <lb/> KEYWORDS <lb/> Bibliography extraction, conditional random field (CRF), error detection, OCR, digital library. <lb/></front>

			<body> 1. INTRODUCTION <lb/> Nowadays many publishers and academic societies provide articles in digital formats. Owing <lb/>to these services we can quickly obtain articles. Early digital library systems stored articles <lb/>independently from each other. Hence, we needed to make another search to obtain cited <lb/>papers. Recently, they begin to make networked documents where cited papers are linked to <lb/>each other and authors are also connected to the articles they wrote. <lb/>

			<note place="headnote">EMPIRICAL EVALUATION OF CRF-BASED BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/></note>

			<page> 19 <lb/></page>

			The linkage between papers and authors enhances the function of digital library systems in <lb/>various ways. From the viewpoint of information retrieval, it enables us to easily access cited <lb/>papers by just clicking links in a reference list. By following the linkage between authors and <lb/>articles, we can gather articles written by a specific group of authors. From the viewpoint of <lb/>bibliometrics, we can count the number of citations of each paper which is a fundamental <lb/>metric for measuring the quality of articles and journals. Similarly, the number of publications <lb/>of researchers obtained through linkage analysis is also an important metric to measure their <lb/>productivity. <lb/>Since articles are usually published without explicit linkage to their related contents, we <lb/>need to find them from the text of articles. For this purpose, we first need to extract <lb/>bibliographic entities to be linked such as authors and titles appearing in the title pages and <lb/>references. This is an information extraction problem extensively studied in natural language <lb/>processing (NLP) and machine learning (ML) communities. Some researchers applied <lb/>sequence labeling techniques to extract entities (Xin et al., 2008). Entity extraction is also <lb/>studied as a problem of document layout analysis in pattern recognition community (Nagy et <lb/>al., 1992). After extracting entities, various machine learning techniques were also applied to <lb/>entity matching problem. <lb/>Most of the studies on entity extraction and linkage analysis focus on improving the <lb/>extraction and linkage accuracies as much as possible. This approach leads to so-called best <lb/>effort systems. For information retrieval, best effort systems are reasonable, however, for <lb/>analysis of articles or researchers as in bibliometrics, the quality of extracted linkage should be <lb/>assured. In early studies of entity linkage, Fellegi and Sunter proposed an entity linkage model <lb/>that assures linkage accuracy (Fellegi and Sunter, 1969). Most entity linkage systems judge <lb/>whether a given pair of entities is identical or not, i.e., the systems are regarded as a binary <lb/>classifier. In Fellegi-Sunter model, systems classify a pair of entities into three categories, i.e., <lb/>identical, unknown, and not identical. The pairs judged as unknown are manually classified. <lb/>By introducing human judgment, the model assures the quality of linkage. <lb/>In this study, we aim to develop an entity extraction model that assures the quality as in <lb/>Fellegi-Sunter model. The first step for the model construction is to develop a method that can <lb/>detect unknown results of entity extraction. In this paper, we define the problem as <lb/>bibliography extraction from a title page of research papers. We first describe our CRF-based <lb/>bibliography extraction briefly and then empirically discuss the effectiveness of several <lb/>measures proposed for error detection of CRF-based bibliography labeling. <lb/>As for bibliography extraction from PDFs, Okada et al. proposed a method to extract <lb/>bibliographies from reference strings (Okada et al., 2004). They combined a support vector <lb/>machine (SVM) and a hidden Markov model (HMM) where the SVM is used for handling <lb/>features of each token in reference strings, whereas the HMM is used for handling features of <lb/>label transition. Peng et al. proposed a CRF-based method of extracting bibliographies from <lb/>the title pages and reference strings in research papers in PDF format (Peng and McCallum, <lb/>2004). They correctly labeled entire entities in title pages of research papers with 73.3% <lb/>accuracy using 13 bibliographic labels defined for title pages. They compared CRFs with <lb/>HMMs and SVMs and experimentally showed that the CRF outperformed the other methods. <lb/>None of these studies, however, discussed how to detect errors and pass them to human <lb/>judgment to assure the quality. <lb/>For extracting bibliographies from legacy articles that are digitized via scanning and OCR <lb/>processing, we need methods that are robust against noises caused by OCRs. Takasu et al. <lb/>proposed a robust method of extracting references from scanned research papers and applied it <lb/>

			<note place="headnote">IADIS International Journal on Computer Science and Information Systems <lb/></note>

			<page> 20 <lb/></page>

			to articles in various journals (Takasu, 2003). Their method was based on HMM and could <lb/>handle OCR errors. We also developed a method of extracting bibliographies from a title page <lb/>of OCRed academic articles. The method uses a CRF to assign labels to text lines in title <lb/>pages. The input of the CRF is the text lines serialized by the OCR. Since OCRed documents <lb/>involve physical layout features such as height of lines and distance between lines, we <lb/>exploited the layout features as well as textual features obtained from the text in lines to <lb/>improve the extraction accuracy (Ohta et al., 2008). <lb/> 2. CRF-BASED BIBLIOGRAPHY EXTRACTION <lb/>2.1 CRF <lb/> A CRF is a statistical sequence labeling framework proposed by Lafferty et al. (Lafferty et al., <lb/>2001) for part-of-speech tagging and syntactical analysis. CRFs outperform other popular <lb/>models, such as HMMs and maximum entropy models, when the true data distribution has <lb/>higher order dependencies than the models, which is often the case under practical <lb/>circumstances. Moreover, CRFs have performed well in many studies in fields ranging from <lb/>bioinformatics to natural language processing (Kudo et al., 2004). <lb/>We adopt a common linear-chain CRF for text line labeling. That is, we define a <lb/>conditional probability of a label sequence <lb/> n <lb/> y <lb/>y ,..., <lb/> 1 <lb/> = <lb/> y <lb/> given an input token sequence <lb/> n <lb/> x <lb/>x ,..., <lb/> 1 <lb/> = <lb/> x <lb/> as follows: <lb/> ), <lb/>) <lb/>, <lb/>, <lb/>( <lb/>exp( <lb/>) <lb/>( <lb/>1 <lb/>) <lb/>( <lb/> 1 <lb/>1 <lb/>1 <lb/> ∑ ∑ <lb/> = <lb/>= <lb/>− <lb/> = <lb/> n <lb/>i <lb/>K <lb/>k <lb/>i <lb/>i <lb/>k <lb/>k <lb/> y <lb/>y <lb/>f <lb/>Z <lb/>| <lb/>p <lb/> x <lb/>x <lb/>x <lb/>y <lb/> λ <lb/> where <lb/> ) <lb/>(x <lb/> Z <lb/> is the normalization constant that makes the probability of all candidate label <lb/>sequences sum to one, <lb/> ) <lb/>, <lb/>, <lb/>( 1 <lb/> x <lb/> i <lb/>i <lb/>k <lb/> y <lb/>y <lb/>f <lb/> − <lb/> is an arbitrary feature function over the ith label  i <lb/> y  , its <lb/>previous label 1 <lb/> − <lb/> i <lb/> y  , and the input sequence x, and  k <lb/> λ  is a learned weight associated with the <lb/>feature function  k <lb/> f  . <lb/>The CRF assigns the label sequence * <lb/> y  to the given token sequence x that maximizes Eq. <lb/>(1), i.e., <lb/> ). <lb/>| <lb/>( <lb/>max <lb/>arg <lb/> * <lb/> x <lb/>y <lb/>y <lb/> y <lb/> p <lb/> = <lb/> Note that the input token sequence x is the sequence of text line IDs, while the label sequence <lb/> y is the sequence of bibliographic element names such as a title, authors, and an abstract. <lb/>Our CRF-based labeling uses the CRF++ package (http://crfpp.sourceforge.net/) which is <lb/>an open source implementation of CRFs for labeling sequential data. When training the CRF, <lb/>we set the learning parameters such as balancing the degree of fitting to default values given <lb/>by the CRF++. <lb/> 2.2 CRF-Based Bibliographic Labeling <lb/> We label each text line in the title page of an academic article as an appropriate bibliographic <lb/>element. Bibliographic elements include paper titles, authors, abstracts, and whatever other <lb/>components found in title pages of the target journal papers. It should be noted that a <lb/>(1) <lb/>(2) <lb/> 
			
			<note place="headnote">EMPIRICAL EVALUATION OF CRF-BASED BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/></note> 
			
			<page>21 <lb/></page>
			
			bibliographic element includes at least a text line produced by the OCR and is often comprised <lb/>of several lines. <lb/>For page layout analysis and character recognition, we have developed an OCR system in <lb/>collaboration with an OCR vendor. For each scanned page, the OCR system produces not only <lb/>recognized text, but also XML markup indicating the bounding rectangles for the characters, <lb/>words, lines, and blocks. The labeling target is the text lines composed of one or more words. <lb/>Moreover, these XML elements have the layout attributes of x, y, width, and height, and <lb/>therefore, we know where the text blocks, lines, words, or characters are located in the page <lb/>and how large they are. <lb/>We prepared nine kinds of bibliographic element labels listed in Table 1 for extracting <lb/>them from three target academic journals published in Japan, i.e., IPSJ Journal (IPSJ), IEICE <lb/>Trans. Commun. in English (IEICE-E), and IEICE Trans. Inf. &amp; Syst. in Japanese (IEICE-J). <lb/>In Table 1, prefixes of &quot; j- &quot; and &quot; e- &quot; respectively stand for Japanese and English, and &quot; type &quot; is <lb/>the article type specifically defined for IEICE-E. Note that different journals have different <lb/>bibliographic elements in their title pages. <lb/>Table 2 summarizes the set of adopted feature templates that automatically generate a set <lb/>of feature functions for the text line labeling. As for visual features reflecting layout <lb/>information of title pages, we take into account not only a line&apos;s location and size, i.e., &lt;x(0)&gt;, <lb/>&lt;y(0)&gt;, &lt;w(0)&gt;, and &lt;h(0)&gt;, but also the gap between lines, &lt;g(0)&gt;, and the size and number <lb/>of characters constituting each line, i.e., &lt;cw(0)&gt;, &lt;ch(0)&gt;, and &lt;#c(0)&gt;. As for linguistic <lb/>features reflecting textual information of OCRed text, we adopt proportions of several kinds of <lb/>characters in the text line, i.e., &lt;ec(0)&gt;, &lt;kc(0)&gt;, &lt;jc(0)&gt;, and &lt;s(0)&gt;, and appearances of <lb/>characteristic keywords, &lt;kw(0)&gt;, which seem correlated with a specific bibliographic <lb/>element, e.g., &quot; university &quot; often found at authors&apos; affiliations. <lb/>An example of the feature functions generated by the bigram feature template <lb/>&lt;y(−1),y(0)&gt; is as follows: <lb/> . <lb/>otherwise <lb/>0 <lb/>authors <lb/>-<lb/>j <lb/>, <lb/>title <lb/>-<lb/>j <lb/>if <lb/>1 <lb/>) <lb/>, <lb/>, <lb/>( <lb/> 1 <lb/>1 <lb/> ⎩ <lb/> ⎨ <lb/>⎧ <lb/>= <lb/>= <lb/>= <lb/> − <lb/>− <lb/> i <lb/> i <lb/>i <lb/>i <lb/>k <lb/> y <lb/>y <lb/>y <lb/>y <lb/>f <lb/> x <lb/> This label bigram reflects the syntactic constraints of bibliographic elements, i.e., that the <lb/>authors&apos; area typically follows the title area and is followed by the abstract area, and so on. <lb/>The number of generated feature functions depends on that of kinds of bibliographic labels. <lb/>As for IPSJ papers, for example, the number of feature functions generated by the unigram <lb/>feature template, e.g., &lt;i(0)&gt;, is <lb/> N <lb/> × <lb/> 7 <lb/> , where 7 is the number of bibliographic labels used for <lb/>IPSJ shown in Table 1 and N is the number of different unigrams, i.e., line IDs. The number of <lb/>bigram feature functions generated by &lt;y(−1),y(0)&gt; amounts to <lb/> 7 <lb/>7× . <lb/> Table 1. Bibliographic element labels <lb/>Bibl. element label type <lb/>j-title <lb/>j-authors j-abstract <lb/>j-keywords e-title <lb/>e-authors <lb/>e-abstract other <lb/>IPSJ <lb/>-<lb/>yes <lb/>yes <lb/>yes <lb/>-<lb/>yes <lb/>yes <lb/>yes <lb/>yes <lb/>IEICE-E <lb/>yes <lb/>-<lb/>-<lb/>-<lb/>-<lb/>yes <lb/>yes <lb/>yes <lb/>yes <lb/>IEICE-J <lb/>-<lb/>yes <lb/>yes <lb/>yes <lb/>yes <lb/>yes <lb/>yes <lb/>-<lb/>yes <lb/> (3) <lb/>

			<note place="headnote">IADIS International Journal on Computer Science and Information Systems <lb/></note>

			<page> 22 <lb/></page>

			Table 2. Feature templates for labeling text lines <lb/>Type <lb/>Feature <lb/>Description <lb/>unigram &lt;i(0)&gt; <lb/>Current line ID <lb/>&lt;x(0)&gt; <lb/>Ratio of current line abscissa to its average <lb/>&lt;y(0)&gt; <lb/>Ratio of current line ordinate to its average <lb/>&lt;w(0)&gt; <lb/>Ratio of current line width to its average <lb/>&lt;h(0)&gt; <lb/>Ratio of current line height to its average <lb/>&lt;g(0)&gt; <lb/>Ratio of gap between current and preceding lines to its average <lb/>&lt;cw(0)&gt; <lb/>Ratio of average characters&apos; width in current line to average in all lines <lb/>&lt;ch(0)&gt; <lb/>Ratio of average characters&apos; height in current line to average in all lines <lb/>&lt;#c(0)&gt; <lb/>Ratio of # of characters in current line to its average <lb/>&lt;ec(0)&gt; <lb/>Proportion of alphanumerics in current line <lb/>&lt;kc(0)&gt; <lb/>Proportion of kanji in current line <lb/>&lt;jc(0)&gt; <lb/>Proportion of hiragana and katakana in current line <lb/>&lt;s(0)&gt; <lb/>Proportion of symbols in current line <lb/>&lt;kw(0)&gt; <lb/>Presence of any of predefined keywords in current line <lb/>bigram <lb/>&lt;y(-1),y(0)&gt; Previous and current labels <lb/> 3. DETECTION OF BIBLIOGRAPHY EXTRACTION ERRORS <lb/>3.1 Confidence Measure <lb/> We propose three confidence measures for evaluating the difficulty of CRF-based labeling in <lb/>order to detect labeling errors. These measures should highly correlate with the accuracy of <lb/>labeling. Therefore, we need to know how much the measures correlate with the accuracy and <lb/>how the correlation is affected by the accuracy. We first explain two measures which we <lb/>originally proposed for active sampling (Ohta et al., 2010). We then propose the other <lb/>confidence measure on the basis of the entropy of label assignment to each token (Settles and <lb/>Craven, 2008). <lb/> 3.1.1 Normalized Likelihood <lb/> As we described in section 2, a CRF calculates the hidden label sequence, y, which maximizes <lb/>the conditional probability given by Eq. (1). Higher <lb/> ) <lb/>| <lb/>( *  x <lb/>y <lb/> p <lb/> means more confident <lb/>assignment of labels. In contrast, lower <lb/> ) <lb/>| <lb/>( *  x <lb/>y <lb/> p <lb/> means that it is hard for the CRF to assign <lb/>labels to the token sequence. <lb/>The conditional probability is affected by the length of the token sequence, x; therefore, we <lb/>use the following normalized likelihood as a confidence measure: <lb/> , <lb/>| <lb/>| <lb/>)) <lb/>| <lb/>( <lb/>log( <lb/>: <lb/>) <lb/>( <lb/> * <lb/> x <lb/>x <lb/>y <lb/>x <lb/> p <lb/>c NLH <lb/> = <lb/> where | <lb/>| x  denotes the length of the token sequence, x, i.e., the number of text lines in a title <lb/>page. We denote the normalized likelihood as NLH. <lb/> 3.1.2 Minimum Probability of Token Assignment <lb/> NLH is a confidence measure on the basis of label assignment to all tokens in a sequence, x. <lb/> The second measure is based on the confidence in assigning a label to a single token in the <lb/> (4) <lb/>

			<note place="headnote">EMPIRICAL EVALUATION OF CRF-BASED BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/></note> 
			
			<page>23 <lb/></page> 
			
			sequence. For sequence x, let  i <lb/> Y  denote a random variable for assigning a label to the ith token <lb/>in x, i.e.,  i <lb/> x  . Let L be a set of labels. For label l in L, <lb/> ) <lb/>( <lb/> l <lb/>Y <lb/>p i  =  denotes the marginal <lb/>probability that label l is assigned to  i <lb/> x  . We can then regard the maximum probability, <lb/> ) <lb/>( <lb/>max <lb/> l <lb/>Y <lb/>p i <lb/>L <lb/>l <lb/> = <lb/> ∈ <lb/> , indicates confidence in labeling  i <lb/> x  . Using the confidence, we define the <lb/>second confidence measure as follows: <lb/> ). <lb/>( <lb/>max <lb/>min <lb/>: <lb/>) <lb/>( <lb/> | <lb/>| <lb/> l <lb/>Y <lb/>p <lb/>c <lb/> i <lb/>L <lb/>l <lb/>i <lb/>MP <lb/> = <lb/>= <lb/> ∈ <lb/>≤ x <lb/> x <lb/> We denote the probability as MP. <lb/> 3.1.3 Maximum Token Entropy <lb/> The NLH focuses only on the most likely label sequence, * <lb/> y  , and the MP focuses only on the <lb/>largest marginal probability of an assigned label, <lb/> ) <lb/>( <lb/>max <lb/> l <lb/>Y <lb/>p i <lb/>L <lb/>l <lb/> = <lb/> ∈ <lb/> . However, we consider that <lb/>the distribution of label assignment probabilities over all the possible label sequences also <lb/>reflects the difficulty of labeling. Therefore, we propose using entropy of labeling as follows. <lb/>We take into consideration not only the most probable label assigned to each token but <lb/>also the other labels to determine the third confidence measure. If there are many other label <lb/>sequences with almost the same probability as the most probable one has, the CRF is <lb/>considered less confident in labeling and so is in the assigned label sequence. While the CRF <lb/>is considered confident in its labeling when it assigns to a token one label with a probability of <lb/>nearly one and other labels have a probability of nearly zero. Therefore, we propose the <lb/>following maximum token entropy as the third confidence measure: <lb/> . <lb/>) <lb/>( <lb/>log <lb/>) <lb/>( <lb/>max <lb/>: <lb/>) <lb/>( <lb/> | <lb/>| <lb/> ∑ <lb/> ∈ <lb/>≤ <lb/> = <lb/>= <lb/>− <lb/>− <lb/>= <lb/> L <lb/>l <lb/>i <lb/>i <lb/>i <lb/>MTE <lb/> l <lb/>Y <lb/>p <lb/>l <lb/>Y <lb/>p <lb/>c <lb/> x <lb/> x <lb/> The minus sign in front is simply to ensure that <lb/> ) <lb/>(x <lb/> MTE <lb/> c <lb/> acts as a confidence measure just like <lb/> ) <lb/>(x <lb/> NLH <lb/> c <lb/> and <lb/> ) <lb/>(x <lb/> MP <lb/> c <lb/> . We denote the maximum token entropy as MTE. <lb/> 3.2 Error Detection Strategy <lb/> We need to detect labeling errors among a set of CRF-labeled token sequences. We consider <lb/>that less-confident sequences are more likely to be erroneous than more-confident ones. <lb/>Therefore, we detect such less-confident sequences as errors as follows: <lb/>1. Calculate the confidence measures, c.(x), for each token sequence x in the set of CRF-<lb/>labeled data, <lb/>2. Order the sequences in ascending order w.r.t. c.(x), which can be regarded as difficulty <lb/>order, and <lb/>3. Choose top-ranked token sequences as errors. <lb/>After detecting errors, we can manually check the detected token sequences to assign <lb/>correct labels, which is expected considerably easier than manually checking all the sequences. <lb/>(5) <lb/> (6) <lb/>

			<note place="headnote">IADIS International Journal on Computer Science and Information Systems <lb/></note>

			<page> 24 <lb/></page>

			4. EMPIRICAL EVALUATION <lb/>4.1 Experimental Setup <lb/> The CRF-based bibliography extractor extracts bibliographic components from scanned and <lb/>OCRed title pages of research papers by labeling text line sequences. We first describe <lb/>experiments on extraction accuracies and then those on detection of extraction errors by using <lb/>the confidence measures. We evaluated the performance of our CRF-based bibliography <lb/>extractor and the effectiveness of the confidence measures for detecting errors by using three <lb/>kinds of academic papers as follows. <lb/>1. Japanese papers issued by the Information Processing Society of Japan (IPSJ): We used <lb/>those issued in 2003. This dataset consisted of 479 papers. <lb/>2. English papers issued by the Institute of Electronics, Information and Communication <lb/>Engineers in Japan (IEICE-E): We used those issued in 2003 and this dataset consisted of <lb/>473 papers. <lb/>3. Japanese papers issued by the Institute of Electronics, Information and Communication <lb/>Engineers in Japan (IEICE-J): We used those issued in 2003 and 2004 and this dataset <lb/>consisted of 174 papers. <lb/>We applied five-fold cross validation to each dataset. We used real data since our OCR <lb/>outputs were difficult to simulate. This is because the OCR outputs included errors caused by <lb/>layout analysis as well as those by character recognition. The accuracy of the abstract was <lb/>99%, but that of the references was 97%. The misrecognitions were mainly caused by the <lb/>mixture of Japanese and English characters, as well as the various fonts and punctuation <lb/>symbols appearing in the references. <lb/> 4.2 Bibliography Extraction Accuracies <lb/> We used the accuracy with which a CRF assigned labels to each token in the test token <lb/>sequences as the evaluation metric. A CRF was only regarded as having succeeded in labeling <lb/>a token sequence when it assigned correct labels to all tokens in the sequence. In other words, <lb/>if a CRF assigned an incorrect label to a token and correctly labeled all other tokens in a <lb/>sequence, x, the CRF was regarded as having failed in assigning labels to the sequence, x. <lb/> Therefore, the labeling accuracy was <lb/> . <lb/>sequences <lb/>test <lb/>of <lb/># <lb/>total <lb/>sequences <lb/>labeled <lb/>correctly <lb/>of <lb/># <lb/> We repeated the experiment with 30 random sampling of training data. That is, we <lb/>randomly chose 20, 100, and 300 samples from the training dataset 30 times for each number <lb/>of samples. The resultant extraction accuracies are the average for these 30 trials and shown in <lb/>Table 3. Note here that the numbers of test sequences were 95.8 (IPSJ), 94.6 (IEICE-E), and <lb/>34.8 (IEICE-J) on average. <lb/>As seen in Table 3, the erroneously labeled test sequences decreased with the increase in <lb/>the number of training samples. The result of 300 training samples for IEICE-J is not given <lb/>because the total number of samples of this journal was 174 as described in section 4.1. We <lb/>experimented with the three different numbers of training samples to evaluate the <lb/>effectiveness of the proposed confidence measures for various accuracy levels. <lb/>

			<note place="headnote">EMPIRICAL EVALUATION OF CRF-BASED BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/></note>

			<page> 25 <lb/></page>

			Table 3. Extraction accuracies (%) and # of erroneously labeled test sequences (in parentheses) <lb/># of training samples <lb/>20 <lb/>100 <lb/>300 <lb/>IPSJ <lb/>83.4% (15.9) <lb/>91.9% (7.8) <lb/>93.8% (6.0) <lb/>IEICE-E <lb/>IEICE-J <lb/>69.5% (28.8) <lb/>65.7% (12.0) <lb/>89.7% (9.8) <lb/>79.8% (7.0) <lb/>95.9% (3.9) <lb/>-<lb/> 4.3 Extraction Robustness against Text Line Permutation <lb/> We found not a few text line permutations in our experimental dataset. That is, the order of <lb/>OCRed text lines of some articles was different from that in which human readers read them <lb/>because of erroneous layout analysis. Therefore, we conducted the following experiments to <lb/>evaluate the robustness of the CRF-based labeling against such text line permutation. <lb/>We first determined the correct order of bibliographies&apos; appearance in a title page based on <lb/>that of human readers for each journal as follows: <lb/>1. IPSJ: (other)  →  j-title  →  j-authors  →  j-abstract  →  e-title  →  e-authors  →  e-abstract <lb/> →  other <lb/>2. IEICE-E: type  →  e-title  →  e-authors  →  e-abstract  →  other <lb/>3. IEICE-J: (other)  →  j-title  →  j-authors  →  e-title  →  e-authors  →  (other)  →  j-abstract <lb/> → <lb/> j-keywords  →  other <lb/>Here &quot; (other) &quot; matches an &quot; other &quot; line zero or more times while &quot; other &quot; matches an &quot; other &quot; <lb/>line one or more times. We then separated the experimental dataset into two: one was the <lb/>samples which conformed to the above bibliography order and the other was those which did <lb/>not. Table 4 summarizes the resultant classification. As seen in the table, IPSJ and IEICE-J <lb/>had a relatively small number of articles including text line permutations while IEICE-E had <lb/>many such articles. Note here that the articles which conformed to the above bibliography <lb/>order were not necessarily completely permutation-free because we did not check the <lb/>permutation of text lines in the same kind of bibliography. <lb/>For evaluating the robustness of our labeling, we conducted the experiment by using the <lb/>permutation-free samples obtained through the classification. The resultant accuracies are <lb/>shown in Table 5. In this table, the results of 100 and 300 training samples for IEICE-E are <lb/>not given because the total number of permutation-free samples of this journal was 73 as <lb/>shown in Table 4. Comparing Table 5 to Table 3, we can see that extraction from the <lb/>permutation-free data was easier than from the original data irrespective of journal. Especially, <lb/>the accuracy of IEICE-E with 20 training samples increased remarkably when we used only <lb/>the permutation-free data, which indicates that eliminating permutation could lead to better <lb/>accuracy. <lb/> Table 4. The number of classified samples <lb/>Total <lb/>Permutation <lb/>Ratio (%) <lb/>IPSJ <lb/>479 <lb/>24 <lb/>5.01 <lb/>IEICE-E <lb/>IEICE-J <lb/>473 <lb/>174 <lb/>400 <lb/>17 <lb/>84.56 <lb/>9.77 <lb/>

			<note place="headnote">IADIS International Journal on Computer Science and Information Systems <lb/></note>

			<page> 26 <lb/></page>

			Table 5. Extraction accuracies (%) and # of erroneously labeled test sequences (in parentheses) for <lb/>permutation-free data <lb/># of training samples <lb/>20 <lb/>100 <lb/>300 <lb/>IPSJ <lb/>87.4% (11.5) <lb/>96.0% (3.7) <lb/>97.4% (2.4) <lb/>IEICE-E <lb/>IEICE-J <lb/>91.0% (1.3) <lb/>69.8% (9.5) <lb/>-<lb/>82.3% (5.6) <lb/>-<lb/>-<lb/> 4.4 Extraction Error Detection <lb/> The task in the error detection experiment was to find erroneous label sequences among the <lb/>sequences labeled by the CRF by using the three confidence measures. For this purpose, we <lb/>first randomly chose 20, 100, and 300 samples from the training dataset and learned the CRF <lb/>using them. Next, we made the CRF label the test sequences and then detected erroneously <lb/>labeled sequences in accordance with each calculated confidence measure. Since all labeled <lb/>test sequences were ranked by each confidence measure in ascending order, we detected top-n-<lb/>ranked sequences as errors. Therefore, we calculated recall and precision of erroneous labeling <lb/>detection as follows: <lb/> . <lb/>seqs <lb/>detected <lb/>of <lb/># <lb/>total <lb/>error <lb/>including <lb/>actually <lb/>seqs <lb/>detected <lb/>of <lb/># <lb/>Precision <lb/>, <lb/>seqs <lb/>labeled <lb/>y <lb/>erroneousl <lb/>of <lb/># <lb/>errors <lb/>including <lb/>actually <lb/>seqs <lb/>detected <lb/>of <lb/># <lb/>Recall <lb/> = <lb/> = <lb/> Note here that &quot; total # of detected seqs &quot; equals the rank cut-off, n. <lb/> Figure 1 plots the recall and precision of error detection when we used 20 training samples <lb/>for learning the CRF and applied each confidence measure to rank labeled test sequences with <lb/>varying the rank cut-off n. Graphs (a), (b), and (c) respectively correspond to recalls and <lb/>precisions for the IPSJ, IEICE-E, and IEICE-J datasets. In addition, Figure 2 shows the recall-<lb/>precision curves for the three datasets when the three confidence measures were applied. As <lb/>we can see in Figures 1 and 2, the retrieval effectiveness of erroneously labeled sequence <lb/>search was better in IPSJ dataset than in IEICE-E and IEICE-J datasets. Comparing the three <lb/>confidence measures, NLH and MP were better than MTE. For example, NLH showed the <lb/>best performance among the three measures in Figure 2 (b) while MP showed the best <lb/>performance in Figure 2 (c). However, NLH was best at low recall level and MP was best at <lb/>high recall level in Figure 2 (a). It should also be noted that the recall did not saturate <lb/>irrespective of the kinds of confidence measure until we detected all the test sequences in <lb/>IEICE-J dataset as shown in Figure 1 (c). <lb/>Figure 3 also shows the recall and precision of error detection for the three datasets when <lb/>we used 100 training samples for learning the CRF. In addition, Figure 4 shows the recall-<lb/>precision curves for the experiments. As seen in Figures 3 and 4, the retrieval effectiveness of <lb/>erroneously labeled sequence search was better in IPSJ dataset than in IEICE-E and IEICE-J <lb/>datasets. Comparing the results in IEICE-E and IEICE-J datasets, those in IEICE-J were <lb/>slightly better because its precision remained at about 0.6 while its recall increased to about <lb/>0.7 as shown in Figure 4 (c). Comparing the confidence measures, it is difficult to determine <lb/>which one was the best measure for detecting errors because their performances differed in <lb/>different datasets and at different recall levels even in the same dataset. For example, NLH <lb/>showed the best performance among the three measures when its recall remained under about <lb/>0.6 in Figure 4 (a); however, it became the worst when its recall exceeded this recall level. <lb/>

			<note place="headnote">EMPIRICAL EVALUATION OF CRF-BASED BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/></note>

			<page> 27 <lb/></page>

			Figure 1. Recall and precision w.r.t. rank cut-off n (# of training articles = 20) <lb/>Figure 2. Recall-precision curves (# of training articles = 20) <lb/>Figure 3. Recall and precision w.r.t. rank cut-off n (# of training articles = 100) <lb/>

			<note place="headnote">IADIS International Journal on Computer Science and Information Systems <lb/></note>

			<page>28 <lb/></page>

			Figure 4. Recall-precision curves (# of training articles = 100) <lb/> Figure 5 shows the recall and precision of error detection with varying the rank cut-off n <lb/> for the IPSJ and IEICE-E datasets when we used 300 training samples for learning the CRF. <lb/>Figure 6 shows the recall-precision curves of the experiments. There are no graphs for IEICE-<lb/>J dataset because the total number of articles in this dataset was 174. As seen in Figures 5 and <lb/>6, the retrieval effectiveness was much better in IPSJ dataset than in IEICE-E dataset. We can <lb/>also say that NLH was the best performer irrespective of dataset throughout almost all recall <lb/>levels. However, the performance in IEICE-E dataset shown in Figure 6 (b) was much poorer <lb/>than those shown in Figures 2 (b) and 4 (b). This is considered partly because the extraction <lb/>accuracy improved in accordance with the increase in the number of training samples as <lb/>shown in Table 3. That is, we had to search for only 3.9 erroneously labeled sequences when <lb/>using 300 training samples while 28.8 and 9.8 sequences when using 20 and 100 training <lb/>samples, respectively. Figures 5 (a) and 6 (a) also show that the proposed measures such as <lb/>NLH were good indicators of labeling confidence. Hence we could practically improve the <lb/>labeling quality if we manually checked only a small fraction of CRF-labeled data with low <lb/>confidence. We discuss the applicability of the confidence measures to controlling the tradeoff <lb/>between assured bibliographic quality and necessary human intervention for achieving the <lb/>quality in section 4.5. <lb/> Figure 5. Recall and precision w.r.t. rank cut-off n (# of training articles = 300) <lb/> 
			
			<note place="headnote">EMPIRICAL EVALUATION OF CRF-BASED BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/></note> 
			
			<page>29 <lb/></page> 
			
			Figure 6. Recall-precision curves (# of training articles = 300) <lb/> 4.5 Bibliographic Quality and Human Post-editing Cost <lb/> Finally, we examined the relationship between bibliographic quality assured by human post-<lb/>editing and its cost. Let us suppose a task of realizing 97% bibliographic accuracy by the <lb/>CRF-based bibliography extraction and the manual post-editing of detected articles as <lb/>extraction errors by using the proposed confidence measures. <lb/>For example, Table 3 shows that the CRF-based extraction achieved 93.8 % accuracy and <lb/>there were 6.0 articles with extraction errors when it was applied to IPSJ dataset with 300 <lb/>training samples. We could achieve more than 97% accuracy if we detected 52% of the 6.0 <lb/>articles with errors. As seen in Figure 5 (a), the recall exceeded 52% for the first time when <lb/>the rank cut-off of NLH was four, and hence the four articles could be regarded as a manual <lb/>checking cost to assure the 97% accuracy. By the same way, we estimated the deemed human <lb/>cost, i.e., the number of articles that had to be manually checked after the CRF-based <lb/>extraction for achieving 97% accuracy by using NLH for all the extraction results shown in <lb/>Table 3. Table 6 summarizes the estimated number of articles that had to be manually checked <lb/>and its ratio to the total. <lb/>Table 6 shows that we had to check many articles manually, i.e., 31% of the test articles <lb/>for IPSJ dataset, 67% for IEICE-E dataset, and 92% for IEICE-J dataset when we used only 20 <lb/>training samples. This is because the accuracies of the CRF-based extraction were poor as <lb/>shown in Table 3. However, we could assure 97% accuracy by checking only 4% (IPSJ) and <lb/>3% (IEICE-E) of the test articles when we used 300 training samples. <lb/>Table 7 summarizes the estimated number of articles that had to be manually checked to <lb/>assure 99% accuracy with human post-editing. As seen in the table, more than half of the test <lb/>articles had to be checked, except in IPSJ dataset, when we used only 20 training samples, <lb/>which is far from practical. However, we could assure 99% accuracy by checking only 10% <lb/>(IPSJ) and 11% (IEICE-E) of the test articles when we used 300 training samples. <lb/>

			<note place="headnote"> IADIS International Journal on Computer Science and Information Systems <lb/></note>

			<page> 30 <lb/></page>

			Table 6. # of articles that had to be manually checked and its ratio (%) to the total (in parentheses) for <lb/>97% accuracy <lb/># of training samples <lb/>20 <lb/>100 <lb/>300 <lb/>IPSJ <lb/>30 (31.3%) <lb/>8 (8.4%) <lb/>4 (4.2%) <lb/>IEICE-E <lb/>IEICE-J <lb/>63 (66.6%) <lb/>32 (92.0%) <lb/>18 (19.0%) <lb/>13 (37.4%) <lb/>3 (3.2%) <lb/>-<lb/>Table 7. # of articles that had to be manually checked and its ratio (%) to the total (in parentheses) for <lb/>99% accuracy <lb/># of training samples <lb/>20 <lb/>100 <lb/>300 <lb/>IPSJ <lb/>43 (44.9%) <lb/>17 (17.7%) <lb/>10 (10.4%) <lb/>IEICE-E <lb/>IEICE-J <lb/>76 (80.3%) <lb/>34 (97.7%) <lb/>49 (51.8%) <lb/>18 (51.7%) <lb/>10 (10.6%) <lb/>-<lb/> 5. CONCLUSION <lb/> This paper reports an empirical evaluation of CRF-based bibliography extraction from <lb/>scanned research papers. We specifically proposed three confidence measures for detecting <lb/>bibliography labeling errors in order to assure bibliographic quality: i) normalized likelihood, <lb/>ii) minimum probability of token assignment, and iii) maximum token entropy. Experiments <lb/>showed that all the confidence measures reasonably indicated the labeling accuracies and <lb/>could be used for labeling error detection for three academic journals used in the experiment. <lb/>Moreover, this paper also discusses the tradeoff between the quality of bibliographic data <lb/>assured by human post-editing of detected errors and its cost. The experiments showed that <lb/>more than 99% accuracy could be assured for two of the journals if the post-editing was <lb/>applied to about 10% of the articles detected as errors by using one of the proposed confidence <lb/>measures. Note that the accuracies of the CRF-based bibliography extraction were about 94% <lb/>for one journal and about 96% for the other by themselves. <lb/>We also observed the detection capabilities of the confidence measures were different in <lb/>different journals, which suggests needs for a further investigation concerning this matter. <lb/>Therefore, we plan to experiment on other journals for examining their applicability to various <lb/>journals. <lb/> ACKNOWLEDGEMENT <lb/> This work was supported by two JSPS Grants-in-Aid for Scientific Research (B) (23300040 <lb/>and 24300097), a JSPS Grant-in-Aid for Young Scientists (B) (23700119), and the <lb/>Collaborative Research Program of National Institute of Informatics. <lb/>
		
		</body>
		
		<back>

			<note place="headnote"> EMPIRICAL EVALUATION OF CRF-BASED BIBLIOGRAPHY EXTRACTION FROM <lb/>RESEARCH PAPERS <lb/></note>

			<page> 31 <lb/></page>

			<listBibl> REFERENCES <lb/> Fellegi, I. P. and Sunter, A. B., 1969. A theory of record linkage. Journal of American Statistical <lb/>Association, Vol. 64, No. 328, pp. 204-211. <lb/>Kudo, T. et al, 2004. Applying conditional random fields to Japanese morphological analysis. Proc. of <lb/>EMNLP 2004, pp. 230-237. <lb/>Lafferty, J. et al, 2001. Conditional random fields: Probabilistic models for segmenting and labeling <lb/>sequence data. Proc. of 18th International Conference on Machine Learning, pp. 282-289. <lb/>Nagy, G. et al, 1992. A prototype document image analysis for technical journals. IEEE Computer, Vol. <lb/>25, No. 7, pp. 10-22. <lb/>Ohta, M. et al, 2010. Empirical evaluation of active sampling for crf-based analysis of pages. Proc. of <lb/>IEEE IRI 2010, pp. 13-18. <lb/>Ohta, M. et al, 2008. Bibliographic element extraction from scanned documents using conditional <lb/>random fields. Proc. of ICDIM 2008, pp. 99-104. <lb/>Okada, T. et al, 2004. Bibliographic component extraction using support vector machines and hidden <lb/>markov models. Proc. of ECDL 2004, LNCS 3232, pp. 501-512. <lb/>Peng, F. and McCallum, A., 2004. Accurate information extraction from research papers using <lb/>conditional random fields. Proc. of HLT-NAACL 2004, pp. 329-336. <lb/>Settles, B. and Craven, M., 2008. An analysis of active learning strategies for sequence labeling tasks. <lb/> Proc. of EMNLP 2008, pp. 1070-1079. <lb/>Takasu, A., 2003. Bibliographic attribute extraction from erroneous references based on a statistical <lb/>model. Proc. of JCDL 2003, pp. 49-60. <lb/>Xin, X. et al, 2008. Academic conference homepage understanding using constrained hierarchical <lb/>conditional random fields. Proc. of ACM Conf. on Information and Knowledge Management (CIKM <lb/>&apos;98), pp. 1301-1310. </listBibl>

		</back>
	</text>
</tei>
