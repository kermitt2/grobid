<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_W09-1206"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43–48, <lb/>Boulder, Colorado, June 2009. c <lb/> 2009 Association for Computational Linguistics <lb/> Multilingual Semantic Role Labeling <lb/> Anders Björkelund <lb/>Love Hafdell <lb/>Pierre Nugues <lb/> Department of Computer Science, Lund University <lb/>S-221 00 Lund, Sweden <lb/> fte04abj@student.lth.se <lb/> love hafdell@hotmail.com <lb/>Pierre.Nugues@cs.lth.se <lb/> Abstract <lb/> This paper describes our contribution to the <lb/>semantic role labeling task (SRL-only) of the <lb/>CoNLL-2009 shared task in the closed chal-<lb/>lenge (Hajič et al., 2009). Our system con-<lb/>sists of a pipeline of independent, local clas-<lb/>sifiers that identify the predicate sense, the ar-<lb/>guments of the predicates, and the argument <lb/>labels. Using these local models, we carried <lb/>out a beam search to generate a pool of candi-<lb/>dates. We then reranked the candidates using <lb/>a joint learning approach that combines the lo-<lb/>cal models and proposition features. <lb/>To address the multilingual nature of the data, <lb/>we implemented a feature selection procedure <lb/>that systematically explored the feature space, <lb/>yielding significant gains over a standard set <lb/>of features. Our system achieved the second <lb/>best semantic score overall with an average la-<lb/>beled semantic F1 of 80.31. It obtained the <lb/>best F1 score on the Chinese and German data <lb/>and the second best one on English. <lb/></front>

			<body> 1 Introduction <lb/> In this paper, we describe a three-stage analysis ap-<lb/>proach that uses the output of a dependency parser <lb/>and identifies the arguments of the predicates in a <lb/>sentence. The first stage consists of a pipeline of <lb/>independent classifiers. We carried out the pred-<lb/>icate disambiguation with a set of greedy classi-<lb/>fiers, where we applied one classifier per predicate <lb/>lemma. We then used a beam search to identify <lb/>the arguments of each predicate and to label them, <lb/>yielding a pool of candidate propositions. The sec-<lb/>ond stage consists of a reranker that we applied to <lb/>the candidates using the local models and proposi-<lb/>tion features. We combined the score of the greedy <lb/>classifiers and the reranker in a third stage to select <lb/>the best candidate proposition. Figure 1 shows the <lb/>system architecture. <lb/>We evaluated our semantic parser on a set of seven <lb/>languages provided by the organizers of the CoNLL-<lb/>2009 shared task: Catalan and Spanish (Taulé et <lb/>al., 2008), Chinese (Palmer and Xue, 2009), Czech <lb/>(Hajič et al., 2006), English (Surdeanu et al., 2008), <lb/>German (Burchardt et al., 2006), and Japanese <lb/>(Kawahara et al., 2002). Our system achieved an <lb/>average labeled semantic F1 of 80.31, which cor-<lb/>responded to the second best semantic score over-<lb/>all. After the official evaluation was completed, we <lb/>discovered a fault in the training procedure of the <lb/>reranker for Spanish. The revised average labeled <lb/>semantic F1 after correction was 80.80. <lb/> 2 SRL Pipeline <lb/> The pipeline of classifiers consists of a predicate <lb/>disambiguation (PD) module, an argument identi-<lb/>fication module (AI), and an argument classifica-<lb/>tion (AC) module. Aside from the lack of a pred-<lb/>icate identification module, which was not needed, <lb/>as predicates were given, this architecture is identi-<lb/>cal to the one adopted by recent systems (Surdeanu <lb/>et al., 2008), as well as the general approach within <lb/>the field (Gildea and Jurafsky, 2002; Toutanova et <lb/>al., 2005). <lb/>We build all the classifiers using the L2-<lb/>regularized linear logistic regression from the LIB-<lb/>LINEAR package (Fan et al., 2008). The package <lb/>implementation makes models very fast to train and <lb/>

			<page> 43 <lb/></page>

			N candidates <lb/> N candidates <lb/>Reranker <lb/>Local features + proposition features <lb/>Global model <lb/>Linear combination of models <lb/>Local classifier pipeline <lb/>Sense disambiguation <lb/> greedy search <lb/> Argument identification <lb/> beam search <lb/> Argument labeling <lb/> beam search <lb/> Reranked <lb/>candidates <lb/> Figure 1: System architecture. <lb/> use for classification. Since models are logistic, they <lb/>produce an output in the form of probabilities that <lb/>we use later in the reranker (see Sect. 3). <lb/> 2.1 Predicate Disambiguation <lb/> We carried out a disambiguation for all the lem-<lb/>mas that had multiple senses in the corpora and we <lb/>trained one classifier per lemma. We did not use the <lb/>predicate lexicons and we considered lemmas with a <lb/>unique observed sense as unambiguous. <lb/>English required a special processing as the sense <lb/>nomenclature overlapped between certain nominal <lb/>and verbal predicates. For instance, the nominal <lb/>predicate plan.01 and the verbal predicate plan.01 <lb/> do not correspond to the same semantic frame. <lb/>Hence, we trained two classifiers for each lemma <lb/> plan that could be both a nominal and verbal predi-<lb/>cate. <lb/> Table 1: Feature sets for predicate disambiguation. <lb/> ca ch cz en ge sp <lb/> PredWord <lb/> • <lb/> • <lb/>• <lb/> PredPOS <lb/> • <lb/>• <lb/> PredDeprel <lb/> • <lb/>• <lb/>• <lb/> PredFeats <lb/> • <lb/>• <lb/>• <lb/> PredParentWord • <lb/>• <lb/>• <lb/>• <lb/>• <lb/> PredParentPOS <lb/> • <lb/>• <lb/>• <lb/> PredParentFeats <lb/> • <lb/>• <lb/> DepSubCat <lb/> • <lb/>• <lb/>• <lb/>• <lb/>• <lb/> ChildDepSet <lb/> • <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/> ChildWordSet <lb/> • <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/> ChildPOSSet <lb/> • <lb/>• <lb/>• <lb/>• <lb/>• <lb/> 2.2 Argument Identification and Classification <lb/> We implemented the argument identification and <lb/>classification as two separate stages, because it en-<lb/>abled us to apply and optimize different feature sets <lb/>in each step. Arguments were identified by means <lb/>of a binary classifier. No pruning was done, each <lb/>word in the sentence was considered as a potential <lb/>argument to all predicates of the same sentence. <lb/>Arguments were then labeled using a multiclass <lb/>classifier; each class corresponding to a certain la-<lb/>bel. We did not apply any special processing with <lb/>multiple dependencies in Czech and Japanese. In-<lb/>stead, we concatenated the composite labels (i.e. <lb/>double edge) to form unique labels (i.e. single edge) <lb/>having their own class. <lb/> 2.3 Identification and Classification Features <lb/> For the English corpus, we used two sets of features <lb/>for the nominal and the verbal predicates both in the <lb/>AI and AC steps. This allowed us to create different <lb/>classifiers for different kinds of predicates. We ex-<lb/>tended this approach with a default classifier catch-<lb/>ing predicates that were wrongly tagged by the POS <lb/>tagger. For both steps, we used the union of the two <lb/>feature sets for this catch-all class. <lb/>We wanted to employ this procedure with the two <lb/>other languages, Czech and Japanese, where predi-<lb/>cates had more than one POS type. As feature selec-<lb/>tion (See Sect. 2.4) took longer than expected, par-<lb/>ticularly in Czech due to the size of the corpus and <lb/>the annotation, we had to abandon this idea and we <lb/>trained a single classifier for all POS tags in the AI <lb/>and AC steps. <lb/>For each data set, we extracted sets of features <lb/>similar to the ones described by Johansson and <lb/>Nugues (2008). We used a total of 32 features that <lb/>we denote with the prefixes: Pred-, PredParent-, <lb/>Arg-, Left-, Right-, LeftSibling-, and RightSibling-<lb/>for, respectively, the predicate, the parent of the <lb/>predicate, the argument, the leftmost and rightmost <lb/>dependents of the argument, and the left and right <lb/>

			<page> 44 <lb/></page>

			Table 2: Feature sets for argument identification and classification. <lb/> Argument identification <lb/>Argument classification <lb/>ca ch cz <lb/>en <lb/>ge ja sp ca ch cz <lb/>en <lb/>ge ja sp <lb/> PredWord <lb/> • <lb/> N <lb/> • <lb/> PredPOS <lb/>N <lb/> • <lb/>• <lb/> V <lb/> • <lb/> PredLemma <lb/>N <lb/> • <lb/>• <lb/>• <lb/>• N,V • <lb/>• <lb/> PredDeprel <lb/>Sense <lb/> • <lb/>• <lb/> V <lb/> • <lb/>• <lb/>• <lb/>• N,V • <lb/>• <lb/>• <lb/> PredFeats <lb/> • <lb/>• <lb/>• <lb/> PredParentWord <lb/>V <lb/> • <lb/> V <lb/> • <lb/> PredParentPOS <lb/>V <lb/>V <lb/> • <lb/> PredParentFeats <lb/> • <lb/> DepSubCat <lb/> • <lb/>• <lb/> ChildDepSet <lb/> • <lb/>• <lb/>• <lb/>• <lb/> V <lb/> • <lb/>• <lb/>• <lb/> ChildWordSet <lb/>N <lb/> • <lb/>• <lb/> ChildPOSSet <lb/> • <lb/>• <lb/> N <lb/> • <lb/> ArgWord <lb/> • <lb/>• <lb/> N,V • <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• N,V • <lb/>• <lb/>• <lb/> ArgPOS <lb/> • <lb/>• <lb/> N,V • <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• N,V <lb/> • <lb/> ArgFeats <lb/> • <lb/>• <lb/>• <lb/>• <lb/>• <lb/> ArgDeprel <lb/> • <lb/>• <lb/>• <lb/> V <lb/> • <lb/>• <lb/>• <lb/>• <lb/>• <lb/> V <lb/> • <lb/>• <lb/> DeprelPath <lb/> • <lb/>• <lb/>• N,V • <lb/>• <lb/>• <lb/>• <lb/>• <lb/> V <lb/> • <lb/> POSPath <lb/> • <lb/>• <lb/>• N,V • <lb/>• <lb/>• <lb/>• <lb/> V <lb/> • <lb/>• <lb/> Position <lb/> • N,V <lb/> • <lb/>• <lb/>• <lb/>• N,V <lb/> • <lb/>• <lb/> LeftWord <lb/> • <lb/>• <lb/>• <lb/>• <lb/> N <lb/> • <lb/>• <lb/> LeftPOS <lb/> • <lb/>• <lb/> V <lb/>LeftFeats <lb/> • <lb/>• <lb/>• <lb/> RightWord <lb/> • <lb/> N <lb/> • <lb/>• <lb/> N,V <lb/> • <lb/> RightPOS <lb/>N <lb/> • <lb/>• <lb/> N,V <lb/> • <lb/> RightFeats <lb/> • <lb/>• <lb/> LeftSiblingWord <lb/> • <lb/>• <lb/>• <lb/>• <lb/> N <lb/> • <lb/> LeftSiblingPOS <lb/> • <lb/>• <lb/>• <lb/>• <lb/> N,V <lb/> • <lb/> LeftSiblingFeats <lb/> • <lb/>• <lb/>• <lb/> RightSiblingWord • <lb/>• <lb/> V <lb/> • <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/> RightSiblingPOS <lb/> • <lb/>• <lb/> RightSiblingFeats <lb/> • <lb/> sibling of the argument. The suffix of these names <lb/>corresponds to the column name of the CoNLL for-<lb/>mat, except Word which corresponds to the Form <lb/>column. Additional features are: <lb/> • Sense: the value of the Pred column, e.g. <lb/> plan.01. <lb/> • Position: the position of the argument with re-<lb/>spect to the predicate, i.e. before, on, or after. <lb/> • DepSubCat: the subcategorization frame of the <lb/>predicate, e.g. OBJ+OPRD+SUB. <lb/> • DeprelPath: the path from predicate to argu-<lb/>ment concatenating dependency labels with the <lb/>direction of the edge, e.g. OBJ↑OPRD↓SUB↓. <lb/> • POSPath: same as DeprelPath, but depen-<lb/>dency labels are exchanged for POS tags, e.g. <lb/>NN↑NNS↓NNP↓. <lb/> • ChildDepSet: the set of dependency labels of <lb/>the children of the predicate, e.g. {OBJ, SUB}. <lb/> • ChildPOSSet: the set of POS tags of the chil-<lb/>dren of the predicate, e.g. {NN, NNS}. <lb/> • ChildWordSet: the set of words (Form) of the <lb/>children of the predicate, e.g. {fish, me}. <lb/>

			<page> 45 <lb/></page>

			2.4 Feature Selection <lb/> We selected the feature sets using a greedy forward <lb/>procedure. We first built a set of single features and, <lb/>to improve the separability of our linear classifiers, <lb/>we paired features to build bigrams. We searched <lb/>the space of feature bigrams using the same proce-<lb/>dure. See Johansson (2008, page 83), for a com-<lb/>plete description. We intended to carry out a cross-<lb/>validation search. Due to the lack of time, we re-<lb/>sorted to using 80% of the training set for training <lb/>and 20% for evaluating the features. Table 2 con-<lb/>tains the complete list of single features we used. <lb/>We omitted the feature bigrams. <lb/>Feature selection turned out to be a massive task. <lb/>It took us three to four weeks searching the feature <lb/>spaces, yet in most cases we were forced to interrupt <lb/>the selection process after a few bigram features in <lb/>order to have our system ready in time. This means <lb/>that our feature sets can probably be further opti-<lb/>mized. <lb/>When the training data was initially released, <lb/>we used the exact feature set from Johansson and <lb/>Nugues (2008) to compute baseline results on the <lb/>development set for all the languages. After feature <lb/>selection, we observed an increase in labeled seman-<lb/>tic F1 close to 10% in most languages. <lb/> 2.5 Applying Beam Search <lb/> The AI module proceeds left to right considering <lb/>each word as an argument of the current predicate. <lb/>The current partial propositions are scored by com-<lb/>puting the product of the probabilities of all the <lb/>words considered so far. After each word, the cur-<lb/>rent pool of partial candidates is reduced to the beam <lb/>size, k, and at the end of the sentence, the top k scor-<lb/>ing propositions are passed on to the AC module. <lb/>Given k unlabeled propositions, the AC module <lb/>applies a beam search on each of these propositions <lb/>independently. This is done in a similar manner, <lb/>proceeding from left to right among the identified <lb/>arguments, keeping the l best labelings in its beam, <lb/>and returning the top l propositions, when all iden-<lb/>tified arguments have been processed. This yields <lb/> n = k × l complete propositions, unless one of the <lb/>unlabeled propositions has zero arguments, in which <lb/>case we have n = (k − 1) × l + 1. <lb/> The probability of a labeled proposition according <lb/>to the local pipeline is given by P  Local  = P  AI  × <lb/> P  AC  , where P  AI  and P  AC  is the output probability <lb/>from the AI and AC modules, respectively. In the <lb/>case of empty propositions, P  AC  was set to 1. <lb/> 3 Global Reranker <lb/> We implemented a global reranker following <lb/>Toutanova et al. (2005). To generate training ex-<lb/>amples for the reranker, we trained m AI and AC <lb/>classifiers by partitioning the training set in m parts <lb/>and using m − 1 of these parts for each AI and AC <lb/>classifier, respectively. <lb/>We applied these AI and AC classifiers on the part <lb/>of the corpus they were not trained on and we then <lb/>generated the top n propositions for each predicate. <lb/>We ran the CoNLL evaluation script on the proposi-<lb/>tions and we marked the top scoring one(s) as pos-<lb/>itive. We marked the others negative. If the correct <lb/>proposition was not in the pool of candidates, we <lb/>added it as an extra positive example. We used these <lb/>positive and negative examples as training data for <lb/>the global reranker. <lb/> 3.1 Reranker Features <lb/> We used all the features from the local pipeline for <lb/>all the languages. We built a vector where the AI <lb/>features were prefixed with AI-and the AC features <lb/>prefixed with lab−, where lab was any of the argu-<lb/>ment labels. <lb/>We added one proposition feature to the concate-<lb/>nation of local features, namely the sequence of core <lb/>argument labels, e.g. A0+plan.01+A1. In Catalan <lb/>and Spanish, we considered all the labels prefixed by <lb/>arg0, arg1, arg2, or arg3 as core labels. In Chinese <lb/>and English, we considered only the labels A0, A1, <lb/>A2, A3, and A4. In Czech, German, and Japanese, <lb/>we considered all the labels as core labels. <lb/>Hence, the total size of the reranker vector space <lb/>is |AI| + |L| × |AC| + |G|, where |AI| and |AC| <lb/> denotes the size of the AI and AC vector spaces, re-<lb/>spectively, |L| corresponds to the number of labels, <lb/>and |G| is the size of additional global features. <lb/>We ran experiments with the grammatical <lb/>voice that we included in the string represent-<lb/>ing the sequence of core argument labels, e.g. <lb/> A1+plan.01/Passive+A0. The voice was derived by <lb/>hand-crafted rules in Catalan, English, German, and <lb/>
			
			<page> 46 <lb/></page>

			Spanish, and given in the Feat column in Czech. <lb/>However, we did not notice any significant gain in <lb/>performance. The hand-crafted rules use lexical <lb/>forms and dependencies, which we believe classi-<lb/> fiers are able to derive themselves using the local <lb/>model features. This also applies to Czech, as Pred-<lb/>Feats was a feature used in the local pipeline, both <lb/>in the AI and AC steps. <lb/> 3.2 Weighting the Models <lb/> In Sect. 2.5, we described how the pipeline was used <lb/>to generate the top n propositions, each with its own <lb/>local probability P  Local  . Similar to softmax, we nor-<lb/>malized these local probabilities by dividing each of <lb/>them by their total sum. We denote this normalized <lb/>probability by P <lb/> 񮽙 <lb/> Local  . The reranker gives a proba-<lb/>bility on the complete proposition, P  Reranker  . We <lb/>weighted these probabilities and chose the proposi-<lb/>tion maximizing P  F inal  = (P <lb/> 񮽙 <lb/> Local  )  α  × P  Reranker  . <lb/>This is equivalent to a linear combination of the log <lb/>probabilities. <lb/> 3.3 Parameters Used <lb/> For the submission to the CoNLL 2009 Shared Task, <lb/>we set the beam widths to k = l = 4, yielding can-<lb/>didate pools of size n = 13 or n = 16 (See Sec-<lb/>tion 2.5). We used m = 5 for training the reranker <lb/>and α = 1 for combining the local model with the <lb/>reranker. <lb/> 4 Results <lb/> Our system achieved the second best semantic score, <lb/>all tasks, with an average labeled semantic F1 of <lb/>80.31. It obtained the best F1 score on the Chinese <lb/>and German data and the second best on English. <lb/>Our system also reached the third rank in the out-of-<lb/>domain data, all tasks, with a labeled semantic F1 of <lb/>74.38. Post-evaluation, we discovered a bug in the <lb/>Spanish reranker model causing the poor results in <lb/>this language. After correcting this, we could reach <lb/>a labeled semantic F1 of 79.91 in Spanish. Table 3 <lb/>shows our official results in the shared task as well <lb/>as the post-evaluation update. <lb/>We also compared the performance of a greedy <lb/>strategy with that of a global model. Table 4 shows <lb/>these figures with post-evaluation figures in Spanish. <lb/>Table 5 shows the training time, parsing time, and <lb/>the parsing speed in predicates per second. These <lb/>figures correspond to complete execution time of <lb/>parsing, including loading models into memory, i.e. <lb/>a constant overhead, that explains the low parsing <lb/>speed in German. We implemented our system to <lb/>be flexible for easy debugging and testing various <lb/>ideas. Optimizing the implementation would reduce <lb/>execution times significantly. <lb/> Table 3: Summary of submitted results: closed challenge, <lb/>semantic F1. * denotes the post-evaluation results ob-<lb/>tained for Spanish after a bug fix. <lb/> Unlabeled Labeled <lb/> Catalan <lb/>93.60 <lb/>80.01 <lb/>Chinese <lb/>84.76 <lb/>78.60 <lb/>Czech <lb/>92.63 <lb/>85.41 <lb/>English <lb/>91.17 <lb/>85.63 <lb/>German <lb/>92.13 <lb/>79.71 <lb/>Japanese <lb/>83.45 <lb/>76.30 <lb/>Spanish <lb/>92.69 <lb/>76.52 <lb/>Spanish* <lb/>93.76 <lb/>79.91 <lb/>Average <lb/>90.06 <lb/>80.31 <lb/>Average* <lb/>90.21 <lb/>80.80 <lb/> Table 4: Improvement of reranker. * denotes the post-<lb/>evaluation results obtained for Spanish after a bug fix. <lb/> Greedy Reranker Gain <lb/> Catalan <lb/>79.54 <lb/>80.01 0.47 <lb/>Chinese <lb/>77.84 <lb/>78.60 0.76 <lb/>Czech <lb/>84.99 <lb/>85.41 0.42 <lb/>English <lb/>84.44 <lb/>85.63 1.19 <lb/>German <lb/>79.01 <lb/>79.71 0.70 <lb/>Japanese <lb/>75.61 <lb/>76.30 0.69 <lb/>Spanish <lb/>79.28 <lb/>76.52 -2.76 <lb/>Spanish* <lb/>79.28 <lb/>79.91 0.63 <lb/>Average <lb/>80.10 <lb/>80.31 0.21 <lb/>Average* <lb/>80.10 <lb/>80.80 0.70 <lb/> 5 Conclusion <lb/> We have built and described a streamlined and ef-<lb/>fective semantic role labeler that did not use any <lb/>lexicons or complex linguistic features. We used a <lb/>generic feature selection procedure that keeps lan-<lb/>guage adaptation minimal and delivers a relatively <lb/>even performance across the data sets. The system is <lb/>

			<page> 47 <lb/></page>

			Table 5: Summary of training and parsing times on an Apple Mac Pro, 3.2 GHz. <lb/> Training Parsing (Greedy) Speed (Greedy) Parsing (Reranker) Speed (Reranker) <lb/> (min) <lb/>(min:sec) <lb/>(pred/sec) <lb/>(min:sec) <lb/>(pred/sec) <lb/>Catalan <lb/>46 <lb/>1:10 <lb/>71 <lb/>1:21 <lb/>62 <lb/>Chinese <lb/>139 <lb/>2:35 <lb/>79 <lb/>3:45 <lb/>55 <lb/>Czech <lb/>299 <lb/>18:47 <lb/>40 <lb/>33:49 <lb/>22 <lb/>English <lb/>421 <lb/>6:25 <lb/>27 <lb/>8:51 <lb/>20 <lb/>German <lb/>15 <lb/>0:21 <lb/>26 <lb/>0:22 <lb/>25 <lb/>Japanese <lb/>48 <lb/>0:37 <lb/>84 <lb/>1:02 <lb/>50 <lb/>Spanish <lb/>51 <lb/>1:15 <lb/>69 <lb/>1:47 <lb/>48 <lb/>robust and can handle incorrect syntactic parse trees <lb/>with a good level of immunity. While input parse <lb/>trees in Chinese and German had a labeled syntac-<lb/>tic accuracy of 78.46 (Hajič et al., 2009), we could <lb/>reach a labeled semantic F1 of 78.60 and 79.71 in <lb/>these languages. We also implemented an efficient <lb/>global reranker in all languages yielding a 0.7 av-<lb/>erage increase in labeled semantic F1. The reranker <lb/>step, however, comes at the expense of parsing times <lb/>increased by factors ranging from 1.04 to 1.82. <lb/></body>

			<listBibl> References <lb/> Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea <lb/>Kowalski, Sebastian Padó, and Manfred Pinkal. 2006. <lb/>The SALSA corpus: a German corpus resource for <lb/>lexical semantics. In Proceedings of the 5th Interna-<lb/>tional Conference on Language Resources and Evalu-<lb/>ation (LREC-2006), Genoa, Italy. <lb/>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui <lb/>Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-<lb/>brary for large linear classification. Journal of Ma-<lb/>chine Learning Research, 9:1871–1874. <lb/>Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-<lb/>beling of semantic roles. Computational Linguistics, <lb/> 28(3):245–288. <lb/>Jan Hajič, Jarmila Panevová, Eva Hajičová, Petr <lb/>Sgall, Petr Pajas, Janň <lb/> Stěpánek, Jiří Havelka, Marie <lb/>Mikulová, and Zdeněk <lb/>Zabokrtsk´ 006. Prague De-<lb/>pendency Treebank 2.0. <lb/>Jan Hajič, Massimiliano Ciaramita, Richard Johans-<lb/>son, Daisuke Kawahara, Maria Ant onia Martí, Lluís <lb/>M` arquez, Adam Meyers, Joakim Nivre, Sebastian <lb/>Padó, Jaň <lb/>Stěpánek, Pavel Straňák, Mihai Surdeanu, <lb/>Nianwen Xue, and Yi Zhang. 2009. The CoNLL-<lb/>2009 shared task: Syntactic and semantic depen-<lb/>dencies in multiple languages. In Proceedings of <lb/>the 13th Conference on Computational Natural Lan-<lb/>guage Learning (CoNLL-2009), June 4-5, Boulder, <lb/>Colorado, USA. <lb/>Richard Johansson and Pierre Nugues. <lb/>2008. <lb/>Dependency-based <lb/>syntactic–semantic <lb/>analysis <lb/>with PropBank and NomBank. In Proceedings of the <lb/>Shared Task Session of CoNLL-2008. <lb/> Richard Johansson. 2008. Dependency-based Semantic <lb/>Analysis of Natural-language Text. Ph.D. thesis, Lund <lb/>University, December 5. <lb/>Daisuke Kawahara, Sadao Kurohashi, and Kôiti Hasida. <lb/>2002. Construction of a Japanese relevance-tagged <lb/>corpus. In Proceedings of the 3rd International <lb/>Conference on Language Resources and Evaluation <lb/>(LREC-2002), pages 2008–2013, Las Palmas, Canary <lb/>Islands. <lb/>Martha Palmer and Nianwen Xue. 2009. Adding seman-<lb/>tic roles to the Chinese Treebank. Natural Language <lb/>Engineering, 15(1):143–172. <lb/>Mihai Surdeanu, Richard Johansson, Adam Meyers, <lb/>Lluís M` arquez, and Joakim Nivre. 2008. The CoNLL-<lb/>2008 shared task on joint parsing of syntactic and se-<lb/>mantic dependencies. In Proceedings of the 12th Con-<lb/>ference on Computational Natural Language Learning <lb/>(CoNLL-2008). <lb/> Mariona Taulé, Maria Ant onia Martí, and Marta Re-<lb/>casens. 2008. AnCora: Multilevel Annotated Corpora <lb/>for Catalan and Spanish. In Proceedings of the 6th <lb/>International Conference on Language Resources and <lb/>Evaluation (LREC-2008), Marrakesh, Morroco. <lb/>Kristina Toutanova, Aria Haghighi, and Christopher D. <lb/>Manning. 2005. Joint learning improves semantic role <lb/>labeling. In Proceedings of ACL-2005. <lb/></listBibl>

			<page> 48 </page>


	</text>
</tei>
