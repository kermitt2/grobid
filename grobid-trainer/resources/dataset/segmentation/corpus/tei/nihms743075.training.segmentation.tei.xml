<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>KnowSim: A Document Similarity Measure on Structured <lb/>Heterogeneous Information Networks <lb/>Chenguang Wang † , Yangqiu Song ‡ , Haoran Li † , Ming Zhang † , and Jiawei Han ‡ <lb/>Chenguang Wang: wangchenguang@pku.edu.cn; Yangqiu Song: yqsong@illinois.edu; Haoran Li: <lb/>lihaoran_2012@pku.edu.cn; Ming Zhang: mzhang_cs@pku.edu.cn; Jiawei Han: hanj@illinois.edu <lb/> † School of EECS, Peking University <lb/> ‡ Department of Computer Science, University of Illinois at Urbana-Champaign <lb/>Abstract <lb/>As a fundamental task, document similarity measure has broad impact to document-based <lb/>classification, clustering and ranking. Traditional approaches represent documents as bag-of-<lb/>words and compute document similarities using measures like cosine, Jaccard, and dice. However, <lb/>entity phrases rather than single words in documents can be critical for evaluating document <lb/>relatedness. Moreover, types of entities and links between entities/words are also informative. We <lb/>propose a method to represent a document as a typed heterogeneous information network (HIN), <lb/>where the entities and relations are annotated with types. Multiple documents can be linked by the <lb/>words and entities in the HIN. Consequently, we convert the document similarity problem to a <lb/>graph distance problem. Intuitively, there could be multiple paths between a pair of documents. <lb/>We propose to use the meta-path defined in HIN to compute distance between documents. Instead <lb/>of burdening user to define meaningful meta-paths, an automatic method is proposed to rank the <lb/>meta-paths. Given the meta-paths associated with ranking scores, an HIN-based similarity <lb/>measure, KnowSim, is proposed to compute document similarities. Using Freebase, a well-known <lb/>world knowledge base, to conduct semantic parsing and construct HIN for documents, our <lb/>experiments on 20Newsgroups and RCV1 datasets show that KnowSim generates impressive <lb/>high-quality document clustering. <lb/></front>

			<body>I. Introduction <lb/>Document similarity is a fundamental task, and can be used in many applications such as <lb/>document classification, clustering and ranking. Traditional approaches use bag-of-words <lb/>(BOW) as document representation and compute the document similarities using different <lb/>measures such as cosine, Jaccard, and dice. However, the entity phrases rather than just <lb/>words in documents can be critical for evaluating the relatedness between texts. For <lb/>example, &quot;New York&quot; and &quot;New York Times&quot; represent different meanings. &quot;George <lb/>Washington&quot; and &quot;Washington&quot; are similar if they both refer to person, but can be rather <lb/>different otherwise. If we can detect their names and types (coarse-grained types such as <lb/>person, location and organization; fine-grained types such as politician, musician, country, <lb/>and city), they can help us better evaluate whether two documents are similar. Moreover, the <lb/></body>

			<front>HHS Public Access <lb/>Author manuscript <lb/>Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/>Published in final edited form as: <lb/>Proc IEEE Int Conf Data Min. 2015 November ; 2015: 1015-1020. doi:10.1109/ICDM.2015.131. <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></front>

			<body>links between entities or words are also informative. For example, as Fig. 1 shown in [1], the <lb/>similarity between the two documents is zero if we use BOW representation since there is no <lb/>identical word shared by them. However, the two documents are related in contents. If we <lb/>can build a link between &quot;Obama&quot; of type Politician in one document and &quot;Bush&quot; of type <lb/>Politician in another, then the two documents become similar in the sense that they both talk <lb/>about politicians and connect to &quot;United States.&quot; Therefore, we can use the structural <lb/>information in the unstructured documents to further improve document similarity <lb/>computation. <lb/>Some existing studies use linguistic knowledge bases such as WordNet [2] or general <lb/>purpose knowledge bases such as Open Directory Project (ODP) [3], Wikipedia [4], [5], [6], <lb/>[7], [8], [9], or knowledge extracted from open domain data such as Probase [10], [11], to <lb/>extend the features of documents to improve similarity measures. However, they treat <lb/>knowledge in such knowledge bases as &quot;flat features&quot; and do not consider the structural <lb/>information contained in the links in knowledge bases. There have been studies on <lb/>evaluating word similarity or string similarity based on WordNet or other knowledge [12] <lb/>considering the structural information [13], and using word similarity to compute short text <lb/>similarity [14], [15]. For example, the distance from words to the root is used to capture the <lb/>semantic relatedness between two words. However, WordNet is designed for single words. <lb/>For named entities, a separate similarity should be designed [14], [16]. These studies do not <lb/>consider the relationships between entities (e.g., &quot;Obama&quot; being related to &quot;United States&quot;). <lb/>Thus, they may still lose structural information even if the knowledge base provides rich <lb/>linked information. For example, nowadays there exist numerous general-purpose <lb/>knowledge bases, e.g., Freebase [17], KnowItAll [18], TextRunner [19], WikiTaxonomy <lb/>[20], DBpedia [21], YAGO [22], NELL [23] and Knowledge Vault [24]. They contain a lot <lb/>of world knowledge about entity types and their relationships and provide us rich <lb/>opportunities to develop a better measure to evaluate document similarities. <lb/>In this paper, we propose KnowSim, a heterogeneous information network (HIN) [25] based <lb/>similarity measure that explores the structural information from knowledge bases to <lb/>compute document similarities. We use Freebase as the source of world knowledge. <lb/>Freebase is a collaboratively collected knowledge base about entities and their organizations <lb/>[17]. We follow [1] to use the world knowledge specification framework including a <lb/>semantic parser to ground any text to the knowledge bases, and a conceptualization-based <lb/>semantic filter to resolve the ambiguity problem when adapting world knowledge to the <lb/>corresponding document. By the specification of world knowledge, we have the documents <lb/>as well as the extracted entities and their relations. Since the knowledge bases provide entity <lb/>types, the resulting data naturally form an HIN. The named entities and their types, as well <lb/>as the documents and the words form the HIN. <lb/>Given a constructed HIN, we use meta-path based similarity [26] to measure the similarity <lb/>between two documents in the network. Rather than asking users to provide meaningful <lb/>meta-path(s), we propose an automatic way to generate meta-paths for a given set of <lb/>documents. In this case, an efficient mechanism should be developed to enumerate all the <lb/>possible meta-paths of interests and compute the best ones. Based on the PageRank-Nibble <lb/>algorithm [27] that can conduct efficient graph pruning locally for a single node, we develop <lb/></body>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 2 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<body>Meta-path Dependent PageRank-Nibble algorithm to locally partition the large-scale HIN <lb/>(in our case, consisting of 108,722 entities and 9,655,466 relations) given a meta-path, and <lb/>then based on the local partition to approximate commuting matrices for all meta-paths. We <lb/>then store all the commuting matrices generated based on the local partition, which saves up <lb/>to 15% space compared to that based on the original network. Thus, the meta-path <lb/>generation process can be approximated in time independent of the size of the underlying <lb/>network with low accuracy loss and high space saving. Then we perform meta-path <lb/>selection based on feature selection algorithms (i.e., maximal spanning tree [28] and <lb/>Laplacian score [29] based methods) by defining the meta-path similarities based on <lb/>document-meta-path co-occurrences. We define an unsupervised knowledge-driven <lb/>document similarity measure, KnowSim, which incorporates the selected meta-paths to <lb/>represent the links between documents. The computation of KnowSim can be done in nearly <lb/>linear time using the precomputed commuting matrices. <lb/> II. Construction of Document HIN <lb/>In this section, we introduce how to generate heterogeneous information network (HIN) for <lb/>the documents based on world knowledge bases. Please find the basic concepts related to <lb/>HIN, such as network schema, meta-path, and commuting matrix in [30]. We use the <lb/>unsupervised semantic parser and conceptualization based semantic filter proposed in [1] to <lb/>generate the semantic meaning of each document. The output is the document associated <lb/>with not only the entities but also the types and relations. In addition to the named entities, <lb/>document and word are also regarded as two types. Following [1], the network contains <lb/>multiple entity types: document , word , named entities <lb/>, and relation types <lb/>connecting the entity types. Different from [1] which uses coarse-grained entity types such <lb/>as Person, Location, and Organization to construct HIN, we prefer to use more fine-grained <lb/>entity types, such as Politician, Musician, and President since they provide refined <lb/>semantics to represent document similarity. However, in Freebase, there are about 1, 500+ <lb/>entity types and 3, 500+ relation types, which will generate an exponential number of meta-<lb/>paths. In previous work [26], [31], meta-paths are provided by users, which is doable for <lb/>networks with simple schema consisting of several types of entities and relations, such as the <lb/>DBLP network (five entity types and four relation types). It is unrealistic to ask a user to <lb/>specify meta-paths for a network with a large number of entities and relations. An automatic <lb/>mechanism should be developed to generate all the interested meta-paths. <lb/>By representing the world knowledge in HIN, two documents can be linked together via <lb/>many meta-paths. Assuming that similar documents are structurally similar defined by <lb/>symmetric meta-paths, we only explore symmetric meta-paths. The calculation based on the <lb/>meta-paths is to compute all the corresponding commuting matrices of interests. <lb/>Consequently, the size of network brings a critical issue since it is impossible to compute all <lb/>the commuting matrices and load them into memory. To make the method practical, we <lb/>propose two ways to prune this computation: (i) prune the large network to generate a more <lb/>compact graph for the interested commuting matrices calculation (Section III), and (ii) use <lb/>unsupervised feature selection approaches to select semantically meaningful meta-paths for <lb/>final document similarity computation (Section IV). <lb/></body>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 3 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<body>III. Offline Meta-Path Calculation <lb/>It is costly to compute the commuting matrix for a meta-path involving multiple entity types <lb/>since it requires a matrix multiplication to compute two consecutive relations connecting <lb/>entity types in the path [26]. It is unnecessary to use the full HIN constructed in the previous <lb/>section, since not all the entities are related. Inspired by Lao et al.&apos; work ([32], [33]), we use <lb/>a meta-path dependent random walk to reduce the complexity of the HIN inference. We <lb/>adopt a similar random walk algorithm which is based on personalized random walk [27] <lb/>with stops to enumerate all the meta-path relevant nodes in the HIN. We employ the <lb/>modified version of approximate personalized PageRank called PageRank-Nibble algorithm <lb/>[27]. The advantage of using this algorithm is that we can have a theoretical guarantee of the <lb/>random walk approximation to the original HIN in the sense of the network structure. The <lb/>goal of PageRank-Nibble algorithm is to find a small, low-conductance component ̂ of a <lb/>large graph = ( , ℰ) that contains a given node v. In our setting, instead of a single given <lb/>node, we need ̂ that contains a node set ̂. Specifically, in our case, we need the set of <lb/>documents so that = ̂ ⊆ . The PageRank-Nibble algorithm starting with a node set ̂ is <lb/>called Meta-path Dependent PageRank-Nibble (as outlined in Algorithm 1). <lb/>Algorithm 1 <lb/>Meta-path Dependent PageRank-Nibble( , , ̂, α, ε) <lb/>Input: A graph , a meta-path , a node set ̂, and two parameters: α and ε. <lb/>Output: A compact graph ̂ of a large graph that contains the given node set ̂. <lb/>1 <lb/>Compute an approximate PageRank vector p with residual vector r initialized with function χ̂ according to <lb/>the given node set ̂, satisfying <lb/>following [27]. The random walk terminates when <lb/>meeting the entities not included in the given meta-path . <lb/>2 <lb/>Check each set <lb/>with j ∈ [1, |Supp(p)|], to see if the conductance: <lb/>is the smallest one. <lb/>3 <lb/>Return ̂ that contains all the nodes <lb/>. Otherwise, return ∅. <lb/>Based on the proof in [27], for any graph, a good approximation can be guaranteed, thus <lb/>satisfy the efficiency bound, which holds independent of the size of the network. So this <lb/>pruning strategy will work on very large networks, such as our specified world knowledge <lb/>HIN. <lb/>After generating the local graph ̂ for meta-path , we compute the commuting matrix <lb/>[26] M for each meta-path based on the local graph. Notice that we only consider the <lb/>symmetric meta-paths, it is easy to see that the commuting matrix can be decomposed. For <lb/>example, suppose the meta-path is <lb/>where <lb/>is the reverse path of l . Then <lb/>the commuting matrix is <lb/>, where M l and <lb/>are the <lb/>commuting matrices for l and <lb/>. Thus, only M l is needed to be precomputed and <lb/>stored. <lb/> The meta-paths are then generated in the following steps. <lb/></body>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 4 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<body>1. Given a maximum length L of the symmetric meta-path <lb/>, enumerate <lb/>all l within ⌈L/2⌉ consisting of different orders of entity types in <lb/>connected. The resulting meta-path set is denoted as P = { }. <lb/>2. For each meta-path ∈ P: <lb/>a. Generate the corresponding local graph ̂ based on the Meta-path <lb/>Dependent PageRank-Nibble given the node set ̂ = {d ∈ }. <lb/>b. Compute the commuting matrices for l and store the commuting matrices. <lb/>IV. HIN-Based Document Similarity <lb/>In this section, we introduce HIN-based document similarity measure, KnowSim. We present <lb/>our meta-path weighting methodology based on two feature selection techniques which can <lb/>speed up the similarity computation using the precomputed commuting matrices. <lb/>Given the document HIN extracted from the world knowledge base, meta-paths can be used <lb/>to compute the similarity between documents. PathSim [26] is proposed to define the <lb/>similarity along a meta-path. However, previous approaches require human to define the <lb/>mata-path(s). Here we should have multiple meta-paths useful for finding similar <lb/>documents. Therefore, it is necessary to provide an automated mechanism to select the most <lb/>meaningful meta-paths to define similarity between documents. <lb/>A. Meta-Path Selection <lb/>We first define the document-meta-path representation, and then use two feature selection <lb/>methods to perform automatic meta-path selection. <lb/>1) Document-Meta-Path Representation-For each meta-path j , we have a <lb/>commuting matrix M j . Suppose we have N documents and M interested (automatically <lb/>generated) meta-paths. Then we can use a tensor T ∈ ℝ M×N×N to encode all the numbers of <lb/>meta-paths, where T j,i,k = M j (i, k). Based on this tensor representation, we can have <lb/>different similarities between documents or between meta-paths. Here we propose to use a <lb/>simplest way based on document-meta-path co-occurrence representation. We generate a <lb/>document meta-path representation matrix D ∈ ℝ N×M where Di,j = Σ k T j,i,k , which means <lb/>that D i,j is the row sum of M j . Summing the i-th row of M j represents the density degree <lb/>of this meta-path j for document i. If the meta-path j is dense for document i in the HIN, then <lb/>most pairs related to document i should have value in M j . Then D i,j will be large. Then we <lb/>can use the distribution of density over all the documents to evaluate the meta-path <lb/>similarity. Specifically, we can define sim(D •,j1 , D •,j2 ) where D •,j1 is the j 1 -th column of D. <lb/>For example, we can use cosine score of two vectors or kernels to define the similarity. <lb/>Moreover, we can define the document similarity based on all the meta-path densities for the <lb/>documents. Specifically, we can define sim(D i1,• , D i2,• ) where D i1,• is the i 1 -th row of D. <lb/>Note that we do not use this document similarity as our final similarity between two <lb/>documents because it is only based on meta-path density. What we need is more elaborate <lb/>document similarity based on each document meta-path pair. We will introduce the meta-<lb/>path specific semantically meaningful similarity in the next subsection. <lb/></body>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 5 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<body>Given the similarities defined above, we introduce two feature selection methods based on <lb/>them to select the most meaningful meta-paths. <lb/>2) Maximal Spanning Tree based Selection-Inspired by the mutual information-<lb/>based feature selection [28], [34], we propose to use maximal spanning tree (MST) to select <lb/>only the meta-paths with the largest dependencies with others. The motivation behind using <lb/>MST is that &quot;features that only weakly influence the remaining domain variables are <lb/>candidates for elimination&quot; for mixture models [28]. Intuitively, if two meta-paths have <lb/>similar density distributions over all the documents, then these two meta-paths are <lb/>dependent. Therefore, we replace the mutual information in the original one with cosine <lb/>similarity due to the consideration of the computational cost. <lb/>3) Laplacian Score based Selection-We also use the Laplacian score to select meta-<lb/>paths [29], [34]. Different from the MST based method that reflects the dependency between <lb/>meta-paths, the Laplacian score represents the power of a meta-path in discriminating <lb/>documents from different clusters. <lb/>B. KnowSim: Knowledge-Driven Document Similarity <lb/>Given the selected meta-paths, we now define our knowledge-driven document similarity <lb/>measure, KnowSim. Intuitively, if two documents are more strongly connected by the <lb/>important (i.e., highly weighted) meta-paths, they tend to be more similar. Formally, we <lb/>have <lb/>Definition 1: KnowSim: a knowledge-driven document similarity measure. Given a <lb/>collection of symmetric meta-paths, denoted as <lb/>, KnowSim between two <lb/>documents d i and d j is defined as: <lb/>(1) <lb/>where p i↝j ∈ m is a path instance between d i and d j following meta-path m , p i↝i ∈ m is <lb/>that between d i and d i , and p j↝j ∈ m is that between d j and d j . We have |{p i↝j ∈ m }| = <lb/>M m (i, j), |{p i↝i ∈ m }| = M m (i, i), and |{p j↝j ∈ m }| = M m (j, j). We use a vector ω <lb/>= [ω 1 , …, ω m , …, ω M′ ] to denote the meta-path weights, where ω m is the weight of meta-<lb/>path m . M′ is the number of selected meta-paths. <lb/>KS(d i , d j ) is defined in two parts: (1) the semantic overlap in the numerator, which is defined <lb/>by the number of meta-paths between documents d i and d j ; and (2) the semantic broadness <lb/>in the denominator, which is defined by the number of total meta-paths between themselves. <lb/>We can see that the larger number of meta-paths between d i and d j , the more similar the two <lb/>documents are, which is further normalized by the semantic broadness of d i and d j . <lb/> Note that KnowSim can be generalized to measure the similarity between any two entities <lb/>rather than documents, which will be very helpful to determine the entity similarity, because <lb/></body>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 6 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<body>we take more link information into consideration rather than a single meta-path. If <lb/>KnowSim only contains a single meta-path, it degenerates to PathSim. <lb/>V. Experiments <lb/>This section reports our experiments which demonstrate the effectiveness and efficiency of <lb/>our approach to measuring document similarity. <lb/>A. Datasets <lb/>We use the following two benchmark datasets to evaluate the document similarity task. <lb/>20Newsgroups (20NG): The 20newsgroups dataset [35] contains about 20,000 newsgroups <lb/>documents evenly distributed across 20 newsgroups. 1 <lb/>RCV1: The RCV1 dataset is a dataset containing manually labeled newswire stories from <lb/>Reuter Ltd [36]. The news documents are categorized with respect to three controlled <lb/>vocabularies: industries, topics and regions. There are 103 categories including all nodes <lb/>except for root in the topic hierarchy. The maximum depth is four, and 82 nodes are leaves. <lb/>We select top category GCAT (Government/Social) to form the document similarity task. In <lb/>total, we have 60,608 documents with 16 leaf categories. <lb/>The ground-truth of document similarity is generated as follows: If two documents are in the <lb/>same group or the same leaf category, their similarity equals to 1; otherwise, it is 0. <lb/>B. World Knowledge Base <lb/>Then we introduce the knowledge base we use. In [1], the authors have demonstrated that <lb/>Freebase is more effective compared to YAGO2, so we only use Freebase as our world <lb/>knowledge source in this experiment. <lb/>Freebase: Freebase 2 is a publicly available knowledge base consisting of entities and <lb/>relations collaboratively collected by its community members. So far, it contains over 2 <lb/>billions relation expressions between 40 millions entities. Moreover, there are 1,500+ entity <lb/>types and 3,500+ relation types in Freebase. We convert a logical form generated by <lb/>unsupervised semantic parser into a SPARQL query and execute it on our copy of Freebase <lb/>using the Virtuoso engine. <lb/>After performing semantic parsing and filtering, the numbers of entities in different <lb/>document datasets with Freebase are summarized in Table I. The numbers of relations <lb/>(logical forms parsed by semantic parsing and filtering) in 20NG and GCAT are 9, 655, 466 <lb/>and 18, 008, 612, respectively. We keep 20 and 43 entity types for 20NG and GCAT <lb/>respectively, because they have relatively larger number of instances. Then 325 and 1, 682 <lb/>symmetric meta-paths are generated based on the MDPN algorithm (Section III), for 20NG <lb/>and GCAT respectively. We can save around 3.8 hours and 19.6 hours for the corresponding <lb/>datasets. The reason is that MDPN shares the similar nature with PageRank-Nibble, which is <lb/></body>

			<note place="footnote">1 http://qwone.com/∼jason/20Newsgroups/ <lb/></note>

			<note place="footnote">2 https://developers.google.com/freebase/ <lb/></note>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 7 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<body>that the running time is independent of the size of the graph. Similar result is found when <lb/>comparing the space usage. By using MDPN, we can save up to 1.4G storage (15.2%) <lb/>compared to storing the exact commuting matrices. In our real setting, we can save 45.5G <lb/>and 235.5G storage for 20NG and GCAT datasets, respectively. Because MDPN only saves <lb/>the nodes that have relatively high degree, which is important in sparse matrix. <lb/>C. Similarity Results <lb/>In this experiment, we compare the performance of our document similarity measure, <lb/>KnowSim, with three representative similarity measures: cosine, Jaccard and dice. We <lb/>denote KnowSim+UNI, KnowSim+MST and KnowSim+LAP as KnowSim with uniform <lb/>weights, weights determined by MST and Laplacian score-based methods introduced in <lb/>Section IV-A. Following [1], we use the specified world knowledge as features to enhance <lb/>cosine, Jaccard and dice. The feature settings are defined as follows. <lb/>• BOW: Traditional bag-of-words model with the tf weighting mechanism. <lb/>• BOW+TOPIC: BOW integrated with additional features from topics generated by <lb/>LDA [37]. According to the number of domains that 20NG and GCAT have, we <lb/>assign 20 topics and 16 topics to 20NG and GCAT, respectively. <lb/>• BOW+ENTITY: BOW integrated with additional features from entities in specified <lb/>world knowledge from Freebase. <lb/>• BOW+TOPIC+ENTITY: BOW integrated with additional features from both <lb/>topics generated by LDA and entities in specified world knowledge from Freebase. <lb/>We employ the widely-used correlation coefficient as the evaluation measure. The <lb/>correlation score is 1 if the similarity results match the ground-truth perfectly and 0 if the <lb/>similarity results are random. In general, the larger the scores, the better the similarity <lb/>results. <lb/>In Table II, we show the performance of all the similarity measures with different <lb/>experimental settings on both 20NG and GCAT datasets. Overall, among all the methods we <lb/>test, KnowSim+LAP consistently performs the best. The reason is that Laplacian score <lb/>could discriminate documents from different clusters, which is strongly correlated to our <lb/>similarity task. We can also see that KnowSim+UNI, KnowSim+MST and KnowSim+LAP <lb/>outperform all the other similarity measures, including the similarity measures with <lb/>specified world knowledge as flat features (BOW+ENTITY). This means that by using <lb/>structural information in HIN extracted from the world knowledge, we can improve the <lb/>document similarity, especially comparing with just using them as flat features. Also, <lb/>KnowSim-based similarity measures perform better than the similarity measures with <lb/>feature setting &quot;BOW+TOPIC.&quot; The reason is again world knowledge could provide the <lb/>structural information between documents rather than using the flat topic distribution. In <lb/>addition, one can also see that KnowSim+UNI performs relatively weaker than the other two <lb/>KnowSim with weighted meta-paths. This means that our meta-path weighting methods do <lb/>help find the important link information (i.e., meta-paths) related to certain domains. <lb/>Moreover, we find the improvement of KnowSim on GCAT is more than that on 20NG. As <lb/>Table I shows, GCAT has more entities and associated types specified by the world <lb/></body>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 8 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<body>knowledge. This means that the more world knowledge we can find or use in the documents, <lb/>the better improvement in the document similarity task. This suggests that if there exists <lb/>world knowledge bases with better precision and coverage, we could get better performance. <lb/>D. Application: Spectral Clustering Using KnowSim Matrix <lb/>To check the quality of different similarity measures in the real application scenario, we <lb/>further use similarity matrices generated above as the weight matrix in the spectral <lb/>clustering [38] for document clustering task. We compare the performance of clustering <lb/>results of using three different KnowSim-based similarity matrices with using the similarity <lb/>matrices generated by other similarity measures. We set the number of clusters as 20 and 16 <lb/>for 20NG and GCAT according to their ground-truth labels, respectively. We employ the <lb/>widely-used normalized mutual information (NMI) [39] as the evaluation measure. The <lb/>NMI score is 1 if the clustering results match the category labels perfectly and 0 if the <lb/>clusters are obtained from a random partition. In general, the larger the scores, the better the <lb/>clustering results. <lb/>As shown in Table III, we illustrate the performance of all the clustering results with <lb/>different similarity matrices on both 20NG and GCAT datasets. The NMI is the average <lb/>NMI of five random trials per experiment setting. Among all the methods we tested, spectral <lb/>clustering with KnowSim+LAP matrix performs the best, which is consistent with the <lb/>similarity correlation results (Table II). Moreover, all of the KnowSim similarity matrix-<lb/>based clustering results consistently outperform the other methods. Note that the three <lb/>KnowSim-based matrices produce higher NMI compared to that with &quot;BOW+ENTITY,&quot; <lb/>which means using the meta-path as link information in the similarity matrix, the link <lb/>information can be passed to the clustering results, where the link information can be very <lb/>useful to group similar documents in the same cluster. We can infer that KnowSim could <lb/>have positive impact on other similarity-based applications, e.g., document classification. <lb/>VI. Conclusion <lb/>In this paper, we use semantic parsing and semantic filtering modules to specify the world <lb/>knowledge to domains, and then model the specified world knowledge in the form of <lb/>heterogeneous information network, which enables to represent the link information for the <lb/>documents. By defining a novel document similarity measure, KnowSim (document <lb/>similarity with world knowledge), the similarity between documents can be measured based <lb/>on the automatically generated meta-paths in the HIN constructed from the documents. <lb/></body>

			<div type="acknowledgement">Acknowledgments <lb/>Chenguang Wang gratefully acknowledges the support by the National Natural Science Foundation of China <lb/>(NSFC Grant Number 61472006) and the National Basic Research Program (973 Program No. 2014CB340405). <lb/>The research is also partially supported by the Army Research Laboratory (ARL) under agreement <lb/>W911NF-09-2-0053, and by DARPA under agreement number FA8750-13-2-0008. Research is also partially <lb/>sponsored by National Science Foundation IIS-1017362, IIS-1320617, and IIS-1354329, HDTRA1-10-1-0120, and <lb/>grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge <lb/>(BD2K) initiative (www.bd2k.nih.gov), and MIAS, a DHS-IDS Center for Multimodal Information Access and <lb/>Synthesis at UIUC. The views and conclusions contained herein are those of the authors and should not be <lb/>interpreted as necessarily representing the official policies or endorsements, either expressed or implied by these <lb/>agencies or the U.S. Government. <lb/></div>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 9 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<listBibl>References <lb/>1. Wang C, Song Y, El-Kishky A, Roth D, Zhang M, Han J. Incorporating world knowledge to <lb/>document clustering via heterogeneous information networks. KDD. 2015:1215-1224. [PubMed: <lb/>26705504] <lb/>2. Hotho A, Staab S, Stumme G. Ontologies improve text document clustering. ICDM. 2003:541-544. <lb/>3. Gabrilovich E, Markovitch S. Feature generation for text categorization using world knowledge. <lb/>IJCAI. 2005:1048-1053. <lb/>4. Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit <lb/>semantic analysis. IJCAI. 2007:1606-1611. <lb/>5. Hu J, Fang L, Cao Y, Zeng HJ, Li H, Yang Q, Chen Z. Enhancing text clustering by leveraging <lb/>Wikipedia semantics. SIGIR. 2008:179-186. <lb/>6. Hu X, Zhang X, Lu C, Park EK, Zhou X. Exploiting wikipedia as external knowledge for document <lb/>clustering. KDD. 2009:389-396. <lb/>7. Hu X, Sun N, Zhang C, Chua TS. Exploiting internal and external semantics for the clustering of <lb/>short texts using world knowledge. CIKM. 2009:919-928. <lb/>8. Song Y, Roth D. On dataless hierarchical text classification. AAAI. 2014:1579-1585. <lb/>9. Song Y, Roth D. Unsupervised sparse vector densification for short text similarity. NAACL. 2015 <lb/>10. Song Y, Wang H, Wang Z, Li H, Chen W. Short text conceptualization using a probabilistic <lb/>knowledgebase. IJCAI. 2011:2330-2336. <lb/>11. Song Y, Wang S, Wang H. Open domain short text conceptualization: A generative + descriptive <lb/>modeling approach. IJCAI. 2015 <lb/>12. Wang C, Duan N, Zhou M, Zhang M. Paraphrasing adaptation for web search ranking. ACL. <lb/>2013:41-46. <lb/>13. Budanitsky A, Hirst G. Evaluating wordnet-based measures of lexical semantic relatedness. <lb/>Computational Linguistics. 2006; 32(1):13-47. <lb/>14. Do Q, Roth D, Sammons M, Tu Y, Vydiswaran V. Robust, light-weight approaches to compute <lb/>lexical similarity. 2009 <lb/>15. Wan X, Peng Y. The earth mover&apos;s distance as a semantic measure for document similarity. CIKM. <lb/>2005:301-302. <lb/>16. Cohen WW, Ravikumar P, Fienberg SE. A comparison of string distance metrics for name-<lb/>matching tasks. IJCAI Workshop on Information Integration. 2003:73-78. <lb/>17. Bollacker KD, Evans C, Paritosh P, Sturge T, Taylor J. Freebase: a collaboratively created graph <lb/>database for structuring human knowledge. SIGMOD. 2008:1247-1250. <lb/>18. Etzioni O, Cafarella M, Downey D. Webscale information extraction in knowitall (preliminary <lb/>results). WWW. 2004:100-110. <lb/>19. Banko M, Cafarella MJ, Soderland S, Broadhead M, Etzioni O. Open information extraction from <lb/>the web. IJCAI. 2007:2670-2676. <lb/>20. Ponzetto SP, Strube M. Deriving a large-scale taxonomy from wikipedia. AAAI. 2007:1440-1445. <lb/>21. Auer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyganiak, R.; Ives, Z. Dbpedia: A nucleus for a <lb/>web of open data. Springer; 2007. <lb/>22. Suchanek FM, Kasneci G, Weikum G. Yago: a core of semantic knowledge. WWW. 2007:697-<lb/>706. <lb/>23. Mitchell TM, Cohen WW, H ER Jr, Talukdar PP, Betteridge J, Carlson A, Mishra BD, Gardner M, <lb/>Kisiel B, Krishnamurthy J, Lao N, Mazaitis K, Mohamed T, Nakashole N, Platanios EA, Ritter A, <lb/>Samadi M, Settles B, Wang RC, Wijaya DT, Gupta A, Chen X, Saparov A, Greaves M, Welling J. <lb/>Never-ending learning. AAAI. 2015:2302-2310. <lb/>24. Dong X, Gabrilovich E, Heitz G, Horn W, Lao N, Murphy K, Strohmann T, Sun S, Zhang W. <lb/>Knowledge vault: A web-scale approach to probabilistic knowledge fusion. KDD. 2014:601-610. <lb/>25. Han J, Sun Y, Yan X, Yu PS. Mining knowledge from databases: An information network analysis <lb/>approach. SIGMOD. 2010:1251-1252. <lb/>26. Sun Y, Han J, Yan X, Yu PS, Wu T. Pathsim: Meta path-based top-k similarity search in <lb/>heterogeneous information networks. PVLDB. 2011:992-1003. <lb/></listBibl>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 10 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<listBibl>27. Andersen R, Chung F, Lang K. Local graph partitioning using pagerank vectors. FOCS. 2006:475-<lb/>486. <lb/>28. Sahami, M. Ph D dissertation. stanford university; 1998. Using machine learning to improve <lb/>information access. <lb/>29. He X, Cai D, Niyogi P. Laplacian score for feature selection. NIPS. 2006:507-514. <lb/>30. Sun Y, Han J. Mining heterogeneous information networks: principles and methodologies. <lb/>Synthesis Lectures on Data Mining and Knowledge Discovery. 2012; 3(2):1-159. <lb/>31. Sun Y, Norick B, Han J, Yan X, Yu PS, Yu X. Integrating meta-path selection with user-guided <lb/>object clustering in heterogeneous information networks. KDD. 2012:1348-1356. <lb/>32. Lao N, Cohen WW. Relational retrieval using a combination of path-constrained random walks. <lb/>Machine learning. 2010; 81(1):53-67. <lb/>33. Lao N, Mitchell T, Cohen WW. Random walk inference and learning in a large scale knowledge <lb/>base. EMNLP. 2011:529-539. <lb/>34. Song Y, Pan S, Liu S, Zhou MX, Qian W. Topic and keyword re-ranking for lda-based topic <lb/>modeling. CIKM. 2009:1757-1760. <lb/>35. Lang K. Newsweeder: Learning to filter netnews. ICML. 1995:331-339. <lb/>36. Lewis DD, Yang Y, Rose TG, Li F. RCV1: A new benchmark collection for text categorization <lb/>research. JMLR. 2004; 5:361-397. <lb/>37. Blei DM, Ng AY, Jordan MI. Latent Dirichlet allocation. JMLR. 2003; 3:993-1022. <lb/>38. Zelnik-manor, L.; Perona, P. Self-tuning spectral clustering. In: Saul, L.; Weiss, Y.; Bottou, L., <lb/>editors. NIPS. 2005. p. 1601-1608. <lb/>39. Strehl A, Ghosh J. Cluster ensembles-a knowledge reuse framework for combining multiple <lb/>partitions. JMLR. 2003; 3:583-617. <lb/></listBibl>

			<note place="headnote">Wang et al. <lb/></note>

			<page>Page 11 <lb/></page>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Author Manuscript <lb/>Wang et al. <lb/></note>

			<page>Page 12 <lb/></page>

			<body>Table I <lb/>Statistics of entities in different datasets with semantic parsing and filtering using Freebase: #(Document) is the number of all documents; similar for <lb/>#(Word) (# of words), #(FBEntity) (# of Freebase entities), #(Total) (the total # of entities), and #Types (the total # of entity types). <lb/>#(Document) #(Word) #(FBEntity) #(Total) #(Types) <lb/>20NG <lb/>19,997 <lb/>60,691 <lb/>28,034 <lb/>108,722 <lb/>2,615 <lb/>GCAT <lb/>60,608 <lb/>95,001 <lb/>110,344 <lb/>265,953 <lb/>2,665 <lb/></body>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/> Author Manuscript <lb/>Author Manuscript <lb/>Wang et al. <lb/></note>

			<page>Page 13 <lb/></page>

			<body>Table II <lb/> Correlation coefficient of different similarity measures on 20NG and GCAT. &quot;BOW&quot; represents bag-of-words as features; &quot;BOW+TOPIC&quot; represents <lb/>bag-of-words plus topics generated by LDA as features; &quot;BOW+ENTITY&quot; represents bag-of-words plus entities as features; &quot;BOW+TOPIC+ENTITY&quot; <lb/>represents bag-of-words plus topics plus entities as features. <lb/>Dataset <lb/>Similarity Measures <lb/>BOW <lb/>BOW+TOPIC BOW+ENTITY BOW+TOPIC+ENTITY <lb/>20NG <lb/>Cosine <lb/>0.2400 <lb/>0.2713 <lb/>0.2473 <lb/>0.2768 <lb/>Jaccard <lb/>0.2352 <lb/>0.2632 <lb/>0.2369 <lb/>0.2650 <lb/>Dice <lb/>0.2400 <lb/>0.2712 <lb/>0.2474 <lb/>0.2767 <lb/>KnowSim+UNI <lb/>0.2860 <lb/>KnowSim+MST <lb/>0.2891 <lb/>KnowSim+LAP <lb/>0.2913 (+5.2%) <lb/>GCAT <lb/>Cosine <lb/>0.3490 <lb/>0.3639 <lb/>0.2473 <lb/>0.3128 <lb/>Jaccard <lb/>0.3313 <lb/>0.3460 <lb/>0.2319 <lb/>0.2991 <lb/>Dice <lb/>0.3490 <lb/>0.3638 <lb/>0.2474 <lb/>0.3156 <lb/>KnowSim+UNI <lb/>0.3815 <lb/>KnowSim+MST <lb/>0.3833 <lb/>KnowSim+LAP <lb/>0.4086 (+12.3%) <lb/></body>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. <lb/></note>

			<note place="headnote">Author Manuscript <lb/>Author Manuscript <lb/> Author Manuscript <lb/>Author Manuscript <lb/>Wang et al. <lb/></note>

			<page>Page 14 <lb/></page>

			<body>Table III <lb/> NMI of clustering on 20NG and GCAT using the similarity matrix generated by different similarity measures. &quot;BOW&quot; represents bag-of-words as <lb/>features; &quot;BOW+TOPIC&quot; represents bag-of-words plus topics generated by LDA as features; &quot;BOW+ENTITY&quot; represents bag-of-words plus entities as <lb/>features; &quot;BOW+TOPIC+ENTITY&quot; represents bag-of-words plus topics plus entities as features. <lb/>Dataset <lb/>Similarity Matrix Source <lb/>BOW <lb/>BOW+TOPIC BOW+ENTITY BOW+TOPIC+ENTITY <lb/>20NG <lb/>Cosine <lb/>0.3440 <lb/>0.3461 <lb/>0.3896 <lb/>0.4247 <lb/>Jaccard <lb/>0.3547 <lb/>0.3517 <lb/>0.3850 <lb/>0.4292 <lb/>Dice <lb/>0.3440 <lb/>0.3457 <lb/>0.3894 <lb/>0.4248 <lb/>KnowSim+UNI <lb/>0.4304 <lb/>KnowSim+MST <lb/>0.4412 <lb/>KnowSim+LAP <lb/>0.4461 (+3.9%) <lb/>GCAT <lb/>Cosine <lb/>0.3932 <lb/>0.4352 <lb/>0.2394 <lb/>0.4106 <lb/>Jaccard <lb/>0.3887 <lb/>0.4292 <lb/>0.2497 <lb/>0.4159 <lb/>Dice <lb/>0.3932 <lb/>0.4355 <lb/>0.2392 <lb/>0.4112 <lb/>KnowSim+UNI <lb/>0.4463 <lb/>KnowSim+MST <lb/>0.4653 <lb/>KnowSim+LAP <lb/>0.4736 (+8.8%) <lb/></body>

			<note place="footnote">Proc IEEE Int Conf Data Min. Author manuscript; available in PMC 2016 March 29. </note>


	</text>
</tei>
