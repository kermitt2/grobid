<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title level="a">A Case for Synthesis of Recursive Quantum Unitary Programs</title>
        <author>
          <persName>
            <forename>Haowei</forename>
            <surname>Deng</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Runzhou</forename>
            <surname>Tao</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Yuxiang</forename>
            <surname>Peng</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Xiaodi</forename>
            <surname>Wu</surname>
          </persName>
        </author>
      </titleStmt>
      <editionStmt>
        <edition>
          <date when="2025-10-29T15:19:10.124216Z">29.10.2025 15:19:10</date>
          <title>grobid.training.segmentation [default]</title>
          <idno type="fileref">10.1145$1$3632901</idno>
        </edition>
      </editionStmt>
      <publicationStmt>
        <publisher>Association for Computing Machinery (ACM)</publisher>
        <availability>
          <licence target="https://creativecommons.org/licenses/by/4.0/"/>
        </availability>
        <date type="publication">2024</date>
        <idno type="DOI">10.1145/3632901</idno>
      </publicationStmt>
      <sourceDesc>
        <bibl>Haowei Deng, Runzhou Tao, Yuxiang Peng, Xiaodi Wu. (2024). A Case for Synthesis of Recursive Quantum Unitary Programs. Proceedings of the ACM on Programming Languages, 8(POPL), 1759-1788. DOI: 10.1145/3632901</bibl>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <appInfo>
        <application version="1.0" ident="pdf-tei-editor" type="editor">
          <ref target="https://github.com/mpilhlt/pdf-tei-editor"/>
        </application>
        <application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-10-29T15:19:10.124216Z" type="extractor">
          <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
          <label type="revision">eb7768b</label>
          <label type="flavor">default</label>
          <label type="variant-id">grobid.training.segmentation</label>
          <ref target="https://github.com/kermitt2/grobid"/>
        </application>
      </appInfo>
    </encodingDesc>
    <revisionDesc>
      <change when="2025-10-29T15:19:10.124216Z" status="draft">
        <desc>Generated with createTraining API</desc>
      </change>
    </revisionDesc>
  </teiHeader>
  <text xmlns="http://www.tei-c.org/ns/1.0" xml:lang="en">
        <front>Mechanizing Refinement Types <lb/>MICHAEL H. BORKOWSKI, UC San Diego, USA <lb/>NIKI VAZOU, IMDEA Software Institute, Spain <lb/>RANJIT JHALA, UC San Diego, USA <lb/>Practical checkers based on refinement types use the combination of implicit semantic subtyping and parametric <lb/>polymorphism to simplify the specification and automate the verification of sophisticated properties of programs. <lb/>However, a formal metatheoretic accounting of the soundness of refinement type systems using this combination <lb/>has proved elusive. We present <lb/>, a core refinement calculus that combines semantic subtyping and parametric <lb/>polymorphism. We develop a metatheory for this calculus and prove soundness of the type system. Finally, we <lb/>give two mechanizations of our metatheory. First, we introduce data propositions, a novel feature that enables <lb/>encoding derivation trees for inductively defined judgments as refined data types, and use them to show that <lb/>LiqidHaskell&apos;s refinement types can be used for mechanization. Second, we mechanize our results in Coq, <lb/>which comes with stronger soundness guarantees than LiqidHaskell, thereby laying the foundations for <lb/>mechanizing the metatheory of LiqidHaskell. <lb/>CCS Concepts: • Software and its engineering → Formal software verification. <lb/>Additional Key Words and Phrases: refinement types, LiquidHaskell <lb/>ACM Reference Format: <lb/>Michael H. Borkowski, Niki Vazou, and Ranjit Jhala. 2024. Mechanizing Refinement Types. Proc. ACM Program. <lb/>Lang. 8, POPL, Article 70 (January 2024), 30 pages. https://doi.org/10.1145/3632912 <lb/></front>

        <body>1 INTRODUCTION <lb/>Refinements constrain types with logical predicates to specify new concepts. For example, the refine-<lb/>ment type Pos Int{ :0 &lt; } describes positive integers and Nat Int{ :0 ≤ } specifies natural <lb/>numbers. Refinement types have been successfully used to specify various properties like secrecy <lb/>[Fournet et al. 2011], resource usage [Knoth et al. 2020], or information flow [Lehmann et al. 2021] that <lb/>can then be verified in programs developed in various programming languages like Haskell [Vazou <lb/>et al. 2014b], Scala [Hamza et al. 2019], and Racket [Kent et al. 2016]. <lb/>The success of refinement types relies on the combination of two essential features. First, implicit <lb/>semantic subtyping uses semantic (SMT-based) reasoning to automatically convert the types of <lb/>expressions without hassling the programmer for explicit type casts. For example, consider a positive <lb/>expression : Pos and a function expecting natural numbers : Nat → Int. To type check the appli-<lb/>cation , the refinement type system will implicitly convert the type of from Pos to Nat, because <lb/>0 &lt; ⇒ 0 ≤ holds semantically. Importantly, refinement types propagate semantic subtyping <lb/>inside type constructors to, for example, treat function arguments in a contravariant manner. Second, <lb/>parametric polymorphism allows the propagation of the refined types through polymorphic function <lb/>interfaces, without the need for extra reasoning. As a trivial example, once we have established that <lb/>is positive, parametric polymorphism should let us conclude that : Pos if, for example, is the <lb/></body>

        <front>Authors&apos; addresses: Michael H. Borkowski, UC San Diego, La Jolla, USA, mborkows@eng.ucsd.edu; Niki Vazou, IMDEA <lb/>Software Institute, Madrid, Spain, niki.vazou@imdea.org; Ranjit Jhala, UC San Diego, USA, rjhala@ucsd.edu. <lb/>© 2024 Copyright held by the owner/author(s). <lb/>ACM 2475-1421/2024/1-ART70 <lb/>https://doi.org/10.1145/3632912 <lb/>Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/>This work is licensed under a Creative Commons Attribution 4.0 International License. <lb/></front>

        <page>70:2 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>identity function : → . As a more interesting example, in § 2 we combine semantic subtyping <lb/>and polymorphism to verify a safe-indexing array of prime numbers. <lb/>The engineering of practical refinement type checkers has galloped far ahead of the development <lb/>of their metatheoretical foundations. In fact, semantic subtyping is very tricky as it is mutually <lb/>defined with typing, leading to metatheoretic proofs with circular dependencies (Figure 2). Unsur-<lb/>prisingly, the addition of polymorphism poses further challenges. As Sekiyama et al. [2017] observe, <lb/>a naïve definition of type instantiation can lose potentially contradicting refinements leading to <lb/>unsoundness. Existing formalizations of refinement types drop semantic subtyping [Hamza et al. <lb/>2019; Sekiyama et al. 2017] or polymorphism [Flanagan 2006; Swamy et al. 2016], or have problematic <lb/>metatheory [Belo et al. 2011]. <lb/>In this paper we formalize <lb/>, a core calculus with a refinement type system that combines <lb/>semantic subtyping with polymorphism, via four concrete contributions. <lb/>1. Reconciliation Our first contribution is a language that combines refinements and polymorphism <lb/>in a way that ensures the metatheory remains sound without sacrificing the expressiveness needed for <lb/>practical verification. To this end, <lb/>introduces a kind system that distinguishes the type variables <lb/>that can be soundly refined (without the risk of losing refinements at instantiation) from the rest, <lb/>which are then left unrefined. In addition our design includes a form of existential typing [Knowles <lb/>and Flanagan 2009] which is essential to synthesize the types -in the sense of bidirectional typing <lb/>-for applications and let-binders in a compositional manner ( § 3, 4). <lb/>2. Foundation Our second contribution is to establish the foundations of <lb/>by proving soundness, <lb/>which says that well-typed expressions cannot get stuck and belong in the denotation of their type <lb/>( § 5). The combination of semantic subtyping, polymorphism, and existentials makes the soundness <lb/>proof challenging with circular dependencies that do not arise in standard (unrefined) calculi. The <lb/>mechanization was simplified by the use of two essential ingredients. First, we use an unrefined base <lb/>language , a classic System F [Pierce 2002], in rules where refinements are not required, cutting two <lb/>potential circularities in the static judgments (Figure 2). Second, we define an implication interface <lb/>that abstractly specifies the properties of implication required to prove type soundess, and show how <lb/>this interface can be implemented via denotational semantics ( § 4.4). <lb/>3. Reification Our third contribution is to introduce data propositions, a novel feature that enables the <lb/>encoding of derivation trees for inductively defined judgments as refined data types, by first reifying <lb/>the propositions and evidence as plain Haskell data, and then using refinements to connect the two. <lb/>Hence, data propositions let us write plain Haskell functions over refined data to provide explicit, <lb/>constructive proofs ( § 6). Without data propositions reasoning about potentially non-terminating <lb/>computations was not possible in LiqidHaskell, thereby precluding even simple metatheoretic <lb/>developments such as the soundness of let alone <lb/>. <lb/>4. Mechanization Our final contribution is to mechanize the metatheory of <lb/>twice: using Liq-<lb/>uidHaskell and Coq. We formalized <lb/>in LiqidHaskell ( § 7) to evaluate the feasibility of such <lb/>substantial metatheoretical formalizations. Our proof is non-trivial, requiring 9,400 lines of code, <lb/>30 minutes to verify, and various modifications in the internals of LiqidHaskell. We translated <lb/>the same proof to Coq ( § 8) to compare the two alternatives. Certain definitions, concretely the <lb/>type denotations, not admissible by LiqidHaskell&apos;s positivity checker, were possible to define <lb/>in Coq using Equations [Sozeau and Mangin 2019]. Further, the Coq development is much faster <lb/>(about 60 seconds to verify), but more difficult to manipulate various partial and mutual recursive <lb/>definitions of the formalization. Finally, Coq comes with stronger foundational soundness guarantees <lb/>than LiqidHaskell. While the metatheory of Coq is well studied, <lb/>lays the foundation for the <lb/>mechanized metatheory of LiqidHaskell. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:3 <lb/></page>

        <body>type ArrayN a N = {i:Nat | i &lt; N} → a <lb/>new :: n:Nat → a → ArrayN a n <lb/>new n x = \i → if 0 ≤ i &amp;&amp; i &lt; n then x else error &quot;Out of Bounds&quot; <lb/>set :: n:Nat → i:{Nat | i &lt; n} → a → ArrayN a n → ArrayN a n <lb/>set n i x a = \j → if i == j then x else a j <lb/>get :: n:Nat → i:{Nat | i &lt; n} → ArrayN a n → a <lb/>get n i a = a i <lb/>Fig. 1. Functional Arrays with refinement types that ensure safe indexing. <lb/>2 REFINEMENT TYPES <lb/>We start by an informal overview of the refined core calculus <lb/>, which we later present formally ( § 3) <lb/>and prove sound ( § 5). Concretely, we present the goals of refinement types ( § 2.1) and how they are <lb/>achieved via the three essential features of semantic subtyping, existential types, and polymorphism <lb/>( § 2.2). We explain how the typing judgements are designed to accommodate these features ( § 2.3) and <lb/>how we addressed the challenges these features impose in the mechanization of the soundness proof <lb/>( § 2.4). Our examples here are presented with the syntax of LiqidHaskell, but can be encoded in <lb/>. <lb/>2.1 The goal of Refinement Types <lb/>Refinement types refine the types of an existing programming language with logical predicates to <lb/>define abstractions not expressible by the underlying type system, which can then be used for static <lb/>(1) error prevention and (2) functional correctness. <lb/>Error Prevention Figure 1 presents the interface of a fixed size array that is encoded in the core <lb/>calculus <lb/>as a function. The function new n x returns an array that contains x when indexed with <lb/>an integer between 0 and n and otherwise throws an &quot;out of bounds&quot; error. To statically ensure that <lb/>this error will never occur, new returns the refined array ArrayN a n, i.e. a function whose domain <lb/>is restricted to integers less than n. The set and get operators manipulate the refined arrays on <lb/>the index i:{Nat | i &lt; n}, i.e. refined to be in-bounds of the array. With this refined interface, <lb/>out-of-bounds indexing is statically ruled out: <lb/>array10 :: ArrayN Int 10 <lb/>array10 = new 10 0 <lb/>good = get 10 4 array10 --OK <lb/>bad = get 10 42 array10 --Refinement Type Error <lb/>Functional Correctness Refinement types are also used to ensure that the program has the intended <lb/>behavior. To achieve this, we use uninterpreted functions to specify behaviors and rely on the type <lb/>system to propagate them. For example, below using the uninterpreted function isPrime we specify <lb/>that some integers are primes, as denoted by the uninterpreted predicate isPrime. <lb/>measure isPrime :: Int → Bool <lb/>type Prime = {v:Int | isPrime v} <lb/>Refinement types are not ideally suited to verifying properties like primality checking, which requires <lb/>reasoning beyond SMT decidable fragments. However, assuming that a function establishes primality, <lb/>refinements can be used to easily track and propagate the invariant: <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:4 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>assume checkPrime :: x:Int → {v:Bool | v ⇔isPrime x} <lb/>nextPrime :: Nat → Prime <lb/>nextPrime x = if checkPrime x then x else nextPrime (x+1) <lb/>The path-sensitivity of refinement types (Rule T-If of Figure 7) ensures that the function nextPrime <lb/>returns only values that pass the primality check. <lb/>Note on recursion Our core calculus does not explicitly support recursion. But it can be extended <lb/>with primitive constants (as long as they satisfy the consistency condition in Requirement 1 below). <lb/>So, to encode inductive definitions, like nextPrime in our system, we use the fixpoint constant fix: <lb/>fix :: (a → a) → a <lb/>nextPrime = fix @(Nat→ Prime) (\f x→ if checkPrime x then x else f (x+1)) <lb/>Importantly, our calculus is fully polymorphic, in the sense that type variables can be instantiated <lb/>with refined types. So, the type variable of fix can be instantiated with the refined type Nat → Prime <lb/>to get the desired type of nextPrime. Here, for emphasis, we make this instantiation explicit, but <lb/>in real systems, like LiqidHaskell, the refined type application is inferred. <lb/>Primes Array Example As a bigger example, consider an example where refinements are used for <lb/>both error prevention and functional correctness. The function primes n generates an array with <lb/>the first n prime numbers: <lb/>primes :: n:Nat → ArrayN Prime n <lb/>primes n = (fix go) 1 0 (new n (nextPrime 1)) <lb/>where go f i p acc = if i &lt; n <lb/>then let p&apos; = nextPrime (p+1) in <lb/>go f (i+1) p&apos; (set n i p&apos; acc) <lb/>else acc <lb/>Since primes typechecks under the safe array interface of Figure 1, no out-of-bounds errors will <lb/>occur. At the same time, all elements of the array are set by a result nextPrime and thus primes <lb/>returns an array of prime numbers. <lb/>2.2 The essence of Refinement Types <lb/>The practicality of refinement types, as illustrated in the examples above, is due to the combination <lb/>of three essential features: <lb/>(1) Semantic Subtyping: The user does not need to provide any explicit type casts, because sub-<lb/>typing is implicit and semantic. For example, to type check get 10 4 array10 (from § 2.1), the <lb/>type of 4 :: {v:Int | v == 4} is implicitly converted to {v:Int | 0 ≤ v &lt; 10} <lb/>(2) Decidability: The semantic casts are reduced to logical implications checked by an SMT solver. <lb/>Refinement types are designed to generate decidable logical implications, to ensure predictable <lb/>verification and also permit type inference [Rondon et al. 2008] that makes verification practical, <lb/>e.g. the primes definition requires zero annotations. <lb/>(3) Polymorphism: Polymorphism on refinement types permits instantiation of type variables with <lb/>any refined type. For example, the same array interface can be used to describe primes, functions <lb/>with positive domains, and any other concept encoded as a refinement type. <lb/>2.3 The design of Refinement Types <lb/>Next, we develop a minimal calculus <lb/>that shows how Refinement type systems enjoy the three <lb/>essential features of § 2.2. <lb/>has four judgements that relate expressions ( ), types ( ), kinds ( ), <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:5 <lb/></page>

        <body>predicates ( ), and environments (Γ): (1) typing (Γ ⊢ : ), (2) subtyping (Γ ⊢ 1 ⪯ 2 ), (3) well-formedness <lb/>(Γ ⊢ : ), and (4) implication checking (Γ ⊢ 1 ⇒ 2 ). In § 4 we define the judgements in detail. Here, <lb/>we present the design decisions that ensure the three essential features of refinement types. <lb/>2.3.1 Semantic Subtyping. Refinement types rely on implicit semantic subtyping, that is, type conver-<lb/>sion (from subtypes) happens without any explicit casts and is checked semantically via logical validity. <lb/>For example, in the application get 10 4 array10 (of Fig. 1), the type of 4 was implicitly converted. <lb/>To see how, consider an environment Γ that contains the array interface. Let Γ ⊆ {get : : Int → : <lb/>Int{ : &lt; } → ArrayN → } For brevity, we ignore the requirement that and are natural num-<lb/>bers and, as in Fig. 1, we use ArrayN as shorthand for Int{ : &lt; } → . The application (get 10) 4 <lb/>will type check as below, using the T-Sub rule to implicitly convert the type of the argument and the <lb/>S-Base rule to check that 4 is a valid index by checking the validity of the formula ∀ . = 4 ⇒ &lt; 10. <lb/>... <lb/>Γ ⊢ get 10 : Int{ : &lt; 10} → ... <lb/>Γ ⊢ 4 : Int{ : = 4} <lb/>∀ . = 4 ⇒ &lt; 10 <lb/>Γ ⊢ Int{ : = 4} ⪯ Int{ : &lt; 10} <lb/>S-Base <lb/>Γ ⊢ 4 : Int{ : &lt; 10} <lb/>T-Sub <lb/>Γ ⊢ get 10 4 : ArrayN 10 → <lb/>Importantly, most refinement type systems use syntax-directed rules to destruct subtyping obliga-<lb/>tions into basic (semantic) implications. For example, in Figure 8 the rule S-Fun states that functions <lb/>are covariant on the result and contravariant on the arguments. Thus, a refinement type system can, <lb/>without any casts, decide that 20 : ArrayN 20 is a suitable argument for the higher order function <lb/>get 10 4 : ArrayN 10 → and type check the expression get 10 4 20 . <lb/>2.3.2 Decidability. As illustrated in the previous type derivation, refinement type checking essen-<lb/>tially generates a set of verification conditions (VCs) whose validity implies type safety. Importantly, <lb/>the refinement type checking rules are designed to generate VCs in the logical language used by the <lb/>user-provided specifications. In general, let L be a logical language that contains equality and con-<lb/>junction. If all the user-specified predicates belong to L, then the VCs will be in L as well. In practice <lb/>(e.g. in Liquid Haskell [Vazou et al. 2014a] and Flux [Lehmann et al. 2023]), L is the qualifier-free <lb/>logic of equality, uninterpreted functions, and linear arithmetic (QF-EUFLIA). <lb/>To achieve this logical-language preservation, special care is taken in type checking function <lb/>declarations and applications. <lb/>Function Declarations Function declarations are checked using the refinement type rule for let <lb/>bindings (Rule T-Let also in Figure 7). <lb/>Γ ⊢ : <lb/>Γ ⊢ : <lb/>: ,Γ ⊢ : <lb/>Γ ⊢ let = in : <lb/>T-Let <lb/>The type checking must infer the type of the function, but that could be user-annotated (e.g. <lb/>could be ′ : ). <lb/>Importantly, the body is checked without knowledge of the definition of . The exact encoding of <lb/>the body of the function definitions (for example, as done in Dafny [Leino 2010] or Prusti [Astrauskas <lb/>et al. 2022]) requires the use of ∀-quantifiers in the SMT solver, thus potentially leading to undecidabil-<lb/>ity. Instead, refinement types only use the refinement type specifications of functions, providing a fast <lb/>but incomplete verification technique. For example, given only the specifications of get and set, and <lb/>not their exact definitions, it is not possible to show that get after set returns the value that was set. <lb/>getSet :: n:Int → i:{Nat|i&lt;n} → x:a → ArrayN a n → {v:a|x == v} <lb/>getSet n i x a = get n i (set n i x a) --Refinement Type Error <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:6 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>Function Application For decidable type checking, refinement types use an existential type [Knowles <lb/>and Flanagan 2009] to check dependent function application, i.e. the TApp-Exists rule below, instead <lb/>of the standard type-theoretic TApp-Exact rule. <lb/>Γ ⊢ : : → <lb/>Γ ⊢ : <lb/>Γ ⊢ : [ / ] <lb/>TApp-Exact <lb/>Γ ⊢ : : → <lb/>Γ ⊢ : <lb/>Γ ⊢ : ∃ : . <lb/>TApp-Exists <lb/>To understand the difference, consider some expression of type Pos and the identity function <lb/>: Pos <lb/>: :Int → Int{ : = } <lb/>The application <lb/>is typed as Int{ : = } with the TApp-Exact rule, which has two problems. First, <lb/>the information that is positive is lost. To regain this information the system needs to re-analyze the <lb/>expression breaking compositional reasoning. Second, the arbitrary expression enters the refine-<lb/>ment logic potentially breaking decidability. Using the TApp-Exists rule, both of these problems are <lb/>addressed. Typing first uses subtyping on to track the actual type of the argument, thus weakening <lb/>the type of to : :Pos → Int{ : = }. With this, the type of <lb/>becomes ∃ :Pos.Int{ : = } <lb/>preserving the information that the application argument is positive, while the variable cannot <lb/>break any carefully crafted decidability guarantees. <lb/>Knowles and Flanagan [2009] introduce the existential application rule and show that it preserves <lb/>the decidability and completeness of the refinement type system. An alternative approach for <lb/>decidable and compositional type checking is to ensure that all the application arguments are <lb/>variables by ANF transforming the original program [Flanagan et al. 1993]. ANF is more amicable to <lb/>implementation as it does not require the definition of one more type form. However, ANF is more <lb/>problematic for the metatheory, as ANF is not preserved by evaluation. Additionally, existentials let <lb/>us synthesize types for let-binders in a bidirectional style: when typing let = 1 in 2 , the existential <lb/>lets us eliminate from the type synthesized for 2 , yielding a precise, algorithmic system [Cosman <lb/>and Jhala 2017]. Thus, we choose to use existential types in <lb/>. <lb/>2.3.3 Polymorphism. Polymorphism is a precious type abstraction [Wadler 1989], but combined <lb/>with refinements, it can lead to imprecise or, worse, unsound systems. As an example, below we <lb/>present the function max with four potential type signatures. <lb/>Definition max = <lb/>.if &lt; then else <lb/>Attempt 1: <lb/>Monomorphism max :: <lb/>:Int → :Int → Int{ : ≤ ∧ ≤ } <lb/>Attempt 2: Unrefined Polymorphism max :: <lb/>: → : → <lb/>Attempt 3: <lb/>Refined Polymorphism max :: <lb/>: → : → { : ≤ ∧ ≤ } <lb/>: <lb/>Kinded Polymorphism max :: ∀ : . : → : → { : ≤ ∧ ≤ } <lb/>As a first attempt, we give max a monomorphic type, stating that the result of max is an integer greater <lb/>than or equal to each of its arguments. This type is insufficient because it forgets any information <lb/>known for max&apos;s arguments. For example, if both arguments are positive, the system cannot decide <lb/>that max x y is also positive. To preserve the argument information we give max a polymorphic <lb/>type, as a second attempt. Now the system can deduce that max x y is positive, but forgets that it <lb/>is also greater than or equal to both x and y. In a third attempt, we naively combine the benefits of <lb/>polymorphism with refinements to give max a very precise type that is sufficient to propagate the <lb/>arguments&apos; properties (positivity) and max behavior (inequality). <lb/>Unfortunately, refinements on arbitrary type variables are dangerous for two reasons. First, the <lb/>type of max implies that the system allows comparison of any values (including functions). Second, <lb/>if refinements on type variables are allowed, then, for soundness [Belo et al. 2011], all the types that <lb/>substitute variables should be refined. For example, as detailed in §6 of [Jhala and Vazou 2021], if a <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:7 <lb/></page>

        <body>Γ ⊢ : <lb/>TYPING <lb/>Γ ⊢ ⪯ <lb/>SUBTYPING <lb/>Γ ⊢ <lb/>: <lb/>WELL-FORMEDNESS <lb/>Γ ⊢ ⇒ <lb/>IMPLICATION <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>6 <lb/>5 <lb/>Fig. 2. Dependencies of Typing Judgements in Refinement Types. (Dashed lines do not exist in our formalism.) <lb/>type variable is refined with false (i.e. { :false}) and gets instantiated with an unrefined function <lb/>type ( : → ), then the false refinement is lost and the system becomes unsound. <lb/>Base Kind when Refined To preserve the benefits of refinements on type variables, without the <lb/>complications of refining function types, we introduce a kind system that separates the type variables <lb/>that can be refined from the ones that cannot. To do so, we extend the standard well-formedness <lb/>rule of refinement types to also perform kind checking (Γ ⊢ : ). Variables with the base kind <lb/>can be refined, compared, and only substituted by base, refined types. The other type variables have <lb/>kind ★ and can only be trivially refined with true. With this kind system, we have a simple and <lb/>convenient way to encode comparable values and we can give max a polymorphic and precise type <lb/>that naturally rejects non-comparable (e.g. function) arguments. This simple kind system could be <lb/>further stratified, i.e. if some base types did not support comparison, and it could be implemented <lb/>via typeclass constraints, if our system contained data types. <lb/>2.4 The soundness of Refinement Types <lb/>In this work we establish two soundness theorems for refinement types that precisely relate typ-<lb/>ing judgments Γ ⊢ : with the high-level goals of error prevention (type safety) and functional <lb/>correctness (denotational soundness). <lb/>1. Type Safety ensures that well-typed programs do not get stuck at runtime. It says that if an <lb/>expression has a type (∅ ⊢ : ) and evaluates to another expression ( ↩→ * ′ ), then either evaluation <lb/>reached a value or it can take another step ( ′ ↩→ ′′ ). In <lb/>, we use the primitive error to denote <lb/>program errors (such as out-of-bounds indexing of Figure 1). The error primitive neither is a value <lb/>nor takes a step. Thus, if an expression type checks, via type safety, we know that error will not <lb/>be reached at runtime. Theorem 5.3 formally defines type safety and it is proved via the preservation <lb/>and progress lemmas. Type safety ensures that programs will not get stuck, but does not ensure that <lb/>they satisfy their functional specifications. This is ensured by the second soundness theorem. <lb/>2. Denotational Soundness states that if an expression has a type (∅ ⊢ : ), then it belongs in the <lb/>denotations of this type ( ∈ <lb/>). For example, the denotation of the type {i:Nat | i ≤ 42} is <lb/>the set of integers between 0 and 42. In § 4 we inductively define the denotations of each type and <lb/>Theorem 5.1 formally encodes denotational soundness. <lb/>This work, for the first time, mechanizes the soundness of refinement types with semantic subtyp-<lb/>ing, existential types, and polymorphism. This mechanization was challenging for three main reasons: <lb/>Challenge 1: Circularities Figure 2 presents the dependencies of the four typing judgements in <lb/>refinement types. As we saw in the example of § 2.3.1 (and can be confirmed in the rules defined <lb/>in § 4), typing depends on subtyping (arrow 1) which in turn depends on implication checking <lb/>(arrow 5). Subtyping depends on typing (arrow 2; because of rule S-Wit of Figure 8), so typing and <lb/>subtyping have a circular dependency we cannot break. Typing also depends on well-formedness <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:8 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>(arrow 3) that checks that types, especially the ones inferred by the system, are well-formed: all the <lb/>variables appearing in the refinements are bound in the type environment and refinements are of <lb/>boolean type. To check the type of the refinements the system could use typing thus introducing one <lb/>more dependency (arrow 4) and yet another circle. We break this dependency by using an unrefined <lb/>calculus (system ) that erases refinements, to check that refinements are well-typed booleans. <lb/>The final potential circle is introduced when implication depends on typing (arrow 6). In § 4.4.3 we <lb/>define implication via type denotations, but as observed by Greenberg [2013], in this case, special <lb/>care should be taken so that the system is monotonic and thus well-defined. To avoid this dangerous <lb/>circularity we again use typing of (and not <lb/>) to define denotations and thus implication. <lb/>In summary, circularities in typing judgements are problematic for two reasons: <lb/>(1) Circularities increase the complexity of proof mechanization. Concretely, because typing and <lb/>subtyping have a circular dependency, the metatheoretical lemmas (substitution, weakening, <lb/>narrowing, etc.) require versions for both typing and subtyping, which are proved by mutual <lb/>induction. If well-formedness was also included in this circularity (arrow 4), then the complexity <lb/>of the proofs would greatly increase, but would not necessarily be impossible. <lb/>(2) Second, circularities are problematic because they can lead to non well-defined systems. Con-<lb/>cretely, Greenberg [2013] describes an older refinement type system in which typing appeared <lb/>in the left hand side of subtyping and, as such, it was non-monotonic and thus not well-defined. <lb/>This situation corresponds to the red arrow 6 in fig. 2, which would make the proof impossible <lb/>due to the typing judgment occurring in a negative position in the implication judgment. <lb/>Challenge 2: Implications The second mechanization challenge was the encoding of implication. <lb/>In the bibliography of refinement types, implication has been defined in three ways: <lb/>(1) Using denotations (of types as sets of terms) defined via operational semantics [Flanagan 2006; <lb/>Vazou et al. 2018]. This encoding is more convenient when proving the soundness of the system, <lb/>since implication and thus subtyping and typing, directly connect with operational semantics, <lb/>making the proof of soundness more direct. However, the implementation of this encoding of <lb/>implication is not realistic, since it is not decidable. <lb/>(2) Using logical implication [Gordon and Fournet 2010; Rondon et al. 2008]. The encoding of the <lb/>implication as a logical implication is the closest to the implementation of a refinement system, <lb/>where an SMT is used to check logical implications. Yet, to prove soundness, a claim should be <lb/>made that logical implication checked by the SMT correctly approximates the runtime semantics <lb/>of the system (i.e. presented in rule I-Log of § 4.4.2) which has never been mechanized. <lb/>(3) By axiomatization [Lehmann and Tanter 2016]. A final approach is to leave the implication unin-<lb/>terpreted and axiomatize it with all the properties required to prove soundness. This approach is <lb/>the easiest to mechanize, but it is dangerous, since in the past the axioms assumed for implication <lb/>were inconsistent, thus soundness was &quot;proved with flawed premises&quot; (as quoted from Table 1 <lb/>of [Sekiyama et al. 2017]). <lb/>Our mechanization follows a combination of the first and the third approach. We specify the interface <lb/>of implication (via Requirement 2 of § 4.4.1 which is encoded as an inductive data type in the proof <lb/>mechanization) to articulate the exact properties required by the soundness proof. Then, in § 4.4.3, we <lb/>implement the implication interface using the denotational semantics of the system. This encoding has <lb/>two major benefits. First, the denotational implementation ensures that our interface is consistent. Sec-<lb/>ond, the development of the interface leaves room for the implementation of alternative implication <lb/>&quot;oracles&quot;, e.g. closer to SMT solvers. Even though we did not mechanize this alternative implemen-<lb/>tation, in § 4.4.2 we present how logical implications are derived from the implication judgement. <lb/>Challenge 3: Proof Complexity All the three essential features of refinement types add complexity to <lb/>the mechanization of the soundness proof. Polymorphism requires the extension of well-formedness <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:9 <lb/></page>

        <body>Primitives <lb/>::= true | false | 0,1,2,... | ∧,¬ | ≤, ≤,=, = <lb/>Values <lb/>::= <lb/>| , ,... | . | Λ : . <lb/>Terms <lb/>::= <lb/>| 1 2 | [ ] | let = 1 in 2 | : | if then 1 else 2 | error <lb/>Fig. 3. Syntax of Primitives, Values, and Expressions. <lb/>Kinds <lb/>::= <lb/>| ★ <lb/>base and star kind <lb/>Predicates <lb/>::= { | ∃Γ.Γ ⊢ : Bool} <lb/>boolean-typed terms <lb/>Base Types <lb/>::= Bool | Int | <lb/>bool, ints, and type variables <lb/>Types <lb/>::= <lb/>{ : } <lb/>refined base type <lb/>| <lb/>: → <lb/>function type <lb/>| <lb/>∃ : . <lb/>existential type <lb/>| <lb/>∀ : . <lb/>polymorphic type <lb/>Environments Γ ::= ∅ | Γ, : | Γ, : <lb/>variable and type bindings <lb/>Fig. 4. Syntax of Types. The grey boxes are the extensions to needed by <lb/>. We use for -only types. <lb/>to kind checking. Semantic subtyping makes type checking not syntax-directed (thus inversion is <lb/>not trivial § 5.3) and dependent upon subtyping. In turn, the existential types required for decidabil-<lb/>ity make subtyping dependent upon type checking. Due to this mutual dependency, the standard <lb/>metatheoretical lemmas (substitution, weakening, narrowing, etc.) require versions for both typing <lb/>and subtyping, which are proved by mutual induction. Thus, the combination of the three essential <lb/>for refinement types features makes the metatheoretical development more complex and prone to un-<lb/>soundness. Once, we have carefully broken the various circularities and eliminated potential sources <lb/>of unsoundness, we get unsurprising, albeit strenuous, proofs of the soundness of refinement typing. <lb/>3 LANGUAGE <lb/>To cut the circularities in the metatheory, we formalize refinements using two calculi. The first <lb/>is the base language : a classic System F [Pierce 2002] with call-by-value semantics extended <lb/>with primitive Int and Bool types and operations. The second is the refined language <lb/>which <lb/>extends with refinements. By using the first calculus to express the typing judgments for our <lb/>refinements, we avoid making the well-formedness (in rule WF-Refn in § 4.1) and the implication <lb/>(in type denotations of Figure 9) judgments mutually dependent with the typing judgments. We use <lb/>the grey highlights for the extensions to required for <lb/>. <lb/>3.1 Syntax <lb/>We start by describing the syntax of terms and types in the two calculi. <lb/>Constants, Values and Terms Figure 3 summarizes the syntax of terms in both calculi. The <lb/>primitives include Int and Bool constants, boolean operations, the polymorphic comparison and <lb/>equality, and their curried versions. Values are constants, binders and -and type-abstractions. <lb/>Finally, the terms comprise values, value-and type-applications, let-binders, annotated expressions, <lb/>conditionals, and runtime errors. The types in annotations are, potentially wrong, specifications <lb/>written by the user and checked by the type checker. <lb/>Kinds &amp; Types Figure 4 shows the syntax of the types, with the grey boxes indicating the extensions <lb/>to <lb/>required by <lb/>. In <lb/>, only base types can be refined: we do not permit refinements for <lb/>functions and polymorphic types. <lb/>enforces this restriction using two kinds which denote types <lb/>that may ( ) or may not (★) be refined. The (unrefined) base types comprise Int, Bool, and type <lb/>variables . The simplest type is of the form { : } comprising a base type and a refinement that <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:10 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>Operational Semantics <lb/>↩→ ′ <lb/>↩→ ( , ) <lb/>E-Prim <lb/>[ ] ↩→ ( ,⌊ ⌋) <lb/>E-TPrim <lb/>↩→ ′ <lb/>: ↩→ ′ : <lb/>E-PAnn <lb/>: ↩→ <lb/>E-Ann <lb/>↩→ ′ <lb/>1 ↩→ ′ 1 <lb/>E-PLApp <lb/>↩→ ′ <lb/>↩→ ′ E-PRApp <lb/>( . ) ↩→ [ / ] <lb/>E-App <lb/>(Λ : . ) [ ] ↩→ [ / ] <lb/>E-TApp <lb/>↩→ ′ <lb/>[ ] ↩→ ′ [ ] <lb/>E-PTApp <lb/>↩→ ′ <lb/>let = in ↩→ let = ′ in <lb/>E-PLet <lb/>let = in ↩→ [ / ] <lb/>E-Let <lb/>↩→ ′ <lb/>if then 1 else 2 ↩→ if ′ then 1 else 2 <lb/>E-PIf <lb/>if true then 1 else 2 ↩→ 1 <lb/>E-IfT <lb/>if false then 1 else 2 ↩→ 2 <lb/>E-IfF <lb/>{ : }[ / ] <lb/>{ : [ / ]}, ≠ <lb/>( : → ) [ / ] <lb/>:( [ / ]) → [ / ] <lb/>(∃ : . ) [ / ] <lb/>∃ : ( [ / ]). [ / ] <lb/>(∀ : . ) [ / ] <lb/>∀ : . [ / ] <lb/>{ : }[ / ] <lb/>refine( , [ / ], ) <lb/>refine( { : }, , ) <lb/>{ : [ / ] ∧ } <lb/>refine(∃ : . , , ) <lb/>∃ : .refine( , , ) <lb/>refine( : → ,_,_) <lb/>: → <lb/>refine(∀ : . ,_,_) <lb/>∀ : . <lb/>Fig. 5. The small-step semantics and type substitution. <lb/>restricts to the subset of values that satisfy i.e. for which evaluates to true. We use refined <lb/>base types to build up dependent function types (where the input parameter can appear in the <lb/>output type&apos;s refinement), existential and polymorphic types. In the sequel, we write to abbreviate <lb/>{ :true} and call types refined with only true &quot;trivially refined&quot; types. <lb/>Refinement Erasure The reduction semantics of our polymorphic primitives are defined using <lb/>an erasure function that returns the unrefined, version of a refined <lb/>type: <lb/>⌊ { : }⌋ , ⌊ : → ⌋ ⌊ ⌋ → ⌊ ⌋, ⌊∃ : . ⌋ ⌊ ⌋, and ⌊∀ : . ⌋ ∀ : .⌊ ⌋ <lb/>Environments Figure 4 describes the syntax of typing environments Γ which contain both term <lb/>variables bound to types and type variables bound to kinds. These variables may appear in types <lb/>bound later in the environment. In our formalism, environments grow from right to left. <lb/>Note on Variable Representation Our metatheory requires that all variables bound in the envi-<lb/>ronment are distinct. Our mechanization enforces this invariant via the locally nameless represen-<lb/>tation [Aydemir et al. 2005]: free and bound variables are distinct objects in the syntax, as are type <lb/>and term variables. All free variables have unique names which never conflict with bound variables <lb/>represented as de Bruijn indices. This eliminates the possibility of capture in substitution and the <lb/>need to perform alpha-renaming during substitution. The locally nameless representation avoids <lb/>technical manipulations such as index shifting by using names instead of indices for free variables <lb/>(we discuss alternatives in § 9). To simplify the presentation of the syntax and rules, we use names <lb/>for bound variables to make the dependent nature of the function arrow clear. <lb/>3.2 Dynamic Semantics <lb/>Figure 5 summarizes the substitution-based, call-by-value, contextual, small-step semantics for both <lb/>calculi. We specify the reduction semantics of the primitives using the functions and . <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:11 <lb/></page>

        <body>Substitution The key difference with standard formulations is the notion of substitution for type <lb/>variables at (polymorphic) type-application sites as shown in rule E-TApp. Type substitution is <lb/>defined at the bottom left of Figure 5 and it is standard except for the last line which defines the <lb/>substitution of a type variable in a refined type variable { : } with a (potentially refined) type <lb/>. To do this substitution, we combine with the type by using refine( , , ) which essentially <lb/>conjoins the refinement to the top-level refinement of a base-kinded . For existential types, <lb/>refine pushes the refinement through the existential quantifier. Function and quantified types are <lb/>left unchanged as they cannot instantiate a refined type variable (which must be of base kind). <lb/>Primitives The function ( , ) evaluates the application of built-in monomorphic primitives. <lb/>The reductions are defined in a curried manner, i.e. ≤ <lb/>evaluates to ( (≤, ), ). Currying gives <lb/>us unary relations like ≤ which is a partially evaluated version of the ≤ relation. The function <lb/>( ,⌊ ⌋) specifies the reduction rules for type application on the polymorphic built-in primitives. <lb/>(∧,true) <lb/>. <lb/>(≤, ) <lb/>≤ <lb/>(=,Bool) <lb/>= <lb/>(∧,false) <lb/>.false <lb/>( ≤, ) <lb/>( ≤ ) <lb/>(=,Int) <lb/>= <lb/>(¬,true) <lb/>false <lb/>(=, ) <lb/>= <lb/>(≤,Bool) <lb/>≤ <lb/>(¬,false) <lb/>true <lb/>( =, ) <lb/>( = ) <lb/>(≤,Int) <lb/>≤ <lb/>Determinism Our soundness proof uses the determinism property of the operational semantics. <lb/>Lemma 3.1 (Determinism). For every expression , 1) there exists at most one term ′ s.t. ↩→ ′ , 2) <lb/>there exists at most one value s.t. ↩→ * , and 3) if is a value there is no term ′ s.t. ↩→ ′ . <lb/>4 STATIC SEMANTICS <lb/>The static semantics of our calculi comprise four main judgment forms: ( § 4.1) well-formedness <lb/>judgments that determine when a type or environment is syntactically well-formed (in and <lb/>); <lb/>( § 4.2) typing judgments that stipulate that a term has a particular type in a given context (in and <lb/>); ( § 4.3) subtyping judgments that establish when one type can be viewed as a subtype of another <lb/>(in <lb/>); and ( § 4.4) implication judgments that establish when one predicate implies another (in <lb/>). Next, we present the static semantics of <lb/>by describing the rules that establish each of these <lb/>judgments. We use grey to highlight the antecedents and rules specific to <lb/>. <lb/>4.1 Well-formedness <lb/>Judgments The judgment Γ ⊢ : says that the type is well-formed in the environment Γ and has <lb/>kind . The judgment ⊢ Γ says that the environment Γ is well formed, meaning that it only binds to <lb/>well-formed types. Well-formedness is also used in the (unrefined) system , where Γ ⊢ : means <lb/>that the (unrefined) type is well-formed in environment Γ and has kind and ⊢ Γ means that <lb/>the free type variables of the environment Γ are bound earlier in the environment. <lb/>Rules Figure 6 summarizes the rules that establish the well-formedness of types and environments. <lb/>Rule WF-Base states that the two closed base types (Int and Bool, refined with true in <lb/>) are <lb/>well-formed and have base kind. Similarly, rule WF-Var says that a type variable is well-formed <lb/>with kind so long as : is bound in the environment. The rule WF-Refn stipulates that a refined <lb/>base type { : } is well-formed with base kind in some environment if the unrefined base type <lb/>has base kind in the same environment and if the refinement predicate has type Bool in the <lb/>environment augmented by binding a fresh variable to type . Note that if ≡ then we can only <lb/>form the antecedent Γ ⊢ { :true} : when : ∈ Γ (rule WF-Var), which prevents us from refining <lb/>star-kinded type variables. To break a circularity in which well-formedness judgments appear in the <lb/>antecedents of typing judgments and a typing judgment appears in the antecedents of WF-Refn, <lb/>we use the judgment to check that has type Bool. Finally, rule WF-Kind simply states that if <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:12 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>Well-formed Type <lb/>Γ ⊢ : <lb/>∈ {Bool,Int} <lb/>Γ ⊢ <lb/>{ :true} : <lb/>WF-Base <lb/>: ∈ Γ <lb/>Γ ⊢ <lb/>{ :true} : <lb/>WF-Var <lb/>Γ ⊢ : <lb/>Γ ⊢ :★ <lb/>WF-Kind <lb/>Γ ⊢ { :true} : <lb/>∀ ∉ Γ. : ,⌊Γ⌋ ⊢ [ / ] : Bool <lb/>Γ ⊢ { : } : <lb/>WF-Refn <lb/>Γ ⊢ <lb/>: <lb/>∀ ∉ Γ. : , Γ ⊢ <lb/>[ / ] : <lb/>Γ ⊢ <lb/>: → :★ <lb/>WF-Func <lb/>Γ ⊢ <lb/>: ∀ ∉ Γ. : ,Γ ⊢ [ / ] : <lb/>Γ ⊢ ∃ : . : <lb/>WF-Exis <lb/>∀ ′ ∉ Γ. ′ : ,Γ ⊢ [ ′ / ] : <lb/>Γ ⊢ ∀ : . :★ <lb/>WF-Poly <lb/>Well-formed Environment <lb/>⊢ Γ <lb/>⊢ ∅ <lb/>WFE-Emp <lb/>Γ ⊢ <lb/>: <lb/>⊢ Γ <lb/>∉ Γ <lb/>⊢ : ,Γ <lb/>WFE-Bind <lb/>⊢ Γ <lb/>∉ Γ <lb/>⊢ <lb/>: ,Γ <lb/>WFE-TBind <lb/>Fig. 6. Well-formedness of types and environments. The rules for exclude the grey boxes. <lb/>a type is well-formed with base kind in some environment, then it is also well-formed with star <lb/>kind. This rule is required by our metatheory to convert base to star kinds in type variables. <lb/>As for environments, the empty environment is well-formed. A well-formed environment remains <lb/>well-formed after binding a fresh term or type variable to resp. any well-formed type or kind. <lb/>4.2 Typing <lb/>The judgment Γ ⊢ : states that the term has type in the context of environment Γ. We write <lb/>Γ ⊢ : to indicate that term has the (unrefined) type in the (unrefined) context Γ. Figure 7 <lb/>summarizes the rules that establish typing for both and <lb/>, with grey for the <lb/>extensions. <lb/>Typing Primitives The type of a built-in primitive is given by the function ty( ), which is defined <lb/>for every constant of our system. Below we present essential examples of the ty( ) definition. <lb/>ty(true) <lb/>Bool{ : = true} <lb/>ty(∧) <lb/>:Bool → :Bool → Bool{ : = ∧ } <lb/>ty(3) <lb/>Int{ : = 3} <lb/>ty(≤) <lb/>∀ : . : → : → Bool{ : = ( ≤ )} <lb/>ty( ≤) <lb/>:Int → Bool{ : = ( ≤ )} ty(=) <lb/>∀ : . : → : → Bool{ : = ( = )} <lb/>We note that the = used in the refinements is the polymorphic equals with type applications elided. <lb/>Further, we use ≤ to represent an arbitrary member of the infinite family of primitives 0 ≤,1 ≤,2 ≤,.... <lb/>For we erase the refinements using ⌊ty( )⌋. The rest of the definition is similar. <lb/>Our choice to make the typing and reduction of constants external to our language, i.e. given by <lb/>the functions ty( ) and ( ), makes our system easily extensible with further constants, including <lb/>a terminating fix constant to encode induction. The requirement, for soundness, is that these two <lb/>functions together satisfy the following four conditions. <lb/>Reqirement 1. (Primitives) For every primitive , <lb/>(1) If ty( ) = { : }, then ∅ ⊢ ty( ) : and ∅ ⊢ true ⇒ [ / ]. <lb/>(2) If ty( ) = : → or ty( ) =∀ : . , then ∅ ⊢ ty( ) :★. <lb/>(3) If ty( ) = : → , then for all such that ∅ ⊢ : , ∅ ⊢ ( , ) : [ / ]. <lb/>(4) If ty( ) =∀ : . , then for all such that ∅ ⊢ <lb/>: , ∅ ⊢ ( , ) : [ / ]. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:13 <lb/></page>

        <body>Typing <lb/>Γ ⊢ : <lb/>Γ ⊢ : ty( ) <lb/>T-Prim <lb/>: ∈ Γ <lb/>Γ ⊢ : <lb/>Γ ⊢ : self( , , ) <lb/>T-Var <lb/>Γ ⊢ : <lb/>Γ ⊢ : <lb/>Γ ⊢ : : <lb/>T-Ann <lb/>Γ ⊢ : <lb/>Γ ⊢ : <lb/>Γ ⊢ ⪯ <lb/>Γ ⊢ : <lb/>T-Sub <lb/>Γ ⊢ : <lb/>Γ ⊢ : : → <lb/>Γ ⊢ <lb/>: ∃ : . <lb/>T-App <lb/>Γ ⊢ <lb/>: <lb/>∀ ∉ Γ. : ,Γ ⊢ [ / ] : <lb/>[ / ] <lb/>Γ ⊢ . : : → <lb/>T-Abs <lb/>∀ ′ ∉ Γ. <lb/>′ : ,Γ ⊢ [ ′ / ] : [ ′ / ] <lb/>Γ ⊢ Λ : . :∀ : . <lb/>T-TAbs <lb/>Γ ⊢ : <lb/>Γ ⊢ :∀ : . <lb/>Γ ⊢ [ ] : [ / ] <lb/>T-TApp <lb/>Γ ⊢ : <lb/>Γ ⊢ : <lb/>∀ ∉ Γ. : ,Γ ⊢ [ / ] : [ / ] <lb/>Γ ⊢ let = in : <lb/>T-Let <lb/>Γ ⊢ : Bool { : } <lb/>Γ ⊢ : <lb/>∀ ∉ Γ. : Bool{ : ∧ }, Γ ⊢ 1 : <lb/>∀ ∉ Γ. : Bool{ : ∧¬ }, Γ ⊢ 2 : <lb/>Γ ⊢ if then 1 else 2 : <lb/>T-If <lb/>Fig. 7. Typing rules. The judgment Γ ⊢ : is defined by excluding the grey boxes. <lb/>Theorem 3 of [Vazou et al. 2014b] proves that a terminating fix constant satisfies requirement 1. <lb/>To type constants, rule T-Prim gives the type ( ) to any built-in primitive , in any context. <lb/>Typing Variables with Selfification Rule T-Var establishes that any variable that appears as <lb/>: in environment Γ can be given the selfified type [Ou et al. 2004] self( , , ) provided that Γ ⊢ : . <lb/>This rule is crucial in practice, to enable path-sensitive &quot;occurrence&quot; typing [Tobin-Hochstadt and <lb/>Felleisen 2008], where the types of variables are refined by control-flow guards. For example, suppose <lb/>we want to establish : ⊢ ( . ) : : → { : = }, and not just : ⊢ ( . ) : → . The latter <lb/>would result if T-Var merely stated that Γ ⊢ : whenever : ∈ Γ. Instead, we strengthen the T-Var <lb/>rule to be selfified. Informally, to get information about into the refinement level, we need to say <lb/>that is constrained to elements of type that are equal to itself. In order to express the exact <lb/>type of variables, below we define the &quot;selfification&quot; function that strengthens a refinement with <lb/>the condition that a value is equal to itself. Since abstractions do not admit equality, we only selfify <lb/>the base types and the existential quantifications of them. <lb/>self (∃ : . , , ) ∃ : .self ( , , ) self( { : }, , ) <lb/>{ : ∧ = } <lb/>self( : → ,_,_) <lb/>: → <lb/>self( { : }, ,★) <lb/>{ : } <lb/>self(∀ : . ,_,_) ∀ : . <lb/>Typing Applications with Existentials Our rule T-App states the conditions for typing a term <lb/>application <lb/>. Under the same environment, we must be able to type at some function type <lb/>: → and at . Then we can give <lb/>the existential type ∃ : . . The use of existential types <lb/>in rule T-App is one of the distinctive features of our language and was introduced by Knowles and <lb/>Flanagan [2009]. As overviewed in § 2.3.2, we chose this form of T-App over the conventional form of <lb/>Γ ⊢ <lb/>: [ / ] because our version prevents the substitution of arbitrary expressions (e.g. functions <lb/>and type abstractions) into refinements. As an alternative, we could have used ANF (A-Normal <lb/>Form [Flanagan et al. 1993]), but our metatheory would be more complex since ANF is not preserved <lb/>under the small step operational semantics. <lb/>Other Typing Rules Our rule T-TApp states that whenever a term has polymorphic type ∀ : . , <lb/>then for any well-formed type with kind , we can give the type [ / ] to the type application [ ]. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:14 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>Subtyping <lb/>Γ ⊢ ⪯ <lb/>Γ ⊢ 2 ⪯ 1 ∀ ∉ Γ. <lb/>: 2 ,Γ ⊢ 1 [ / ] ⪯ 2 [ / ] <lb/>Γ ⊢ : 1 → 1 ⪯ : 2 → 2 <lb/>S-Fun <lb/>Γ ⊢ : <lb/>Γ ⊢ ⪯ ′ [ / ] <lb/>Γ ⊢ ⪯ ∃ : . ′ <lb/>S-Wit <lb/>∀ ∉ free( ) ∪Γ. <lb/>: ,Γ ⊢ [ / ] ⪯ ′ <lb/>Γ ⊢ ∃ : . ⪯ ′ <lb/>S-Bind <lb/>∀ ′ ∉ Γ. <lb/>′ : ,Γ ⊢ 1 [ ′ / ] ⪯ 2 [ ′ / ] <lb/>Γ ⊢∀ : . 1 ⪯ ∀ : . 2 <lb/>S-Poly <lb/>∀ ∉ Γ. <lb/>: ,Γ ⊢ 1 [ / ] ⇒ 2 [ / ] <lb/>Γ ⊢ { : 1 } ⪯ { : 2 } <lb/>S-Base <lb/>Fig. 8. Subtyping Rules. <lb/>For the variant of T-TApp, we erase the refinements (via ⌊ ⌋) before checking well-formedness and <lb/>performing the substitution. Rule T-Ann establishes that an explicit annotation : indeed has type <lb/>when the underlying has type and is well-formed. The version of the rule erases the refinements <lb/>and uses ⌊ ⌋. Rule T-If states that a conditional expression if then 1 else 2 has the type when the <lb/>guard can be given type Bool refined by and 1 (resp. 2 ) can be given type in the environment Γ aug-<lb/>mented by the knowledge we have about the type and semantics of the guard . The extension of the en-<lb/>vironment Γ with a fresh variable that captures the semantics of the guard when checking the two paths <lb/>is critical to permit path-sensitive reasoning. Finally, rule T-Sub tells us that we can exchange a subtype <lb/>for a supertype in a judgment Γ ⊢ : provided is well-formed and Γ ⊢ ⪯ , which we present next. <lb/>4.3 Subtyping <lb/>The subtyping judgment Γ ⊢ ⪯ , defined in Figure 8, stipulates that the type is a subtype of the type <lb/>in the environment Γ and is used in the subsumption typing rule T-Sub (of Figure 7). <lb/>Subtyping Rules Rules S-Bind and S-Wit establish subtyping for existential types [Knowles and <lb/>Flanagan 2009], resp. when the existential appears on the left or right. Rule S-Bind allows us to ex-<lb/>change a universal quantifier (a variable bound to some type in the environment) for an existential <lb/>quantifier. If we have a judgment of the form : ,Γ ⊢ [ / ] ⪯ ′ where does not appear free in <lb/>either ′ or in the context Γ, then we can conclude that ∃ : . is a subtype of ′ . Rule S-Wit states <lb/>that if type is a subtype of ′ [ / ] for some value of type , then we can discard the specific <lb/>witness for and quantify existentially to obtain that is a subtype of ∃ : . ′ . <lb/>Refinements enter the scene in the rule S-Base which specifies that a refined base type { : 1 } <lb/>is a subtype of another { : 2 } in context Γ when 1 implies 2 in the environment Γ augmented <lb/>by binding a fresh variable to the unrefined type . <lb/>4.4 Implication <lb/>The implication judgment Γ ⊢ 1 ⇒ 2 states that the implication 1 ⇒ 2 holds under the assumptions <lb/>captured by the context Γ. In refinement type implementations [Swamy et al. 2016; Vazou et al. <lb/>2014a], this relation is implemented as an external automated (usually SMT) solver. Since external <lb/>solvers are not easy to encode in mechanized proofs, we follow an approach that decouples the <lb/>mechanization from the implementation. Concretely, first we define the interface of the implication <lb/>( § 4.4.1) that precisely captures all the requirements that the implication judgement should satisfy <lb/>to establish the soundness of <lb/>. Then, we define two alternative implementations of the interface: <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:15 <lb/></page>

        <body>a logical implementation ( § 4.4.2) that is used in refinement type implementations and a denotational <lb/>implementation ( § 4.4.3) that we used to complete our mechanized proof. <lb/>4.4.1 Implication&apos;s Interface. In our mechanization, following Lehmann and Tanter [2016], we <lb/>encode implication as an axiomatized judgment that satisfies the requirements below. <lb/>Reqirement 2 (Implication Interface). The implication relation satisfies the below statements: <lb/>(1) (Reflexivity) Γ ⊢ ⇒ . <lb/>(2) (Transitivity) If Γ ⊢ 1 ⇒ 2 and Γ ⊢ 2 ⇒ 3 , then Γ ⊢ 1 ⇒ 3 . <lb/>(3) (Faithfulness) Γ ⊢ ⇒ true. <lb/>(4) (Introduction) If Γ ⊢ 1 ⇒ 2 and Γ ⊢ 1 ⇒ 3 , then Γ ⊢ 1 ⇒ 2 ∧ 3 . <lb/>(5) (Conjunction) Γ ⊢ 1 ∧ 2 ⇒ 1 and Γ ⊢ 1 ∧ 2 ⇒ 2 . <lb/>(6) (Repetition) Γ ⊢ 1 ∧ 2 ⇒ 1 ∧ 1 ∧ 2 . <lb/>(7) (Evaluation) If 1 ↩→ * <lb/>2 , then Γ ⊢ 1 ⇒ 2 and Γ ⊢ 2 ⇒ 1 . <lb/>(8) (Narrowing) If Γ 1 , : ,Γ 2 ⊢ 1 ⇒ 2 and Γ 2 ⊢ ⪯ , then Γ 1 , : ,Γ 2 ⊢ 1 ⇒ 2 . <lb/>(9) (Weaken) If Γ 1 ,Γ 2 ⊢ 1 ⇒ 2 , , ∉ Γ, then Γ 1 , : ,Γ 2 ⊢ 1 ⇒ 2 and Γ 1 , : ,Γ 2 ⊢ 1 ⇒ 2 . <lb/>(10) (Subst I) If Γ 1 , : ,Γ 2 ⊢ 1 ⇒ 2 and Γ 2 ⊢ : , then Γ 1 [ / ],Γ 2 ⊢ 1 [ / ] ⇒ 2 [ / ]. <lb/>(11) (Subst II) If Γ 1 , : ,Γ 2 ⊢ 1 ⇒ 2 and Γ 2 ⊢ : , then Γ 1 [ / ],Γ 2 ⊢ 1 [ / ] ⇒ 2 [ / ]. <lb/>(12) (Strengthening) If : { : },Γ ⊢ 1 ⇒ 2 , then : ,Γ ⊢ [ / ] ∧ 1 ⇒ [ / ] ∧ 2 . <lb/>This interface precisely explicates the requirements of the implication checker to establish the <lb/>soundness of the entire refinement type system. The first six statements are standard properties of <lb/>implication. Evaluation is used to prove that built-in constants satisfy the Requirement 1 and the rest, <lb/>as captured by their name, are required to prove the narrowing (5.10), weakening (5.9), substitution <lb/>(5.8) lemmas hold in <lb/>. <lb/>Our requirements are very similar to Assumption 1 of [Knowles and Flanagan 2009]. Our Strength-<lb/>ening and Subst II cases are required for polymorphism, thus they do not appear in Knowles and <lb/>Flanagan [2009]&apos;s assumption. Instead they require Consistency and Exact Quantification. We do <lb/>not require Exact Quantification since our relation captures the minimum requirements to prove <lb/>soundness. Instead of explicitly requiring Consistency, in § 4.4.3 we define (and mechanize) an <lb/>implementation, i.e. inhabitant, of the interface thus show our assumptions are consistent. <lb/>4.4.2 Logical Implementation (non mechanized). The logical implementation of Γ ⊢ 1 ⇒ 2 checks <lb/>that the logical implication 1 ⇒ 2 is valid assuming the refinements of the base types in Γ: <lb/>|= LOGIC ∧{ [ / ] | : { : } ∈ Γ} ⇒ 1 ⇒ 2 <lb/>Γ ⊢ 1 ⇒ 2 <lb/>I-Log <lb/>This encoding is imprecise, since some information is ignored from the environment Γ, but when <lb/>the language of refinements is decidable, implication checking is also decidable and can be efficiently <lb/>checked by an SMT solver. LiqidHaskell, for example, uses this encoding to reduce type checking <lb/>to decidable implications checked by Z3 [de Moura and Bjørner 2008], while the soundness of this <lb/>implementation (concretely statement 7 of Requirement 2) is hinted by Theorem 2 of [Vazou et al. <lb/>2014b]. Chen [2022] defines a mechanization of a refinement type system in Agda that uses a similar <lb/>encoding of implication where logical implications are checked using Agda&apos;s logic. <lb/>4.4.3 Denotational Implementation (mechanized). The denotational implementation of Γ ⊢ 1 ⇒ 2 <lb/>checks that if 1 evaluates to true, so does 2 . <lb/>∀ ∈ Γ . • 1 ↩→ * true ⇒ • 2 ↩→ * true <lb/>Γ ⊢ 1 ⇒ 2 <lb/>I-Den <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:16 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>{ : } <lb/>{ |∅ ⊢ : ∧ [ / ] ↩→ * true} <lb/>: → <lb/>{ |∅ ⊢ : ⌊ ⌋ → ⌊ ⌋ ∧ (∀ ∈ <lb/>. <lb/>↩→ * ′ s.t. ′ ∈ [ / ] } <lb/>∃ : . <lb/>{ |(∅ ⊢ : ⌊ ⌋) ∧ (∃ ∈ <lb/>. ∈ [ / ] } <lb/>∀ : . <lb/>{ |(∅ ⊢ :∀ : .⌊ ⌋) ∧ (∀ .(∅ ⊢ <lb/>: ) ⇒ [ ] ↩→ * ′ s.t. ′ ∈ [ / ] } <lb/>Γ <lb/>{ |∀( : ) ∈ Γ. ( ) ∈ • ∧ ∀( : ) ∈ Γ.∅ ⊢ ( ) : }. <lb/>Fig. 9. Denotations of Types and Environments. <lb/>The refinements 1 and 2 are boolean expressions, so evaluation uses the operational semantics <lb/>of Figure 5. But, they are open expressions with variables bound in Γ, so before evaluation we apply <lb/>the closing substitution that belongs to the denotation of Γ, as defined next. <lb/>Closing Substitutions. A closing substitution is a sequence of value bindings to variables: = ( 1 ↦ → <lb/>1 , ..., ↦ → , 1 ↦ → 1 , ..., ↦ → ) with all , distinct. We write ( ) to refer to if = and <lb/>we use ( ) to refer to if = . We define • to be the type derived from by substituting for all <lb/>variables in : • <lb/>[ 1 / 1 ]•••[ / ] [ 1 / 1 ]•••[ / ]. <lb/>Denotational Semantics. Figure 9 defines the denotations of types and environments. Follow-<lb/>ing Flanagan [2006], each closed type has a denotation <lb/>containing the set of closed values of <lb/>the appropriate base type which satisfy the type&apos;s refinement predicate. (The denotation of a type <lb/>variable is not defined as we only require denotations for closed types.) We lift the notion of <lb/>denotations to environments Γ as the set of closing substitutions, i.e. value and type bindings for <lb/>the variables in Γ, such that the values respect the denotations of the respective Γ-bound types and <lb/>the types are well formed with respect to the corresponding kinds. <lb/>Revisiting rule I-Den. The premise of the rule I-Den quantifies over all closing substitutions in <lb/>the denotations of the typing environment (i.e. ∀ ∈ Γ ). This quantification has two consequences. <lb/>First, the environment denotation appears in a negative position on the premise of the rule. Inspect-<lb/>ing Figure 9, the environment denotation uses the type denotation, which in turn uses type checking, <lb/>thus rendering a potential circularity between type and implication checking (arrow 6 of Figure 2). <lb/>Because of the negative occurrence, this mutual dependency would lead to a non-monotonic and <lb/>thus non-well defined system. To break this circularity, we use &apos;s type checking in the definition <lb/>of type denotations. <lb/>Second, the quantification is over all closing substitutions which are infinite. For example, a typing <lb/>environment that binds to an integer (i.e. : Int ∈ Γ) has infinitely many closing substitutions <lb/>mapping to a different integer. Thus, the denotational implementation cannot be used to implement <lb/>a decidable type checker. On the positive side, the denotational implementation connects implication <lb/>checking to the operational semantics thus it is amicable to mechanization. Concretely, we proved <lb/>( § 8) that the denotational implementations satisfies the statements of Requirement 2. <lb/>5 SOUNDNESS <lb/>Our development for ( § 5) follows the standard presentation of System F&apos;s metatheory by Pierce <lb/>[2002]. The main difference is that ours includes well-formedness of types and environments, which <lb/>help with mechanization [Rémy 2021] and are crucial in <lb/>when formalizing refinements. <lb/>Figure 10 charts the overall landscape of our formal development as a dependency graph of the <lb/>main lemmas which establish meta-theoretic properties of the different judgments for and <lb/>. <lb/>Nodes shaded light grey represent lemmas in the metatheories for both and <lb/>. The dark grey <lb/>nodes denote lemmas that only appear in <lb/>. An arrow shows a dependency: the lemma at the tail <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:17 <lb/></page>

        <body>Weakening Lemma (5.9) <lb/>Weakening Lemma <lb/>Substitution Lemma (5.8) <lb/>Substitution Lemma <lb/>Denotational Soundness (5.1) <lb/>Inversion <lb/>Weaken: tv in sub <lb/>Weaken: tv in typ <lb/>Weaken: var in sub <lb/>Weaken: var in typ <lb/>Weaken: tv in wf <lb/>Weaken: var in wf <lb/>Substitute: tv in sub <lb/>Substitute: tv in typ <lb/>Substitute: var in sub <lb/>Substitute: var in typ <lb/>Substitute: tv in wf <lb/>Substitute: var in wf <lb/>Implication Interf. (Rq. 2) <lb/>Den. Sound: subtyping <lb/>Denot. Sound: typing <lb/>Selfified Den. (5.2) <lb/>Exact Subtypes (5.11) <lb/>Exact Types (5.11) <lb/>Narrowing Lemma (5.10) <lb/>Transitivity (5.6) <lb/>Inversion of Typing (5.7) <lb/>Primitives (Req. 1) <lb/>Polym. Prim. (Req. 1) <lb/>Progress (5.4) <lb/>Preservation (5.5) <lb/>Values Stuck <lb/>Det. Semantics <lb/>Fig. 10. Dependencies in the metatheory. We write &quot;var&quot; and &quot;tv&quot; to resp. abbreviate term and type variables. <lb/>is used in the proof of the lemma at the head. Solid arrows are dependencies in <lb/>only. The chart <lb/>already shows that the metatheory of the refined calculus <lb/>is much more complex that the one <lb/>of the unrefined system , as also shown by the summary of our mechanization (Table 1). <lb/>5.1 Denotational Soundness <lb/>Denotational soundness connects syntactic typing and subtyping with the type denotations (of Fig-<lb/>ure 9). For typing it states that if Γ ⊢ : , then when is closed by any closing substitution of Γ it <lb/>evaluates to a value that belongs in the denotation of the closed . For subtyping, if Γ ⊢ ⪯ , then <lb/>under all closing substitutions, the denotation of the former type is contained in the latter: <lb/>Theorem 5.1. (Denotational Soundness) <lb/>(1) If Γ ⊢ : and ⊢ Γ and ∈ Γ then ( ) ↩→ * ∈ ( ) for some value . <lb/>(2) If Γ ⊢ 1 ⪯ 2 and ⊢ Γ and Γ ⊢ 1 : 1 and Γ ⊢ 2 : 2 and ∈ Γ then ( 1 ) ⊆ ( 2 ) . <lb/>The proof is by mutual induction on the structure of the judgments Γ ⊢ : and Γ ⊢ 1 ⪯ 2 respectively. <lb/>Our rule T-Var mentions selfification, so we use Lemma 5.2 for that case. <lb/>Lemma 5.2. (Selfified Denotations) If ∅ ⊢ : , ∅ ⊢ : , ↩→ * for some ∈ <lb/>then ∈ self( , , ) . <lb/>This lemma captures the intuition that if ∈ { : } (i.e. if has base type and [ / ] ↩→ * true), <lb/>then we have ∈ { : ∧ = } as ( ∧ = ) [ / ] certainly evaluates to true. <lb/>5.2 Type Safety <lb/>The type safety theorem states that a well-typed term does not get stuck: i.e. either evaluates to a <lb/>value or can step to another term (progress) of the same type (preservation). <lb/>Theorem 5.3. (Type Safety of <lb/>and ) <lb/>(1) (Type Safety) If ∅ ⊢ : and ↩→ * ′ , then ′ is a value or ′ ↩→ ′′ for some ′′ . <lb/>(2) (No Error) If ∅ ⊢ : and ↩→ * ′ , then ′ ≠ error. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:18 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>The No Error property explicitly states that well-typed terms cannot evaluate to the term error <lb/>(that encodes stuck terms) and is a direct implication of type safety. We prove type safety by induction <lb/>on the length of the sequence of steps ↩→ * ′ , using preservation and progress. <lb/>Progress The progress lemma says a well-typed term is a value or steps to some other term. <lb/>Lemma 5.4. (Progress) If ∅ ⊢ : , then is a value or ↩→ ′ for some ′ . <lb/>The proof is by induction on the typing derivation using the primitives Requirement 1, that we <lb/>proved for our built-in primitives, and the inversion of typing lemma. <lb/>Preservation The preservation lemma states that typing is preserved by evaluation. <lb/>Lemma 5.5. (Preservation) If ∅ ⊢ : and ↩→ ′ , then ∅ ⊢ ′ : . <lb/>The proof is by structural induction on the derivation of the typing judgment and implicitly uses <lb/>the inversion lemma. We use the determinism of the operational semantics (lemma 3.1) and the <lb/>canonical forms lemma to case split on to determine ′ . The interesting cases are for T-App and <lb/>T-TApp that require a substitution Lemma 5.8. Next, let&apos;s see the three main lemmas used in the <lb/>preservation and progress proofs. <lb/>5.3 Inversion of Typing Judgments <lb/>The region of Figure 10 labelled &quot;Inversion&quot; accounts for the fact that, due to subtyping chains, the <lb/>typing judgment in <lb/>is not syntax-directed. First, we establish that subtyping is transitive: <lb/>Lemma 5.6. (Transitivity) If Γ ⊢ 1 : 1 , Γ ⊢ 3 : 3 , ⊢ Γ, Γ ⊢ 1 ⪯ 2 , Γ ⊢ 2 ⪯ 3 , then Γ ⊢ 1 ⪯ 3 . <lb/>The proof consists of a case-split on the possible rules for Γ ⊢ 1 ⪯ 2 and Γ ⊢ 2 ⪯ 3 . When the last <lb/>rule used in the former is S-Wit and the latter is S-Bind, we require the substitution Lemma 5.8. As <lb/>Aydemir et al. [2005], we use the narrowing Lemma 5.10 for the transitivity for function types. <lb/>Inverting Typing Judgments We use the transitivity of subtyping to prove some non-trivial lemmas <lb/>that let us &quot;invert&quot; the typing judgments to recover information about the underlying terms and <lb/>types. We describe the non-trivial case which pertains to type and value abstractions: <lb/>Lemma 5.7. (Inversion of T-Abs, T-TAbs) <lb/>(1) If Γ ⊢ ( . ) : : → and ⊢ Γ, then for all ∉ Γ, : ,Γ ⊢ [ / ] : [ / ]. <lb/>(2) If Γ ⊢ (Λ 1 : 1 . ) :∀ : . and ⊢ Γ, then for all ′ ∉ Γ, ′ : ,Γ ⊢ [ ′ / 1 ] : [ ′ / ]. <lb/>If Γ ⊢ ( . ) : : → , then we cannot directly invert the typing judgment to get a judgment <lb/>for the body of . . Perhaps the last rule used was T-Sub, and inversion only tells us that there <lb/>exists a type 1 such that Γ ⊢ ( . ) : 1 and Γ ⊢ 1 ⪯ : → . Inverting again, we may in fact find a <lb/>chain of types +1 ⪯ ⪯ ••• ⪯ 2 ⪯ 1 which can be arbitrarily long. But the proof tree must be finite <lb/>so eventually we find a type : → such that Γ ⊢ ( . ) : : → and Γ ⊢ : → ⪯ : → <lb/>(by transitivity) and the last rule was T-Abs. Then inversion gives us that for any ∉ Γ we have <lb/>: ,Γ ⊢ : [ / ]. To get the desired typing judgment, we must use the narrowing Lemma 5.10 to <lb/>obtain : ,Γ ⊢ : [ / ]. Finally, we use T-Sub to derive : ,Γ ⊢ : [ / ]. <lb/>5.4 Substitution Lemma <lb/>In <lb/>, unlike unrefined calculi such as , typing and subtyping are mutual dependent. Due to <lb/>this dependency, both the substitution the weakening lemmas must now be proven in a mutually <lb/>recursive form: <lb/>Lemma 5.8. (Substitution) <lb/>(1) If Γ 1 , : ,Γ 2 ⊢ ⪯ , ⊢ Γ 2 , and Γ 2 ⊢ : , then Γ 1 [ / ],Γ 2 ⊢ [ / ] ⪯ [ / ]. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:19 <lb/></page>

        <body>(2) If Γ 1 , : ,Γ 2 ⊢ : , ⊢ Γ 2 , and Γ 2 ⊢ : , then Γ 1 [ / ],Γ 2 ⊢ [ / ] : [ / ]. <lb/>(3) If Γ 1 , : ,Γ 2 ⊢ ⪯ , ⊢ Γ 2 , and Γ 2 ⊢ <lb/>: , then Γ 1 [ / ],Γ 2 ⊢ [ / ] ⪯ [ / ]. <lb/>(4) If Γ 1 , : ,Γ 2 ⊢ : , ⊢ Γ 2 , and Γ 2 ⊢ <lb/>: , then Γ 1 [ / ],Γ 2 ⊢ [ / ] : [ / ]. <lb/>The proof goes by induction on the derivation trees. The main difficulty arises in substituting <lb/>some type for variable in Γ 1 , : ,Γ 2 ⊢ { 1 : } ⪯ { 2 : } because must be strengthened by the <lb/>refinements and respectively. Because we encoded our typing rules using cofinite quantifica-<lb/>tion [Aydemir et al. 2008] the proof does not require a renaming lemma, but the rules that lookup <lb/>environments (rules T-Var and WF-Var) do need a weakening Lemma: <lb/>Lemma 5.9. (Weakening) If , ∉ Γ 1 ,Γ 2 , then <lb/>(1) if Γ 1 ,Γ 2 ⊢ : then Γ 1 , : ,Γ 2 ⊢ : and Γ 1 , : ,Γ 2 ⊢ : . <lb/>(2) if Γ 1 ,Γ 2 ⊢ ⪯ then Γ 1 , : ,Γ 2 ⊢ ⪯ and Γ 1 , : ,Γ 2 ⊢ ⪯ . <lb/>The proof is by mutual induction on the derivation of the typing and subtyping judgments. <lb/>5.5 Narrowing <lb/>The narrowing lemma says that whenever we have a judgment where a binding : appears in the <lb/>binding environment, we can replace by any subtype . The intuition here is that the judgment <lb/>holds under the replacement because we are making the context more specific. <lb/>Lemma 5.10. (Narrowing) If Γ 2 ⊢ &lt;: , Γ 2 ⊢ <lb/>: , and ⊢ Γ 2 then <lb/>(1) if Γ 1 , : ,Γ 2 ⊢ : , then Γ 1 , : ,Γ 2 ⊢ : . <lb/>(2) if Γ 1 , : ,Γ 2 ⊢ 1 &lt;: 2 , then Γ 1 , : ,Γ 2 ⊢ 1 &lt;: 2 . <lb/>(3) if Γ 1 , : ,Γ 2 ⊢ : , then Γ 1 , : ,Γ 2 ⊢ : . <lb/>The narrowing proof requires an exact typing Lemma 5.11 which says that both subtyping and <lb/>typing is preserved after selfification. <lb/>Lemma 5.11. (Exact Typing) <lb/>(1) If Γ ⊢ : , ⊢ Γ, Γ ⊢ : , and Γ ⊢ ⪯ , then Γ ⊢ self ( , , ) ⪯ self ( , , ). <lb/>(2) If Γ ⊢ : , ⊢ Γ, and Γ ⊢ : , then Γ ⊢ : self ( , , ). <lb/>6 LIQUIDHASKELL &amp; REFINED DATA PROPOSITIONS <lb/>In § 7 we present how we proved <lb/>soundness in LiqidHaskell. To do so, we developed refined <lb/>data propositions, a novel feature of LiqidHaskell that made such a meta-theoretic proof possible. <lb/>6.1 LiquidHaskell <lb/>LiqidHaskell&apos;s core proof system is <lb/>, that is, it is using the typing judgement presented in fig. 7 <lb/>to check if a Haskell program satisfies its refinement type annotations. The expression language <lb/>checked by LiqidHaskell is GHC&apos;s intermediate language (CoreSyn [Sulzmann et al. 2007]) which <lb/>is a superset of <lb/>that also includes literals, datatypes, and coercions. Thus, LiqidHaskell&apos;s typing <lb/>judgement is extended to include these constructs. To guess the unknown types of fig. 7 (i.e. in the <lb/>rules T-Sub and T-Let) and make the typing judgment algorithmic LiqidHaskell implements the <lb/>refinement type inference algorithm of liquid types [Rondon et al. 2008]. To check the implications <lb/>LiqidHaskell uses the I-Log rule of § 4.4.2 which are automatically discharged by an SMT solver. <lb/>LiquidHaskell as a Theorem Prover. Equipped with the SMT solver, LiqidHaskell can be used <lb/>to prove theorems over theories known to the SMT solver. For example, that addition over integers <lb/>is associative and that for every integer there exists a larger one, as encoded by the below functions: <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:20 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>assoc :: x:Int → y:Int → {v:() | x + y == y + x } <lb/>assoc _ _ = () <lb/>exLg :: x:Int → (y::Int,{v:() | y &gt; x }) <lb/>exLg x = (x+1, ()) <lb/>These definitions use lambda abstraction and dependent pairs to respectively encode the universal <lb/>and existential quantifiers. To encode logical terms, such as y &gt; x, they refine the unit type with <lb/>such terms. Building upon this idea, LiqidHaskell has been extensively used to prove theorems, <lb/>using recursive Haskell definitions to encode inductive proofs and refinement reflection [Vazou <lb/>et al. 2018] to allow user-defined terminating functions into the refinement logic. Yet, the proving <lb/>power of LiqidHaskell was limited because only provably terminating functions can be used in the <lb/>refinement logic and the proofs were implicitly performed by the SMT solver. Thus, the programmer <lb/>could not inspect the proof terms. <lb/>6.2 Refined Data Propositions <lb/>Refined data propositions encode Coq-style inductive predicates permit constructive reasoning <lb/>about potentially non-terminating properties, as required for meta-theoretic proofs. <lb/>Refined data propositions encode inductive predicates in LiqidHaskell by refining Haskell&apos;s <lb/>data types, allowing the programmer to write plain Haskell functions to provide constructive proofs <lb/>for user-defined propositions. Here, for exposition, we present the four steps we followed in the <lb/>mechanization of <lb/>to define the &quot;has-type&quot; proposition and then use it to type the primitive one. <lb/>Step 1: Reifying Propositions as Data Our first step is to represent the propositions of interest <lb/>as plain Haskell data. For example, we can define the following types (suffixed Pr for &quot;proposition&quot;): <lb/>data HasTyPr = HasTyPr <lb/>Env Expr Type <lb/>data IsSubTyPr = IsSubTyPr Env Type Type <lb/>Thus, HasTyPr e t and IsSubTyPr s t resp. represent the propositions ⊢ : and ⊢ ⪯ . <lb/>Step 2: Reifying Evidence as Data Next, we reify evidence, i.e. derivation trees as data by defining <lb/>Haskell data types with a single constructor per derivation rule. For example, we define the data type <lb/>HasTyEv to encode the typing rules of Figure 7, with constructors that match the names of each rule. <lb/>data HasTyEv where <lb/>TPrim :: Env → Prim → HasTyEv <lb/>TSub :: Env → Expr → Type → Type → HasTyEv → IsSubTyEv → HasTyEv <lb/>... <lb/>Using these data one can construct derivation trees. For instance, TPrim Empty (PInt 1):: <lb/>HasTyEv is the tree that types the primitive one under the empty environment. <lb/>Step 3: Relating Evidence to its Propositions Next, we specify the relationship between the <lb/>evidence and the proposition that it establishes, via a refinement-level uninterpreted function: <lb/>measure hasTyEvPr :: HasTyEv → HasTyPr <lb/>measure isSubTyEvPr :: IsSubTyEv → IsSubTyPr <lb/>The above signatures declare that hasTyEvPr (resp. isSubTyEvPr) is a refinement-level function <lb/>that maps has-type (resp. is-subtype) evidence to its corresponding proposition. We can now use <lb/>these uninterpreted functions to define type aliases that denote well-formed evidence that establishes <lb/>a proposition. For example, consider the (refined) type aliases <lb/>type HasTy <lb/>e t = {ev:HasTyEv | hasTyEvPr ev == HasTyPr <lb/>e t } <lb/>type IsSubTy <lb/>s t = {ev:IsSubTyEv | isSubTyEvPr ev == IsSubTyPr <lb/>s t } <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:21 <lb/></page>

        <body>The definition stipulates that the type HasTy e t is inhabited by evidence (of type HasTyEv) <lb/>that establishes the typing proposition HasTyPr e t. Similarly IsSubTy s t is inhabited by <lb/>evidence (of type IsSubTyEv) that establishes the subtyping proposition IsSubTyPr s t. Note <lb/>that the first three steps have only defined separate data types for propositions and evidence, and <lb/>specified the relationship between them via uninterpreted functions in the refinement logic. <lb/>Step 4: Refining Evidence to Establish Propositions Finally, we implement the relationship <lb/>between evidence and propositions refining the types of the evidence data constructors (rules) with <lb/>pre-conditions that require the rules&apos; premises and post-conditions that ensure the rules&apos; conclusions. <lb/>For example, we connect the evidence and proposition for the typing relation by refining the data <lb/>constructors for HasTyEv using their respecting typing rule from Figure 7. <lb/>data HasTyEv where <lb/>TPrim :: :Env → c:Prim → HasTy (Prim c) (ty c) <lb/>TSub :: :Env → e:Expr → s:Type → t:Type <lb/>→ HasTy e s → IsSubTy s t → HasTy e t <lb/>... <lb/>The constructors TPrim and TSub respectively encode the rules T-Prim and T-Sub (with well-<lb/>formedness elided for simplicity). The refinements on the input types, which encode the premises <lb/>of the rules, are checked whenever these constructors are used. The refinement on the output type <lb/>(being evidence of a specific proposition) is axiomatized to encode the conclusion of the rules. For <lb/>example, the type for TSub says that &quot;for all , , , , given evidence that ⊢ : and ⊢ ⪯ &quot;, the <lb/>constructor returns &quot;evidence that ⊢ : &quot;. <lb/>Implementation of Data Propositions Data propositions are a novel feature required to encode <lb/>inductive propositions in the mechanization of <lb/>. (Parker et al. [2019] developed a LiqidHaskell <lb/>metatheoretic proof but before data propositions and thus had to axiomatize a terminating eval-<lb/>uation relation; see § 9.) Refined data propositions are implemented as part of LiqidHaskell&apos;s <lb/>existing refined data types that already supported subtyping on constructor arguments using variant <lb/>and contravariant rules, as described but not formalized in [Jhala and Vazou 2021]. The essential <lb/>extension to support data propositions is that by refining the output types of inductive data types, <lb/>LiqidHaskell can support constructive derivation-tree-style proofs. To use this feature in practice, <lb/>we had to extend the refinement logic of LiqidHaskell to use existing SMT support to make data <lb/>constructors injective, i.e. if is a constructor then ∀ , . ( ) = ( ) ⇒ = . Thus, refined data types <lb/>and injectivity are the two required components to implement data propositions. <lb/>7 LIQUIDHASKELL MECHANIZATION <lb/>We mechanized type safety (Theorem 5.3) of <lb/>in both Coq 8.15.1 and LiqidHaskell 8.10.7.1 <lb/>(submitted as anonymous supplementary material). In LiqidHaskell we use refined data proposi-<lb/>tions ( § 6) to specify the static (e.g. typing, subtyping, well-formedness) and dynamic (i.e. small-step <lb/>transitions and their closure) semantics of <lb/>. Other that the development of data propositions, we <lb/>extended LiqidHaskell with two more features during the development of this proof. First, we <lb/>implemented an interpreter that critically dropped the verification time from 10 hours to only 29 <lb/>minutes ( §7.3). Second, we implemented a (Coq-style) strictly-positive-occurrence checker to ensure <lb/>data propositions are well defined, since early versions of our proof used negative occurrences. <lb/>The LiqidHaskell mechanization is simplified by SMT-automation ( § 7.1) and consists of proofs <lb/>implemented as recursive functions that construct evidence to establish propositions by induction <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:22 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>( § 7.2). Note that while Haskell types are inhabited by diverging ⊥ values, LiqidHaskell&apos;s to-<lb/>tality, termination, and type checks ensure that all cases are handled, the induction (recursion) is <lb/>well-founded, and that the proofs (programs) indeed inhabit the propositions (types). <lb/>7.1 SMT Solvers, Arithmetic, and Set Theory <lb/>The most tedious part in mechanization of metatheories is the establishment of invariants about <lb/>variables, for example uniqueness and freshness. LiqidHaskell offers a built-in, SMT automated <lb/>support for the theory of sets, which simplifies establishing such invariants. <lb/>Intrinsic Verification LiqidHaskell embeds the functions of the standard Data.Set Haskell <lb/>library as SMT set operators. Given a Haskell function, e.g. the set of free variables in an expression, <lb/>this embedding, combined with SMT&apos;s support for set theory, lets LiqidHaskell prove properties <lb/>about free variables &quot;for free&quot;. For example, consider the function subFV x vx e which substitutes <lb/>the variable x with vx in e. The refinement type of subFV describes the free variables of the result. <lb/>subFV :: x:VName → vx:{Expr | isVal vx } → e:Expr <lb/>→ {e&apos;:Expr | fv e&apos; ⊆ (fv vx ∪ (fv e \ x)) &amp;&amp; (isVal e ⇒ isVal e&apos;)} <lb/>subFV x vx (EVar y) <lb/>= if x == y then vx else EVar y <lb/>subFV x vx (ELam e) = ELam (subFV x vx e) <lb/>subFV x vx (EApp e e&apos;) = EApp (subFV x vx e) (subFV x vx e&apos;) <lb/>... --other cases <lb/>The refinement type specifies that the free variables after substitution is a subset of the free vari-<lb/>ables in the two argument expressions, excluding x, i.e. fv( [ / ]) ⊆ fv( ) ∪ (fv( ) \ { }). This <lb/>specification is proved intrinsically, i.e. the definition of subFV is the proof (no user aid is required) <lb/>and, importantly, the specification is automatically established each time the function subFV is <lb/>called without any need for explicit hints. The specification of subFV above shows another example <lb/>of SMT-based proof simplification. It intrinsically proves that the value property is preserved by <lb/>substitution, using the Haskell boolean function isVal that defines when an expression is a value. <lb/>7.2 Inductive Proofs as Recursive Functions <lb/>The majority of our proofs are by induction on derivations. These proofs are recursive Haskell <lb/>functions that operate over refined data propositions. LiqidHaskell ensures the proofs are valid by <lb/>checking that they are inductive (i.e. the recursion is well-founded), handle all cases (i.e. the function <lb/>is total), and establish the desired properties (i.e. witnesses the appropriate proposition). <lb/>Preservation (Lemma 5.5) relates the HasTy data proposition of § 6 with a Step data proposition <lb/>that encodes Figure 5 and is proved by induction on the type derivation tree. Below we present a <lb/>snippet of the proof, where the subtyping case is by induction while the primitive case is impossible: <lb/>preservation :: e:Expr → t:Type → e&apos;:Expr → HasTy Empty e t <lb/>→ Step e e&apos; → HasTy Empty e&apos; t <lb/>preservation _ e _ t e&apos; (TSub Empty e t&apos; t e _ has _ t&apos; t&apos; _ sub _ t) e _ step _ e&apos; <lb/>= TSub Empty e&apos; t&apos; t (preservation e t&apos; e&apos; e _ has _ t&apos; e _ step _ e&apos;) t&apos; _ sub _ t <lb/>preservation e _ t e&apos; (TPrim _ _ ) step <lb/>= impossible &quot;value&quot; ? lemValStep e e&apos; step --↩→ ′ ⇒ ¬(isVal ) <lb/>... <lb/>impossible :: {v:String | false} → a <lb/>lemValStep :: e:Expr → e&apos;:Expr → Step e e&apos; → {¬(isVal e)} <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:23 <lb/></page>

        <body>In the TSub case we note that LiqidHaskell knows that the argument _ e is equal to the subtyping <lb/>parameter e. The termination checker ensures the inductive call happens on a smaller derivation <lb/>subtree. The TPrim case is by contradiction since primitives cannot step: we proved values cannot <lb/>step in the lemValStep lemma, which is combined via the (?) combinator of type a → b → a with <lb/>the fact that e is a value to allow the call of the false-precondition impossible. <lb/>LiqidHaskell&apos;s totality checker ensures all cases of HasTyEv are covered and the termination <lb/>checker ensures the proof is well-founded. <lb/>7.3 <lb/>antitative Results <lb/>We provide a mechanically checked proof of the type safety in § 5, that only assumes the requirements <lb/>1 and 2. Concretely, we assumed the primitives Requirement 1 for some constants of <lb/>because it <lb/>was too strenuous to mechanically prove without interactive aid. In LiqidHaskell type denotations <lb/>(of Figure 9) cannot be currently encoded: since they include ∀-quantification they could only be <lb/>encoded as data propositions, but the strictly-positive-occurrence checker rejects the definition of the <lb/>function denotation. Due to this limitation, we can neither define the denotational implementation <lb/>of the implication ( § 4.4.3) nor prove the denotational soundness (Theorem 5.1). <lb/>Representing Binders One main challenge in the mechanized metatheory is the syntactic rep-<lb/>resentation of variables and binders [Aydemir et al. 2005]. The named representation has severe <lb/>difficulties because of variable capturing substitutions and the nameless (a.k.a. de Bruijn) requires <lb/>heavy index shifting. The variable representation of <lb/>is locally nameless representation [Aydemir <lb/>et al. 2008; Pollack 1993], where free variables are named, but bound variables are represented by <lb/>deBruijn indices. Our mechanization still resembles the paper and pencil proofs (performed before <lb/>mechanization), yet it clearly addresses the following two problems with named bound variables. <lb/>First, when different refinements are strengthened (as in Figure 5) the variable capturing problem <lb/>reappears because we are substituting underneath a binder. Second, subtyping usually permits <lb/>alpha-renaming of binders, which breaks a required invariant that each <lb/>derivation tree is a valid <lb/>tree after erasure. <lb/>Table 1 summarizes the development of our metatheory, which was checked using LiqidHaskell <lb/>8.10.7.1 and a Lenovo ThinkPad T15p laptop with an Intel Core i7-11800H processor. Our mechanized <lb/>proofs are substantial. The entire LiqidHaskell development comprises over 12,800 lines across <lb/>about 35 files. Currently, the whole LiqidHaskell proof can be checked in 29 minutes, which makes <lb/>interactive development difficult, especially compared to the Coq proof ( § 8) that is checked in about <lb/>60 seconds. While incremental modular checking provides a modicum of interactivity, improving <lb/>the ergonomics of LiqidHaskell, i.e. verification time and actionable error messages, remains an <lb/>important direction for future work. <lb/>8 COQ MECHANIZATION <lb/>Our Coq mechanization proves both type safety and denotation soundness, i.e. all the statements <lb/>of § 5 and serves as a comparison for the metatheoretical development abilities of the two theorem <lb/>provers. In Coq, Req. 1 is proved (using Coq&apos;s interactive development) and type denotations (of <lb/>Figure 9) are defined as recursive functions using Equations [Sozeau and Mangin 2019], which make <lb/>both the definition the denotational implementation of the implication ( § 4.4.3) and the proof the <lb/>denotational soundness (Theorem 5.1) possible. To fairly compare the two developments in terms <lb/>of effort and ergonomics, we did not use external Coq libraries because no such libraries exist yet <lb/>for LiqidHaskell. Vazou et al. [2017] previously compared LiqidHaskell and Coq as theorem <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:24 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>LiqidHaskell Mechanization <lb/>Coq Mechanization <lb/>Subject <lb/>Files <lb/>Time (m) <lb/>Spec <lb/>Proof <lb/>Files <lb/>Spec <lb/>Proof <lb/>Definitions <lb/>6 <lb/>1 <lb/>1805 <lb/>374 <lb/>7 <lb/>941 <lb/>190 <lb/>Basic Properties <lb/>8 <lb/>4 <lb/>646 <lb/>2117 <lb/>8 <lb/>1201 <lb/>2360 <lb/>Soundness <lb/>4 <lb/>3 <lb/>138 <lb/>685 <lb/>4 <lb/>173 <lb/>773 <lb/>Weakening <lb/>4 <lb/>1 <lb/>379 <lb/>467 <lb/>4 <lb/>110 <lb/>568 <lb/>Substitution <lb/>4 <lb/>7 <lb/>458 <lb/>846 <lb/>4 <lb/>158 <lb/>859 <lb/>Exact Typing <lb/>2 <lb/>4 <lb/>70 <lb/>230 <lb/>2 <lb/>33 <lb/>182 <lb/>Narrowing <lb/>1 <lb/>1 <lb/>88 <lb/>166 <lb/>1 <lb/>54 <lb/>262 <lb/>Inversion <lb/>1 <lb/>1 <lb/>124 <lb/>206 <lb/>1 <lb/>57 <lb/>258 <lb/>Primitives <lb/>3 <lb/>4 <lb/>120 <lb/>277 <lb/>3 <lb/>89 <lb/>508 <lb/>Soundness <lb/>1 <lb/>1 <lb/>14 <lb/>181 <lb/>1 <lb/>12 <lb/>233 <lb/>Denotational Soundness <lb/>-<lb/>-<lb/>-<lb/>-<lb/>13 <lb/>815 <lb/>3010 <lb/>Total <lb/>35 <lb/>29 <lb/>3842 <lb/>5549 <lb/>49 <lb/>3643 <lb/>9203 <lb/>Table 1. <lb/>antitative mechanization details. We split each development into sets of modules pertaining to <lb/>regions of Figure 10 and for each we count lines of specification (definitions, lemma statements) and of proof. <lb/>provers, but their mechanizations were an order of magnitude smaller than ours and did not use data <lb/>propositions ( § 6), which permit constructive LiqidHaskell proofs. <lb/>Coq vs. LiquidHaskell Coq has a tiny TCB and strong foundational mechanized soundness <lb/>guarantees [Sozeau et al. 2020]. In contrast, LiqidHaskell trusts the Haskell compiler (GHC), the <lb/>SMT solver (Z3), and its constraint generation rules which have not been formalized. This work, <lb/>, <lb/>serves precisely that purpose: by formalizing and mechanizing a significant subset of LiqidHaskell, <lb/>leaving out literals, casts, and data types. As far as the user experience is concerned, Coq metatheo-<lb/>retical developments are much faster to check, which was expected since LiqidHaskell comes with <lb/>expensive inference, and can be aided by relevant libraries. The two tools come with different kinds <lb/>of automation: tactics vs. SMT, which we found to be useful in complementary parts of the proofs, <lb/>pointing the way to possible improvements for both verification styles. Finally, LiqidHaskell <lb/>facilitates reasoning over mutually defined and partial functions. <lb/>Negative Occurrences and Coq&apos;s Equations Our original LiqidHaskell mechanization defined <lb/>denotations as refined data propositions and proved denotational soundness. Though, we realized <lb/>that the definition of the function type denotation has a negative occurrence and permitting negative <lb/>occurrences can, in general, lead to unsoundness [Coquand and Paulin 1990]. Our mechanization <lb/>is the first big-scale user of LiqidHaskell&apos;s data propositions thus it was not surprising that it <lb/>revealed this potential unsoundness. To remove this source of unsoundness in LiqidHaskell, <lb/>we implemented a Coq-style positivity checker that unsurprisingly rejected the type denotation <lb/>definitions. A similar challenge appears in the proof of strong normalization of the simply-type <lb/>lambda calculus that because of negative occurrences cannot use inductive propositions [Pierce et al. <lb/>2022]. There, the solution is to use a recursive function expr → type → Prop because a definition <lb/>doesn&apos;t need to be computable. In our Coq mechanization, we followed a similar solution, but since <lb/>our definition was not structurally recursive and was needed for the proofs, we used the full power of <lb/>Coq&apos;s Equations [Sozeau and Mangin 2019] to define the type denotations. Unfortunately, a similar <lb/>approach cannot currently carry over to LiqidHaskell because all Haskell functions must be <lb/>computable and all LiqidHaskell annotations must be decidable. Therefore, quantifiers are neither <lb/>allowed on the right-hand side of Haskell definitions nor in the refinements. <lb/>Tactics and Automation Coq&apos;s tactics and automation often permit shorter proofs as lemmas and <lb/>constructors can be used with the apply tactic without writing out all arguments. For example, in <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:25 <lb/></page>

        <body>LiqidHaskell safety (thm. 5.3) is encoded using Haskell&apos;s Either for disjunction and dependent <lb/>pairs for existentials. (Steps is defined, using data propositions, as the closure of Step.) <lb/>safety :: e 0 :Expr → t:Type → e:Expr → HasTy Empty e 0 t → Steps e 0 e <lb/>→ Either {isVal e} (e ::Expr, Step e e ) <lb/>safety _ e 0 t _ e e 0 _ has _ t e 0 _ evals _ e = case e 0 _ evals _ e of <lb/>Refl e 0 → progress e 0 t e 0 _ has _ t <lb/>--0 = <lb/>AddStep e 0 e 1 e 0 _ step _ e 1 e e 1 _ eval _ e → --0 ↩→ 1 ↩→ * <lb/>safety e 1 t e (preservation e 0 t e 0 _ has _ t e 1 e 0 _ step _ e 1 ) e 1 _ eval _ e <lb/>The reflexive case is proved by progress. In the inductive case the evaluation sequence is 0 ↩→ <lb/>1 ↩→ * and the proof goes by induction, using preservation to ensure that 1 is typed. In Coq safety <lb/>is proved without any of the three fully applied calls above: <lb/>Theorem safety : forall (e 0 e:expr) (t:type), <lb/>Steps e 0 e → HasTy Empty e 0 t → isVal e \/ exists e , Steps e e . <lb/>Proof. intros; induction H. <lb/>-( * Refl * ) apply progress with t; assumption. <lb/>-( * Add * ) apply IHSteps; apply preservation with e; assumption. Qed. <lb/>Mutual Recursion LiqidHaskell makes it easy to define and work with mutually recursive data <lb/>types, such as our typing and subtyping judgments, and to prove mutually inductive lemmas. Mutu-<lb/>ally recursive types are not a natural fit for Coq: the automatically generated induction principles do <lb/>not work, so we need to use the Scheme keyword to generate suitable principles. Theorems involving <lb/>these types cannot be broken up into separate lemmas for each type involved. Rather, one combined <lb/>statement must be given, which is difficult to use in the rewrite tactic. <lb/>Another weakness of Coq is that all information about the hypothesis is lost during the induc-<lb/>tion tactic, so the normal structural induction tactic only works when a judgment contains no <lb/>information, i.e. the data constructor is instantiated solely with universally quantified variables. For <lb/>instance, in the proof of the weakening Lemma 5.9, to do structural induction on HasTy (concat <lb/>g g&apos;)e t we must introduce a universally quantified variable g0 and strengthen the theorem with <lb/>the hypothesis g0 = concat g g&apos;. While the standard library contains an &quot;experimental&quot; tactic <lb/>dependent induction, we also need to work with the special mutual induction principles that we <lb/>generate for our types, so we have to directly instantiate the principle with a strengthened, complex <lb/>hypothesis. By contrast, in LiqidHaskell we can state two separate mutually recursive lemma <lb/>functions for weakening: one for typing and one for subtyping. Then we may call either lemma <lb/>in their own proofs on any smaller instance of the typing (resp. subtyping) judgment. In practice, <lb/>developments in Coq sidestep some of these issues by collapsing the language of terms, types, etc. into <lb/>a single inductive data type. This approach has the advantage of reducing the number of substitution <lb/>operations, but allows highly ungrammatical combinations like App Bool False into our syntax. <lb/>We could still use this approach combined with a pre-term encoding common in Coq developments, <lb/>but we preferred to keep a closer comparison to the LiqidHaskell mechanization. <lb/>Partial Functions LiqidHaskell facilitates the definition of partial Haskell functions and proves <lb/>totality with respect to the refined types, usually automatically. For instance, our syntax does not <lb/>contain an explicit error value, so we only want the function ( , ) to be defined where can step in <lb/>our semantics. This is straightforward in LiqidHaskell: we define a predicate isCompat :: Prim <lb/>→ Value → Bool and refine the input types of to satisfy isCompat. In Coq a more roundabout <lb/>approach is needed: we have to define isCompat as an inductive type and include this object as an <lb/>explicit argument to our function. However, this makes it harder to prove the determinism of our <lb/>semantics due to the dependence on the proof object. One solution would be to define a partial version <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:26 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>of with type Prim → Expr → option Expr and prove the two functions always agree regardless <lb/>of proof object, e.g. using subset types; but since each value comes wrapped with a term-level proof <lb/>object, agreement proofs would require a Proof Irrelevance axiom. <lb/>9 RELATED WORK <lb/>We discuss the most closely related work on the metatheory of unrefined and refined type systems. <lb/>Hybrid &amp; Contract Systems Flanagan [2006] formalizes on paper a monomorphic lambda calculus <lb/>with refinement types that differs from our <lb/>in two ways. First, in Flanagan [2006]&apos;s type checking <lb/>is hybrid: the developed system is undecidable and inserts runtime casts when subtyping cannot be <lb/>statically decided. Second, the original system lacks polymorphism. Sekiyama et al. [2017] extended <lb/>hybrid types with polymorphism, but unlike <lb/>, their system does not support semantic subtyping. <lb/>For example, consider a divide by zero-error. The refined types for div and 0 could be given by <lb/>div :: Int → Int{ : ≠ 0} → Int and 0 :: Int{ : = 0}. This system will compile div 1 0 by inserting <lb/>a cast on 0: ⟨Int{ : = 0} ⇒ Int{ : ≠ 0}⟩, causing a definite runtime failure that could have easily <lb/>been prevented statically. Having removed semantic subtyping, the metatheory of Sekiyama et al. <lb/>[2017] is highly simplified. Static refinement type systems (as summarized by Jhala and Vazou [2021]) <lb/>usually restrict the definition of predicates to quantifier-free first-order formulae that can be decided <lb/>by SMT solvers. This restriction is not preserved by evaluation that can substitute variables with any <lb/>value, thus allowing expressions that cannot be encoded in decidable logics, like lambdas, to seep into <lb/>the predicates of types. In contrast, we allow predicates to be any language term (including lambdas) to <lb/>prove soundness via preservation and progress: our meta-theoretical results trivially apply to systems <lb/>that, for efficiency of implementation, restrict their source languages. Finally, none of the above <lb/>systems (hybrid, contracts or static refinement types) come with a machine checked soundness proof. <lb/>Semantic Subtyping Semantic subtyping is not a unique feature of refinement types. For example, <lb/>Frisch et al. [2002] use the set theoretic models of types to decide subtyping. Castagna and Frisch [2005] <lb/>present an algorithm that decides semantic subtyping for a core calculus with functional types. Like <lb/>, Castagna and Frisch [2005] introduce a denotational interpretation of types to break the circular-<lb/>ity between the typing and subtyping relations. Unlike <lb/>, their system does not have polymorphism <lb/>and, crucially, has no notion of dependency (no refinement type-style binder of arguments). Moreover, <lb/>their subtyping algorithm is different than our refinement based algorithm: it is neither type directed <lb/>nor efficient (i.e. it requires backtracking), and cannot be automated by an external SMT solver. <lb/>Mechanizations of Refinement Types Lehmann and Tanter [2016]&apos;s Coq formalization of a <lb/>monomorphic, refined calculus differs from <lb/>in two ways. First, their axiomatized implication, <lb/>which is similar to our implication interface, allows them to restrict the language of refinements to <lb/>decidable logics but provides no formal connection between subtyping and evaluation. Instead, we <lb/>also provide the denotational implementation of the implication interface, thus establish denotation <lb/>soundness. Second, <lb/>includes polymorphism, existentials, and selfification which are critical <lb/>for context-sensitive refinement typing, but make the metatheory more challenging. Hamza et al. <lb/>[2019] present System FR, a polymorphic, refined language with a mechanized metatheory of about <lb/>30K lines of Coq. Compared to our system, their notion of subtyping is not semantic, but relies <lb/>on a reducibility relation. For example, even though System FR will deduce that Pos is a subtype <lb/>of Int, it will fail to derive that Int → Pos is subtype of Pos → Int as reduction-based subtyping <lb/>cannot reason about contra-variance. Because of this more restrictive notion of subtyping, their <lb/>mechanization requires neither the indirection of denotational soundness nor an implication proving <lb/>oracle. Further, System FR&apos;s support for polymorphism is limited in that it disallows refinements <lb/>on type variables, thereby precluding many practically useful specifications. Recently, Chen [2022] <lb/>formalized a refinement type system as an embedding of refinement types in Agda. This system is <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:27 <lb/></page>

        <body>verified in a few thousand lines of Agda. This formalism differs significantly from ours in that as <lb/>an embedding it is built on top of a rich theorem prover and cannot be used to refine some existing <lb/>programming language. Further, it does not support higher-order functions, polymorphism, semantic <lb/>subtyping, neither be automated by an external solver since soundness reduces to Agda&apos;s soundness. <lb/>Finally, Ghalayini and Krishnaswami [2023] mechanize refinement types with explicit proof terms <lb/>in 15K lines of Lean code. They use a categorical, denotational semantics soundness statement, but <lb/>their calculus by design supports neither semantic subtyping nor polymorphism. <lb/>Metatheory in LiquidHaskell LWeb [Parker et al. 2019] also used LiqidHaskell to prove <lb/>metatheory, the non-interference of LWeb , a core calculus that extends the LIO formalism with <lb/>database access. The LWeb proof did not use refined data propositions, which were not present at <lb/>development time, and thus it has two major weaknesses compared to our present development. <lb/>First, LWeb assumes termination of LWeb &apos;s evaluation function; without refined data propositions <lb/>metatheory can be developed only over terminating functions. This was not a critical limitation since <lb/>non-interference was only proved for terminating programs. However, in our proof the requirement <lb/>that evaluation of <lb/>terminates would be too strict. In our encoding with refined data propositions <lb/>such an assumption was not required. Second, the LWeb development is not constructive: the struc-<lb/>ture of an assumed evaluation tree is logically inspected instead of the more natural case splitting <lb/>permitted only with refined data propositions. This constructive way to develop metatheories is <lb/>more compact (e.g. there is no need to logically inspect derivation trees) and akin to the standard <lb/>meta-theoretic developments of constructive tools like Coq and Isabelle. <lb/>10 CONCLUSIONS &amp; FUTURE WORK <lb/>We presented and formalized, for the first time, the soundness of <lb/>, a refinement calculus with <lb/>semantic subtyping, existential types, and parametric polymorphism, which are critical for practical <lb/>refinement typing. Our metatheory is mechanized in both Coq and LiqidHaskell, the latter using <lb/>the novel feature of refined data propositions to reify derivations as (refined) Haskell datatypes, <lb/>using SMT to automate invariants about variables. <lb/>While our proof can be mechanized in other proof assistants like Agda [Norell 2007], Isabelle [Nip-<lb/>kow et al. 2002], Beluga [Pientka 2010], Dafny [Leino 2010], or F* [Martínez et al. 2019], our goal here <lb/>is not to compare LiqidHaskell against every system. Instead, our primary contribution is to, for the <lb/>first time, establish the soundness of the combination of features critical for practical refinement typing <lb/>and show that such a proof can be mechanized as a plain program with refinement types. Looking <lb/>ahead, we envision two lines of work on mechanizing metatheory of and with refinement types. <lb/>1. Mechanization of Refinements <lb/>covers a crucial but small fragment of the features of modern <lb/>refinement type checkers. The immediate next step is to extend the language to include literals, casts, <lb/>and data types, thus covering all GHC&apos;s core calculus. Next, <lb/>can be extended to more sophisticated <lb/>features of refinement types, such as abstract and bounded refinements and refinement reflection. <lb/>Similarly, our current work axiomatizes the requirements of the semantic implication checker (i.e. <lb/>SMT solver). It would be interesting to implement a solver and verify that it satisfies that contract, <lb/>or alternatively, show how proof certificates [Necula 1997] could be used in place of such axioms. <lb/>2. Mechanization with Refinements While this work shows that non-trivial meta-theoretic proofs <lb/>are possible with SMT-based refinement types, our experience is that much remains to make such <lb/>developments pleasant. For example, programming would be far more convenient with support <lb/>for automatically splitting cases or filling in holes as done in Agda [Norell 2007] and envisioned by <lb/>Redmond et al. [2021]. Similarly, when a proof fails, the user has little choice but to think really hard <lb/>about the internal proof state and what extra lemmas are needed to prove their goal. Finally, the <lb/>stately pace of verification -9400 lines across 35 files take about 30 minutes -hinders interactive <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:28 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <body>development. Thus, rapid incremental checking, lightweight synthesis, and actionable error messages <lb/>would go a long way towards improving the ergonomics of verification, and hence remain important <lb/>directions for future work. <lb/></body>

        <div type="availability">11 DATA AVAILABILITY STATEMENT <lb/>The source code for our mechanizations in Coq and LiqidHaskell, together with instructions on <lb/>how to replicate the results, are available on Zenodo [Borkowski et al. 2023a]. Additionally, a virtual <lb/>appliance for Oracle VM VirtualBox is available on Zendo [Borkowski et al. 2023b] to assist with <lb/>replication. <lb/></div>

        <div type="acknowledgement">ACKNOWLEDGMENTS <lb/>We thank James Parker for a helpful discussion about data propositions and the anonymous reviewers <lb/>for the useful comments and suggestions. This work was supported by the NSF grants CNS-2120642, <lb/>CNS-2155235, CCF-1918573, CCF-1911213, CCF-1955457, the Horizon Europe ERC Starting Grant <lb/>CRETE (GA: 101039196), the US Office of Naval Research HACKCRYPT (Ref. N00014-19-1-2292), and <lb/>generous gifts from Microsoft Research. <lb/></div>

        <listBibl>REFERENCES <lb/>V. Astrauskas, A. Bílý, J. Fiala, Z. Grannan, C. Matheja, P. Müller, F. Poli, and A. J. Summers. 2022. The Prusti Project: <lb/>Formal Verification for Rust (invited). In NASA Formal Methods (14th International Symposium). Springer, 88-108. <lb/>https://link.springer.com/chapter/10.1007/978-3-031-06773-0_5 <lb/>Brian E. Aydemir, Aaron Bohannon, Matthew Fairbairn, J. Nathan Foster, Benjamin C. Pierce, Peter Sewell, Dimitrios <lb/>Vytiniotis, Geoffrey Washburn, Stephanie Weirich, and Steve Zdancewic. 2005. Mechanized Metatheory for the Masses: <lb/>The PoplMark Challenge. In Theorem Proving in Higher Order Logics, 18th International Conference, TPHOLs 2005, Oxford, <lb/>UK, August 22-25, 2005, Proceedings (Lecture Notes in Computer Science, Vol. 3603), Joe Hurd and Thomas F. Melham (Eds.). <lb/>Springer, 50-65. https://doi.org/10.1007/11541868_4 <lb/>Brian E. Aydemir, Arthur Charguéraud, Benjamin C. Pierce, Randy Pollack, and Stephanie Weirich. 2008. Engineering <lb/>formal metatheory. In Proceedings of the 35th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, <lb/>POPL 2008, San Francisco, California, USA, January 7-12, 2008, George C. Necula and Philip Wadler (Eds.). ACM, 3-15. <lb/>https://doi.org/10.1145/1328438.1328443 <lb/>João Filipe Belo, Michael Greenberg, Atsushi Igarashi, and Benjamin C. Pierce. 2011. Polymorphic Contracts. In Programming <lb/>Languages and Systems -20th European Symposium on Programming, ESOP 2011, Held as Part of the Joint European Con-<lb/>ferences on Theory and Practice of Software, ETAPS 2011, Saarbrücken, Germany, March 26-April 3, 2011. Proceedings (Lecture <lb/>Notes in Computer Science, Vol. 6602), Gilles Barthe (Ed.). Springer, 18-37. https://doi.org/10.1007/978-3-642-19718-5_2 <lb/>Michael H. Borkowski, Niki Vazou, and Ranjit Jhala. 2023a. <lb/>Artifact for &quot;Mechanizing Refinement Types&quot;. <lb/>https://doi.org/10.5281/zenodo.8425960 <lb/>Michael H. Borkowski, Niki Vazou, and Ranjit Jhala. 2023b. Artifact Virtual Machine for &quot;Mechanizing Refinement Types&quot;. <lb/>https://doi.org/10.5281/zenodo.8425176 <lb/>Giuseppe Castagna and Alain Frisch. 2005. A Gentle Introduction to Semantic Subtyping. In Proceedings of the 7th ACM <lb/>SIGPLAN International Conference on Principles and Practice of Declarative Programming (Lisbon, Portugal) (PPDP &apos;05). <lb/>Association for Computing Machinery, New York, NY, USA, 198-208. https://doi.org/10.1145/1069774.1069793 <lb/>Zilin Chen. 2022. A Hoare Logic Style Refinement Types Formalisation. In Proceedings of the 7th ACM SIGPLAN International <lb/>Workshop on Type-Driven Development (Ljubljana, Slovenia) (TyDe 2022). Association for Computing Machinery, New <lb/>York, NY, USA, 1-14. https://doi.org/10.1145/3546196.3550162 <lb/>Thierry Coquand and Christine Paulin. 1990. Inductively Defined Types. In COLOG-88, Per Martin-Löf and Grigori Mints <lb/>(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 50-66. https://doi.org/10.1007/3-540-52335-9_47 <lb/>Benjamin Cosman and Ranjit Jhala. 2017. Local refinement typing. Proc. ACM Program. Lang. 1, ICFP (2017), 26:1-26:27. <lb/>https://doi.org/10.1145/3110270 <lb/>Leonardo de Moura and Nikolaj Bjørner. 2008. Z3: An Efficient SMT Solver. In Tools and Algorithms for the Construction and <lb/>Analysis of Systems, C. R. Ramakrishnan and Jakob Rehof (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 337-340. <lb/>https://doi.org/10.1007/978-3-540-78800-3_24 <lb/>Cormac Flanagan. 2006. Hybrid Type Checking. In Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on <lb/>Principles of Programming Languages (Charleston, South Carolina, USA) (POPL &apos;06). Association for Computing Machinery, <lb/>New York, NY, USA, 245-256. https://doi.org/10.1145/1111037.1111059 <lb/></listBibl>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <note place="headnote">Mechanizing Refinement Types <lb/></note>

        <page>70:29 <lb/></page>

        <listBibl>Cormac Flanagan, Amr Sabry, Bruce F. Duba, and Matthias Felleisen. 1993. The Essence of Compiling with Continu-<lb/>ations. In Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation <lb/>(Albuquerque, New Mexico, USA) (PLDI &apos;93). Association for Computing Machinery, New York, NY, USA, 237-247. <lb/>https://doi.org/10.1145/155090.155113 <lb/>Cédric Fournet, Markulf Kohlweiss, and Pierre-Yves Strub. 2011. Modular Code-Based Cryptographic Verification. In <lb/>Proceedings of the 18th ACM Conference on Computer and Communications Security (Chicago, Illinois, USA) (CCS &apos;11). <lb/>Association for Computing Machinery, New York, NY, USA, 341-350. https://doi.org/10.1145/2046707.2046746 <lb/>Alain Frisch, Giuseppe Castagna, and Véronique Benzaken. 2002. Semantic Subtyping. In 17th IEEE Symposium on Logic <lb/>in Computer Science (LICS 2002), 22-25 July 2002, Copenhagen, Denmark, Proceedings. IEEE Computer Society, 137-146. <lb/>https://doi.org/10.1109/LICS.2002.1029823 <lb/>Jad Elkhaleq Ghalayini and Neel Krishnaswami. 2023. Explicit Refinement Types. Proc. ACM Program. Lang. 7, ICFP, Article <lb/>195 (aug 2023), 28 pages. https://doi.org/10.1145/3607837 <lb/>Andrew D. Gordon and C. Fournet. 2010. Principles and Applications of Refinement Types. In Logics and Languages for <lb/>Reliability and Security. IOS Press. https://doi.org/10.3233/978-1-60750-100-8-73 <lb/>Michael Greenberg. 2013. <lb/>Manifest Contracts. <lb/>Ph.D. Dissertation. University of Pennsylvania. <lb/>https: <lb/>//repository.upenn.edu/edissertations/468/ <lb/>Jad Hamza, Nicolas Voirol, and Viktor Kuncak. 2019. System FR: formalized foundations for the stainless verifier. Proc. ACM <lb/>Program. Lang. 3, OOPSLA (2019), 166:1-166:30. https://doi.org/10.1145/3360592 <lb/>Ranjit Jhala and Niki Vazou. 2021. Refinement Types: A Tutorial. Found. Trends Program. Lang. 6, 3-4 (2021), 159-317. <lb/>https://doi.org/10.1561/2500000032 <lb/>Andrew M. Kent, David Kempe, and Sam Tobin-Hochstadt. 2016. Occurrence Typing modulo Theories. In Proceedings of <lb/>the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation (Santa Barbara, CA, USA) (PLDI <lb/>&apos;16). Association for Computing Machinery, New York, NY, USA, 296-309. https://doi.org/10.1145/2908080.2908091 <lb/>Tristan Knoth, Di Wang, Adam Reynolds, Jan Hoffmann, and Nadia Polikarpova. 2020. Liquid resource types. Proc. ACM <lb/>Program. Lang. 4, ICFP (2020), 106:1-106:29. https://doi.org/10.1145/3408988 <lb/>Kenneth Knowles and Cormac Flanagan. 2009. Compositional Reasoning and Decidable Checking for Dependent Contract <lb/>Types. In Proceedings of the 3rd Workshop on Programming Languages Meets Program Verification (Savannah, GA, USA) <lb/>(PLPV &apos;09). Association for Computing Machinery, New York, NY, USA, 27-38. https://doi.org/10.1145/1481848.1481853 <lb/>Nico Lehmann, Adam T. Geller, Niki Vazou, and Ranjit Jhala. 2023. Flux: Liquid Types for Rust. Proc. ACM Program. Lang. <lb/>7, PLDI, Article 169 (jun 2023), 25 pages. https://doi.org/10.1145/3591283 <lb/>Nico Lehmann, Rose Kunkel, Jordan Brown, Jean Yang, Niki Vazou, Nadia Polikarpova, Deian Stefan, and Ranjit Jhala. 2021. <lb/>STORM: Refinement Types for Secure Web Applications. In 15th USENIX Symposium on Operating Systems Design and Im-<lb/>plementation (OSDI 21). USENIX Association, 441-459. https://www.usenix.org/conference/osdi21/presentation/lehmann <lb/>Nico Lehmann and Éric Tanter. 2016. Formalizing Simple Refinement Types in Coq. In 2nd International Workshop on Coq <lb/>for Programming Languages (CoqPL&apos;16). St. Petersburg, FL, USA. <lb/>K. Rustan M. Leino. 2010. Dafny: An Automatic Program Verifier for Functional Correctness. In Logic for Programming, <lb/>Artificial Intelligence, and Reasoning (LPAR). https://doi.org/10.1007/978-3-642-17511-4_20 <lb/>Guido Martínez, Danel Ahman, Victor Dumitrescu, Nick Giannarakis, Chris Hawblitzel, Catalin Hritcu, Monal <lb/>Narasimhamurthy, Zoe Paraskevopoulou, Clément Pit-Claudel, Jonathan Protzenko, Tahina Ramananandro, Aseem <lb/>Rastogi, and Nikhil Swamy. 2019. Meta-F*: Proof Automation with SMT, Tactics, and Metaprograms. In European <lb/>Symposium on Programming (ESOP). https://doi.org/10.1007/978-3-030-17184-1_2 <lb/>George C. Necula. 1997. Proof-Carrying Code. In Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles <lb/>of Programming Languages (Paris, France) (POPL &apos;97). Association for Computing Machinery, New York, NY, USA, 106-119. <lb/>https://doi.org/10.1145/263699.263712 <lb/>Tobias Nipkow, Markus Wenzel, and Lawrence C. Paulson. 2002. Isabelle/HOL -A Proof Assistant for Higher-Order Logic. <lb/>https://link.springer.com/book/10.1007/3-540-45949-9 <lb/>Ulf Norell. 2007. a practical programming language based on dependent type theory. Ph.D. Dissertation. Chalmers. <lb/>Xinming Ou, Gang Tan, Yitzhak Mandelbaum, and David Walker. 2004. Dynamic Typing with Dependent Types. In Exploring <lb/>New Frontiers of Theoretical Informatics, Jean-Jacques Levy, Ernst W. Mayr, and John C. Mitchell (Eds.). Springer US, <lb/>Boston, MA, 437-450. https://doi.org/10.1007/1-4020-8141-3_34 <lb/>James Parker, Niki Vazou, and Michael Hicks. 2019. LWeb: information flow security for multi-tier web applications. Proc. <lb/>ACM Program. Lang. 3, POPL (2019), 75:1-75:30. https://doi.org/10.1145/3290388 <lb/>Brigitte Pientka. 2010. Beluga: Programming with Dependent Types, Contextual Data, and Contexts. In Functional and Logic <lb/>Programming, Matthias Blume, Naoki Kobayashi, and Germán Vidal (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, <lb/>1-12. https://doi.org/10.1007/978-3-642-12251-4_1 <lb/>Benjamin C. Pierce. 2002. Types and Programming Languages. MIT Press. https://www.cis.upenn.edu/~bcpierce/tapl/ <lb/></listBibl>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. <lb/></note>

        <page>70:30 <lb/></page>

        <note place="headnote">Michael H. Borkowski, Niki Vazou, and Ranjit Jhala <lb/></note>

        <listBibl>Benjamin C. Pierce, Arthur Azevedo de Amorim, Chris Casinghino, Marco Gaboardi, Michael Greenberg, Cătălin Hriţcu, <lb/>Vilhelm Sjöberg, Andrew Tolmach, and Brent Yorgey. 2022. Programming Language Foundations. Software Foundations, <lb/>Vol. 2. Electronic textbook. https://softwarefoundations.cis.upenn.edu/ <lb/>Randy Pollack. 1993. Closure Under Alpha-Conversion. In Types for Proofs and Programs, International Workshop TYPES&apos;93, <lb/>Nijmegen, The Netherlands, May 24-28, 1993, Selected Papers (Lecture Notes in Computer Science, Vol. 806), Henk Barendregt <lb/>and Tobias Nipkow (Eds.). Springer, 313-332. https://doi.org/10.1007/3-540-58085-9_82 <lb/>Patrick Redmond, Gan Shen, and Lindsey Kuper. 2021. Toward Hole-Driven Development with Liquid Haskell. CoRR <lb/>abs/2110.04461 (2021). arXiv:2110.04461 https://arxiv.org/abs/2110.04461 <lb/>Patrick M. Rondon, Ming Kawaguci, and Ranjit Jhala. 2008. Liquid Types. In Proceedings of the 29th ACM SIGPLAN Conference <lb/>on Programming Language Design and Implementation (Tucson, AZ, USA) (PLDI &apos;08). Association for Computing Machinery, <lb/>New York, NY, USA, 159-169. https://doi.org/10.1145/1375581.1375602 <lb/>Didier Rémy. 2021. Type systems for programming languages. Course notes. https://www.doc.ic.ac.uk/~svb/TSfPL/ <lb/>Taro Sekiyama, Atsushi Igarashi, and Michael Greenberg. 2017. Polymorphic Manifest Contracts, Revised and Resolved. <lb/>ACM Trans. Program. Lang. Syst. 39, 1 (2017), 3:1-3:36. https://doi.org/10.1145/2994594 <lb/>Matthieu Sozeau, Simon Boulier, Yannick Forster, Nicolas Tabareau, and Théo Winterhalter. 2020. Coq Coq Cor-<lb/>rect. Verification of Type Checking and Erasure for Coq, in Coq. In Principles of Programming Languages (POPL). <lb/>https://doi.org/10.1145/3371076 <lb/>Matthieu Sozeau and Cyprien Mangin. 2019. Equations Reloaded: High-Level Dependently-Typed Functional Programming <lb/>and Proving in Coq. Proc. ACM Program. Lang. 3, ICFP, Article 86 (jul 2019), 29 pages. https://doi.org/10.1145/3341690 <lb/>Martin Sulzmann, Manuel M. T. Chakravarty, Simon Peyton Jones, and Kevin Donnelly. 2007. System F with Type <lb/>Equality Coercions. In Proceedings of the 2007 ACM SIGPLAN International Workshop on Types in Languages Design <lb/>and Implementation (Nice, Nice, France) (TLDI &apos;07). Association for Computing Machinery, New York, NY, USA, 53-66. <lb/>https://doi.org/10.1145/1190315.1190324 <lb/>Nikhil Swamy, Cătălin Hriţcu, Chantal Keller, Aseem Rastogi, Antoine Delignat-Lavaud, Simon Forest, Karthikeyan <lb/>Bhargavan, Cédric Fournet, Pierre-Yves Strub, Markulf Kohlweiss, Jean-Karim Zinzindohoue, and Santiago Zanella-<lb/>Béguelin. 2016. Dependent Types and Multi-Monadic Effects in F*. In Principles of Programming Languages (POPL). <lb/>https://doi.org/10.1145/2837614.2837655 <lb/>Sam Tobin-Hochstadt and Matthias Felleisen. 2008. The Design and Implementation of Typed Scheme. In Proceedings of the <lb/>35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (San Francisco, California, USA) <lb/>(POPL &apos;08). Association for Computing Machinery, New York, NY, USA, 395-406. https://doi.org/10.1145/1328438.1328486 <lb/>Niki Vazou, Leonidas Lampropoulos, and Jeff Polakow. 2017. A Tale of Two Provers: Verifying Monoidal String Matching in <lb/>Liquid Haskell and Coq. In Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell (Oxford, UK) (Haskell <lb/>2017). Association for Computing Machinery, New York, NY, USA, 63-74. https://doi.org/10.1145/3122955.3122963 <lb/>Niki Vazou, Eric L. Seidel, and Ranjit Jhala. 2014a. LiquidHaskell: Experience with Refinement Types in the Real World. <lb/>In Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell (Gothenburg, Sweden) (Haskell &apos;14). Association for <lb/>Computing Machinery, New York, NY, USA, 39-51. https://doi.org/10.1145/2633357.2633366 <lb/>Niki Vazou, Eric L. Seidel, Ranjit Jhala, Dimitrios Vytiniotis, and Simon Peyton-Jones. 2014b. Refinement Types for Haskell. <lb/>In Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming (Gothenburg, Sweden) <lb/>(ICFP &apos;14). Association for Computing Machinery, New York, NY, USA, 269-282. https://doi.org/10.1145/2628136.2628161 <lb/>Niki Vazou, Anish Tondwalkar, Vikraman Choudhury, Ryan G. Scott, Ryan R. Newton, Philip Wadler, and Ranjit Jhala. <lb/>2018. Refinement reflection: complete verification with SMT. Proc. ACM Program. Lang. 2, POPL (2018), 53:1-53:31. <lb/>https://doi.org/10.1145/3158141 <lb/>Philip Wadler. 1989. Theorems for Free!. In Proceedings of the Fourth International Conference on Functional Programming <lb/>Languages and Computer Architecture (Imperial College, London, United Kingdom) (FPCA &apos;89). Association for Computing <lb/>Machinery, New York, NY, USA, 347-359. https://doi.org/10.1145/99370.99404 <lb/></listBibl>

        <front>Received 2023-07-11; accepted 2023-11-07 <lb/></front>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 70. Publication date: January 2024. </note>


	</text>

</TEI>