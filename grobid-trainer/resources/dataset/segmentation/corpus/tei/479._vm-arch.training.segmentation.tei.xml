<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Virtual Memory Architecture in SunOS <lb/>Robert A. Gingell <lb/>Joseph P. Moran <lb/>William A. Shannon <lb/>Sun Microsystems, Inc. <lb/>2550 Garcia Ave. <lb/>Mountain View, CA 94043 <lb/>ABSTRACT <lb/>A new virtual memory architecture for the Sun implementation of the UNIX † <lb/>operating system is described. Our goals included unifying and simplifying the concepts <lb/>the system used to manage memory, as well as providing an implementation that fit well <lb/>with the rest of the system. We discuss an architecture suitable for environments that <lb/>(potentially) consist of systems of heterogeneous hardware and software architectures. <lb/>The result is a page-based system in which the fundamental notion is that of mapping <lb/>process addresses to files. <lb/></front>

			<body>1. Introduction and Motivation <lb/>The UNIX operating system has traditionally provided little support for memory sharing between <lb/>processes, and no support for facilities such as file mapping. For some communities, the lack of such facil-<lb/>ities has been a barrier to the adoption of UNIX, or has hampered the development of applications that <lb/>might have benefited from their availability. Our own desire to provide a shared libraries capability has <lb/>provided additional incentive for us to explore providing new memory management facilities in the system. <lb/>We have also found ourselves faced with having to support a variety of interfaces. These included <lb/>the partially implemented interfaces we have had in our 4.2BSD-derived kernel [JOY 83] and those specified <lb/>by AT&amp;T for System V [AT&amp;T 86]. Aggravating these situations were the variations on those interfaces <lb/>being developed by a number of vendors that were incompatible with or extended the original proposals. <lb/>Also, entirely new interfaces have been proposed and implemented, most notably in Carnegie-Mellon&apos;s <lb/>MACH [ACCE 86]. There has been no market movement to suggest which, if any, of these would become <lb/>dominant, and in some cases a specific interface lacked an important capability (such as System V&apos;s lack of <lb/>file mapping). <lb/>Finally, our existing implementation is too constraining a base from which to provide the new func-<lb/>tionality we wanted. It is targeted to traditional models of UNIX memory management and specifically <lb/>towards the hardware model of the VAX. ‡ The work required to enhance the current implementation <lb/>appeared to be adding its own new wart to an increasingly baroque implementation, and we were con-<lb/>cerned for its long-term maintainability. <lb/>Thus, we decided to create a new Virtual Memory (VM) system for Sun&apos;s implementation of UNIX, <lb/>SunOS. This paper describes the architecture of this new system: the goals we had for its design and the <lb/>constraints under which we operated, the concepts it embodies, the interfaces it offers the UNIX application <lb/>programmer and its relationship to the rest of the system. Although our primary intent is to discuss the <lb/>architectural issues, information relating to the project and its implementation is provided to add context to <lb/>the presentation. <lb/></body>

			<note place="footnote"> † UNIX is a trademark of Bell Laboratories. <lb/> ‡ VAX is a trademark of Digital Equipment Corporation <lb/></note>

			<page>1 <lb/></page>

			<body>2. Goals/Non-Goals <lb/>Beyond the previously mentioned functional issues of memory sharing and file mapping, our goals <lb/>for the new architecture were: <lb/>Unify memory handling. Our primary architectural goal was to find the general concepts <lb/>underlying all of the functions we wanted to provide or could envision, and then to provide <lb/>them as the basis for all VM operations. If successful, we should be able to reimplement exist-<lb/>ing kernel functions (such as fork and exec ) in terms of these new mechanisms. We also <lb/>hoped to replace many of the existing memory management schemes in the kernel with facili-<lb/>ties provided by the new VM system. <lb/>Non-kernel implementation of many functions. If we were successful in identifying and <lb/>providing the right mechanisms as kernel operations, then it seemed likely that many functions <lb/>that otherwise would have had to be provided in the kernel could in fact be implemented as <lb/>library routines. In particular, we wanted to be able to provide capabilities such as shared <lb/>libraries and the System V interfaces as applications of these basic mechanisms. <lb/>Improved portability. The existing system was targeted towards a specific machine architec-<lb/>ture. In many cases, attributes of this architecture had crept cancerously through the code that <lb/>implements software-defined functionality. We therefore wanted to describe software-defined <lb/>objects using data structures appropriate to the software, and relegate machine-dependent code <lb/>to a lower system layer accessed through a well-defined and narrow interface. <lb/>Consistent with environment. We wanted our system to fit well with the UNIX concepts we <lb/>were not changing. It would not be acceptable to build the world&apos;s most wonderful memory <lb/>management system if it was completely incompatible with the rest of the system and its <lb/>environment. Particularly important to us in this respect was the use of the file system as the <lb/>name space for the objects supported by the system. Moreover, we sell systems that are <lb/>intended to operate in highly networked environments, and thus we could not create a system <lb/>that presented barriers to the networked environment. <lb/>In addition to these architectural goals, there were other goals we had for the project as a whole. These <lb/>project goals were: <lb/>Maintain performance. Although it is always desirable to tag a project with the label <lb/>&apos;&apos;improves performance&apos;&apos;, we chose the apparently more conservative goal of simply provid-<lb/>ing more functionality for the same cost in terms of overall system performance. While the <lb/>new functionality might enable increased application performance, the performance of the sys-<lb/>tem itself seemed uncertain. Further, when one considers that we replaced a mature imple-<lb/>mentation with one which has not been subjected to several years of tuning, getting back to <lb/>current performance levels appeared to be an ambitious goal, something later experience has <lb/>proven correct. <lb/>Engineer for the future. We wanted to build an implementation that would be amenable to <lb/>anticipated future requirements, such as kernel support for &apos;&apos;lightweight&apos;&apos; processes [KEPE 85] <lb/>and multiprocessors. <lb/>When engaging in a large project, it is often as important to know what one&apos;s goals are not. In the <lb/>architectural arena, our principal &apos;&apos;non-goals&apos;&apos; were: <lb/>New external interfaces. As previously noted, a large number of groups were already work-<lb/>ing on the refinement and definition of interfaces. To the extent possible, we wanted to use <lb/>such interfaces as had already been defined by others, and to provide those that were <lb/>sufficiently defined to be implementable and that the market was demanding. <lb/>Compatible internal interfaces. An unfortunate characteristic of UNIX is the existence of <lb/>programs that have some understanding of the system&apos;s internals and use this information to <lb/>rummage through the kernel by reading the memory device. The changes to the system we <lb/>contemplated clearly made it impossible for us to try to support these programs, and thus we <lb/>decided not to fool ourselves into trying. <lb/></body>

			<page>2 <lb/></page>

			<body>Relevant project non-goals included: <lb/>Pageable kernel. We did not intend to produce an implementation in which the kernel itself <lb/>was paged − beyond a general desire in principle for the kernel to use less physical memory, <lb/>we would have satisfied no specific functional goal by having the kernel pageable. However, <lb/>it has turned out that a considerable portion of the memory that was previously &apos;&apos;wired down&apos;&apos; <lb/>for kernel use is in fact now paged, although kernel code remains physically locked. <lb/>Massive policy changes. Our interests lay in changing the mechanisms and what they pro-<lb/>vide, not in the policies by which they were administered. Although we would eventually like <lb/>to support an integrated view of process and memory scheduling using techniques such as <lb/>working set page replacement policies and balance set scheduling, we decided to defer these to <lb/>future efforts. <lb/>3. Constraints <lb/>Working within the framework of an existing system imposed a number of constraints on what we <lb/>could do. The constraints were not always limits on our flexibility; in fact, those reflecting specific custo-<lb/>mer requirements provided data that guided us through a number of design decisions. A major constraint <lb/>was that of compatibility with previous versions of the system − ultimately, compatibility drove many deci-<lb/>sions. <lb/>One such decision was that the new system would execute existing a.out files. This was necessary to <lb/>preserve the utility of the programs already in use by customers and third parties. An important implication <lb/>is that the system must provide a binary-compatible interface for existing programs, which means that <lb/>existing system calls that perform memory management functions must continue to work. In our case, this <lb/>meant supporting our partial implementation of the 4.2BSD mmap(2) system call, which we used to map <lb/>devices, such as frame buffers, into a process&apos;s address space. <lb/>Although the system had to be binary-compatible, we did not feel constrained to leave it source-<lb/>compatible, nor to use mmap as the principal interface to the memory management facilities of the system. <lb/>Users with programs that used interfaces we changed in this manner would have to change their programs <lb/>the next time they compiled them, but they would not be forced to recompile just to install and continue <lb/>operating on the new system. <lb/>A wide variety of customer requirements implied that the interfaces we would offer would have to <lb/>present very few constraints on a process&apos;s use of its address space. Some applications wanted to manage <lb/>their address space completely, including the ability to assign process addresses for objects and to use a <lb/>large, sparsely populated address space. Our own desire to build a base on which many different interfaces <lb/>could be easily constructed suggested that we wanted as much flexibility as possible in user level address <lb/>space management. However, other factors and requirements suggested that the system should also be able <lb/>to control many details of an address space. One such factor was the introduction of a virtual address <lb/>cache in the Sun-3/200 family of processors, where system control of address assignment would have a <lb/>beneficial impact on performance. We also wanted to use copy-on-write techniques to enhance the level of <lb/>sharing in the system, and to do this efficiently required page-level protection. <lb/>4. New Architecture: General Concepts <lb/>This section describes in general terms the abstractions and properties of the new VM system, and <lb/>some reflections on the decisions that led to their creation. In many cases, our decisions were not based on <lb/>obvious considerations, but rather &apos;&apos;fell out&apos;&apos; of a large number of small issues. Although this makes the <lb/>decisions more difficult to explain, the process by which they were reached increased our confidence that, <lb/>given our goals and constraints, we had in fact reached the best conclusion. <lb/>4.1. Pages vs. Segments <lb/>Our earliest decision was that the basic kernel facilities would operate on pages, rather than seg-<lb/>ments. The major factors in this decision included: <lb/>compatibility with current systems (the 4.2BSD mmap is page-based); <lb/></body>

			<page>3 <lb/></page>

			<body>implementing efficient copy-on-write facilities required maintenance of per-page information <lb/>anyway; <lb/>pages appeared to offer the greatest opportunity to satisfy customer requirements for flexibil-<lb/>ity; and <lb/>segments could be built as an abstraction on top of the page-based interface by library routines. <lb/>The major advantage to a segment-based mechanism appeared simply to be that it was a &apos;&apos;better&apos;&apos; pro-<lb/>gramming abstraction. Since we could still build abstraction from the page-based mechanisms, and in fact <lb/>gained some flexibility in building different forms of the abstraction as libraries, providing segments <lb/>through the kernel appeared to offer little benefit and possibly even presented barriers to accomplishing <lb/>some of our goals. <lb/>Although we believed we could gain the architectural advantages of segments through library rou-<lb/>tines built on our page-based system, another potential advantage to a segment-based system was the <lb/>opportunity to implement a compact representation for a sparsely populated address space. However, since <lb/>we needed per-page information to implement per-page copy-on-write and perform other physical storage <lb/>management, at the very least we would end up with a mix of page-and segment-oriented data structures. <lb/>We recognized that we could keep the major implementation advantage of a segment-based system, i.e., <lb/>the concise description of the mapping for a range of addresses, by viewing it as an optimization (a sort of <lb/>run-length encoding) of the per-page data structure (a similar scheme is used in MACH.) <lb/>4.2. Virtual Memory, Address Spaces, and Mapping <lb/>The system&apos;s virtual memory consists of all its available physical memory resources. Examples <lb/>include file systems (both local and remote), pools of unnamed memory (also known as private or <lb/>anonymous storage, and implemented by the processor&apos;s primary memory and swap space), and other ran-<lb/>dom access memory devices. Named objects in the virtual memory are referenced through the UNIX file <lb/>system. This does not imply that all file system objects are in the virtual memory, but simply that all <lb/>named objects in the virtual memory are named in the file system. One of the strengths of UNIX has been <lb/>the use of a single name-space for system objects, and we wished to build upon that strength. Some objects <lb/>in the virtual memory, such as process private memory and our implementation of System V shared <lb/>memory segments, do not have names. Although the most common form of object is the UNIX &apos;&apos;regular <lb/>file&apos;&apos;, previous work on SunOS has allowed for many different implementations of objects, which the sys-<lb/>tem manipulates as an abstraction of the original UNIX inode, called a vnode [KLEI 86]. <lb/>A process&apos;s address space is defined by mappings onto the address spaces of one or more objects in <lb/>the system&apos;s virtual memory. As previously discussed, the system provides a page-based interface, and <lb/>thus each mapping is constrained to be sized and aligned with the page boundaries defined by the system <lb/>on which the process is executing. Each page may be mapped (or not) independently, and thus the pro-<lb/>grammer may treat an address space as a simple vector of pages. It should be noted that the only valid pro-<lb/>cess address is one which is mapped to some object, and in particular there is no memory associated with <lb/>the process itself − all memory is represented by virtual memory objects. <lb/>Each object in the virtual memory has an object address space defined by some physical storage, the <lb/>specific form being object-specific. A reference to an object address accesses the physical storage that <lb/>implements the address within the object. The virtual memory&apos;s associated physical storage is thus <lb/>accessed by transforming process addresses to object addresses, and then to the physical store. The <lb/>system&apos;s VM management facilities may interpose one or more layers of logical caching on top of the <lb/>actual physical storage used to implement an object, a fact that has implications for coherency, discussed <lb/>below. <lb/>A given process page may map to only one object, although a given object address may be the sub-<lb/>ject of many process mappings. The amount of the object&apos;s address space covered by a mapping is an <lb/>integral multiple of the page size as seen by the process performing the mapping. An important charac-<lb/>teristic of a mapping is that the object to which the mapping is made is not required to be affected by the <lb/>mere existence of the mapping. The implications of this are that it cannot, in general, be expected than an <lb/>object has an &apos;&apos;awareness&apos;&apos; of having been mapped, or of which portions of its address space are accessed <lb/>by mappings; in particular, the notion of a &apos;&apos;page&apos;&apos; is not a property of the object. Establishing a mapping <lb/></body>

			<page>4 <lb/></page>

			<body>to an object simply provides the potential for a process to access or change the object&apos;s contents. <lb/>The establishment of mappings provides an access method that renders an object directly addressable <lb/>by a process. Applications may find it advantageous to access the storage resources they use directly rather <lb/>than indirectly through read and write. Potential advantages include efficiency (elimination of unnecessary <lb/>data copying) and reduced complexity (e.g., updates changed to a single step rather than a read, modify <lb/>buffer, write cycle). The ability to access an object and have it retain its identity over the course of the <lb/>access is unique to this access method, and facilitates the sharing of common code and data. <lb/>It is important to note that this access method view of the VM system does not directly provide shar-<lb/>ing. Thus, although our motivations included providing shared memory, we have actually only provided <lb/>the mechanisms for applications to build such sharing. For the system to provide not only an access method <lb/>but also the semantics for such access is not only difficult or impossible, it is not clear that it is the correct <lb/>thing to do in a highly heterogeneous environment. However, useful forms of sharing can be built in such <lb/>environments, as the previous mechanisms for sharing in the kernel (such as the shared program text and <lb/>file data buffer cache) have been subsumed by kernel programming building on top of these mechanisms. <lb/>4.3. Networking, Heterogeneity, and Coherence <lb/>Many of the factors that drove our adoption of the access method view of a VM system originated <lb/>from our goal of providing facilities that &apos;&apos;fit&apos;&apos; with their expected environment. A major characteristic of <lb/>our environment is the extensive use of networking to access file systems that would be part of the system&apos;s <lb/>virtual memory. These networks are not constrained to consist of similar hardware or a common operating <lb/>system; in fact, the opposite is encouraged. Making extensive assumptions about the properties of objects <lb/>or their access creates potentially extensive barriers to accommodating heterogeneity. These properties <lb/>include such system variables as page sizes and the ability of an object to synchronize its uses. While a <lb/>given set of processes may apply a set of mechanisms to establish and maintain various properties of <lb/>objects, a given operating system should not impose them on the rest of the network. <lb/>As it stands, the access method view of a virtual memory maintains the potential for a given object <lb/>(say a text file) to be mapped by systems running our memory management system but also accessed by <lb/>systems for which the notion of a virtual memory or storage management techniques such as paging would <lb/>be totally foreign, such as PC-DOS. Such systems could continue to share access to the object, each using <lb/>and providing its programs with the access method appropriate to that system. The alternative would be to <lb/>prohibit access to the object by less capable systems, an alternative we find unacceptable. <lb/>A new consideration arises when applications use an object as a communications channel, or other-<lb/>wise attempt to access it simultaneously. In addition to providing the mapping functions described previ-<lb/>ously, the VM management facilities also manage a storage hierarchy in which the processor&apos;s primary <lb/>memory is often used as a cache for data from the virtual memory. Since the system cannot assume either <lb/>that the object will coordinate accesses to it, nor that other systems will in fact cooperate with such coordi-<lb/>nation, it does not attempt on its own to synchronize the &apos;&apos;virtual memory cache&apos;&apos; it maintains. This is not <lb/>to say that such objects can not exist, nor that systems will not cooperate; simply that in general the system <lb/>can not make such an assumption. Even within a single system, the sharing that results is a consequence of <lb/>the system&apos;s attempt to use its cache resources efficiently, not part of its defined functionality. <lb/>However, the lack of cache synchronization is not the limitation it might first appear. Applications <lb/>that intend to share an object must employ a synchronization mechanism around their access and this <lb/>requirement is independent of the access method they use. The scope and nature of the mechanism <lb/>employed is best left to the application to decide. While today applications sharing a file object must access <lb/>and update it indirectly using read and write, they must coordinate their access using semaphores or file <lb/>locking or some application-specific protocol. In such environments, either caching is totally disabled <lb/>(resulting in performance limitations) or the applications must employ a function such as fsync to ensure <lb/>that the object is updated. Coherency of shared objects is not a new issue, and the introduction of a new <lb/>access method simply exposes a new manifestation of an old problem. All that is required in an environ-<lb/>ment where mapping replaces read and write as the access method is that an operation comparable to fsync <lb/>be provided. <lb/></body>

			<page>5 <lb/></page>

			<body>Thus, the nature and scope of synchronization over shared objects is something that is application-<lb/>defined from the outset. If the system attempted to impose any automatic semantics for sharing, it might <lb/>prohibit other useful forms of mapped access that have nothing whatsoever to do with communication or <lb/>sharing. By providing the mechanism to support coherency, and leaving it to cooperating applications to <lb/>apply the mechanism, our design meets the needs of applications without providing barriers to hetero-<lb/>geneity. Note that this design does not prohibit the creation of libraries that provide coherent abstractions <lb/>for common application needs. Not all abstractions on which an application builds need be supplied by the <lb/>&apos;&apos;operating system&apos;&apos;. <lb/>4.4. Historical Acknowledgements <lb/>Many of the concepts we have described are not new. MULTICS [ORGA 72] supported the notion of <lb/>file/process memory integration that is fundamental to our system. TENEX [BOBR 72] [MURP 72] supported <lb/>a page-based environment together with the notion of a process page map independent of the object being <lb/>mapped. <lb/>5. External Interfaces: System Calls <lb/>The applications programmer gains access to the facilities of the new VM system through several <lb/>sets of system calls. At present, we have defined our principal interface to be a refinement of those pro-<lb/>vided with 4.2BSD. We also provide interfaces for System V&apos;s shared memory operations. The new sys-<lb/>tem also impacted other system calls and facilities. These are described further below. Although these <lb/>represent the initial interfaces we intend to support, others may be provided in the future in response to <lb/>market demand. <lb/>5.1. 4.2BSD-based Interfaces <lb/>The 4.2BSD UNIX specification [JOY 83] included the definition of a number of system calls for map-<lb/>ping files, although the system did not implement them. Earlier releases of SunOS included partial imple-<lb/>mentations of these calls to support mapping devices such as frame buffers into a process&apos;s address space. <lb/>The basic concepts embedded in the interface were very close to our own, namely a page-based system <lb/>providing mappings from process addresses to objects identified with file descriptors, and thus working <lb/>from this base was a natural thing to do. <lb/>However, we had problems with the 4.2BSD interfaces due to their sketchy definition. Although the <lb/>intent was well understood, the lack of an implementation left many semantic issues unresolved or ambigu-<lb/>ous. We required some facilities that were not part of the specification, and other facilities were part of the <lb/>specification but seemed superfluous. Thus, although we did manage to avoid creating an entirely new <lb/>interface, we did find ourselves refining an existing, but unimplemented one. The process of refinement <lb/>involved many people; in fact most were external to Sun and involved exchanges utilizing a &apos;&apos;VM <lb/>interest&apos;&apos; mailing list supported and maintained by the developers at UC Berkeley, CSRG. Table 1 sum-<lb/>marizes our refined interface, and the following sections expand on various areas of refinements. <lb/>5.1.1. mmap <lb/>The mmap(2) system call is used to establish mappings from a process&apos;s address space to an object. <lb/>Its definition is: <lb/>caddr_t mmap(addr, len, prot, flags, fd, off) <lb/>mmap establishes a mapping between the process&apos;s address space at an address paddr for len bytes to the <lb/>object specified by fd at offset off for len bytes. The value of paddr is an implementation-dependent func-<lb/>tion of the parameter addr and values of flags, further described below. A successful mmap call returns <lb/>paddr as its result. The address ranges covered by [paddr, paddr + len) and [off, off + len) must be legiti-<lb/>mate for the address space of a process and the object in question, respectively. The mapping established <lb/>by mmap replaces any previous mappings for the process&apos;s pages in the range [paddr, paddr + len). <lb/>The parameter prot determines whether read, execute, write or some combination of accesses are <lb/>permitted to the pages being mapped. The values desired are expressed by or&apos;ing the flags values <lb/>PROT_READ, PROT_EXECUTE, and PROT_WRITE. It is not expected that all implementations <lb/></body>

			<page>6 <lb/></page>

			<body>Table 1 − Refined 4.2BSD Interfaces <lb/>Call <lb/>Function <lb/>madvise(addr, len, behav) <lb/>caddr_t addr; int len, behav; <lb/>Gives advice about the handling of <lb/>memory over a range of addresses. <lb/>mincore(addr, len, vec) <lb/>caddr_t addr; int len; result char *vec; <lb/>Determines residency of memory <lb/>pages. (Will be replaced by more <lb/>general map reading function.) <lb/>caddr_t <lb/>mmap(addr, len, prot, flags, fd, off) <lb/>caddr_t addr; int len, prot, flags, fd; <lb/>off_t off; <lb/>Establish mapping from address <lb/>space to object named by fd. <lb/>mprotect(addr, len, prot) <lb/>caddr_t addr; int len, prot; <lb/>Change protection on mapped <lb/>pages. <lb/>msync(addr, len, flags) <lb/>caddr_t addr; int len, flags; <lb/>Synchronizes and/or invalidates <lb/>cache of mapped data. <lb/>munmap(addr, len) <lb/>caddr_t addr; int len; <lb/>Removes mapping of address <lb/>range. <lb/>literally provide all possible combinations. <lb/>PROT_WRITE is often implemented as <lb/>PROT_READ|PROT_WRITE, and PROT_EXECUTE as PROT_READ|PROT_EXECUTE. However, no <lb/>implementation will permit a write to succeed where PROT_WRITE has not been set. The behavior of <lb/>PROT_WRITE can be influenced by setting MAP_PRIVATE in the flags parameter. <lb/>The parameter flags provides other information about the handling of the pages being mapped. The <lb/>options are defined by a field describing an enumeration of the &apos;&apos;type&apos;&apos; of the mapping, and a bit-field <lb/>specifying other options. The enumeration currently defines two values, MAP_SHARED and <lb/>MAP_PRIVATE. The bit-field values are MAP_FIXED and MAP_RENAME. The &apos;&apos;type&apos;&apos; value chosen <lb/>determines whether stores to the mapped addresses are actually propagated to the object being mapped <lb/>(MAP_SHARED) or directed to a copy of the object (MAP_PRIVATE). If the latter is specified, the ini-<lb/>tial write reference to a page will create a private copy of the page of the object and redirect the mapping to <lb/>the copy. The mapping type is retained across a fork(2). The mapping &apos;&apos;type&apos;&apos; only affects the disposition <lb/>of stores by this process − there is no insulation from changes made by other processes. If an application <lb/>desires such insulation, it should use the read system call to make a copy of the data it wishes to keep pro-<lb/>tected. <lb/>MAP_FIXED informs the system that the value of paddr must be addr, exactly. The use of <lb/>MAP_FIXED is discouraged, as it may prevent an implementation from making the most effective use of <lb/>system resources. <lb/>When MAP_FIXED is not set, the system uses addr as a hint in an implementation-defined manner <lb/>to arrive at paddr. The paddr so chosen will be an area of the address space that the system deems suitable <lb/>for a mapping of len bytes to the specified object. All implementations interpret an addr value of zero as <lb/>granting the system complete freedom in selecting paddr, subject to constraints described below. A non-<lb/>zero value of addr is taken to be a suggestion of a process address near which the mapping should be <lb/>placed. When the system selects a value for paddr, it will never place a mapping at address 0, nor will it <lb/>replace any extant mapping, nor map into areas considered part of the potential data or stack &apos;&apos;segments&apos;&apos;. <lb/>In the current SunOS implementation, the system strives to choose alignments for mappings that maximize <lb/>the performance of systems with a virtual address cache. <lb/>MAP_RENAME causes the pages currently mapped in the range [paddr, paddr + len) to be effec-<lb/>tively renamed to be the object addresses in the range [off, off + len). The currently mapped pages must be <lb/>mapped as MAP_PRIVATE. MAP_RENAME implies a MAP_FIXED interpretation of addr. fd must be <lb/>open for write. MAP_RENAME affects the size of the memory object referenced by fd: the size is max(off <lb/>+ len -1, flen) (where flen was the previous length of the object). After the pages are renamed, a mapping <lb/></body>

			<page>7 <lb/></page>

			<body>to them is reestablished with the parameters as specified in the renaming mmap. <lb/>The addition of MAP_FIXED and corresponding changes in the default interpretation of addr and <lb/>mmap&apos;s return value represent the principal change made to the original 4.2BSD specification. The change <lb/>was made to remove the burden of managing a process&apos;s address space from applications that did not wish <lb/>it. <lb/>5.1.2. Additions <lb/>We added one new system call, msync. msync has the interface <lb/>msync(addr, len, flags) <lb/>msync causes all modified copies of pages over the range [addr, addr + len) in system caches to be flushed <lb/>to the objects mapped by those addresses. msync optionally invalidates such cache entries so that further <lb/>references to the pages will cause the system to obtain them from their permanent storage locations. The <lb/>flags argument provides a bit-field of values which influences msync&apos;s behavior. The bit names and their <lb/>interpretations are: <lb/>MS_ASYNC <lb/>Return immediately <lb/>MS_INVALIDATE Invalidate caches <lb/>MS_ASYNC causes msync to return immediately once all I/O operations are scheduled; normally, <lb/>msync will not return until all I/O operations are complete. MS_INVALIDATE causes all cached copies of <lb/>data from mapped objects to be invalidated, requiring them to be re-obtained from the object&apos;s storage <lb/>upon the next reference. <lb/>5.1.3. Unchanged Interfaces <lb/>Two 4.2BSD calls were implemented without change. They were mprotect for changing the protec-<lb/>tion values of mapped pages, and munmap for removing a mapping. <lb/>5.1.4. Removed: mremap <lb/>We deleted one system call, mremap. Upon reading the 4.2BSD specification, we had the impres-<lb/>sion that mremap was the mapping equivalent of the UNIX mv command. However, discussions with those <lb/>involved in its original specification created confusion as to whether it was in fact supposed to be the <lb/>equivalent of mv, cp, or ln. In the presence of the uncertainty and lacking any other motivation to include <lb/>it, mremap was dropped from the system. <lb/>5.1.5. Open Issues <lb/>Two 4.2BSD system calls, madvise and mincore, remain unspecified. madvise is intended to provide <lb/>information to the system to influence its management policies. Since a major rework of such policies was <lb/>deferred to a future release, we decided to defer full specification and implementation of madvise until that <lb/>time. <lb/>mincore was specified to return the residency status of a group of pages. Although the intent was <lb/>clear, we felt that a more comprehensive interface for obtaining the status of a mapping was required. <lb/>However, at present, this revised interface has not been defined. <lb/>Also unspecified is an interface for locking pages in memory. We envision either a new mlock sys-<lb/>tem call, or a variation on madvise. <lb/>5.2. System V Shared Memory <lb/>The &apos;&apos;System V Interface Definition&apos;&apos; [AT&amp;T 86] defines a number of operations on entities called <lb/>&apos;&apos;shared memory segments&apos;&apos;. Early in our project, we had hoped to implement these operations not as sys-<lb/>tem calls but rather as library routines which built the System V abstractions out of the basic mechanisms <lb/>supplied by the kernel. Unfortunately, System V shared memory is almost, but not completely the same as, <lb/>a UNIX file. The primary differences are: <lb/></body>

			<page>8 <lb/></page>

			<body>name space: a shared memory segment exists in a name space different from that of the tradi-<lb/>tional UNIX file system; and <lb/>ownership and access: a shared memory segment separates the notion of &apos;&apos;creator&apos;&apos; from <lb/>&apos;&apos;owner&apos;&apos;. <lb/>Together, these differences motivated a kernel-based implementation to allocate and manage the different <lb/>name space (which shared implementation with other System V-specific objects such as semaphores), and <lb/>to administer the different ownership and access control operations. <lb/>Although the databases peculiar to these differences are maintained inside the kernel, the implemen-<lb/>tation of the objects and access are built from the standard notions. Specifically, the memory object <lb/>representing the shared memory segment exists as an unnamed object in the system&apos;s virtual memory, and <lb/>the operation which attaches processes to it performs the internal equivalent of an mmap. <lb/>Implementation plans call for the object used to represent the shared memory segment to be sup-<lb/>ported by an anonymous memory-based file system. /tmp could be implemented as a file system of this <lb/>type, potentially eliminating all I/O operations for temporary files and simply supporting them out of the <lb/>processor&apos;s memory resources. <lb/>5.3. Other System Calls and Facilities <lb/>The new VM system has had an impact on other areas of the system as well, either extending or <lb/>slightly altering the semantics of existing operations. <lb/>5.3.1. &apos;&apos;Segments&apos;&apos; <lb/>Traditionally, the address space of a UNIX process has consisted of three segments: one each for <lb/>write-protected program code (text), a heap of dynamically allocated storage (data), and the process&apos;s <lb/>stack. Under the new system, a process&apos;s address space is simply a vector of pages and there exists no real <lb/>structure to the address space. However, for compatibility purposes, the system maintains address ranges <lb/>that &apos;&apos;should&apos;&apos; belong to such segments to support operations such as extending or contracting the data <lb/>segment&apos;s &apos;&apos;break&apos;&apos;. These are initialized when a program is initiated with exec. <lb/>5.3.2. exec <lb/>exec overlays a process&apos;s address space with a new program to be executed. Under the new system, <lb/>exec performs this operation by performing the internal equivalent of an mmap to the file containing the <lb/>program. The text and initialized data segments are mapped to the file, and the program&apos;s uninitialized <lb/>data and stack areas are mapped to unnamed objects in the system&apos;s virtual memory. The boundaries of the <lb/>mappings it establishes are recorded as representing the traditional &apos;&apos;segments&apos;&apos; of a UNIX process&apos;s <lb/>address space. <lb/>exec establishes MAP_PRIVATE mappings, which has implications for the operation of fork and <lb/>ptrace, as discussed below. The text segment is mapped with only PROT_READ and PROT_EXECUTE <lb/>protections, so that write references to the text produce segmentation violations. The data segment is <lb/>mapped as writable; however any page of initialized data that does not get written may be shared among all <lb/>the processes running the program. <lb/>5.3.3. fork <lb/>Previously, a process created by fork had an address space made from a copy of its parent&apos;s address <lb/>space. Under the new system, the address space is not copied, but the mappings defining it are. Since exec <lb/>specifies MAP_PRIVATE on all the mappings it performs, parent and child thus effectively have copy-on-<lb/>write access to a single set of objects. Further, since the mapping is generally far smaller than the data it <lb/>describes, fork should be considerably more efficient. Any MAP_SHARED mappings in the parent are <lb/>also MAP_SHARED in the child, providing the opportunity for both parent and child to operate on a com-<lb/>mon object. <lb/></body>

			<page>9 <lb/></page>

			<body>5.3.4. vfork <lb/>Berkeley-based systems include a &apos;&apos;VM-efficient&apos;&apos; form of the fork system call to avoid the overhead <lb/>of copying massive processes that simply threw away the copy operation with a subsequent exec call. At <lb/>one point we hoped that the efficiencies gained through a reimplemented fork would obviate the need for <lb/>vfork. Unfortunately, vfork is defined to suspend the parent process until the child performs either an exec <lb/>or an exit and to allow the child full access to the parent&apos;s address space (not a copy) in the interim. A <lb/>number of programs take advantage of this quirk, allowing the child to record data in the address space for <lb/>later examination by the parent. Eliminating vfork would break these programs, a fact we discovered in <lb/>numerous ways when early versions of the system simply treated a vfork as fork. Further, vfork remains <lb/>fundamentally more efficient than even a fork that only copies an address space map, since vfork copies <lb/>nothing. <lb/>However, to encourage programmers at Sun to avoid the use of vfork, we took our time restoring it to <lb/>the system and as a result got many programs &apos;&apos;fixed&apos;&apos;. <lb/>5.3.5. ptrace <lb/>In previous versions of the system, the ptrace system call (used for process debugging) would refuse <lb/>to deposit a breakpoint in a program that was being run by more than one process. This restriction was <lb/>imposed by the nature of the old system&apos;s facility for sharing program code, which was to share the entire <lb/>text portion of an executable file. <lb/>In the new system, the system simply shares file pages among all those who have mappings to them. <lb/>When a mapping is made MAP_PRIVATE, writes by a process to a page to which writes are permitted are <lb/>diverted to a copy of the page − leaving the original object unaffected. ptrace takes advantage of the fact <lb/>that an exec establishes the mapping to the file containing the program and its initialized data as <lb/>MAP_PRIVATE, as it inserts a breakpoint by making a read-only page writable, depositing the breakpoint, <lb/>and restoring the protection. The page on which the breakpoint is deposited, and only that page, is no <lb/>longer shared with other users of the program − and their view of that page is unaffected. <lb/>5.3.6. truncate <lb/>The truncate system call has been changed so that it sets the length of a file. If the newly specified <lb/>length is shorter than the file&apos;s current length, truncate behaves as before. However, if the new length is <lb/>longer, the file&apos;s size is increased to the desired length. When writing a file exclusively through mapping, <lb/>extending through truncate is the only alternative to MAP_RENAME operations for growing a file. <lb/>5.3.7. Resource Limits <lb/>Berkeley-based systems include functions for limiting the consumption of certain system resources. <lb/>We have introduced a new resource limit: RLIMIT_PRIVATE. This limit controls the amount of &apos;&apos;private <lb/>memory&apos;&apos; that a process may dynamically allocate from the system&apos;s source of unnamed backing store. In <lb/>many respects, RLIMIT_PRIVATE really describes the limit that RLIMIT_DATA and RLIMIT_STACK <lb/>attempt to capture, namely the amount of swap space a given process may consume. <lb/>6. Internal Interfaces <lb/>The new VM system provides a set of abstractions and operations to the rest of the kernel. In many <lb/>cases, these are used directly as the basis for the system call interfaces described above. In other areas they <lb/>support internal forms of those system call interfaces, allowing the kernel to perform mappings for the <lb/>address space in which it operates. The VM system also relies on services from other areas of the kernel. <lb/>6.1. Internal Role of VM <lb/>In general, the kernel uses the VM system as the manager of a logical cache of memory pages and as <lb/>the object manager for &apos;&apos;address space objects&apos;&apos;. In its role as cache manager, the VM system also <lb/>manages the physical storage resources of the processor, as it uses these resources to implement the cache <lb/>it maintains. The VM system is a particularly effective cache manager, and maintains a high degree of <lb/>sharing over multiple uses of a given page of an object. As such, it has subsumed the functions of older <lb/></body>

			<page>10 <lb/></page>

			<body>data structures, in particular the text table and disk block data buffer cache (the &apos;&apos;buffer cache&apos;&apos;). The VM <lb/>system has replaced the old fixed-size buffer cache with a logical cache that uses all of the system&apos;s page-<lb/>able physical memory. Thus its use as a &apos;&apos;buffer cache&apos;&apos; in the old sense dynamically adapts to the pattern <lb/>of the system&apos;s use − in particular if the system is performing a high percentage of file references, all of the <lb/>system&apos;s pageable physical memory is devoted to a function that previously only had approximately 10% <lb/>of the same resources. The VM system is also responsible for the management of the system&apos;s memory <lb/>management hardware, although these operations are invisible to the machine-independent portions of the <lb/>kernel. <lb/>Kernel algorithms that operate on logical quantities of memory, such as the contents of file pages, do <lb/>so by establishing mappings from the kernel&apos;s address space to the object they wish to access. Those algo-<lb/>rithms that implement the read and write system calls on such memory objects are particularly interesting: <lb/>they operate by creating a mapping to the object and then copying the data to or from user buffers as <lb/>appropriate. When mapping is used in this manner, users of the object are provided with a consistent view <lb/>of the object, even if they mix references through mapped accesses or the read and write system calls. <lb/>Note that the decision to use mapping operations in this way is left to the manager of the object being <lb/>accessed. <lb/>The VM system does not know the semantics of the UNIX operating system. Instead, those proper-<lb/>ties of an address space that are the province of UNIX, such as the notions of &apos;&apos;segments&apos;&apos; and stack-<lb/>growth, are implemented by a layer of UNIX semantics over the basic VM system. By providing only the <lb/>basic abstractions from the VM system itself, we believe we have made it easier to provide future system <lb/>interfaces that may not have UNIX-like characteristics. <lb/>The VM system relies on the rest of the system to provide managers for the objects to which it estab-<lb/>lishes mappings. These managers are expected to provide advice and assistance to the VM system to <lb/>ensure efficient system management, and to perform physical I/O operations on the objects they manage. <lb/>These responsibilities are detailed further below. <lb/>6.2. as layer <lb/>The primary object managed by the VM system is a (process) address space (as). The interfaces <lb/>through which the system requests operations on an as object are summarized in Table 2, and are collec-<lb/>tively referred to as the as-layer of the system. An as contains the memory of the mappings that comprise <lb/>an address space. In addition, it contains a hardware address translation (hat) structure that holds the state <lb/>of the memory management hardware associated with this address space. This structure is opaque to much <lb/>of the VM system, and is interpreted only by a machine-dependent layer of the system, described further <lb/>below. <lb/>An as exists independent of any of its uses, and may be shared by multiple processes, thus setting the <lb/>stage for future integration of a multi-threaded address space capability as described in [KEPE 85]. The <lb/>&apos;&apos;address space&apos;&apos; in which the kernel operates is also described by an as structure, and is the handle by <lb/>which the kernel effects internal mapping operations using as_map. <lb/>The operations permitted on an as generally correspond to the functions provided by the system call <lb/>interface. An implication of this is that just about any operation that the kernel could perform on an <lb/>address space could also be implemented by an application directly. More work is necessary to define an <lb/>interface for obtaining information about an as, to support the generation of core files, and the as-yet <lb/>unspecified interfaces for reading mappings. An additional interface is also needed to support any advice <lb/>operations we might choose to define in the future. <lb/>Internally to an address space, each individual mapping is treated as an object with a &apos;&apos;mapping <lb/>object manager&apos;&apos;. Such mappings are run-length compact encodings describing the mapping being per-<lb/>formed, and may or may not have per-page information recorded depending on the nature of the mapping <lb/>or subsequent references to the object being mapped. Due to a regrettable lack of imagination at a critical <lb/>junction in our design, these &apos;&apos;mapping objects&apos;&apos; are termed segments, and their managers are called &apos;&apos;seg-<lb/>ment drivers&apos;&apos;. <lb/></body>

			<page>11 <lb/></page>

			<body>Table 2 − as operations <lb/>Operation <lb/>Function <lb/>struct as *as_alloc() <lb/>as allocation. <lb/>struct as *as_dup(as) <lb/>struct as *as; <lb/>Duplicates as − used in fork. <lb/>void as_free(as) <lb/>struct as *as; <lb/>as deallocation. <lb/>enum as_res <lb/>as_map(as, addr, size, crfp, crargsp) <lb/>struct as *as; addr_t addr; u_int size; <lb/>int (*crfp)(); caddr_t crargsp; <lb/>Internal mmap. <lb/>Establish a <lb/>mapping to an object using the <lb/>mapping manager routine identified <lb/>in crfp, providing object specific <lb/>arguments in the opaque structure <lb/>crargsp. <lb/>enum as_res <lb/>as_unmap(as, addr, size) <lb/>struct as *as; addr_t addr; u_int size; <lb/>Remove a mapping in as. <lb/>enum as_res <lb/>as_setprot(as, addr, size, prot) <lb/>struct as *as; addr_t addr; <lb/>u_int size, prot; <lb/>Alter protection of mappings in <lb/>as. <lb/>enum as_res <lb/>as_checkprot(as, addr, size, prot) <lb/>struct as *as; addr_t addr; <lb/>u_int size, prot; <lb/>Determine <lb/>whether <lb/>mappings <lb/>satisfy protection required by <lb/>prot. <lb/>enum as_res <lb/>as_fault(as, addr, size, type, rw) <lb/>struct as *as; addr_t addr; u_int size; <lb/>enum fault_type type; enum seg_rw rw; <lb/>Resolves a fault. <lb/>enum as_res <lb/>as_faulta(as, addr, size) <lb/>struct as *as; addr_t addr; u_int size; <lb/>Asynchronous fault − used for <lb/>&apos;&apos;fault-ahead&apos;&apos;. <lb/>6.3. hat layer <lb/>As previously noted, a hat is an object representing an allocation of memory management hardware <lb/>resources. The set of operations on a hat are not visible outside of the VM system, but represent a <lb/>machine-dependent/independent boundary called the hat-layer. Although it provides no services to the rest <lb/>of the system, the hat-layer is of import to those faced with porting the system to various hardware archi-<lb/>tectures. It provides the mapping from the software data structures of an as and its internals to those <lb/>required by the hardware of the system on which it resides. <lb/>We believe that the hat-layer has successfully isolated the hardware-specific requirements of Sun&apos;s <lb/>systems from the machine-independent portions of the VM system and the rest of the kernel. In particular, <lb/>under the old system the addition of support for a virtual address cache permeated many areas of the sys-<lb/>tem. Under the new system, support for the virtual address cache is isolated within the hat layer. <lb/>6.4. I/O Layer <lb/>The primary services the VM system requires of the rest of the kernel are physical I/O operations on <lb/>the objects it maps. These operations occur across an interface called the &apos;&apos;I/O Layer&apos;&apos;. Although used <lb/>mainly to cause physical page frames to be filled (page-in) or drained (page-out) operations, the I/O layer <lb/>also provides an opportunity for the managers of particular objects to map the system-specific page abstrac-<lb/>tion used by the VM system to the representation used by the object being mapped. <lb/></body>

			<page>12 <lb/></page>

			<body>For instance, although the system operates on page-sized allocations, the 4.2BSD UNIX file system <lb/>[MCKU 84] operates on collections of disk blocks that are often not page-sized. Efficient file system perfor-<lb/>mance may also require non-page-sized I/O operations, in order to amortize the overhead of starting opera-<lb/>tions and to maximize the throughput of the particular subsystem involved. Thus, the VM system will pass <lb/>several operations (such as the resolution of a fault on an object address, even one for which the VM sys-<lb/>tem has a cached copy) through the object manager to provide it the opportunity to intercede. The object <lb/>manager for NFS files uses these intercessions to prevent cached pages from becoming stale. Managers for <lb/>network-coherent objects enforce coherence through this technique. <lb/>The I/O layer is to some extent bi-directional, as a given operation requested by the VM system may <lb/>cause the object manager to request several VM-based operations. I/O clustering is an example of this, <lb/>where a request by the VM system to obtain a page&apos;s worth of data may cause the object manager to actu-<lb/>ally schedule an I/O operation for logical pages surrounding the one requested in the hopes of avoiding <lb/>future I/O requests. The old notion of &apos;&apos;read-ahead&apos;&apos; is implemented in this manner, and each object <lb/>manager has the opportunity to recognize and act on patterns of access to a given object in a manner that <lb/>maximizes its performance. <lb/>7. Project Status &amp; Future Work <lb/>The architecture described in this paper has been implemented and ported to the Sun-2 and Sun-3 <lb/>families of workstations. At present, all our major functional goals have been met. The work has con-<lb/>sumed approximately four man-years of effort over a year and a half of real time. A surprisingly large <lb/>amount of effort has been drained by efforts to interpose the VM system as the logical cache manager for <lb/>the file systems, in particular with respect to the 4.2BSD UNIX file system. <lb/>With respect to our performance goals, more tuning work is required before we can claim to meet <lb/>them. However, in some areas dealing with file access, early benchmarks reveal substantial performance <lb/>improvements resulting from the much larger cache available for I/O operations. We expect further perfor-<lb/>mance improvements when more of the system uses the new mechanisms. In particular, we expect an <lb/>implementation of shared libraries to have a substantial impact upon the use of system resources. Future <lb/>uses of mapping include a rewritten standard I/O library to use mmap rather than read and perhaps write, <lb/>thus eliminating the dual copying of data and providing a transparent performance improvement to many <lb/>applications. As sharing increases in the system, we expect the requirements for swap resources to <lb/>decrease. <lb/>Other future work involves refining and completing the interfaces that have not yet been fully <lb/>defined. We plan an investigation of new management policies, especially with respect to different page-<lb/>replacement policies and the better integration of memory and processor scheduling. We would also like to <lb/>port the system to different hardware bases, in particular to the VAX, to test the success of the hat layer in <lb/>isolating machine dependencies from the rest of the system. <lb/>8. Conclusions <lb/>We believe the new VM architecture successfully meets our goals. Reviewing these reveals: <lb/>Unify memory handling. All VM operations have been unified around the single notion of file <lb/>mapping. Extant operations such as fork and exec have been reconstructed and their perfor-<lb/>mance, and in some cases function, has been improved through their use of the new mechan-<lb/>isms. <lb/>Non-kernel implementation of many functions. Although we were disappointed that kernel <lb/>support was required to implement System V shared memory segments, we believe that this <lb/>goal has been largely satisfied. In particular, our implementation of shared libraries [GING 87] <lb/>requires no specific kernel support. We believe the basic operations the interfaces provide will <lb/>permit the construction of other useful abstractions with user-level programming. <lb/>Improved portability. Although more experience is required, we were pleased with the <lb/>degree to which the Sun-3 virtual address cache was easily incorporated into the new system, <lb/>in comparison with the difficulty experienced in integrating it into the previous system. <lb/></body>

			<page>13 <lb/></page>

			<body>Consistent with environment. The new system builds on the abstractions already in UNIX, in <lb/>particular with respect to our use of the UNIX file system as the name space for named virtual <lb/>memory objects. The integrated use of the new facilities in the system has helped to extend <lb/>the previous abstractions in a natural manner. The semantics offered by the basic system <lb/>mechanisms also do not impede the heterogeneous use of objects accessed through the system, <lb/>an important consideration for the networked environments in which we expect the system to <lb/>operate. <lb/>Finally, we have provided the functionality that motivated the work in the first place. <lb/></body>

			<div type="acknowledgement">9. Acknowledgements <lb/>The system was designed by the authors, with Joe Moran providing the bulk of the implementation. <lb/>Bill Joy offered commentary and advice on the architecture, as well as insights into the intents of the <lb/>4.2BSD interface, and an initial sketch of an implementation of the internal VM interfaces. Kirk <lb/>McKusick and Mike Karels of UC Berkeley, CSRG, spent several days discussing the issues with us. The <lb/>other members of Sun&apos;s System Software group gave considerable assistance and advice during the design <lb/>and implementation of the system. <lb/></div>

			<listBibl>10. References <lb/>[ACCE 86] <lb/>Accetta, M., R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, M. Young, <lb/>&apos;&apos;Mach: A New Kernel Foundation for UNIX Development&apos;&apos;, Summer Confer-<lb/>ence Proceedings, Atlanta 1986, USENIX Association, 1986. <lb/>[AT&amp;T 86] <lb/>AT&amp;T, System V Interface Definition, Volume I, 1986 <lb/>[BOBR 72] <lb/>Bobrow, D. G., J. D. Burchfiel, D. L. Murphy, and R. S. Tomlinson, &apos;&apos;TENEX, a <lb/>Paged Time Sharing System for the PDP-10&apos;&apos;, Communications of the ACM, <lb/>Volume 15, No. 3, March 1972. <lb/>[GING 87] <lb/>Gingell, R. A., M. Lee, X. T. Dang, M. S. Weeks, &apos;&apos;Shared Libraries in SunOS&apos;&apos;, <lb/>Summer Conference Proceedings, Phoenix 1987, USENIX Association, 1987. <lb/>[JOY 83] <lb/>Joy, W. N., R. S. Fabry, S. J. Leffler, M. K. McKusick, 4.2BSD System Manual, <lb/>Computer Systems Research Group, Computer Science Division, University of <lb/>California, Berkeley, 1983. <lb/>[KEPE 85] <lb/>Kepecs, J. H., &apos;&apos;Lightweight Processes for UNIX Implementation and Applica-<lb/>tions&apos;&apos;, Summer Conference Proceedings, Portland 1985, USENIX Association, <lb/>1985. <lb/>[KLEI 86] <lb/>Kleiman, S. R., &apos;&apos;Vnodes: An Architecture for Multiple File System Types in Sun <lb/>UNIX&apos;&apos;, Summer Conference Proceedings, Atlanta 1986, USENIX Association, <lb/>1986. <lb/>[MKCU 84] <lb/>McKusick, M. K., W. N. Joy, S. J. Leffler, R. S. Fabry, &apos;&apos;A Fast File System for <lb/>UNIX&apos;&apos;, Transactions on Computer Systems, Volume 2, No. 3, August 1984. <lb/>[MURP 72] <lb/>Murphy, D. L., &apos;&apos;Storage organization and management in TENEX&apos;&apos;, Proceedings <lb/>of the Fall Joint Computer Conference, AFIPS, 1972. <lb/>[ORGA 72] <lb/>Organick, E. I., The Multics System: An Examination of Its Structure, MIT Press, <lb/>1972. <lb/></listBibl>

			<page>14 </page>


	</text>
</tei>
