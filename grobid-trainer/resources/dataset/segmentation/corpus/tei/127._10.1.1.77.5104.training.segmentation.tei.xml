<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>In: Multimedia Systems, Volume 2, Number 6, (January 1995) pages 267-279. <lb/>An Empirical Study of Delay Jitter Management Policies * <lb/>Donald L. Stone Kevin Jeffay <lb/>University of North Carolina at Chapel Hill <lb/>Department of Computer Science <lb/>Chapel Hill, NC 27599-3175 USA <lb/>{stone, jeffay}@cs.unc.edu <lb/>July 1994 <lb/>Abstract: This paper presents an empirical study of several policies for managing the effect <lb/>of delay jitter on the playout of audio and video in computer-based conferences. The problem <lb/>addressed is that of managing the fundamental tradeoff between display with low latency and <lb/>display with few gaps. We describe a particular policy called queue monitoring which <lb/>observes delay jitter over time and dynamically adjusts display latency in order to support <lb/>low-latency conferences with an acceptable gap rate. Queue monitoring is evaluated by <lb/>comparing it with two policies from the literature in a study based on measurements from a <lb/>computer-based conferencing system. Our results show that queue monitoring performs as <lb/>well or better than the other policies over the range of observed network loads. More <lb/>importantly, we show that queue monitoring performs better on those network loads for <lb/>which the other policies exhibit poor performance. <lb/></front>

			<body>1. Introduction <lb/>This work concerns network and operating system support for computer-based video <lb/>conferencing. Our goal is to support high-performance conferences using todayÕs <lb/>networks based on asynchronous communications (e.g., ethernet, token ring, FDDI, T3, <lb/>etc.). In this environment, transmission times vary and hence audio and video data may <lb/>occasionally arrive at their destination after the time at which they should have been <lb/>displayed. When this occurs, a ÒgapÓ appears in the playout that can adversely affect <lb/>usersÕ perception of the conference. The problem we consider here is that of ameliorating <lb/>the effect of variable network transmission delays in order to support good quality audio <lb/>and video playout. In particular, we are interested in supporting small internetworks <lb/>consisting of several physical networks connected via bridges and routers (e.g., a building <lb/>or campus-area network). Support for such networks will be necessary if, as we believe, <lb/>conventional LANs continue to be widely used in the Òlast mileÓ to the desktop. <lb/>For the audio and video hardware considered in our work, data units called frames are <lb/>acquired from an audio or video source at a precise rate (e.g., one frame every 33 ms. for <lb/></body>

			<front>* This work supported by the National Science Foundation (grant numbers CCR-9110938 and ICI-9015443), <lb/>and by the IBM and Intel Corporations. <lb/></front>

			<page>2 <lb/></page>

			<body>video), and must be delivered to a conference participantÕs display at the same precise <lb/>rate. Prior to delivery, each frame must be digitized, compressed, transmitted over the <lb/>network, and decompressed as shown in Figure 1. Once decompressed, a frame is <lb/>buffered in a display queue until it can be displayed. <lb/>Conference Sender <lb/>Transmission <lb/>Digitization <lb/>Compression <lb/>Inter-<lb/>network <lb/>Conference Receiver <lb/>Reception <lb/>Play-Out <lb/>Decompression <lb/>Display <lb/>Queue <lb/> Figure 1: System overview. <lb/>For each conference receiver, the display latency of a frame is defined as the total time <lb/>from acquisition at the sender to display at the receiver. For purposes of discussion, it is <lb/>convenient to divide display latency into two components: the total time from acquisition <lb/>to decompression, called the end-to-end delay of a frame, and the time a frame is buffered <lb/>between decompression and display, called the display queuing delay. If we assume that <lb/>the time required to digitize, compress, or decompress frames is not constant, or if we <lb/>assume that the delays incurred transmitting frames across the network can vary, then the <lb/>end-to-end delays experienced by frames will vary. Variance in end-to-end delay is called <lb/>delay jitter. <lb/>Ideally, frames should be displayed continuously with each successive frame displayed <lb/>immediately after its predecessor. Since frames are generated and displayed at fixed <lb/>intervals, continuous playout can occur only if each frame is played with the same <lb/>display latency. Thus, if frames are to be played continuously, jitter in the end-to-end <lb/>delay of each frame must be compensated for by varying the display queuing delay of the <lb/>frame. <lb/>However, continuous playout is not always possible. For example, consider a case where <lb/>a frame incurs a particularly long end-to-end delay. As a result, the frame may not be <lb/>ready to be displayed at the time when the display of the preceding frame is complete. In <lb/>such a case, there will necessarily be a gap in the playout (e.g., for a video stream, this <lb/>would mean that no video frame is displayed for a 33 ms. interval). More precisely, a gap <lb/>will occur whenever a frame arrives with an end-to-end delay greater than the display <lb/>latency of the previous frame. <lb/>In general, the lower the display latency, the higher the probability of encountering an <lb/></body>

			<page>3 <lb/></page>

			<body>end-to-end delay sufficient to cause a gap. Thus there is a fundamental tradeoff between <lb/>display latency and gap frequency. A conferencing system must explicitly manage this <lb/>tradeoff so as to provide good quality playout to the user. <lb/>One approach to managing this tradeoff is to prevent delay jitter completely by using a <lb/>network that provides transmission with constant delay. This is the approach used in <lb/>computer-based conferencing systems based on networks that provide isochronous <lb/>services (such as ATM). Another approach is to minimize delay jitter by reserving <lb/>resources in the network so as to provide guaranteed bounds on delay and delay jitter. <lb/>For example, Ferrari [1] presents a scheme in which clients requiring real-time <lb/>communication services specify their traffic characteristics and performance requirements. <lb/>In response, the system reserves sufficient resources at each node on the network (e.g., <lb/>processor capacity, buffer space) to guarantee the performance requirements of the client. <lb/>Unfortunately, isochronous delivery and resource reservation are not available in the <lb/>network environments we wish to support. More importantly, even if the network <lb/>supported transmission with constant delay, jitter in the end-to-end delay can occur as a <lb/>result of variations in the time required to digitize, compress, and decompress frames, as <lb/>well as variations in the time required to communicate data between the audio/video <lb/>subsystem and the network transport system (all of which can be affected by operating <lb/>system scheduling decisions). For each of these reasons, we assume that delay jitter is a <lb/>fundamental phenomenon and therefore the tradeoff between display latency and gap <lb/>frequency must be explicitly managed in any conferencing system. <lb/> In this paper, we evaluate three policies for managing the tradeoff between display <lb/>latency and gap frequency. Two policies, the I-policy and the E-policy, are taken from <lb/>the literature; the third, queue monitoring, is a new policy that we have developed. The <lb/>performance of these policies is evaluated with an empirical study based on an <lb/>experimental computer-based video conferencing system (described in [3]). In the study, <lb/>traces of the end-to-end delays experienced by frames are recorded during the execution of <lb/>the conferencing system under a variety of (real) network loads. Each trace is used as <lb/>input to a simulator which determines the display latency and gap rate that would result <lb/>from applying each policy to a conference with the corresponding sequence of end-to-end <lb/>delays. The simulation results demonstrate that queue monitoring always performs at <lb/>least as well, and often performs better than either the I-policy or the E-policy over the <lb/>range of observed network conditions. <lb/>The following section discusses the basic principles of managing the tradeoff between <lb/>display latency and gap frequency, and describes the I-and E-policies. The queue <lb/>monitoring policy is presented in Section 3. Section 4 discusses the metrics we propose <lb/>for evaluating delay jitter management policies and the problems inherent in formulating <lb/>such metrics. Section 5 describes the experiments and presents the results of the trace-<lb/>driven simulations. We conclude in Section 6 with a summary of our findings. <lb/></body>

			<page>4 <lb/></page>

			<body>2. The Effect of Delay Jitter <lb/>In order to sustain continuous playout without gaps, every frame must be played with a <lb/>fixed display latency that is greater than the worst-case end-to-end delay that will be <lb/>encountered during a conference. It is not clear however that our primary goal should be <lb/>playout with no gaps. Display latency is only one important factor in determining the <lb/>perceived quality of the playout [2]. It is likely that in many conferencing applications, <lb/>as long as gaps occur infrequently, playout with low latency and some gaps will be <lb/>preferable to playout with high latency and no gaps. Therefore, if we always play frames <lb/>with a display latency greater than the worst-case delay and if the worst-case delay is <lb/>rarely observed in practice, then we will display most frames with latency higher than <lb/>necessary to support good quality playout. <lb/>If we are willing to accept gaps in the playout, then we can choose to play frames with a <lb/>display latency less than the worst-case end-to-end delay. However, we must now <lb/>address the issue of what should be done with a frame that arrives after the time at which <lb/>it should have been displayed. There are two choices: either the frame can be discarded <lb/>or it can be displayed. These choices define two delay jitter management policies which <lb/>Naylor and Kleinrock call the I-Policy and the E-Policy [6]. Under the I-policy, frames <lb/>are displayed with a single fixed display latency which is a parameter of the policy, and <lb/>each frame that arrives with an end-to-end delay greater than this latency is discarded. <lb/>Under the E-policy, late frames are displayed at the next opportunity. This causes the <lb/>display of all frames after the late frame to be delayed and thus the E-policy has the effect <lb/>of increasing the display latency of all subsequent frames. <lb/>Figure 2 illustrates the behavior of the I-and E-policies in response to late frames. Figure <lb/>2a shows the acquisition and display times for eight frames. Tick marks on the upper <lb/>timeline indicate acquisition times, the times at which new frames are acquired. Tick <lb/>marks on the lower timeline indicate display initiation times, the times at which the new <lb/>frames are displayed. Each diagonal arrow represents the end-to-end delay of an <lb/>individual frame, extending from the time at which it was acquired to the time it is placed <lb/>in the display queue. Throughout these examples, the acquisition time of a frame (i.e., <lb/>points a, b, c, etc.) is used to refer to individual frames. <lb/>Figure 2b shows the effect of executing the I-Policy on the pattern of acquisition times, <lb/>display initiation times, and end-to-end delays shown in Figure 2a. In this example, the <lb/>display latency parameter of the I-Policy is 2 frame times. (For simplicity in the <lb/>examples, time is represented as multiples of the time taken to acquire or display a <lb/>frame). The top graph in Figure 2b shows the display queue length at each display <lb/>initiation time. The bottom graph shows the display latency of the frame being displayed <lb/>at each display initiation time. In addition, each latency bar is labeled with the acquisition <lb/>time of the frame that is displayed at that display initiation time. In this example, frames <lb/>b, d, and f arrive with end-to-end delays longer than two frame times and are discarded. <lb/>Thus, use of the I-policy results in three gaps occurring in the playout at display <lb/>initiation times 4,6, and 8. <lb/></body>

			<page>5 <lb/></page>

			<body>Acquisition Time <lb/> 1 <lb/>1 0 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>a <lb/>h <lb/>g <lb/>f <lb/>e <lb/>d <lb/>c <lb/>b <lb/>Display Initiation Time <lb/>a) Delay jitter. <lb/>Display Queue Length <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Queue <lb/>Length <lb/>Display Queue Length <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Queue <lb/>Length <lb/>Display Latency <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Time <lb/>(in frames) <lb/>a <lb/>h <lb/>g <lb/>e <lb/>c <lb/>g <lb/>f <lb/>e <lb/>d <lb/>c <lb/>b <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Display Latency <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>Time <lb/>(in frames) <lb/>a <lb/>b) I-policy. <lb/>c) E-policy. <lb/>Figure 2: Behavior of I-policy and E-policy. <lb/>Figure 2c shows the effect of executing the E-policy. As with the I-policy, the late arrival <lb/>of frame b causes a gap at display initiation time 4. However, after frame b arrives, it is <lb/>placed in the display queue and eventually played at display initiation time 5 with a <lb/>latency of 3 frame times. This delay results in each frame after b also being played with a <lb/>display latency of 3 frame times. Because 3 frame times is longer than the end-to-end <lb/>delay experienced by any frame after b, there are no gaps after display initiation time 4. <lb/>The example shown in Figure 2 illustrates an advantage of the E-policy. The E-policy <lb/>starts playing frames with the lowest possible initial display latency and then adjusts <lb/>display latency upward in response to delay jitter. The overall effect of the E-policy is <lb/>to find a display latency which is sufficient to play frames without gaps by dynamically <lb/>adjusting the latency to be higher than any end-to-end delay yet observed. <lb/>Figure 3 illustrates a situation in which the I-policy performs better than the E-policy. In <lb/>this example, all frames except frames c and d arrive with an end-to-end delay of less than <lb/>a frame time and experience negligible delay jitter. Frames c and d arrive late because of <lb/>some temporary increase in network activity. Each policy results in gaps at display <lb/>initiation times 4 and 5, however, the I-policy with a display latency of one frame time <lb/></body>

			<page>6 <lb/></page>

			<body>Acquisition Time <lb/> 1 <lb/>1 0 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>a <lb/>h <lb/>g <lb/>f <lb/>e <lb/>d <lb/>c <lb/>b <lb/>Display Initiation Time <lb/>i <lb/>a) Delay jitter. <lb/>Display Queue Length <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Queue <lb/>Length <lb/>Display Queue Length <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Queue <lb/>Length <lb/>Display Latency <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Time <lb/>(in frames) <lb/>g <lb/>f <lb/>e <lb/>d <lb/>c <lb/>a b <lb/>g <lb/>f <lb/>e <lb/>d <lb/>c <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>Display Initiation Time <lb/>Display Latency <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>Time <lb/>(in frames) <lb/>a b <lb/>b) I-policy. <lb/>c) E-policy. <lb/>Figure 3: Behavior of the I-policy and E-policy after a Òburst.Ó <lb/>plays the frames after the gap with low latency while the E-policy plays the frames after <lb/>the gap with high latency. Under the E-policy, a single ÒburstÓ of activity on the network <lb/>that causes a few frames to arrive late results in a permanent increase in display latency. <lb/>In each policy, we are choosing a display latency at which to play frames, either <lb/>explicitly in the case of the I-policy, or implicitly in the case of the E-policy. A good <lb/>choice for display latency will depend on many factors. First, the acceptable rate of gaps <lb/>and the acceptable display latency may vary depending on the application (e.g., speech <lb/>and music may have different gap and latency requirements) and the current requirements <lb/>of the user (e.g., a surgeon viewing an operation will have different requirements than <lb/>participants in a televised lecture). Second, the display latency required to maintain an <lb/>acceptable gap-rate will depend on the expected level of delay jitter, which will vary as a <lb/>result of congestion both at participantsÕ machines and on the network. The dynamic <lb/>nature of these factors motivates the design of a delay jitter management policy which <lb/>dynamically chooses an appropriate display latency so as to adapt to new requirements <lb/>and conditions. <lb/></body>

			<page>7 <lb/></page>

			<body>3. Queue Monitoring <lb/>The problem of choosing a display latency has been extensively studied in the context of <lb/>packet audio systems (systems for the transmission of audio streams across packet-<lb/>switched networks). In many applications, audio is modeled as a sequence of <lb/>ÒtalkspurtsÓ (some period of time in which audio data must be acquired, transmitted, and <lb/>played) separated by Òsilent periodsÓ (some period of time in which there is no <lb/>significant audio activity, so audio need not be acquired or played). Naylor and Kleinrock <lb/>proposed that a display latency be chosen at the beginning of each talkspurt by observing <lb/>the transmission delays of the last m audio fragments, discarding the k largest delays, and <lb/>choosing the greatest remaining delay. For their particular model of audio quality, they <lb/>stated a rule of thumb for choosing m and k (m &gt; 40 and k = .07*m) which usually <lb/>resulted in good quality audio. More recent work on practical solutions for the Internet <lb/>(as exemplified by Nevot [7]), has also used a scheme by which a display latency is <lb/>chosen at the beginning of each talkspurt on the basis of observed delay jitter. <lb/>More generally, consider an oracle that has perfect knowledge of the end-to-end delays of <lb/>future frames and hence can choose the best display latency at which to play each frame. <lb/>Such an oracle can adjust display latency in response to changes in delay jitter (perhaps <lb/>due to changes in network congestion) in order to achieve the best possible balance <lb/>between display latency and gaps. Display latency can be adjusted upward by artificially <lb/>introducing a gap (i.e., delaying the playout of the next frame) and can be adjusted <lb/>downward by discarding frames. <lb/>If we assume that the delay jitter in the near future can be predicted by observing the <lb/>delay jitter in the recent past, we can construct delay jitter management policies that are <lb/>approximations to the oracle. (This is analogous to the working set concept of page <lb/>replacement in virtual memory management.) The Naylor and Kleinrock policy for <lb/>choosing a display latency is one example of such a policy. This policy, however, is <lb/>difficult to implement as measuring end-to-end delays at runtime requires synchronized <lb/>clocks. <lb/>Instead of measuring end-to-end delays, we can directly measure the impact of delay jitter <lb/>at a receiver by observing the length of the display queue over time. Once every frame <lb/>time, a frame is removed from the display queue to be played. For example, for video <lb/>frames displayed at 30 frames per second, a frame is removed every 33 ms. Since frames <lb/>are also acquired and transmitted once per frame time, on average one frame will arrive <lb/>and be placed in the display queue and one frame will be removed from the display queue <lb/>during each frame time. If end-to-end delays are constant, the queue length measured at <lb/>each display initiation time should be constant (as it was between display initiation times <lb/>7 and 10 in the example shown in Figure 3c). If end-to-end delay increases sufficiently, it <lb/>may happen that no frame arrives during the playout of a frame. In that case, the length <lb/>of the display queue will decrease by 1 frame (e.g., display initiation time 6 in Figure 2c). <lb/>If end-to-end delays decrease sufficiently, more than one frame may arrive during the <lb/>playout of a frame, and the length of the display queue will increase (e.g., display <lb/>initiation time 6 in Figure 3c). <lb/></body>

			<page>8 <lb/></page>

			<body>Over time, the length of the display queue will vary depending on the range of end-to-end <lb/>delays encountered by frames. If we assume the level of delay jitter in the near future will <lb/>be the same as the level in the recent past, then while end-to-end delays may vary, they <lb/>will not vary outside the range that we have observed recently. This implies that the <lb/>length of the display queue will remain at least as long as the minimum length that has <lb/>been observed in the recent past. Furthermore, as long as the display queue contains at <lb/>least one frame at each display initiation time, there will be no gaps in the playout. Thus, <lb/>if the minimum display queue length observed recently was at least two frames, we can <lb/>discard a frame to reduce display latency without causing a gap. <lb/>A policy for decreasing display latency based on observing queue lengths has been used <lb/>to govern the behavior of the audio display queue in the Pandora system [5]. Whenever <lb/>frames are added to the display queue (called the clawback buffer), the length of the queue <lb/>is checked against a target value. In the Pandora system, the target is 2 frames (which <lb/>corresponds to 4 ms. of audio data). If the length of the display queue is greater than this <lb/>target for a sufficiently long interval (8 seconds in the Pandora system), incoming audio <lb/>frames are discarded. Because this has the effect of shortening the display queue, audio <lb/>data that arrives after this time will be played with a lower display latency. <lb/>We propose a display queue management policy called queue monitoring that is a <lb/>variation of the policy used in Pandora. For each possible display queue length we define <lb/>a threshold value. The threshold value for queue length n specifies a duration (measured <lb/>in frame times) after which, if the display queue has continuously contained more than n <lb/>frames, we will conclude that display latency can be reduced without increasing the <lb/>frequency of gaps. Since we expect that large variations in end-to-end delay will occur <lb/>infrequently, and small variations will occur much more frequently, threshold values for <lb/>long queue lengths specify short durations while those for short queue lengths specify <lb/>long durations. This policy has the effect of reducing display latency quickly after long <lb/>delays due to large bursts of network traffic, but approaching minimal latency slowly. <lb/>To implement queue monitoring, we associate an array of counters and threshold values <lb/>with the display queue. Each time we wish to display a new frame, we first perform a <lb/>Òthresholding operation.Ó If the display queue has length m, then counters 2 through m Ð <lb/>1 are incremented and all other counters are reset. If any counter exceeds its associated <lb/>threshold value, all the counters are reset and the oldest frame is discarded from the <lb/>display queue. The oldest remaining frame is then displayed. <lb/>An important principle in this implementation is that the thresholding operation will <lb/>never discard frames unless the display queue contains more than two frames. The last <lb/>frame in the display queue should never be discarded because there must be a frame <lb/>available for display after the thresholding operation completes. Similarly, if the second-<lb/>to-last frame in the display queue were discarded, then even minimal delay jitter could <lb/>potentially cause a gap. For example, this can occur in a situation where a frame arrives <lb/>immediately before the thresholding operation and results in a display queue with length <lb/>2. If one of those frames is discarded and the other is displayed, then the queue would be <lb/>empty. Then, if the next frame has a slightly larger end-to-end delay, it may not arrive in <lb/></body>

			<page>9 <lb/></page>

			<body>time to be displayed. Therefore, in the following discussion, thresholds will only be <lb/>defined for queue lengths greater than two. <lb/>4. Evaluating Delay Jitter Management Policies <lb/>The remainder of the paper is a study comparing the effectiveness of the queue <lb/>monitoring policy to that of the I-and the E-policy. We are hampered in this task by the <lb/>lack of metrics for comparing the performance of delay jitter management policies. <lb/>Clearly, if policy A results in lower display latency and less gaps than policy B, it is <lb/>performing better. However, if policy A results in a lower display latency and a higher <lb/>gap rate as compared with policy B, which has performed better? The answer will <lb/>depend on a number of factors including the display latency, the gap rate, the resolution <lb/>of the display, and the userÕs particular audio and video requirements. It may also <lb/>depend on the distribution of gaps throughout the measurement interval, the number of <lb/>display latency changes, and the distribution of periods of high and low display latency <lb/>throughout the interval. A proper standard for comparing policies would take each of <lb/>these factors into account in making a judgment. <lb/>Unfortunately, we are not aware of such a standard. Therefore we have adopted a simple <lb/>and arbitrary comparison rule for the analysis in this paper. The performance of a policy <lb/>is evaluated along two dimensions: average display latency and average gap rate. We <lb/>assume that only differences in display latency of more than 15 ms. and differences in gap <lb/>rate of more than 1 gap per minute are significant. Under these rules policy A is declared <lb/>to have done better than policy B if it is better in one dimension and the same or better in <lb/>the other dimension. Two policies are declared to have done equally well if they are the <lb/>same in both dimensions and are declared to be incomparable if each has done better in <lb/>one dimension. <lb/>Given this comparison rule, we can evaluate and compare the effectiveness of policies for <lb/>a particular execution of the system. However, it is still difficult to compare results of <lb/>multiple executions. One fundamental difficulty arises because the video hardware that <lb/>acquires frames at the sender is not synchronized with the display at the receiver. To <lb/>illustrate the effect this has on display latency, assume there is no end-to-end delay (i.e., <lb/>acquisition and arrival of frames are simultaneous). Despite this fact, we must wait until <lb/>the next display initiation time to display each new frame. Depending on the <lb/>synchronization difference between the video hardware acquiring the frames and the <lb/>display, each frame may have to wait up to one frame time before being displayed. This <lb/>synchronization time is a random variable and varies between executions. Therefore, <lb/>when comparing results of multiple executions, differences in latency of as much as one <lb/>frame time are not significant. <lb/>The second difficulty in comparing multiple executions arises from our working definition <lb/>of the I-policy. Ideally, the I-policy should enforce a particular display latency. <lb/>However, this would require that the clocks at the acquisition and display workstations <lb/>be synchronized. In our work we only assume synchronized clocks for measurement <lb/></body>

			<page>10 <lb/></page>

			<body>purposes (i.e., we do not use synchronized clocks to guide the execution of the system), <lb/>so we cannot completely implement such a policy. Instead, we use a variant of the I-<lb/>policy which buffers the first frame for a fixed number of frame times before displaying it <lb/>and then displays all subsequent frames with the same display latency. The effect of this <lb/>definition is to make the display latency enforced by a particular I-policy in an execution, <lb/>a function of the end-to-end delay of the first frame that is received (i.e., a random <lb/>variable). <lb/>The goal of the study presented in this paper is to determine which of several policies <lb/>results in the best quality playout. Because of the difficulties involved in comparing the <lb/>results of multiple executions, and because our comparison rule determines relative, rather <lb/>than absolute performance, we restrict direct comparisons to determining the relative <lb/>performance of two policies on single conference executions. This allows us to conclude <lb/>only that one policy outperforms another on a particular execution. To show that one <lb/>policy outperforms another in general, we must show that it performs better on some <lb/>executions and as well or better on all executions. This method of pairwise comparison is <lb/>the basis of the performance evaluations presented in the next section. <lb/>5. The Study <lb/>In this section, we present the results of a study that we have performed to gauge the <lb/>effectiveness of the queue monitoring policy in a building-sized internetwork. In this <lb/>study, we compared the performance of the queue monitoring policy with the <lb/>performance of the I-and E-policies on video conferences transmitted over the main <lb/>network supporting our department. The experiments presented were performed using <lb/>the audio portion of an experimental video conferencing system that we have developed. <lb/>In each experiment, we have recorded a trace of the end-to-end delay experienced by each <lb/>frame. These traces are then used as input to a simulator which determines the average <lb/>display latency and average gap rate that would result from applying each policy to a <lb/>conference with the corresponding sequence of end-to-end delays. The results are <lb/>compared using the comparison rule defined in Section 4. <lb/>The video conferencing system used for the experiments runs on IBM PS/2 workstations. <lb/>These workstations run a real-time operating system and network transport software <lb/>specially tailored for real-time communications. Network packets use the UDP/IP <lb/>format. Acquisition and display of audio and video data is performed using IBM-Intel <lb/>ActionMedia 750 hardware at a rate of 30 video frames per second (256x240 resolution <lb/>and 8-bit color pixels; each compressed video frame occupies around 6000-8000 bytes) <lb/>and 60 audio frames per second (a 2-channel audio stream generated at a bit rate of 128 <lb/>Kb per second and packaged into 16.5 ms ÒframesÓ). The full system has been <lb/>extensively described elsewhere [3,4]. <lb/>The building network consists of several 10 Mb Ethernets and 16 Mb token rings <lb/>interconnect by bridges and routers. It supports approximately 400 UNIX workstations <lb/>and Macintosh personal computers. The workstations share a common filesystem using <lb/>a mix of NFS and AFS. The application mix running on these workstations should be <lb/></body>

			<page>11 <lb/></page>

			<body>typical of most academic computer science departments. We are linked to the Internet <lb/>through the campus internetwork which also includes a number of other departmental <lb/>networks (generally smaller than the one just described) connected via a central broadband <lb/>network. In all, this environment should be typical of those generally found in the Òlast <lb/>mileÓ to the desktop. <lb/>In each experiment, we acquired, transmitted, and displayed audio and video for a series <lb/>of 10 minute intervals. Audio frames were transmitted in individual packets and video <lb/>frames were broken into 1350 byte fragments. Each packet was routed across a lightly <lb/>loaded private token ring to a gateway, through a segment of the departmental ethernet to <lb/>a bridge, through a second segment of the departmental ethernet to another gateway, and <lb/>back across the private token ring to the display machine. <lb/>Twenty-four runs of the system were performed over the course of a typical day <lb/>(between 6 am and 5 pm) covering lightly and heavily loaded periods. Four additional <lb/>runs were performed between midnight and 1 am. For each run, we recorded a trace of the <lb/>acquisition time and the arrival time of each audio frame. In addition, we recorded each <lb/>display initiation time (i.e., the times at which new frames were displayed). Before each <lb/>run, we ran a protocol to measure the difference in clock times between the acquisition <lb/>and display machines. After each run, the recorded times were adjusted to account for the <lb/>difference in clock times and to account for clock drift (measured in a separate <lb/>experiment). Finally, the traces were used as input for a trace-driven simulation of the <lb/>display-side of a conference. For the given sequence of arrivals and display initiation <lb/>times, the simulator determined the time at which each frame would be displayed under <lb/>each of the three policies. The output of the simulator was the average display latency <lb/>and average gap rate that would result from using each policy on the run. <lb/>Figure 4 gives some basic data on the 28 runs. ÒTime of DayÓ is the time the run was <lb/>initiated. Average and maximum delays are calculated from the end-to-end delays <lb/>experienced by audio frames. Lost and duplicate frames are counts of lost and duplicated <lb/>packets which contained an audio frame. No out of order packets were observed. <lb/>Figure 5 provides a more detailed look at two runs, one with low delay jitter and one with <lb/>high delay jitter. These figures are histograms of the end-to-end delays experienced by <lb/>audio frames in runs 2 and 24. The y-axis shows a count of the number of frames with <lb/>end-to-end delays within each 5 ms. interval (e.g., a count of frames with an end-to-end <lb/>delay of 30-35 ms.). Note that the y-axis is plotted on a log scale. <lb/>Figure 6 shows the simulation results for each of the 28 runs. For each, we simulated the <lb/>queue monitoring policy with a threshold of 120 frame times at all queue lengths. The <lb/>effect of these threshold settings is to reduce display latency by one audio frame time <lb/>(i.e., 16.5 ms.) whenever the display queue contains more than 2 audio frames for 120 <lb/>continuous frame times (i.e., 2 seconds). We also simulated the E-policy and the (variant) <lb/>I-policy with 2 and 3 frame times of display latency. In the table, these policies are <lb/>labeled QM-120, E, I-2, and I-3 respectively. For each policy, the table shows the <lb/>resulting average display latency (in ms.) and the average gap rate (in gaps per minute). <lb/></body>

			<page>12 <lb/></page>

			<body>Run <lb/>Time of <lb/>Day <lb/>Avg. Delay <lb/>ms. <lb/>Max. Delay <lb/>ms. <lb/>Lost <lb/>Frames <lb/>Duplicate <lb/>Frames <lb/>1 <lb/>06:03 <lb/>38 <lb/>76 <lb/>1 <lb/>0 <lb/>2 <lb/>06:25 <lb/>38 <lb/>88 <lb/>3 <lb/>0 <lb/>3 <lb/>06:36 <lb/>37 <lb/>171 <lb/>5 <lb/>0 <lb/>4 <lb/>06:47 <lb/>37 <lb/>105 <lb/>1 <lb/>0 <lb/>5 <lb/>08:03 <lb/>38 <lb/>115 <lb/>1 <lb/>0 <lb/>6 <lb/>08:14 <lb/>37 <lb/>73 <lb/>2 <lb/>0 <lb/>7 <lb/>08:25 <lb/>38 <lb/>184 <lb/>7 <lb/>0 <lb/>8 <lb/>08:36 <lb/>39 <lb/>157 <lb/>1 <lb/>0 <lb/>9 <lb/>10:02 <lb/>41 <lb/>186 <lb/>23 <lb/>0 <lb/>10 <lb/>10:16 <lb/>40 <lb/>124 <lb/>4 <lb/>0 <lb/>11 <lb/>10:31 <lb/>41 <lb/>213 <lb/>7 <lb/>0 <lb/>12 <lb/>10:49 <lb/>40 <lb/>140 <lb/>6 <lb/>0 <lb/>13 <lb/>11:57 <lb/>39 <lb/>110 <lb/>5 <lb/>0 <lb/>14 <lb/>12:08 <lb/>41 <lb/>138 <lb/>5 <lb/>0 <lb/>15 <lb/>12:19 <lb/>41 <lb/>133 <lb/>3 <lb/>0 <lb/>16 <lb/>12:34 <lb/>40 <lb/>187 <lb/>11 <lb/>0 <lb/>17 <lb/>14:02 <lb/>41 <lb/>189 <lb/>11 <lb/>0 <lb/>18 <lb/>14:13 <lb/>42 <lb/>141 <lb/>3 <lb/>0 <lb/>19 <lb/>14:42 <lb/>39 <lb/>107 <lb/>4 <lb/>0 <lb/>20 <lb/>14:54 <lb/>40 <lb/>131 <lb/>12 <lb/>0 <lb/>21 <lb/>16:01 <lb/>39 <lb/>171 <lb/>9 <lb/>0 <lb/>22 <lb/>16:21 <lb/>39 <lb/>128 <lb/>2 <lb/>0 <lb/>23 <lb/>16:33 <lb/>39 <lb/>86 <lb/>2 <lb/>1 <lb/>24 <lb/>16:55 <lb/>42 <lb/>242 <lb/>14 <lb/>1 <lb/>25 <lb/>00:05 <lb/>38 <lb/>80 <lb/>4 <lb/>0 <lb/>26 <lb/>00:16 <lb/>38 <lb/>128 <lb/>0 <lb/>0 <lb/>27 <lb/>00:27 <lb/>38 <lb/>134 <lb/>8 <lb/>0 <lb/>28 <lb/>00:38 <lb/>38 <lb/>83 <lb/>2 <lb/>0 <lb/>Figure 4: All runs, basic data. <lb/>For each run, the rightmost columns show the comparison between the queue monitoring <lb/>policy and the other policies (using the comparison rule defined in Section 0). A Ô+Õ <lb/>indicates that queue monitoring did better, a Ô0Õ means the two were equivalent, a Ô-Õ <lb/>means that queue monitoring did worse, and a blank space means the two policies were <lb/>incomparable. <lb/>Figure 6 shows that the QM-120 policy performed as well or better than the I-2 policy <lb/>for every run except run 11. For that run, QM-120 is incomparable with I-2 because it <lb/>has a somewhat lower latency and a much higher gap rate. In particular, the gap rate <lb/>produced by the I-2 policy is extremely high (approximately 1 gap every 2 seconds, <lb/>compared with about 1 gap every 12 seconds for the QM-120 policy). From these <lb/>results, we can conclude that in general, the QM-120 policy is more effective than the I-2 <lb/>policy. <lb/>The QM-120 policy also performed as well or better than the I-3 policy for every run <lb/>except run 15. On that run, QM-120 policy is judged to perform worse than I-3 because <lb/></body>

			<page>13 <lb/></page>

			<body>End-To-End Delays (ms.) <lb/>Number of Frames <lb/>0 <lb/>1 <lb/>10 <lb/>100 <lb/>1000 <lb/>10000 <lb/>100000 <lb/>0 <lb/>50 <lb/>100 <lb/>150 <lb/>200 <lb/>250 <lb/>a) Run 2. <lb/>End-To-End Delays (ms.) <lb/>Number of Frames <lb/>0 <lb/>1 <lb/>10 <lb/>100 <lb/>1000 <lb/>10000 <lb/>100000 <lb/>0 <lb/>50 <lb/>100 <lb/>150 <lb/>200 <lb/>250 <lb/>b) Run 24. <lb/>Figure 5: Distribution of end-to-end delays. <lb/>the difference between the display latency produced by the QM-120 policy and that <lb/>produced by the I-3 policy (13 ms.) is not significant (according to the comparison rule), <lb/>while the difference in gap rates (1.2 gaps per minute) is significant. Thus, we conclude <lb/>that, in general, the QM-120 policy is more effective than the I-3 policy. Furthermore, <lb/>QM-120 always results in a display latency lower than that produced by I-3. Therefore, <lb/>all I-policies with display latencies larger than 3 frame times cannot perform better than <lb/>the QM-120 policy (by our comparison rule). However, because gap rates for the I-<lb/>policies with larger display latency will decrease, these policies may be incomparable <lb/>with QM-120. Nevertheless, the QM-120 policy resulted in relatively low gap rates for <lb/>every run. <lb/>Overall then, we conclude that if our goal in managing delay jitter is to minimize display <lb/>latency with an acceptable gap frequency, then the QM-120 policy outperforms all I-<lb/>policies. For other goals, we make no conclusions. <lb/></body>

			<page>14 <lb/></page>

			<body>Run <lb/>I-Policy 2 <lb/>(I-2) <lb/>I-Policy 3 <lb/>(I-3) <lb/>E-Policy <lb/>QM <lb/>(QM-120) <lb/>QM <lb/>vs. <lb/>QM <lb/>vs. <lb/>QM <lb/>vs. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>I2 <lb/>I3 <lb/>E <lb/>1 <lb/>80 <lb/>0.1 <lb/>97 <lb/>0.1 <lb/>75 <lb/>0.2 <lb/>66 <lb/>0.3 <lb/>0 <lb/>+ <lb/>0 <lb/>2 <lb/>75 <lb/>0.5 <lb/>91 <lb/>0.3 <lb/>72 <lb/>0.5 <lb/>66 <lb/>0.6 <lb/>0 <lb/>+ <lb/>0 <lb/>3 <lb/>69 <lb/>3.6 <lb/>86 <lb/>2.8 <lb/>140 <lb/>0.9 <lb/>68 <lb/>1.4 <lb/>+ <lb/>+ <lb/>+ <lb/>4 <lb/>65 <lb/>0.7 <lb/>82 <lb/>0.4 <lb/>104 <lb/>0.6 <lb/>65 <lb/>0.6 <lb/>0 <lb/>+ <lb/>+ <lb/>5 <lb/>71 <lb/>0.6 <lb/>88 <lb/>0.4 <lb/>93 <lb/>0.5 <lb/>68 <lb/>0.5 <lb/>0 <lb/>+ <lb/>+ <lb/>6 <lb/>70 <lb/>0.3 <lb/>86 <lb/>0.2 <lb/>76 <lb/>0.4 <lb/>70 <lb/>0.5 <lb/>0 <lb/>+ <lb/>0 <lb/>7 <lb/>73 <lb/>2.9 <lb/>90 <lb/>1.6 <lb/>106 <lb/>1.2 <lb/>72 <lb/>1.9 <lb/>+ <lb/>+ <lb/>+ <lb/>8 <lb/>62 <lb/>5.1 <lb/>79 <lb/>2.4 <lb/>106 <lb/>0.9 <lb/>75 <lb/>1.3 <lb/>+ <lb/>+ <lb/>+ <lb/>9 <lb/>81 <lb/>23.0 <lb/>98 <lb/>12.6 <lb/>118 <lb/>2.8 <lb/>87 <lb/>7.6 <lb/>+ <lb/>+ <lb/>10 <lb/>70 <lb/>14.6 <lb/>87 <lb/>3.6 <lb/>113 <lb/>0.8 <lb/>78 <lb/>3.9 <lb/>+ <lb/>0 <lb/>11 <lb/>66 <lb/>25.2 <lb/>83 <lb/>6.9 <lb/>133 <lb/>1.4 <lb/>83 <lb/>4.8 <lb/>+ <lb/>12 <lb/>71 <lb/>9.6 <lb/>87 <lb/>3.4 <lb/>114 <lb/>0.9 <lb/>76 <lb/>2.7 <lb/>+ <lb/>0 <lb/>13 <lb/>67 <lb/>9.6 <lb/>84 <lb/>2.8 <lb/>96 <lb/>0.8 <lb/>72 <lb/>2.1 <lb/>+ <lb/>0 <lb/>14 <lb/>72 <lb/>15.1 <lb/>88 <lb/>3.9 <lb/>101 <lb/>1.1 <lb/>80 <lb/>3.9 <lb/>+ <lb/>0 <lb/>15 <lb/>76 <lb/>4.4 <lb/>92 <lb/>1.7 <lb/>117 <lb/>0.9 <lb/>79 <lb/>2.9 <lb/>+ <lb/>-<lb/>16 <lb/>68 <lb/>18.6 <lb/>85 <lb/>8.0 <lb/>114 <lb/>1.8 <lb/>80 <lb/>6.6 <lb/>+ <lb/>+ <lb/>17 <lb/>77 <lb/>22.0 <lb/>93 <lb/>12.1 <lb/>146 <lb/>1.8 <lb/>88 <lb/>7.5 <lb/>+ <lb/>+ <lb/>18 <lb/>76 <lb/>13.0 <lb/>93 <lb/>4.1 <lb/>131 <lb/>0.7 <lb/>85 <lb/>4.8 <lb/>+ <lb/>0 <lb/>19 <lb/>66 <lb/>5.0 <lb/>82 <lb/>1.3 <lb/>87 <lb/>0.9 <lb/>72 <lb/>1.8 <lb/>+ <lb/>0 <lb/>+ <lb/>20 <lb/>73 <lb/>11.3 <lb/>90 <lb/>4.1 <lb/>98 <lb/>1.6 <lb/>77 <lb/>3.7 <lb/>+ <lb/>0 <lb/>21 <lb/>70 <lb/>12.8 <lb/>87 <lb/>6.1 <lb/>159 <lb/>1.5 <lb/>76 <lb/>3.6 <lb/>+ <lb/>+ <lb/>22 <lb/>79 <lb/>1.4 <lb/>95 <lb/>0.5 <lb/>100 <lb/>0.6 <lb/>77 <lb/>1.0 <lb/>0 <lb/>+ <lb/>+ <lb/>23 <lb/>75 <lb/>0.4 <lb/>91 <lb/>0.2 <lb/>84 <lb/>0.4 <lb/>74 <lb/>0.6 <lb/>0 <lb/>+ <lb/>0 <lb/>24 <lb/>77 <lb/>39.6 <lb/>93 <lb/>15.0 <lb/>104 <lb/>1.8 <lb/>87 <lb/>5.2 <lb/>+ <lb/>+ <lb/>25 <lb/>65 <lb/>0.8 <lb/>81 <lb/>0.4 <lb/>66 <lb/>0.7 <lb/>65 <lb/>0.7 <lb/>0 <lb/>+ <lb/>0 <lb/>26 <lb/>64 <lb/>5.4 <lb/>81 <lb/>0.9 <lb/>122 <lb/>0.6 <lb/>69 <lb/>1.5 <lb/>+ <lb/>0 <lb/>+ <lb/>27 <lb/>70 <lb/>7.8 <lb/>87 <lb/>3.8 <lb/>107 <lb/>1.3 <lb/>73 <lb/>3.5 <lb/>+ <lb/>0 <lb/>28 <lb/>76 <lb/>0.3 <lb/>93 <lb/>0.2 <lb/>75 <lb/>0.3 <lb/>73 <lb/>0.4 <lb/>0 <lb/>+ <lb/>0 <lb/>QM Better <lb/>18 <lb/>18 <lb/>8 <lb/>QM Equivalent <lb/>9 <lb/>9 <lb/>6 <lb/>QM Worse <lb/>0 <lb/>1 <lb/>0 <lb/>Incomparable <lb/>1 <lb/>0 <lb/>14 <lb/>Figure 6: Results for all runs. <lb/>With respect to the E-policy, the QM-120 policy performs as well or better on 14 runs <lb/>and is incomparable on 14 runs. The runs in which QM-120 performs as well or better <lb/>tend to be the runs with low delay jitter (i.e., the early morning and late evening runs), <lb/>while the incomparable runs are those with high delay jitter. In every run, QM-120 <lb/>resulted in smaller display latency and a somewhat higher gap rate (5.2 gaps per minute is <lb/>the largest difference). So again we conclude that if the goal in managing delay jitter is to <lb/>minimize display latency with an acceptable gap frequency, then the QM-120 policy <lb/>outperforms the E-policy. For other goals, we make no conclusions. <lb/>In the second part of the study, we investigated the effect of varying the threshold <lb/>parameter on the effectiveness of the queue monitoring policy. For each of the 28 runs, <lb/>we simulated the queue monitoring policy with a single threshold defined for all queue <lb/></body>

			<page>15 <lb/></page>

			<body>Run <lb/>QM (30) <lb/>QM (60) <lb/>QM (120) <lb/>QM (600) <lb/>QM (3600) <lb/>120 120 120 120 <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>vs. <lb/>30 <lb/>vs. <lb/>60 <lb/>vs. <lb/>600 <lb/>vs. <lb/>3600 <lb/>1 <lb/>64 <lb/>0.3 <lb/>65 <lb/>0.3 <lb/>66 <lb/>0.3 <lb/>73 <lb/>0.3 <lb/>75 <lb/>0.2 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>65 <lb/>0.7 <lb/>65 <lb/>0.7 <lb/>66 <lb/>0.6 <lb/>66 <lb/>0.6 <lb/>67 <lb/>0.6 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>3 <lb/>67 <lb/>1.7 <lb/>67 <lb/>1.4 <lb/>68 <lb/>1.4 <lb/>74 <lb/>1.4 <lb/>103 <lb/>1.1 <lb/>0 <lb/>0 <lb/>0 <lb/>+ <lb/>4 <lb/>65 <lb/>0.6 <lb/>65 <lb/>0.6 <lb/>65 <lb/>0.6 <lb/>69 <lb/>0.6 <lb/>83 <lb/>0.6 <lb/>0 <lb/>0 <lb/>0 <lb/>+ <lb/>5 <lb/>67 <lb/>0.5 <lb/>68 <lb/>0.5 <lb/>68 <lb/>0.5 <lb/>69 <lb/>0.5 <lb/>81 <lb/>0.5 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>6 <lb/>70 <lb/>0.5 <lb/>70 <lb/>0.5 <lb/>70 <lb/>0.5 <lb/>70 <lb/>0.5 <lb/>76 <lb/>0.4 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>7 <lb/>70 <lb/>2.3 <lb/>71 <lb/>1.9 <lb/>72 <lb/>1.9 <lb/>77 <lb/>1.7 <lb/>95 <lb/>1.4 <lb/>0 <lb/>0 <lb/>0 <lb/>+ <lb/>8 <lb/>68 <lb/>2.0 <lb/>70 <lb/>1.5 <lb/>75 <lb/>1.3 <lb/>83 <lb/>1.0 <lb/>97 <lb/>1.0 <lb/>0 <lb/>0 <lb/>0 <lb/>+ <lb/>9 <lb/>77 <lb/>13.1 <lb/>83 <lb/>9.0 <lb/>87 <lb/>7.6 <lb/>102 <lb/>4.9 <lb/>117 <lb/>3.0 <lb/>+ <lb/>+ <lb/>-<lb/>10 <lb/>72 <lb/>6.6 <lb/>75 <lb/>5.0 <lb/>78 <lb/>3.9 <lb/>89 <lb/>1.6 <lb/>98 <lb/>1.0 <lb/>+ <lb/>+ <lb/>-<lb/>11 <lb/>72 <lb/>8.3 <lb/>76 <lb/>6.3 <lb/>83 <lb/>4.8 <lb/>98 <lb/>3.4 <lb/>124 <lb/>1.7 <lb/>+ <lb/>+ <lb/>12 <lb/>72 <lb/>5.3 <lb/>74 <lb/>3.3 <lb/>76 <lb/>2.7 <lb/>86 <lb/>1.9 <lb/>103 <lb/>1.2 <lb/>+ <lb/>0 <lb/>0 <lb/>13 <lb/>69 <lb/>3.5 <lb/>70 <lb/>2.7 <lb/>72 <lb/>2.1 <lb/>82 <lb/>1.4 <lb/>91 <lb/>1.0 <lb/>+ <lb/>0 <lb/>0 <lb/>14 <lb/>74 <lb/>6.7 <lb/>76 <lb/>6.0 <lb/>80 <lb/>3.9 <lb/>92 <lb/>1.8 <lb/>99 <lb/>1.2 <lb/>+ <lb/>+ <lb/>-<lb/>15 <lb/>76 <lb/>3.1 <lb/>77 <lb/>3.1 <lb/>79 <lb/>2.9 <lb/>89 <lb/>1.7 <lb/>106 <lb/>1.2 <lb/>0 <lb/>0 <lb/>-<lb/>16 <lb/>71 <lb/>8.6 <lb/>74 <lb/>7.7 <lb/>80 <lb/>6.6 <lb/>96 <lb/>2.9 <lb/>112 <lb/>1.9 <lb/>+ <lb/>+ <lb/>17 <lb/>79 <lb/>10.0 <lb/>83 <lb/>8.9 <lb/>88 <lb/>7.5 <lb/>106 <lb/>3.9 <lb/>131 <lb/>2.2 <lb/>+ <lb/>+ <lb/>18 <lb/>78 <lb/>7.5 <lb/>81 <lb/>6.2 <lb/>85 <lb/>4.8 <lb/>100 <lb/>2.5 <lb/>123 <lb/>1.0 <lb/>+ <lb/>+ <lb/>19 <lb/>68 <lb/>3.0 <lb/>69 <lb/>2.3 <lb/>72 <lb/>1.8 <lb/>80 <lb/>1.1 <lb/>86 <lb/>0.9 <lb/>+ <lb/>0 <lb/>0 <lb/>0 <lb/>20 <lb/>74 <lb/>5.0 <lb/>74 <lb/>4.3 <lb/>77 <lb/>3.7 <lb/>85 <lb/>2.3 <lb/>94 <lb/>1.7 <lb/>+ <lb/>0 <lb/>-<lb/>21 <lb/>72 <lb/>5.0 <lb/>73 <lb/>4.4 <lb/>76 <lb/>3.6 <lb/>88 <lb/>2.4 <lb/>121 <lb/>1.6 <lb/>+ <lb/>0 <lb/>-<lb/>22 <lb/>70 <lb/>1.6 <lb/>72 <lb/>1.3 <lb/>77 <lb/>1.0 <lb/>79 <lb/>0.9 <lb/>90 <lb/>0.7 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>23 <lb/>73 <lb/>0.6 <lb/>74 <lb/>0.6 <lb/>74 <lb/>0.6 <lb/>74 <lb/>0.6 <lb/>81 <lb/>0.5 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>24 <lb/>81 <lb/>8.5 <lb/>83 <lb/>6.4 <lb/>87 <lb/>5.2 <lb/>97 <lb/>2.7 <lb/>104 <lb/>1.9 <lb/>+ <lb/>+ <lb/>-<lb/>25 <lb/>65 <lb/>0.7 <lb/>65 <lb/>0.7 <lb/>65 <lb/>0.7 <lb/>66 <lb/>0.7 <lb/>66 <lb/>0.7 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>26 <lb/>66 <lb/>1.7 <lb/>67 <lb/>1.5 <lb/>69 <lb/>1.5 <lb/>79 <lb/>0.9 <lb/>94 <lb/>0.6 <lb/>0 <lb/>0 <lb/>0 <lb/>+ <lb/>27 <lb/>70 <lb/>3.9 <lb/>71 <lb/>3.5 <lb/>73 <lb/>3.5 <lb/>82 <lb/>2.9 <lb/>101 <lb/>1.6 <lb/>0 <lb/>0 <lb/>0 <lb/>28 <lb/>73 <lb/>0.4 <lb/>73 <lb/>0.4 <lb/>73 <lb/>0.4 <lb/>73 <lb/>0.4 <lb/>74 <lb/>0.4 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>QM-120 Better <lb/>13 8 <lb/>0 <lb/>5 <lb/>QM-120 Equivalent <lb/>15 20 17 <lb/>9 <lb/>QM-120 Worse <lb/>0 <lb/>0 <lb/>7 <lb/>0 <lb/>Incomparable <lb/>0 <lb/>0 <lb/>4 <lb/>14 <lb/>Figure 7: Effect of varying thresholds. <lb/>lengths. We considered thresholds of 30 frame times (1/2 second), 60 frame times (1 <lb/>second), 120 frame times (2 seconds), 600 frame times (10 seconds), and 3600 frame <lb/>times (1 minute). Figure 7 shows the simulation results. In the figure, these policies are <lb/>labeled QM-30, QM-60, QM-120, QM-600, and QM-3600 respectively. The rightmost <lb/>columns show the comparison between QM-120 and the other policies. <lb/>Figure 7 shows that QM-600 performs best relative to QM-120. On 24 runs, QM-600 is <lb/>judged to have provided better or equivalent performance. On the other four <lb/>incomparable runs, QM-120 produced slightly better average display latency while QM-<lb/>600 produced approximately one-third to one-quarter the number of gaps. QM-120 <lb/>performed as well or better than QM-30 and QM-60 on every run. Finally, QM-120 <lb/>performed as well or better than QM-3600 on half the runs and was incomparable on the <lb/>remainder. <lb/></body>

			<page>16 <lb/></page>

			<body>It is interesting to compare the performance of QM-30 and QM-60 with that of the I-2 <lb/>policy. The QM-30 policy and QM-60 policy produced average display latencies that <lb/>were similar to or better than those produced by I-2. Furthermore, both produced similar <lb/>or better gap rates. For example, on run 9, the I-2 policy produced a display latency of <lb/>81 ms. with a gap rate of 23.0 gaps/min. The QM-60 policy produced a display latency <lb/>of 83 ms. with a gap rate of 9.0 gaps/min. Thus, by choosing an appropriate threshold <lb/>value, we can use queue monitoring to produce behavior similar to that produced by an I-<lb/>policy, but with better performance on runs where an I-policy behaves badly. <lb/>The results for the QM-3600 policy are also interesting. In general, the QM-3600 policy <lb/>produced results similar to those produced by the E-policy, with a slightly lower average <lb/>display latency and a slightly higher gap rate. Thus, the QM-3600 policy is <lb/>incomparable with the QM-120 policy on exactly the same runs that the E-policy was <lb/>incomparable with the QM-120 policy. Again, by choosing an appropriate threshold <lb/>value, we can use queue monitoring to produce behavior similar to that produced by the <lb/>E-policy, but with better performance on runs where the E-policy behaves badly. <lb/>From these results, we conclude that thresholds are a useful tunable parameter for the <lb/>general queue monitoring policy. A range of thresholds produces a range of results, from <lb/>low-latency with many gaps to high-latency with few gaps. For our comparison rule and <lb/>for the threshold values we examined, a threshold of 10 seconds for all queue lengths <lb/>performs best. Given a particular network environment and a particular comparison rule, <lb/>it should be possible to find an optimal threshold value. <lb/>The final part of the study examined the performance of the general queue monitoring <lb/>policy. Recall that individual thresholds can be defined for each queue length, whereas in <lb/>the previous simulations we used the same threshold value for all queue lengths. These <lb/>thresholds can be arbitrary, but for purposes of this study, we defined a particular rule <lb/>for setting the threshold values. This rule has two parameters: a threshold value for a <lb/>queue of length 3, referred to as the base threshold, and a decay factor which specifies a <lb/>rate at which the thresholds decrease with increasing queue length. For example, a queue <lb/>monitoring policy with a base threshold of 3600 and a decay factor of 2 would have the <lb/>threshold values: 3600 for queues of length 3, 1800 for queues of length 4, 900 for queues <lb/>of length 5, etc. In Figure 8, we show the results for three policies: base threshold 120 <lb/>with decay factor 2, base threshold 600 with decay factor 2, and base threshold 3600 with <lb/>decay factor 2. In the figure these are labeled QM-(120,2), QM-(600,2) and QM-<lb/>(3600,2). The figure also shows the QM-120 policy which is the base for comparison <lb/>with the other policies. <lb/>With a base threshold of 120 frame times, decreasing the thresholds for longer queue <lb/>lengths did not improve the QM-120 policy. One run (run 24) was worse, and the runs <lb/>that were equivalent according to the comparison rule generally had comparable latencies <lb/>and slightly worse gap rates. On the other hand, for a base threshold of 600 frame times, <lb/>decreasing the thresholds did improve the QM-600 policy. Relative to the QM-120 <lb/>policy, QM-(600,2) performed as well or better on every run. Furthermore, it performed <lb/>better than QM-120 on 11 runs, where QM-600 only performed better than QM-120 on <lb/></body>

			<page>17 <lb/></page>

			<body>Run <lb/>QM (120) <lb/>QM (120,2) <lb/>QM (600,2) <lb/>QM (3600,2) <lb/>120 <lb/>120 <lb/>120 <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>Latency <lb/>ms. <lb/>Gaps <lb/>/min. <lb/>vs. <lb/>120,2 <lb/>vs. <lb/>600,2 <lb/>vs. <lb/>3600,2 <lb/>1 <lb/>66 <lb/>0.3 <lb/>66 <lb/>0.3 <lb/>73 <lb/>0.3 <lb/>75 <lb/>0.2 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>66 <lb/>0.6 <lb/>66 <lb/>0.6 <lb/>66 <lb/>0.6 <lb/>67 <lb/>0.6 <lb/>0 <lb/>0 <lb/>0 <lb/>3 <lb/>68 <lb/>1.4 <lb/>67 <lb/>1.6 <lb/>68 <lb/>1.4 <lb/>78 <lb/>1.4 <lb/>0 <lb/>0 <lb/>0 <lb/>4 <lb/>65 <lb/>0.6 <lb/>65 <lb/>0.6 <lb/>68 <lb/>0.6 <lb/>82 <lb/>0.6 <lb/>0 <lb/>0 <lb/>+ <lb/>5 <lb/>68 <lb/>0.5 <lb/>68 <lb/>0.5 <lb/>68 <lb/>0.5 <lb/>72 <lb/>0.5 <lb/>0 <lb/>0 <lb/>0 <lb/>6 <lb/>70 <lb/>0.5 <lb/>70 <lb/>0.5 <lb/>70 <lb/>0.5 <lb/>76 <lb/>0.4 <lb/>0 <lb/>0 <lb/>0 <lb/>7 <lb/>72 <lb/>1.9 <lb/>71 <lb/>1.9 <lb/>72 <lb/>1.8 <lb/>82 <lb/>1.5 <lb/>0 <lb/>0 <lb/>0 <lb/>8 <lb/>75 <lb/>1.3 <lb/>74 <lb/>1.5 <lb/>79 <lb/>1.0 <lb/>89 <lb/>1.0 <lb/>0 <lb/>0 <lb/>0 <lb/>9 <lb/>87 <lb/>7.6 <lb/>85 <lb/>8.5 <lb/>97 <lb/>5.7 <lb/>113 <lb/>3.2 <lb/>0 <lb/>-<lb/>10 <lb/>78 <lb/>3.9 <lb/>78 <lb/>4.2 <lb/>88 <lb/>1.7 <lb/>97 <lb/>1.0 <lb/>0 <lb/>-<lb/>11 <lb/>83 <lb/>4.8 <lb/>81 <lb/>5.2 <lb/>91 <lb/>3.6 <lb/>110 <lb/>2.1 <lb/>0 <lb/>-<lb/>12 <lb/>76 <lb/>2.7 <lb/>75 <lb/>2.8 <lb/>82 <lb/>2.0 <lb/>94 <lb/>1.3 <lb/>0 <lb/>0 <lb/>13 <lb/>72 <lb/>2.1 <lb/>72 <lb/>2.1 <lb/>81 <lb/>1.4 <lb/>91 <lb/>1.0 <lb/>0 <lb/>0 <lb/>14 <lb/>80 <lb/>3.9 <lb/>79 <lb/>4.0 <lb/>90 <lb/>2.0 <lb/>99 <lb/>1.2 <lb/>0 <lb/>-<lb/>15 <lb/>79 <lb/>2.9 <lb/>78 <lb/>2.9 <lb/>85 <lb/>1.8 <lb/>100 <lb/>1.3 <lb/>0 <lb/>-<lb/>16 <lb/>80 <lb/>6.6 <lb/>77 <lb/>7.1 <lb/>90 <lb/>3.6 <lb/>106 <lb/>2.1 <lb/>0 <lb/>-<lb/>17 <lb/>88 <lb/>7.5 <lb/>82 <lb/>8.2 <lb/>95 <lb/>5.5 <lb/>119 <lb/>2.9 <lb/>0 <lb/>-<lb/>18 <lb/>85 <lb/>4.8 <lb/>84 <lb/>4.9 <lb/>98 <lb/>2.7 <lb/>120 <lb/>1.1 <lb/>0 <lb/>-<lb/>19 <lb/>72 <lb/>1.8 <lb/>72 <lb/>1.8 <lb/>79 <lb/>1.1 <lb/>83 <lb/>0.9 <lb/>0 <lb/>0 <lb/>0 <lb/>20 <lb/>77 <lb/>3.7 <lb/>76 <lb/>3.7 <lb/>85 <lb/>2.3 <lb/>94 <lb/>1.7 <lb/>0 <lb/>-<lb/>21 <lb/>76 <lb/>3.6 <lb/>74 <lb/>3.9 <lb/>82 <lb/>2.6 <lb/>98 <lb/>1.8 <lb/>0 <lb/>-<lb/>22 <lb/>77 <lb/>1.0 <lb/>77 <lb/>1.0 <lb/>78 <lb/>0.9 <lb/>86 <lb/>0.8 <lb/>0 <lb/>0 <lb/>0 <lb/>23 <lb/>74 <lb/>0.6 <lb/>74 <lb/>0.6 <lb/>74 <lb/>0.6 <lb/>81 <lb/>0.5 <lb/>0 <lb/>0 <lb/>0 <lb/>24 <lb/>87 <lb/>5.2 <lb/>83 <lb/>6.3 <lb/>90 <lb/>4.0 <lb/>100 <lb/>2.2 <lb/>+ <lb/>-<lb/>-<lb/>25 <lb/>65 <lb/>0.7 <lb/>65 <lb/>0.7 <lb/>66 <lb/>0.7 <lb/>66 <lb/>0.7 <lb/>0 <lb/>0 <lb/>0 <lb/>26 <lb/>69 <lb/>1.5 <lb/>69 <lb/>1.5 <lb/>78 <lb/>1.0 <lb/>89 <lb/>0.6 <lb/>0 <lb/>0 <lb/>+ <lb/>27 <lb/>73 <lb/>3.5 <lb/>72 <lb/>3.5 <lb/>78 <lb/>3.1 <lb/>92 <lb/>2.0 <lb/>0 <lb/>0 <lb/>28 <lb/>73 <lb/>0.4 <lb/>73 <lb/>0.4 <lb/>73 <lb/>0.4 <lb/>74 <lb/>0.4 <lb/>0 <lb/>0 <lb/>0 <lb/>QM-120 Better <lb/>1 <lb/>0 <lb/>2 <lb/>QM-120 Equivalent <lb/>27 <lb/>17 <lb/>12 <lb/>QM-120 Worse <lb/>0 <lb/>11 <lb/>1 <lb/>Incomparable <lb/>0 <lb/>0 <lb/>13 <lb/>Figure 8: Effect of multiple thresholds. <lb/>7 runs. In general, QM-(600,2) produced lower display latencies than QM-600 with only <lb/>slightly higher gap rates. Decreasing the thresholds also helped to improve the <lb/>performance of the QM-3600 policy. On a number of runs, the QM-(3600,2) policy <lb/>produced significantly lower display latencies than those produced by QM-3600 with <lb/>only slightly higher gap rates. Relative to the QM-120 policy, QM-(3600,2) performed <lb/>somewhat better than QM-3600. <lb/>From these results, we conclude that the principle of decreasing thresholds has some <lb/>utility. For low base thresholds, decreasing thresholds does not help, and may even hurt <lb/>performance. But for higher base thresholds, decreasing the thresholds seems to help a <lb/>great deal for some network conditions, without adversely affecting performance for other <lb/>network conditions. <lb/></body>

			<page>18 <lb/></page>

			<body>6. Summary and Conclusions <lb/>If we wish to support computer-based video conferences transmitted over interconnected <lb/>local-area networks based on ethernets and token rings, we must address the effect of <lb/>variable network transmission delays (i.e., delay jitter) on the perceived quality of audio <lb/>and video playout. The fundamental effect of delay jitter is to necessitate a tradeoff <lb/>between display with low latency and display with few gaps. We have presented a new <lb/>policy for managing this tradeoff called queue monitoring. This policy operates by <lb/>observing delay jitter over time and dynamically adjusting display latency in order to <lb/>support low-latency conferences with an acceptable gap rate. Overall, we conclude that <lb/>queue monitoring can be an effective policy for ameliorating the effect of delay jitter in a <lb/>building-sized internetwork. <lb/>In this paper, queue monitoring was evaluated by comparing it with two policies from the <lb/>literature: the I-policy and the E-policy. The performance of these policies was <lb/>evaluated by recording the end-to-end delays experienced by audio frames during video <lb/>conferences run over our building network and simulating the effect of each policy on the <lb/>resulting trace. In these conferences, the end-to-end delays experienced by most audio <lb/>frames were in the range of 35-40 ms (including acquisition and processing time as well as <lb/>network transmission time), but the variation in end-to-end delays was as much as 200 <lb/>ms. Furthermore, even at times of day when there was little other activity on the <lb/>network, end-to-end delays varied by as much as 80 ms. Over this range of network <lb/>loads, queue monitoring consistently performed as well or better than either the I-policy <lb/>or the E-policy. Our results have shown that queue monitoring with a base threshold of <lb/>600 frame times (10 seconds) and a decay factor of 2 resulted in the best performance. <lb/>Two runs best illustrate the advantages of the queue monitoring policy over the E-policy <lb/>and the I-policy. Run 3 is an example of a run on which the E-policy performed poorly, <lb/>producing an average display latency of 140 ms. with a gap rate of 0.9 gaps/minute. In <lb/>contrast, the QM-(600,2) policy produced an average display latency of 68 ms. with a <lb/>gap rate of 1.4 gaps/minute. Run 24 is an example of a run on which the I-3 (and the <lb/>other I-policies) performed poorly, producing an average display latency of 93 ms. with a <lb/>gap rate of 15.0 gaps/minute. For this run, QM-(600,2) produced an average display <lb/>latency of 90 ms. with a gap rate of 4.0 gaps/minute. Together, these examples show that <lb/>the queue monitoring policy can adapt to many network conditions to produce good <lb/>quality playout. <lb/>Queue monitoring is also a flexible and tunable policy. Experiments with a range of <lb/>thresholds produced a range of behaviors, from low display latency with frequent gaps, to <lb/>high display latency with few gaps. As a result, we believe that queue monitoring can be <lb/>tuned to meet the requirements of particular applications and to adapt to the demands of <lb/>particular network environments. In addition, queue monitoring has a simple and efficient <lb/>implementation. A particularly important feature of this implementation is that it <lb/>operates without feedback from the audio or video source and without synchronized <lb/>clocks. Thus queue monitoring can be used effectively in computer-based conferences <lb/>with many participants each of which may experience differing network delays. <lb/></body>

			<page>19 <lb/></page>

			<body>There are a number of issues still to be addressed in evaluating the queue monitoring <lb/>policy. First, in this paper we have compared queue monitoring to the I-policy in which <lb/>the display latency remains fixed over an entire run. An alternative would be an I-policy <lb/>for which we choose a new display latency at regular intervals, perhaps based on <lb/>observations of delay jitter in the recent past. <lb/>Second, we have compared policies using average display latency and average gap rate. <lb/>There are many other factors that can influence perceived quality including the <lb/>distribution of gaps throughout a conference, the number of display latency changes, and <lb/>the distribution of periods of high and low display latency throughout a conference. <lb/>Ideally, policies should be compared using a more general measure of audio and video <lb/>quality which accounts for all the factors which determine perceived quality. <lb/>Third, the study presented in this paper is based on audio and video data transmitted over <lb/>a building-area network. We are also interested in determining the extent to which the <lb/>queue monitoring technique scales. Future work will include repeating the study in this <lb/>paper for a succession of larger networks. Such a study will help to identify the types of <lb/>networks which can be supported without resorting to new specialized network services. <lb/>Fourth, the emphasis in this work has been on managing delay jitter. Another aspect of <lb/>our work is a forward error correction (FEC) strategy for minimizing frame loss [4]. By <lb/>decreasing loss, FEC will support the assumptions underlying queue monitoring, namely <lb/>that display queue length only decreases as a result of late arrivals. However, frames that <lb/>are recovered through FEC will incur longer end-to-end delays, thus increasing delay jitter. <lb/>Because of these effects, we must investigate the impact of FEC and other mechanisms <lb/>for reducing loss (e.g., timeouts and retransmission) on queue monitoring and the other <lb/>policies. <lb/>Fifth, work must be done on choosing good threshold values, but this work will require a <lb/>new quality measure. Simulation using a variety of threshold values indicates that large <lb/>changes in threshold values may only produce small changes in average display latency <lb/>and average gap rate. As such, work on choosing threshold values will involve making <lb/>tradeoffs which result in small changes to display latency and gap rate. While simple <lb/>measures of quality may be sufficient to evaluate the gross performance characteristics of <lb/>a policy, proper evaluation of small tradeoffs will require a better quality measure than <lb/>we currently have available. <lb/>Finally, the use of decreasing thresholds for longer queue lengths must be investigated <lb/>further. While the study has shown that the principle of decreasing thresholds can <lb/>improve performance, it is not yet clear what strategy should be used to set those <lb/>thresholds. Among the possibilities are the geometric decrease in threshold values we <lb/>used in this study, and a linear decrease in threshold values suggested by Jones and <lb/>Hopper [5]. <lb/></body>

			<page>20 <lb/></page>

			<listBibl>7. References <lb/>[1] Ferrari, D., ÒDelay Jitter Control Scheme For Packet-Switching Internetworks,Ó <lb/>Computer Communications, Vol. 15, No. 6 (July/August 1992), pp. 367-373. <lb/>[2] Issacs, E., Tang, J.C., ÒWhat Video Can and CanÕt Do for Collaboration: A Case <lb/>Study,Ó Proc. AC M Intl. Conf. on Multimedia, Anaheim, CA, August 1993, pp. <lb/>199-205. <lb/>[3] Jeffay, K., Stone, D.L., Smith, F.D., ÒKernel Support for Live Digital Audio and <lb/>Video,Ó Computer Communications, Vol. 15, No. 6 (July/August 1992), pp. 388-<lb/>395. <lb/>[4] Jeffay, K., Stone, D.L., Smith, F.D., ÒTransport and Display Mechanisms for <lb/>Multimedia Conferencing Across Packet-Switched Networks,Ó Computer Networks <lb/>and ISDN Systems, Vol. 26, No. 10 (July 1994), pp. 1281-1304. <lb/>[5] Jones, A., Hopper, A., ÒHandling Audio and Video Streams in a Distributed <lb/>Environment,Ó Proc. ACM Symp. on Operating Systems Principles, Asheville, NC, <lb/>December 1993, ACM Operating Systems Review, Vol. 27, No. 5, pp. 231-243. <lb/>[6] Naylor, W.E., Kleinrock, L., ÒStream Traffic Communication in Packet-Switched <lb/>Networks: Destination Buffering Considerations,Ó IEEE Trans. on <lb/>Communications, Vol. COM-30, No. 12 (December 1982), pp. 2527-2534. <lb/>[7] Schulzrinne, H., ÒVoice Communication Across the Internet: A Network Voice <lb/>Terminal,Ó Technical Report, Univ. of Massachusetts 1992. </listBibl>


	</text>
</tei>
