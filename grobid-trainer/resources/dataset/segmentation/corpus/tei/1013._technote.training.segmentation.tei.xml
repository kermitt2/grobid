<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Multiple Retrieval Models and Regression Models <lb/>for Prior Art Search <lb/>Patrice Lopez * and Laurent Romary † <lb/>Humboldt Universität zu Berlin -Institut für Deutsche Sprache und Linguistik <lb/> † INRIA -Gemo Research Group <lb/> * European Patent Office, Berlin <lb/>patrice lopez@hotmail.com laurent.romary@loria.fr <lb/>Abstract <lb/>This paper presents the system called PATATRAS (PATent and Article Tracking, <lb/>Retrieval and AnalysiS) realized at the Humboldt University for the IP track of CLEF <lb/>2009. Our approach presents three main characteristics: <lb/>1. The usage of multiple retrieval models (KL, Okapi) and term index definitions <lb/>(lemma, phrase, concept) for the three languages considered in the present track <lb/>(English, French, German) producing ten different sets of ranked results. <lb/>2. The merging of the different results based on multiple regression models using an <lb/>additional validation set created from the patent collection. <lb/>3. The exploitation of patent metadata and of the citation structures for creating <lb/>restricted initial working sets of patents and for producing a final re-ranking <lb/>regression model. <lb/>As we exploit specific metadata of the patent documents and the citation relations <lb/>only at the creation of initial working sets and during the final post ranking step, our <lb/>architecture remains generic and easy to extend. <lb/>Categories and Subject Descriptors <lb/>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Infor-<lb/>mation Search and Retrieval; H.3.4 Systems and Software; H.3.7 Digital Libraries; H.2.3 [Database <lb/>Managment]: Languages-Query Languages <lb/>General Terms <lb/>Measurement, Performance, Experimentation <lb/>Keywords <lb/>Patent, Prior Art Search, Multilinguality, Regression models, Re-ranking <lb/></front>

			<body>1 Motivation <lb/>Our participation to the CLEF IP track was first motivated by our interest in infrastructures <lb/>for technical and scientific literature in general. A large collection of patent publications offers an <lb/>excellent opportunity of experimentation. With several millions of documents, such a collection <lb/>first corresponds to a realistic volume of documents comparable with the largest existing article <lb/> repositories. Patents cover multiple technical and scientific domains while providing rich cross-<lb/>disciplinary relations. This level of complexity regarding multiple thematics is similar, for instance, <lb/>to a large scale repository such as HAL (Hyper Article en Ligne) [11]. In addition, the European <lb/>Patent (EP) publications present a quite unique multilingual dimension, often combining three <lb/>languages (English, French and German) in the same publication (title and claims). Finally, <lb/>patents can be qualified as extreme exemples of noisy, deliberatly vague and misleading wordings <lb/>for the title, abstract and claims parts while maintaining relatively standard technical terminologies <lb/>in the description bodies. <lb/>Our second motivation was to experiment a few fundamental approaches that we consider <lb/>central and constant for any technical and scientific collections, namely first the exploitation of rich <lb/>terminological information and natural language processing techniques, second the exploitation <lb/>of the relations among citations and, third, the exploitation of machine learning for improving <lb/>retrieval and classification results. Obviously none of these points is original, but we believe that <lb/>their appropriate combination can provide a framework that would provide much more than the <lb/>sum of these individual parts. <lb/>Last, we consider that the efficient exploitation and dissemination of patent information are <lb/>currently not satisfactory. While such services as Google Patent and SumoBrain have certainly <lb/>improved this aspect, patent information is still very difficult to access and exploit as technical <lb/>documentation. The applicants have usually no interest to disclose their invention in a manner that <lb/>could facilitate its dissimination. The patent document itself is often poorly structured and follow <lb/>only a minimal review during its examination focused on the claims and legal aspects. However, <lb/>a clear disclosure of an invention is the counterpart toward the public of patent monopoly rights. <lb/>A patent that is impossible to retrieve is in practice a failure of the patent &quot;contract&quot;. This is not <lb/>desirable because it is directed against the public and against the goal of a patent system, which <lb/>is first to motivate and encourage research and innovation. Economic and commercial incentives <lb/>are not the only factor for boosting invention and innovation, openly exchanged knowledge is the <lb/>source and the lifeblood of research and new ideas. For this purpose, better tools for searching and <lb/>discovering technical and scientific information are also desirable for patent information, beyond <lb/>pure economic aspects. <lb/>2 CLEF IP 2009 and the Prior Art Task <lb/>Before starting to describe our system, we introduce some basic definitions and examine in more <lb/>detail the task and how it defers from a standard prior art search for patent examination. We also <lb/>discuss the main differences between a patent document and traditional documents considered in <lb/>usual text retrieval tasks. <lb/>In the following description, the &quot;collection&quot; refers to the data collection of approx. 1,9 millions <lb/>documents corresponding to 1 million European Patents. This collection represents the prior art. <lb/>The &quot;training set&quot; refers to the 500 documents of &quot;training topics&quot; provided with judgements (the <lb/>relevant patents to be retrieved). The &quot;validation set&quot; corresponds to a subset of approx. 4000 <lb/>patents selected by us from the collection and a &quot;patent topic&quot; refers to the patent for which the <lb/>prior art search is done. <lb/>2.1 Prior Art Searches <lb/>The goal of the CLEF IP 2009 track was to identify in the collection the closest prior art to a <lb/>given patent. The evaluation was produced automatically using patent citations introduced during <lb/>the official prosecution of this patent application at the European Patent Office (EPO). The list <lb/>of patent citations, therefore, gathers the patent citations provided by the applicant himself, the <lb/>result of the prior art search performed by the patent examiner, and patent citations introduced <lb/>at a later stage of proceedings (examination and possibly opposition). <lb/>The prior art search as implemented in the CLEF IP 2009 track can be considered globally as <lb/>easier than a real one. The usual starting point of a patent examiner performing a prior art task <lb/>is a set of application documents in only one language, with one or more IPC 1 classes and with <lb/>a very broad set of claims. In the present task, the topic patents were entirely made of examined <lb/>granted patents providing reliable information resulting normally from the search phase: <lb/>• The ECLA 2 classes. They corresponds to a fine classification used for the search, more <lb/>precise than the IPC. <lb/>• The claims of the granted patents. These final claims are drafted taking into account the <lb/>prior art identified in the search report and are often revised for removing clarity issues. In <lb/>addition, this final version of the claims is translated in the three official languages of the <lb/>EPO by a skilled human translator, making crosslingual IR techniques more reliable. <lb/>• A revised description. The first part of the description often acknowledges the most impor-<lb/>tant document of the prior art which has been identified during the search phase. <lb/>While such information alone is not sufficient to perform reliable prior art analysis, it can be <lb/>combined to more standard text retrieval techniques in order to achieve higher accuracy. <lb/>It is also important to stress that other aspects make the task more difficult than a prior art <lb/>search based on a fresh patent application. An examiner can exploit some very useful information <lb/>as the patent families which permit to relate patents from different patent systems (PCT, US, <lb/>etc.). The examiner can also search non EP publications and get search report from other patent <lb/>offices in case the application has already been searched elsewhere. Moreover, as the evaluation is <lb/>made on the basis of the patents cited in the EPO search reports and cited during examination and <lb/>possible opposition phases, the list of relevant documents is by nature partial and often motivated <lb/>by procedural purposes. Typically an examiner stops his research when he has found a very good <lb/>X document that will be used to refuse an application. The goal of a patent examiner is not really <lb/>to search the best of all relevant documents, but a subset or a combination that will support his <lb/>argumentation during the examination phase. Very good prior art documents that are difficult to <lb/>find will often not appear in the search report and a very good automatic retrieval system that <lb/>could present this document highly ranked will not been rewarded. In addition, as non patent <lb/>literature is not considered in this task, the collection and the evaluation can be viewed as biaised: <lb/>when an examiner finds one or two good non patent documents, he may limit his further search in <lb/>the patent database to complementary documents covering minor embodiment aspects. It must <lb/>be noted that the non-patent documents (mainly journal and conference articles) represent at the <lb/>EPO more than 30% of the citations in several technical domains, for instance more than 60% in <lb/>biotechnologies. These different aspects make the final results relatively difficult to generalize to <lb/>a standard prior art search. <lb/>The task remains, however, a good approximation and we consider that all the techniques <lb/>presented in this work remain pertinent for standard prior art searches. In addition, these condi-<lb/>tions fit well an &quot;invalidity search&quot; made by a third party after the grant of patent for engaging <lb/>a possible opposition. Finally, the task is also relevant for the purpose of a technical survey, from <lb/>the point of view of the scientist or the engineer willing to evaluate the novelty and inventiveness <lb/>of a work in term of the patent law. <lb/>2.2 The limits of the textual content <lb/>The textual content of patent documents is known to be difficult to process with traditional text <lb/>processing and text retrieval techniques. As pointed out by [6], patents often make use of non <lb/>standard terminology, vague terms and legalistic language. The claims are usually written in <lb/>a very different style than the description. The description also frequently contains digressions <lb/>and general presentations of the technical fields which do not provide any useful information <lb/></body>

			<note place="footnote">1 International Patent Classification: a hierarchical classification of approx. 60.000 subdivisions used by all <lb/>patent offices and maintained by the WIPO (World Intellectual Property Organization). <lb/></note>

			<note place="footnote">2 European CLAssification: an extension of the IPC corresponding to a hierarchical classification of approx. 135 <lb/>600 subdivisions, about 66 000 more than the IPC. <lb/></note>

			<body>about the contribution of the patent. A patent also contains non-linguistic material that could <lb/>be important: tables, mathematical and chemical formulas, citations, technical drawing, etc. For <lb/>so called drawing-oriented fields (such as mecanics), examiners focus their first attention only on <lb/>drawings and we can suppose that any automatic retrieval based only on text will fail. <lb/>So one could challenge the relevance of any standard technical vocabulary for searching patent <lb/>documents. However, the description, after a general presentation of the state of the art, illustrates <lb/>the claimed &quot;invention&quot; with preferred embodiments which very often use a well accepted technical <lb/>terminology, and exhibit a language much more similar to usual scientific and technical literature. <lb/>2.3 The citation structures <lb/>The patent collection is a very dense network of citations creating a set of interrelations particularly <lb/>interesting to exploit during a prior art search. Table 1 gives a quantitative overview of the citation <lb/>network we observed for the patent collection 3 . The large majority of patents are continuations <lb/>of previous works and previous patents. The citation relations make this development process <lb/>visible. Similarly, fundamental patents which open new technologies subfields are exceptional but <lb/>tends to be cited very frequently in the whole subfield during years. <lb/>[2], for instance, exploits the citation graph of a patent collection for identifying patent thickets, <lb/>i.e. the patent portfolios of several companies overlapping on a similar technical aspect. While the <lb/>author&apos;s goal was to identify pro-competitive technical domains with respect to antitrust regimes, <lb/>this work shows that multiply related patents can be infered from the overall citation network of <lb/>a patent collection. High density of inter-citations between a group of applicants is the evidence <lb/>of cumulative innovation and multiple blocking patents. If a new patent applicant belonging to <lb/>this patent ticket appears, it is very likely that the most relevant prior art documents are already <lb/>present in this patent thicket. <lb/>In addition, as drawings are excluded from the present task, the exploitation of citation rela-<lb/>tions appears the best source of evidence for retrieval in drawing-oriented fields. <lb/>Citations <lb/>Category a <lb/># <lb/>total <lb/>all <lb/>4 854 280 <lb/>X <lb/>581 853 <lb/>Y <lb/>413 981 <lb/>A <lb/>1 849 251 <lb/>D <lb/>2 019 733 <lb/>other <lb/>198 749 <lb/>EP doc. <lb/>all <lb/>1 082 647 <lb/>EP doc. with citation text all <lb/>363 494 <lb/>a X means that the cited document taken alone anticipates the claimed invention. Y indicates that the cited <lb/>document in combination with other prior art documents covers the claimed invention. A means that the document <lb/>is relevant but discloses only partially the claimed invention. Finally, D, which can appear together with the previous <lb/>categories, indicates that the document has been cited by the applicant in the original description. <lb/>Table 1: Overview of citation relations in the patent collection. <lb/>The patents cited in the description of a patent document are potentially highly relevant <lb/>documents. First, the examiner often acknowledges the applicant&apos;s proposed prior art by adding <lb/>this document in the search report (usually as an A document since it is extremely rare that an <lb/>applicant discloses himself, by mistake, a &quot;killing&quot; X document in his application). Second, in case <lb/>the patent document corresponds to a granted patent, Rule 42(1)(b) EPC requires the applicant <lb/>to acknowledge the closest prior art. As a consequence, the closest prior art document, sometimes <lb/>an EP document, is frequently present in the description body of a B patent publication. <lb/></body>

			<note place="footnote">3 Just as a comparison, following Thomson Reuters&apos; Journal Citation Reports, approximately 60 millions citations <lb/>has been made in more than 7000 journals during the time period 1997-2005. <lb/></note>

			<body>Actually, we observed that, in the final XL evaluation set, the European Patents cited in the <lb/>descriptions represent 8,52% of the expected prior art documents of the final topic set. For the <lb/>10.000 topic patents, we found that 4407 were citing in their descriptions at least one EP document <lb/>from the collection, for a total of 7960 cited EP documents for which 5305 are relevant. This is <lb/>21.66% of the relevant documents of these 4 407 topics. With a &quot;run&quot; made only with these cited <lb/>patents, a MAP (Mean Average Precision) of 0.2230 is obtained, with a precision at 5 of 0.2315. <lb/>This shows the potential contribution of this simple source of relevant patents, while it must be <lb/>noted that it is only a partial source since it leaves more than half of the patent topics without <lb/>any citations. <lb/>2.4 Importance of metadata <lb/>In addition to the application content (text and figures) and the citation information, all patent <lb/>publications contain a relatively rich set of well defined metadata. Traditionally at the EPO, the <lb/>basic approach to cope with the volume of the data is, first, to exploit the European patent clas-<lb/>sification (ECLA classes) to create restricted search sets and, second, to limit the broad searches <lb/>to the titles and abstracts. When following this strategy, an examiner has retrieved a set of ap-<lb/>prox. 50 documents, he visualizes each of these documents and performs a careful non automated <lb/>analysis of the whole content including the description text and the figures. Exploiting the ECLA <lb/>classes appears, therefore, as a solid basis for efficiently pruning the search space. <lb/>In addition to the classification metadata, during 30 years of prior art search, the EPO patent <lb/>examiners have developed heuristics for finding interesting documents given the application in <lb/>hand. Since the means for searching text content are still relatively rudimentary (in 2009, only <lb/>a command line boolean search engine is used at the EPO), these heuristics are often based on <lb/>metadata such as the applicant name, the inventor names, other patent office classes or priority <lb/>documents. <lb/>2.5 Multilinguality <lb/>The European Patent documents are by definition highly multilingual. First, each patent is <lb/>associated to one of the three official languages of application. In the collection, the language <lb/>distribution was the following: 69% for English, 23% for German and 7% for French. It indicates <lb/>that all the textual content of a patent will be available at least in this language. Second, granted <lb/>patents also contains the title and the claims in the three official languages. These translations are <lb/>usually of very high quality because they are made by professional human translators. Crosslingual <lb/>retrieval techniques are therefore crucial for patents, not only because the target documents are in <lb/>different languages, but also because a patent document often provides itself reliable multilingual <lb/>information which makes possible the creation of valid queries in each language. <lb/>As it is important not to limit a retrieval only in the main language of the patent application, <lb/>our system needed to deal with different languages for each patent. We have thus decided to build <lb/>different index and different specialized retrieval models for each languages and, in a second time, <lb/>to merge the different results in oder to exploit the benefit of each language. <lb/>
			3 Overall Description of the System <lb/>3.1 System architecture <lb/>As explained in the previous section, there is clear evidence that pure text retrieval techniques <lb/>are insufficient for coping correctly with patent documents. Our proposal is to combine useful <lb/>information from the citation structure and the patent metadata, in particular patent classification <lb/>information, as pre and post processing steps of text-based retrieval techniques. In order to <lb/>exploit multilinguality and different retrieval approaches, we merged the ranked results of multiple <lb/>retrieval models based on machine learning techniques. As illustrated by Figure 1, our system, <lb/>called PATATRAS (PATent and Article Tracking, Retrieval and AnalysiS), relies on four main <lb/>processing steps: <lb/>1. the creation of an initial working set for each patent topic in order to limit the search space, <lb/>2. the application of multiple retrieval models (KL divergence, Okapi) using different index <lb/>models (English lemma, French lemma, German lemma, English phrases and concepts) for <lb/>producing several sets of ranked results, <lb/>3. the merging of the different ranked results based on multiple SVM regression models and a <lb/>linear combination of the normalized ranking scores, <lb/>4. a post-ranking based on a SVM regression model. <lb/>Steps 2 and 3 have been designed as generic processing steps that could be reused for any <lb/>technical and scientific content, patent or non patent. The step 2 uses standard text retrieval <lb/>techniques. The step 3 uses domains information and standard metadata such as the language <lb/>that we can find or obtain automatically in any type of document collections. The patent-specific <lb/>information are exploited in steps 1 and 4. Before presenting in more in details these four different <lb/>steps, we now briefly describe how all the patent documents have been parsed and pre-processed. <lb/>Patent <lb/>Collection <lb/>Patent <lb/>Topic <lb/>Initial <lb/>Working <lb/>Sets <lb/>Index <lb/>Lemma <lb/>EN <lb/>LEMUR 4.9 <lb/>-KL divergence <lb/>-Okapi BM25 <lb/>Index <lb/>Lemma <lb/>FR <lb/>Index <lb/>Lemma <lb/>DE <lb/>Index <lb/>Phrase <lb/>EN <lb/>Index <lb/>Concepts <lb/>Tokenization <lb/>POS tagging <lb/>Phrase <lb/>extraction <lb/>Concept <lb/>tagging <lb/>Query <lb/>Lemmas EN <lb/>Query <lb/>Lemmas FR <lb/>Query <lb/>Lemmas DE <lb/>Query <lb/>Phrases EN <lb/>Query <lb/>Concepts <lb/>Ranked <lb/>results <lb/>(10) <lb/>Merging <lb/>Ranked <lb/>shared <lb/>results <lb/>Post-<lb/>ranking <lb/>Final <lb/>Ranked <lb/>results <lb/>Init <lb/>Figure 1: System architecture overview of PATATRAS for query processing. The arrow represents the <lb/>main data flow from the patent topic to the final set of ranked results. <lb/>3.2 Document parsing <lb/>For each patent in the collection, training and topic set, the following processing were performed: <lb/>1. Parsing of the initial XML documents corresponding to the patent (so called A document <lb/>before a decision and B documents in case of grant). <lb/>2. Representation of all the metadata in a MySQL database, following a comprehensive relation <lb/>model -given the required heavy processing based on the metadata, and although XML <lb/>databases can present some advantages, we needed a very fast and easy to optimize database. <lb/>3. Identification by means of regular expressions of the patents cited in the latest version of the <lb/>description (so from a decreasing order of priority: B2, B1, A2, A1). <lb/>4. For all the textual data associated to the patent: Rule-based tokenization depending on the <lb/>language. <lb/>5. For all the textual data associated to the patent: Part of speech tagging and lemmatization. <lb/>We used our own HMM-based implementation for English and the Tree Tagger [14] for <lb/>French and German. <lb/>3.3 Metadata Database <lb/>We performed a basic normalization and cleaning of all inventor and applicant names: Particules <lb/>and titles were removed from the inventor fields (for instance Professor Dr. Dr. h.c. mult. <lb/>Wolfgang Wahlster becomes simply -with all due respect-Wolfgang Wahlster), business entity <lb/>marks (Inc., GmbH, Kabushi Kaisha) and locations (country names) were removed from applicant <lb/>fields. We also stored the citation texts of the patents cited in a description. The database storing <lb/>all metadata of the collection and all corresponding indexes, but not the textual content, had a <lb/>final size of 2,48 GB. <lb/>4 Indexing models <lb/>4.1 Overview <lb/>The five following indexes were build using the Lemur toolkit [7] (version 4.9): <lb/>• For each of the three language (English, French, German), we built a full index at the lemma <lb/>level. <lb/>• For English, we created an additional phrase index based on phrase as term definition. <lb/>• A crosslingual concept index was finally built using the list of concepts identified in the <lb/>textual material for all three languages. <lb/>
			We did not, therefore, index the collection document by document, but rather considered a <lb/>&quot;meta-document&quot; corresponding to all the publications related to a patent application. In each <lb/>case, the following textual data corresponding to a patent is indexed: <lb/>• the last version of the title, <lb/>• the first version of the description (Following Article 123(2) EPC, we are sure that any <lb/>further publication will not go beyond the scope of the initial version of the description), <lb/>• the last version of the abstract, <lb/>• the last version of the claims. <lb/>4.2 Lemma indexes <lb/>Based on the result of the lemmatization, we considered the lemma present in the textual data <lb/>of the publications corresponding to the patents of the collection: title, abstract, claims of the <lb/>last available publication, and description of the earliest available publication -so A1, A2 or B1 <lb/>in this order. The selection of the lemma as term unit could be view as a stemming removing all <lb/>inflexional suffixes. Only the lemmas corresponding to open grammatical categories (i.e. noun, <lb/>verb, adverb, adjective and number) has been indexed, which could be viewed as applying a <lb/>stop-word list in traditional IR. <lb/>4.3 Phrase indexes <lb/>For the English content, a phrase extraction was realized: based on the part of speech tagging <lb/>results, all the noun phrases were identified and the Dice Coefficient was applied to select the <lb/>phrases [16]. <lb/>4.4 Multilingual Terminological database <lb/>A multilingual terminological database covering multiple technical and scientific domains and <lb/>based on a conceptual model, have been created from the following existing &quot;free&quot; resources: <lb/>MeSH, UMLS, the Gene Ontology, a subset of WordNet corresponding to the technical domains <lb/>as identified in WordNet Domains [9] with the corresponding entries of WOLF (a free French <lb/>WordNet), and a subset of the English, French and German Wikipedia corresponding to technical <lb/>and scientific categories [17]. <lb/>The Wikipedia &quot;dump&quot; XML files were processed with a slightly modified version of Wikiprep <lb/>[18] able to extract multilingual relations in addition to usual structure and text information. <lb/>Similarly as in [4], we interpreted an article as a concept, the title of an article being the preferred <lb/>term and the disambiguation redirections to this article being alternative or variant terms realizing <lb/>this concept. <lb/>A set of 76 general technical domains have been first established derived from the Dewey <lb/>Decimal Classification [3]. For each above-mentioned sources, a mapping has been written between <lb/>the main catgories/domains of the source and the general domains. Two concepts coming from <lb/>two different sources were merged if they share at least one term and one domain. <lb/>The resulting database contains: <lb/>• Approx. 3 millions terms (2,6 millions for English, 190.000 for German, 140.000 for French), <lb/>• 1,4 millions of concepts (71.000 realized in German and 65.000 realized in French), <lb/>Figure 2: View of the multilingual terminological database for the concept corresponding to the term <lb/>Radial Engine. <lb/>• 600.000 definitions, <lb/>• 1 million of semantic relations, <lb/>• approx. 20.000 fully specified acronyms, <lb/>• 123.000 additional &quot;source-specific&quot; categories. <lb/>The terminological database relies on a conceptual model [12] and is currently implemented in <lb/>a MySQL database. A web interface has been developed for browsing the terminological database <lb/>and for performing basic searches, see Figure 2. For the purpose of the CLEF IP tasks, we use <lb/>only the terms and the acronyms information of this terminological resource. <lb/>Wikipedia offers a massive sources of terminological data (more than 1,5 million of terms <lb/>for the technical and scientific categories) and was the only multilingual source. However, the <lb/>quality of the data is clearly not comparable with a well designed domain specific terminology <lb/>as for instance MeSH. In particular, due to overall noisy data, the merging of concepts involving <lb/>Wikipedia entries was not satisfactory. In practice, it was not possible to consider as term variants <lb/>the disambiguation terms given by Wikipedia and we merged concepts involving Wikipedia entries <lb/>only if the article-level term entry was in common with the other concepts for a common domain. <lb/>This work is still ongoing and we expect to improve it in the future. <lb/>4.5 Concept tagging <lb/>The terms of the terminological database have been used for annotating the textual data of the <lb/>whole collection, training and topic sets. A term annotator able to deal with such a large volume <lb/>of data has been developed specifically for this track. This annotator can match term variants <lb/>following morphological variations. The concept disambiguation was realized on the basis of the <lb/>ECLA classes (or by default the IPC classes) of the processed patent. For this purpose, the <lb/>upper level of the IPC (corresponding to the IPC &quot;classes&quot; as defined in the IPC, i.e. the three <lb/>first characters of the classes/subclasses/groups/subgroups) has been mapped to the common <lb/>abovementioned 76 domains. Given the design of the terminological database, a term used in a <lb/>given domain corresponds to a single concept. When an IPC class corresponds to several domains <lb/>(for instance the class G10 is used for music instruments which can correspond to the domains <lb/>acoutics and electronics), or when a term corresponds to several concepts in the same domain (for <lb/>instance engine in the computing field which could be a layout engine or a search engine), a term <lb/>cannot always be disambiguated on the basis of the IPC class. In this case, we have decided in <lb/>the present implementation not to further attempt to select a concept and to skip this term. <lb/>4.6 Retrieval models <lb/>We used the two following well known retrieval models: <lb/>• Unigram language model with KL-Divergence and Jelinek-Mercer smoothing (λ = 0.4), <lb/>• Okapi weighting function BM25 (K1 = 1.5, b = 1.5, K3 = 3). <lb/>The two models have been used with each of the previous five indexes, resulting in the production <lb/>of 10 lists of retrieval results for each topic patent. <lb/>In each case the query was build based on all the available textual data of a topic patent and <lb/>processed similarly as the whole collection in order to create one query per language based on <lb/>lemma, one query based on English phrases and one query based on concepts (independent by <lb/>definition from the language). The query for a given model is, therefore, a representation of the <lb/>whole textual content of the patent. The query is exactly the same representation as the one of <lb/>a document indexed in the collection. The scoring model used for retrieval corresponds, in this <lb/>context, to a distance between two &quot;documents&quot;. In the framework of language model based IR, <lb/>KL divergence is typically used for evaluating the distance between two documents, i.e. between <lb/>two unigram probabilistic distributions. While Okapi BM25 is usually used for ad hoc retrieval, it <lb/>is also known as a reliable scoring function for evaluating the distance between documents. The <lb/>drawback of using a whole document representation as query is the processing time which is always <lb/>related to the size of the query. However, for the present work, we did not consider processing time <lb/>as an issue, as long as the whole set of patent topics could be processed in the track timeframe. For <lb/>both retrieval methods, we did not use query expansion, nor pseudo-relevance feedback, because <lb/>these two techniques did not appear effective during our first experiments. <lb/>The retrieval processes were based on the Lemur toolkit [7], version 4.9. The baseline results <lb/>of the different indexes and retrieval models are presented in Table 2, column (1). These results <lb/>correspond to the application of the retrieval model with on whole collection. <lb/>Model Index <lb/>Language <lb/>(1) <lb/>(2) <lb/>(3) <lb/>(4) <lb/>KL <lb/>lemma <lb/>EN <lb/>0.1068 0.1083 0.1516 0,1589 <lb/>KL <lb/>lemma <lb/>FR <lb/>0.0611 <lb/>0.0612 <lb/>0.1159 <lb/>0,1234 <lb/>KL <lb/>lemma <lb/>DE <lb/>0.0627 <lb/>0.0634 <lb/>0.1145 <lb/>0,1218 <lb/>KL <lb/>phrase <lb/>EN <lb/>0.0717 <lb/>0.0720 <lb/>0.1268 <lb/>0,1344 <lb/>KL <lb/>concept all <lb/>0.0671 <lb/>0.0680 <lb/>0.1414 <lb/>0,1476 <lb/>Okapi lemma <lb/>EN <lb/>0.0806 <lb/>0.0813 <lb/>0.1365 <lb/>0,1454 <lb/>Okapi lemma <lb/>FR <lb/>0.0301 <lb/>0.0303 <lb/>0.1000 <lb/>0,1098 <lb/>Okapi lemma <lb/>DE <lb/>0.0598 <lb/>0.0612 <lb/>0.1195 <lb/>0,1261 <lb/>Okapi phrase <lb/>EN <lb/>0.0328 <lb/>0.0330 <lb/>0.1059 <lb/>0,1080 <lb/>Okapi concept all <lb/>0.0510 <lb/>0.0516 <lb/>0.1323 <lb/>0.1385 <lb/>Table 2: MAP results of the retrieval models, Main Task. (1) Base MAP, Medium set (1 000 queries), <lb/>(2) MAP with citation texts, Medium set, (3) MAP with citation texts, initial working sets, Medium set, <lb/>(4) Map with citation texts, initial working sets, XL set (10 000 queries). <lb/>We can observe that for the same query and the same index, KL divergence always provided <lb/>better results than Okapi. The best result is obtained with KL divergence with English lemma <lb/>representation. The fact that conceptual and phrase based retrievals perform worst than the <lb/>monolingual English lemma representation can appear disappointing given the effort needed to <lb/>implement them. However, it is consistent with previous works which have noted the information <lb/>loss implied by pure conceptual representations as compared to simple stem-based retrieval. <lb/>4.7 Citation texts <lb/>The citation texts of a target patent are all the paragraphs in other patent documents that refers <lb/>to it. Extending the content of a patent with its citation text aims at providing more textual <lb/>descriptions corresponding to the important contributions of the patent according to the other <lb/>patent applicants. Following a similar approach, [10], for instance, tried to exploit the citation <lb/>texts in order to improve the semantic interpretation and the retrieval of text for biomedical <lb/>articles. Moreover, for the case of patent documents, since part of the citation text can be written <lb/>in the other languages than the taget patent, it possibly increases the multilingual description <lb/>of the cited patents. While for technical and scientific articles, the citation text is usually just <lb/>a sentence, citation texts for patents appear to be in a constant manner a whole paragraph. <lb/>Therefore, for each patent document present and cited in the collection, the entire paragraph of <lb/>citation was appended to the textual material of the cited patent. <lb/>The table 2 presents the impact of adding the citation text, see column (2). The improvement <lb/>is low and statistically not significant. We think, however, that this result is encouraging because <lb/>it was obtained with a very limited number of citation texts. Only citations of an EP document <lb/>were considered here. By having a complete collection or patent family information, it would be <lb/>possible to extract much more citation texts (most likely by a factor five to ten) and we could <lb/>expect that the improvement would be more significant. <lb/>
			5 Creation of initial working set <lb/>For each topic patent, we created a prior working set for reducing the search space and the effect <lb/>of term polysemy. The goal here is, for a given topic patent, to select the smallest set of patents <lb/>which has the best chance to contain all the relevant documents. A set S p of patents for a given <lb/>patent p is created by applying successively the following steps: <lb/>1. Put in S p all the patents cited in the description of the topic patent (as identified in step 3 <lb/>of the document parsing) and present in the collection. <lb/>2. Up the citation tree: All patents citing at least one patent of S p are added to S p . <lb/>3. Down the citation tree: All patents cited by at least one patent of S p are added to S p (steps <lb/>2 and 3 were performed iteratively a second times after step the fifth step). <lb/>4. Priority dependencies: All patents having a priority document in common with p or with <lb/>a patent in S p are added in S p . All patents citing a priority document of a patent in S p <lb/>are added to S p . This step permits to gather non unitary and divisional patent applications <lb/>(i.e. a single patent application containing possibly different independent inventions which <lb/>results in several parallel applications) and to exploit the partial information present in the <lb/>collection about patent families. <lb/>5. All patents having the same applicant as the topic patent and at least one common inventor <lb/>are added in S p . <lb/>6. All patents belonging to one of the ECLA class of p (if at least one ECLA class is available) <lb/>are added in S p . <lb/>7. All patents belonging to the ECLA classes most frequently co-occurring in the collection <lb/>with the ECLA classes of p are added in S. <lb/>8. If the working set is below a given limit: All patents having the same applicant and belonging <lb/>to one of the IPC class of p are added to to S p . <lb/>9. If the working set is still below a given limit: All patents belonging to one of the IPC class <lb/>of p are added to to S p . <lb/>Step <lb/># relevant doc. micro recall <lb/># docs <lb/>1. cited patents <lb/>5 305 0.0852 <lb/>7 960 <lb/>2. up the citation tree <lb/>5 853 0.0940 (+10,3%) <lb/>20 245 <lb/>3. down the citation tree <lb/>6 026 0.0967 (+2,9%) <lb/>22 727 <lb/>4. common prority doc.: <lb/>7 999 0.1284 (+32,78%) <lb/>42 168 <lb/>5. same applicant/inventor <lb/>10 768 0.1728 (+34,58%) <lb/>116 099 <lb/>2+3. second iteration <lb/>11 463 0.1840 (+6,48%) <lb/>176 815 <lb/>6. same ECLAs <lb/>35 935 0.5769 (+213,53%) <lb/>5 129 068 <lb/>7. most freq. co-occuring ECLA <lb/>44 559 0.7154 (+24,22%) <lb/>24 859 957 <lb/>8. same applicant and IPC <lb/>44 625 0.7164 (+0.14%) <lb/>24 863 768 <lb/>
			9. same ipc <lb/>45 489 0.7303 (+1.94%) <lb/>26 159 190 <lb/>Total relevant <lb/>62 285 1.0 <lb/>Table 3: Incremental construction of the initial working lists of patents for the Main XL task. <lb/>At the end of this process, if the size of the working set is too low or too large, no working set <lb/>is used and the retrieval is performed on the whole collection. In the submitted runs, the lower <lb/>limit was 10 documents and the upper limit 10.000. The table 3 presents the performance of each <lb/>step in term of increase of the micro recall, i.e. coverage of all the relevant documents of the whole <lb/>set of topics. Note that the recall reported in the official CLEF IP 2009 evaluation summary is <lb/>the macro recall, i.e. the average of recall obtained for each topic patent. The last column gives <lb/>the sum of the number of documents for all working sets. <lb/>The micro recall of the final run was 0.6985, thus lower than the one of the final working lists. <lb/>This difference comes from the working list having a number of patents higher than 1000 and <lb/>where the final processing has not been able to place all the relevant documents in the first 1000 <lb/>patent results. These &quot;missed&quot; patents are the most difficult documents to process: the system <lb/>failed to rank them both on the basis of the textual content and the metadata even with an initial <lb/>working set. Since many working lists contained less than 1000 documents and sometimes less than <lb/>100 documents, the list of results of the final run was frequently less than 1000. Similarly some <lb/>working sets were particularly large, sometimes more than 10 000 documents, but all final runs <lb/>were cut at 1000. The final number of results was, therefore, on average approx. 415 documents <lb/>per patent topic, as compared to approx. 2616 documents in average per initial working set. <lb/>It is also clear that we are missing 26.97% of the expected relevant documents which is a <lb/>relatively high number. Identifying how to capture these documents without expanding too much <lb/>the working sets will require further investigations. <lb/>These different steps correspond to typical search strategies used by the patent examiners <lb/>themselves for building sources of interesting patents. They capture techniques that have emerged <lb/>in patent examination and which are considered to be effective. As the goal of the track is, to a <lb/>large extend, to recreate the patent examiner&apos;s search reports, recreating such restricted working <lb/>sets appears to be a valuable approach. Table 2, column (3) and (4) show the improvement of <lb/>using the initial working sets instead of the whole collection. The retrieval with working sets was <lb/>realized using the &quot;working list&quot; functionality of Lemur in batch mode. <lb/>
			6 Merging of results <lb/>In the previous section, we have observed that conceptual and phrase-based retrievals alone were <lb/>less efficient than English lemma model. However, several works have shown that the conceptual <lb/>and phrase representation can improve a word-based model, for instance the semantic smoothing <lb/>techniques in the language model framework. In our preliminary results, we observed that the <lb/>different retrieval models present a strong potential of complementarity, in particular between <lb/>the different languages and between lemma/concept. The table 4 presents the repartition of the <lb/>best results over the different models. We can observe that concept-based models provide a high <lb/>number of results with higher MAP than the English lemma model. We can also note that each <lb/>language-specific model provides a constant set of best results over all other models. Intuitively, <lb/>for a topic patent essentially described in the main language of application (in particular the whole <lb/>description) and minimally in the two other languages (just the title and the claims), the index <lb/>corresponding to main language should provide better results and should be prioritized. <lb/>Model <lb/># better than <lb/>baseline <lb/># equal <lb/># best <lb/>overall <lb/>KL lemma en <lb/>(baseline) <lb/>-<lb/>-<lb/>1341 <lb/>KL lemma fr <lb/>3480 <lb/>661 <lb/>839 <lb/>KL lemma de <lb/>3392 <lb/>632 <lb/>781 <lb/>KL phrase en <lb/>3559 <lb/>741 <lb/>869 <lb/>KL concept <lb/>4832 <lb/>133 <lb/>1692 <lb/>Okapi lemma en <lb/>3928 <lb/>789 <lb/>956 <lb/>Okapi lemma fr <lb/>3224 <lb/>616 <lb/>626 <lb/>Okapi lemma de <lb/>3494 <lb/>630 <lb/>836 <lb/>Okapi phrase en <lb/>3002 <lb/>649 <lb/>572 <lb/>Okapi concept <lb/>4638 <lb/>114 <lb/>1488 <lb/>Table 4: Complementarity between results sets for the XL patent topic set (10 000 documents). <lb/>Merging multiple results has been used in the context of distributed information retrieval, in <lb/>particular with partially overlapping collections. Merging ranked results from different models for <lb/>the same collection appears well adapted to a patent collection because the different models exploit <lb/>different views, for instance different languages, for retrieving documents in the same collection. <lb/>In addition, we are presently in an exceptional situation where we can exploit a very large amount <lb/>of training data given the number of citations present in the collection. For training purposes, 500 <lb/>complete examples were provided with judgements. Moreover, the collection itself could provide <lb/>a huge number of examples of prior art results. This uncommon aspect makes possible a fully <lb/>supervised learning method. Machine learning algorithms are, therefore, well appropriated to <lb/>weight the different ranked lists so that, given a query, the most reliable models are prioritized. <lb/>The merging of ranked lists of results is here expressed as a regression problem. <lb/>The usage of regression models for merging results was described for instance in [13] and [15]. <lb/>Merging based on regression usually surpasses other combination methods which do not involve <lb/>machine learning, such as the well known CORI merging algorithm. In [15], the precision follow-<lb/>ing a merging of results from different search engines was improved up to 98.9 % as compared <lb/>to a merging based on the CORI algorithm. In [13], even with a very limited number of fea-<lb/>tures used for learning, a merging of retrieval results from different languages based on a logistic <lb/>regression significantly surpasses all other score combination approaches. Regression models ap-<lb/>pears particularly appropriate in the present case, because they permit to adapt the merging on <lb/>a query-by-query basis. <lb/>For each model m, we trained a linear regression model using as input a set of features inferred <lb/>from the query. As a general framework, given a set of examples, ( − → <lb/>x 1 , y 1 ), ..., ( − → <lb/>x N , y N ), where the <lb/>− → x i are vectors of features and the y i are values corresponding to the dependent variable, the goal <lb/>of a linear regression is to find − → w such that and <lb/>Φ( − → w ) = min <lb/>i <lb/>(y i − − → w . <lb/>− → <lb/>xi) 2 <lb/>(1) <lb/>In the present regression training, the observed MAP was used as the dependent variable for <lb/>representing the pertinence of a set of results for a given retrieval model. <lb/>For the realizing the merging, scores were first normalized so that they all lie in the same range. <lb/>We used the standard normalization, basically all the min are shift to 0 and the max is scaled to <lb/>
			1: <lb/>w i = <lb/>v i − v min <lb/>v max − v min <lb/>(2) <lb/>The regression model gives a score c qm for the retrieval model m and for the query q which is <lb/>interpreted as an estimation of the relevance of the results retrieved by m for the query q. <lb/>The merged relevance score s dq for a patent d following a query (here a patent topic) q is <lb/>obtained as a linear combination of the normalized scores w md obtained from each retrieval model <lb/>m: <lb/>s dq = <lb/>m∈M <lb/>c mq w md <lb/>(3) <lb/>This score is computed for each patent appearing in at least one of the result sets produced by <lb/>the different models. The merged result set is build by ranking these patents according to this <lb/>new relevance score. <lb/>The key of the supervised learning approach is to exploit as much training data as possible in <lb/>order to use a rich set of features while avoiding overfitting. To exploit more training data, we <lb/>created from the collection track a supplementary training set of 4 131 patents. We selected patents <lb/>citing at least 4 EP documents, with a language distribution and an IPC class distribution similar <lb/>to the ones of the whole collection. We assembled the initial working sets similarly as explained in <lb/>section 5, but we filtered out patents whose publication dates were after the priority date of the <lb/>considered patent. During our tests for selecting the regression algorithm, we built the merging <lb/>models based on these 4 131 patents, that we called the validation set, and tested them on the <lb/>&quot;normal&quot; training set of 500 topics. For producing the final official runs, we trained the merging <lb/>models on the whole 4.631 training patents. <lb/>The following set of features was used: (f1) the language of proceeding of the patent topic, (f2) <lb/>the size of the query, (f3) the size of the initial working set, (f4-5) the non-normalized minimum <lb/>and the maximum retrieval scores of the set of results, (f6) the range of the non-normalized score <lb/>of the result set, (f7-8) the main IPC trunk (first character of an IPC class) and the IPC class <lb/>(three first characters of an IPC class/group). The average number of words of the phrases was <lb/>also used for the results based on the phrase index (f9). <lb/>Expressing the merging of ranked results as a regression problem permits to use existing ma-<lb/>chine learning software packages which provide excellent evaluation utilities. We have experi-<lb/>mented several regression models: least median squared linear regression, SVM regression (SMO <lb/>and ν-SVM methods) and multilayer perceptron using LibSVM [1] for the ν-SVM regression <lb/>method and the WEKA toolkit [19] for the other methods. For the ν-SVM regression method, <lb/>we used the methodology described in [5] for setting the parameters which includes scaling and <lb/>cross-validation. SVM in general is known to be sensitive to hyperparameter selection (λ). This <lb/>methodology led to a strong improvement as compared to our first random testing and the SMO <lb/>regression. <lb/>Features LeastMedSq <lb/>multilayer <lb/>perceptron <lb/>SMO <lb/>ν-SVM <lb/>f1 <lb/>0.1681 (+5.8%) 0.1711 (+7.7%) 0.1706 (7.4) <lb/>0.1691 (+6.4%) <lb/>f1-6 <lb/>0.1689 (+6.3%) 0.1797 (+13.1%) 0.1807 (+13.7) <lb/>0.1976 (+24.3%) <lb/>all <lb/>0.1786 (+12.4) <lb/>0.1898 (+19.4%) 0.2016 (+26.9%) 0.2281 (+43.5%) <lb/>Table 5: MAP measures following different regression models for merging the 10 ranked results (XL <lb/>patent topic set, 10 000 documents). The number in parenthesis gives the relative improvement over the <lb/>best individual score, English lemma KL model at 0,1589. <lb/>Table 5 gives the breakdown of our experiments based on the 4631 training patents. The <lb/>results obtained with the ν-SVM regression model shows that the methodological aspect is a key <lb/>for achieving better performances. The default parameters have been used for the other regression <lb/>models. The final runs are based on ν-SVM regression employing the Radial Basic Function (RBF) <lb/>as kernel function. <lb/>We also observed that the IPC class is particularly relevant for identifying strong confidence <lb/>of the concept retrieval models. The terminological knowledge base contains a very rich amount <lb/>of terms from the medical, biotech and chemical fields coming from well curated sources (MeSH, <lb/>UMLS and the Gene Ontology). On the contrary, a field like computer sciences suffers from the <lb/>noisy Wikipedia data. <lb/>7 Post-ranking <lb/>Post-ranking, or re-ranking in general, is a particularly attractive technique in Information Re-<lb/>trieval because, first, the usual result of a retrieval model is a weighted n-best list of outputs and, <lb/>second, it is much easier to experiment and optimize a post-ranker for a particular set of features <lb/>than to integrate them in a single model. In the present case, as we used retrieval models designed <lb/>for texts, it would be extremely difficult or impossible to modify these methods for including the <lb/>features we want, namely to prioritize certain patents based on citation relations and metadata. <lb/>The drawback of re-ranking is, of course, that it is limited by the initial model. <lb/>
			While the previous section was focusing on learning to merge ranked results, this step aims at <lb/>learning to rank. Regression here is used to weight a patent result in a ranked list of patents given <lb/>a query. The creation of the inital sets was dealing with recall and we address now precision. The <lb/>goal of the re-ranking of the final merged result is first of all to boost the score of certain patents: <lb/>• Patents initially cited in the description of the topic patent: a boolean feature was used to <lb/>indicate if the retrieved patent was present or not in the citations. <lb/>• Patents having several ECLA and IPC classes in common with the topic patent: two features <lb/>were introduced to indicate the number of common ECLA classes and the number of IPC <lb/>classes. <lb/>• Patents with higher probability of citation as observed within the same IPC class and within <lb/>the set of results: we introduced two features corresponding to a number between 0 and 1 <lb/>representing these two probabilities. <lb/>• Patents having the same applicant and at least one identical inventor: the following features <lb/>were added: a boolean indicating if the applicant was common between the retrieve patent <lb/>and the patent topic, and a number between 0 and 1 indicating the proportion of common <lb/>inventors. <lb/>Similarly as for the creation of the initial working sets, these features correspond to criteria often <lb/>considered by patent examiners when defining their search strategies. Re-ranking based on these <lb/>features permits to obtain a final result closer to a standard EPO search report and thus increases <lb/>the average precision. <lb/>The dependent variable represents the weight adjustment to be applied to the particular patent <lb/>result. In the training data, the dependent variable s p for a patent result p was set as follow: <lb/>s p = <lb/>w max if p is relevant, <lb/>0 <lb/>Otherwise. <lb/>(4) <lb/>where w max is the score of the top result in the current result set. For each retrieve patent p <lb/>in the result set, the regression model produce a score s p which is used to reevaluate the score of <lb/>p to w ′ <lb/>p such that w ′ <lb/>p = w p (s p + 1). The final runs is obtained by re-sorting the list of results and <lb/>by applying a cutoff at rank 1000. <lb/>The regression model was trained using the normal training set and the additional validation <lb/>set presented in the previous section. In order to limit the size of the training data and to avoid <lb/>too many useless negative examples, we considered only 20 negative results per patent. The final <lb/>run is also based on SVM regression, more precisely the WEKA implementation SMOreg using <lb/>Pearson VII Universal Kernel (PUK) function. We did not evaluate other regression algorithms. <lb/>8 Final Results <lb/>8.1 Main task <lb/>Table 6 summarizes the automatic evaluation obtained for our final runs. We processed the entire <lb/>list of queries (the XL set, corresponding to 10 000 patent topics) and, therefore, cover the smaller <lb/>bundles (S and M, respectively 500 and 1000). The list of relevant documents distinguishes two <lb/>types of documents: relevant documents (patents cited by the applicant and and with a A category) <lb/>and very relevant patents (all the other cited patents). <lb/>Measures <lb/>S <lb/>M <lb/>XL <lb/>MAP <lb/>0.2714 0.2783 0.2802 <lb/>Prec. at 5 <lb/>0.2780 0.2766 0.2768 <lb/>Prec. at 10 0.1768 0.1748 0.1776 <lb/>Measures <lb/>S <lb/>M <lb/>XL <lb/>MAP <lb/>0.2832 0.2902 0.2836 <lb/>Prec. at 5 <lb/>0.1856 0.1852 0.1878 <lb/>Prec. at 10 0.1156 0.1133 0.1177 <lb/>Table 6: Evaluation of official runs for all relevant documents (left) and for highly relevant documents <lb/>(right). <lb/>The exploitation of patent metadata and citation information clearly provides a significant <lb/>improvement over a retrieval only based on textual data. By exploiting the same metadata as a <lb/>patent examiner and combining them to robust text retrieval models via prior working sets and re-<lb/>ranking, we created result sets closer to actual search reports. Overall, the exploitation of ECLA <lb/>classes and of the patents cited in the descriptions provided the best improvements. In addition, <lb/>the different regression models proved to be a very effective way of combining complementary <lb/>indexing models. While many models exhibit relatively low individual results, they appear to be <lb/>strongly specialized following discriminant criteria as the application language or the technical <lb/>domain. Similarly a regression model appears an efficient technique for re-ranking a list of ranked <lb/>results following heterogeneous features. <lb/>
			8.2 Language tasks <lb/>While the main task allows the participants to exploit all the available textual information, the <lb/>language tasks limits the usable material to a single language. We applied for the language specific <lb/>tasks a similar approach as for the main tasks, but with the following restrictions: <lb/>• The patents cited in the description (which are used during the creation of initial working <lb/>sets and in the final re-ranking) have been only considered for the main language of a patent <lb/>topic ; <lb/>• The lemma index corresponding to a different language was not used ; <lb/>• The phrase index was used only for English ; <lb/>• The queries for concepts were limited to the concepts extracted in the text of a single <lb/>language. <lb/>We think that we have ensured, therefore, that for building the query and for the exploitation of <lb/>patent metadata, we did not use any elements different from the task&apos;s language. <lb/>Measures <lb/>English French German <lb/>All <lb/>MAP <lb/>0.2358 0.1787 <lb/>0.2092 <lb/>0.2802 <lb/>Prec. at 5 <lb/>0.2365 0.1855 <lb/>0.2122 <lb/>0.2768 <lb/>Prec. at 10 0.1575 0.1338 <lb/>0.1467 <lb/>0.1776 <lb/>Table 7: Evaluation of official runs for the three language tasks and the main multilingual task for the <lb/>XL bundle. <lb/>Table 7 presents the evaluation of the three language tasks compared to the multilingual one. <lb/>We can observe that the exploitation of English language clearly leads to better results as compared <lb/>to the other languages. This result is not surprising because almost 70% of the textual material, in <lb/>particular the descriptions, are in English. However, the combination of all languages was effective <lb/>and has provided the best performance. <lb/>9 Hardware and Processing Time <lb/>We use 3 machines with 2.0GHz Core 2 Duo processor, 2 GB SDRAM (3 headless Mac Mini) and <lb/>a Laptop with a similar configuration and 4 GB RAM. In addition 2 TB of storage have been used <lb/>for the collection files, indexes and all the intermediary processed documents. The four machines <lb/>ran under 64bits Mac OS 10.5.6. Here are some indications about the processing time: <lb/>• The compilation of the terminological database took 28 hours on one machine after a pre-<lb/>processing of 135 hours for the three language Wikipedia XML files. <lb/>• Tokenization and POS tagging took 55 hours-machine. Phrase extraction was the heaviest <lb/>task and took a total of approx. 28 days-machine. <lb/>• Controlled concept indexing took approx. 22 hours-machine. <lb/>• Training a regression model took between 10 minutes (least median squared linear regression) <lb/>to 150 minutes (Multilayer Perceptron) per model. Aggregation of results and post-ranking <lb/>took approx. 90 minutes. <lb/>• Producing the final runs required 5 complete days of processing using entirely the 4 machines, <lb/>i.e. approx. 20 days-machine. <lb/>The total processing time for a topic patent was, therefore, approx. 43 secondes. This is, of <lb/>course, quite long for an online processing. However, given the challenge of processing the whole <lb/>collection of patent documents, we did not address at all the issue of processing time and many <lb/>possibilities for runtime improvement and optimization exist. <lb/>
			10 Future Work <lb/>We have tried in the present work to create a theoritically sound framework that could be gen-<lb/>eralized to non patent and mixed collection of technical and scientific documents. More comple-<lb/>mentary retrieval models and more languages can easily be added to the current architecture of <lb/>PATATRAS. If some metadata are specific to patent information, many of them find their coun-<lb/>terpart in non patent articles. Fine-grained and theoretically well founded taxonomies such as <lb/>MeSH can be used instead of patent classifications. <lb/>Although our system topped the evaluation for all tasks and all size of patent topics, we do <lb/>not consider that any aspects of the present system are finalized. First, we have chosen not to <lb/>focus on query formulation and we have simply selected the whole patent topic representation <lb/>as query. Using more sophisticated query formulation such as parsimonious language models <lb/>and an improved analysis of the patent structures could provide interesting improvements and <lb/>make query expansion and pseudo-relevant feedback effective. Regarding the machine learning <lb/>approach, a larger set of features combined with feature selection methods would need to be used. <lb/>The initial creation of working sets should also be expressed as a machine learning problem so <lb/>that the selection of interesting patents could be realized on patent topic by patent topic basis. <lb/>The exploitation of citation text is also currently disappointing because of the limited number of <lb/>cited EP documents. While this problem could be solved by exploiting patent family information, <lb/>it might also be well tackled for non patent litterature in repository services as HAL or CiteSeer <lb/>by using automatic bibliographical extraction techniques such as the one presented in [8]. <lb/>Finally, the terminological database suffers from the low quality of data of Wikipedia which <lb/>introduces a considerable noise and makes the merging of concepts frequently erroneous. In future, <lb/>we plan to experiment more controlled terminological multi-domain and multilingual large scale <lb/>resources. <lb/></body>

			<listBibl>References <lb/>[1] Chih-Chung Chang and Chih-Jen Lin. LIBSVM : a library for support vector machines, 2001. <lb/>http://www.csie.ntu.edu.tw/˜cjlin/libsvm <lb/>[2] Gavin Clarkson. Objectve Identification of Patent Thickets: A Network Analytic approach. <lb/>Harvard Business School Doctoral Thesis. 2004 <lb/>[3] http://www.oclc.org/dewey/ <lb/>[4] E. Gabrilovich and S. Markovitch. Computing Semantic Relatedness using Wikipedia-based <lb/>Explicit Semantic Analysis. In Proceedings of IJCAI, pages 16061611, 2007. <lb/>[5] Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin. A Practical Guide to Support Vector <lb/>Classication. May 19, 2009. http://www.csie.ntu.edu.tw/ cjlin/papers/guide/guide.pdf <lb/>[6] Marc Krier and Francesco Zacc. Automatic Categorisation Applications at the European <lb/>Patent Office. World patent Information 24:187-196 (2002). <lb/>[7] The Lemur Project. 2001-2008. University of Massachusetts and Carnegie Mellon University. <lb/>http://www.lemurproject.org/ <lb/>[8] Patrice Lopez. GROBID: Combining Automatic Bibliographic Data Recognition and Term <lb/>Extraction for Scholarship Publications. Proceedings of ECDL 2009, 13th European Conference <lb/>on Digital Library, Corfu, Greece, Sept. 27 -Oct. 2, 2009. <lb/>[9] Bernardo Magnini and Gabriela Cavagli. Integrating Subject Field Codes into WordNet. In <lb/>Gavrilidou M., Crayannis G., Markantonatu S., Piperidis S. and Stainhaouer G. (Eds.) Pro-<lb/>ceedings of LREC-2000, Second International Conference on Language Resources and Evalu-<lb/>ation, Athens, Greece, May 31-June 2, 2000, pp. 1413-1418. <lb/>[10] Preslav Nakov, Ariel Schwartz and Marti Hearst. Citances: Citation Sentences for Semantic <lb/>Analysis of Bioscience Text. SIGIR&apos;04 workshop on Search and Discovery in Bioinformatics, <lb/>Sheffield, UK. 2004. <lb/>[11] Laurent Romary and Chris Armbruster. Beyond Institutional Repositories. June 25, 2009. <lb/>Available at SSRN: http://ssrn.com/abstract=1425692 <lb/>[12] Laurent Romary. An abstract model for the representation of multilingual terminological data: <lb/>TMF -Terminological Markup Framework. TAMA (Terminology in Advanced Microcomputer <lb/>Applications), Antwerp (Belgium), February 1-2, 2001. Available at http://hal.inria.fr/inria-<lb/>00100405 <lb/>[13] Jacques Savoy. Combining Multiple Strategies for Effective Monolingual and Cross-Language <lb/>Retrieval. Information Retrieval 7(1-2) :121-148 (2004). <lb/>[14] Helmut Schmid. Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings of <lb/>International Conference on New Methods in Language Processing. September 1994. <lb/>[15] Luo SI and Jamie Callan. Using Sampled Data and Regression to Merge Search Engine <lb/>Results. Proceedings of the 25th annual international ACM SIGIR conference on Research and <lb/>development in information retrieval table of contents, Tampere, Finland, 2002. <lb/>[16] Franck Smadja, Kathleen R. McKeown and Vasileios Hatzivassiloglou. Translating collo-<lb/>cations for bilingual lexicons: A statistical approach. Computational Linguistics 22(1):1-38 <lb/>(1996). <lb/>[17] http://download.wikimedia.org/ <lb/>http://en.wikipedia.org/wiki/Portal:Contents/Categorical index <lb/>[18] http://sourceforge.net/projects/wikiprep <lb/>[19] Ian H. Witten and Eibe Frank. Data Mining: Practical machine learning tools and techniques, <lb/>2nd Edition, Morgan Kaufmann, San Francisco, 2005. </listBibl>


	</text>
</tei>
