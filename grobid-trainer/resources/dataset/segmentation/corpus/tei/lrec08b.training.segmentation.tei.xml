<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front> ParsCit: An open-source CRF reference string parsing package <lb/> Isaac G. Councill  *  , C. Lee Giles  *  , Min-Yen Kan  † <lb/>  *  College of Information Sciences &amp; Technology <lb/>The Pennsylvania State University <lb/> {icouncil,giles}@ist.psu.edu <lb/>  †  Department of Computer Science <lb/>National University of Singapore <lb/>kanmy@comp.nus.edu.sg <lb/> Abstract <lb/> We describe ParsCit, a freely available, open-source implementation of a reference string parsing package. At the core of ParsCit is a <lb/>trained conditional random field (CRF) model used to label the token sequences in the reference string. A heuristic model wraps this <lb/>core with added functionality to identify reference strings from a plain text file, and to retrieve the citation contexts. The package comes <lb/>with utilities to run it as a web service or as a standalone utility. We compare ParsCit on three distinct reference string datasets and show <lb/>that it compares well with other previously published work. <lb/></front>

			<body> 1. Introduction <lb/> In scholarly works, we acknowledge the past contribution <lb/>of fellow scientists by referring to their work through for-<lb/>mal citations. Scientific papers often conclude with a sec-<lb/>tion that lists referenced works in the form of a reference <lb/>list or bibliography. This form of acknowledgment is cru-<lb/>cial in helping readers and reviewers to relate the current <lb/>work to its context within the research community&apos;s dis-<lb/>course. <lb/>An ongoing focus within the bibliographic research com-<lb/>munity is the automatic creation of citation networks from <lb/>underlying source documents. A prerequisite to programat-<lb/>ically recovering links between referring and referred-to <lb/>documents requires a machine to understand the structure <lb/>of the strings in a reference section. Each reference string 1 <lb/> can be viewed as a set of fields (e.g., author, title, year, jour-<lb/>nal) that are represented as a surface string, with implicit <lb/>cues such as punctuation to assist in recovering the encoded <lb/>data. While parsing these reference strings at the end of a <lb/>document is often straightforward for human readers, the <lb/>sheer diversity of different standards espoused by different <lb/>communities, coupled with inadvertent errors on the part of <lb/>authors, makes this process difficult to automate. <lb/>Many methods have been proposed to deal with this se-<lb/>quence labeling problem (Peng and McCallum, 2004; Giles <lb/>et al., 1998). In this paper we describe our implementation <lb/>of ParsCit, a system that uses a core of machine learned <lb/>methods coupled with a heuristic processing framework. <lb/>While many methods that use machine learning have been <lb/>proposed for this exact problem (Huang et al., 2004; Cortez <lb/>et al., 2007), our contribution lies in 1) devising new fea-<lb/>tures useful for this problem, 2) automatically extracting ci-<lb/>tation contexts, 3) packaging our results as a software mod-<lb/>ule that can be called on a standalone basis or as a web <lb/>service, and 4) making our code open-source for the com-<lb/>munity&apos;s benefit. <lb/>

			<note place="footnote"> 1 We use &quot; reference string &quot; to refer to an item in the refer-<lb/>ence section at the end of a document, and &quot; citation &quot; to refer to <lb/>the pointers used in the body of the main text of a document. <lb/></note>

			In the remainder of this paper, we discuss the core learning <lb/>model, and detail the pre-and post-processing steps that <lb/>wrap the sequence labeling model into a working service. <lb/>We then describe the implementation details and usage of <lb/>the toolkit, and conclude with a comparison with related <lb/>work. <lb/> 2. Learning Model <lb/> We first formally define the problem to be solved. We <lb/>say that a reference string R is first broken down into <lb/>a sequence of tokens {r  1  , r  2  , ..., r  n  }. Each token is to <lb/>be assigned the correct label from a set of classes C = <lb/> {c  1  , c  2  , ..., c  m  }. Evidence used in classifying some token <lb/> r  i  can be any data that can be derived from the surface ref-<lb/>erence string, as well as previously-assigned r  1  ...r  i−1  clas-<lb/>sifications. <lb/>This sequence labeling problem is common to a large set <lb/>of NLP tasks, including part-of-speech tagging, chunking, <lb/>and semantic role labeling. It also embodies the reference <lb/>parsing problem tackled here, in which the classes are the <lb/>metadata fields such as author, title, journal, etc. In our im-<lb/>plementation, a total of 13 classes are labeled, correspond-<lb/>ing to common fields used in bibliographic reference man-<lb/>agement software (e.g., EndNote, BibTeX). <lb/>We use a conditional random field (Lafferty et al., 2001) <lb/>formalism to learn a model from labeled training data that <lb/>can be applied to unseen data. This learning model scales <lb/>well, handling large sets of possibly overlapping (i.e., con-<lb/>ditionally dependent) features. We have engineered our <lb/>features to rectify the classification errors made by models <lb/>created from previous work. We list the general category <lb/>of features used by the ParsCit system below; the number <lb/>of individual features used to represent each category are <lb/>given in parentheses. <lb/> Token identity (3): We encode separate features for each <lb/>token in three different forms: 1) as-is, 2) lowercased, <lb/>and 3) lowercased stripped of punctuation. <lb/> N-gram prefix/suffix (9): We encode 4 features for the <lb/> first 1-4 characters of the token, similarly for the last <lb/>1-4 characters. A single feature also examines the last <lb/>character of the token, encoding whether it is upper-<lb/>case, lowercase or numeric. <lb/> Orthographic case (1): We analyze the case of the to-<lb/>ken, assigning it one of four values: Initialcaps, <lb/> M ixedCaps, ALLCAP S, or others. <lb/> Punctuation (1): Similarly, <lb/>we give fine-grained <lb/>distinctions for the punctuation present in <lb/>token: <lb/> leadingQuotes, <lb/>endingQuotes, <lb/>multipleHyphens (occasionally found in page <lb/>ranges), continuingP unctuation (e.g., commas, <lb/>semicolons), stopP unctuation (e.g., periods, double <lb/>quotes), pairedBraces, possibleV olume (e.g., <lb/> &quot; 3(4) &quot; ), or others. <lb/> Number (1): We analyze the token for possible numeric <lb/>properties. The value of this feature can be spe-<lb/>cific, such as year (a value between 19xx and 20xx), <lb/> possibleP ageRange (contains [0 − 9] − [0 − 9]), <lb/> possibleV olume (contains [0 − 9]([0 − 9] * )), ordinal <lb/> (contains number followed with a suffix such as &quot; th &quot; ), <lb/>or a general class: 4digit, 3digit, 2digit, 1digit, <lb/> hasDigit or noDigits. <lb/> Dictionary (6): Separate analyzers check whether the to-<lb/>ken is a key within a hash table of possible pub-<lb/>lisher names, place names, surnames, female and male <lb/>names, and months. <lb/> Location (1): We code the relative location of the token <lb/>within the reference string, discretized into n uniform <lb/>bins (n was set to 12 by experimentation). In most <lb/>styles, more important data such as author, title, year <lb/>is placed towards the beginning of the citation string; <lb/>this feature attempts to capture regularities in position <lb/>on top of the sequence labeling strengths of the CRF <lb/>learner. <lb/> Possible editor (1): This feature indicates whether a token <lb/>such as &quot; eds. &quot; is present anywhere within the refer-<lb/>ence string. <lb/>Note that many of our features make fine-grained distinc-<lb/>tions (e.g., orthographic case, numeric, punctuation evi-<lb/>dence); these increase performance significantly over pre-<lb/>vious work. We also observe that misclassifications of edi-<lb/>tors for authors occur often in previous work; to correct for <lb/>this, we explicitly model the possible editor feature so that <lb/>long-range dependencies are factored out. <lb/>Features are applied to the current token to be tagged and, <lb/>for important features, applied to a contextual window of <lb/>words (window width of -2 to +2). We use the freely-<lb/>available CRF++ package 2 , which makes the application <lb/>of the feature inventory across multiple tokens easy. This <lb/>implementation of the CRF learning model was also se-<lb/>lected as it is licensed using the Lesser GNU Public License <lb/>(LGPL), which is suitable to be embedded in free and com-<lb/>mercial products. <lb/>

			<note place="footnote"> 2 http://crfpp.sourceforge.net/ <lb/></note>

			3. Pre-Processing Steps <lb/> Before reference strings can be properly extracted, it is nec-<lb/>essary to first find the references within an article. Although <lb/>formatting (e.g., font changes) may be of significant help, <lb/>dependency on specific formatting may lead to a loss of <lb/>generality. For this reason, ParsCit assumes only that docu-<lb/>ments are first converted to plain text, encoded using UTF-<lb/>8. Well-formed text extraction is notoriously difficult to do <lb/>with certain types of files (e.g., PDF files), but it is a critical <lb/>requirement for proper extraction. <lb/>Given a plain UTF-8 text file, ParsCit finds the reference <lb/>strings using a set of heuristics. It begins by searching for <lb/>a labeled reference section in the text. Labels may include <lb/>such strings as &quot; References &quot; , &quot; Bibliography &quot; , &quot; References <lb/>and Notes &quot; , or common variations of those strings. Text is <lb/>iteratively split around strings that appear to be reference <lb/>section labels. If a label is found too early in the docu-<lb/>ment according to a configurable parameter (under 40% of <lb/>the whole text, by default) , subsequent matches are sought. <lb/>The final match is considered the starting point of the ref-<lb/>erence section. Processing then begins to find the end point <lb/>by searching for subsequent section labels, such as appen-<lb/>dices, figures, tables, acknowledgments, autobiographies, <lb/>etc., or the end of the document. <lb/>Once the complete reference section is extracted, the next <lb/>phase is to segment individual reference strings. There <lb/>are three general cases for reference string segmentation: <lb/>1) strings are marked with square bracket or parenthetical <lb/>reference indicators (e.g., &quot; [1] &quot; , &quot; (1) &quot; , &quot; [Heckerman02] &quot; , <lb/>etc.), 2) strings are marked with naked numbers (e.g. &quot; 1 &quot; or <lb/> &quot; 1. &quot; ), and 3) strings are unmarked (such as in APA style). <lb/>The first step is therefore to find the marker type for the <lb/>citation list. This is done by constructing a number of reg-<lb/>ular expressions matching common marker styles for cases <lb/>1 and 2, then counting the number of matches to each ex-<lb/>pression in the reference string text. If either case yields <lb/>more matches than 1/6 of the total lines in the citation text, <lb/>the case with the greatest number of matches is indicated. <lb/>In both cases, the same regular expressions that were used <lb/>to find the marker type may be used to indicate the start-<lb/>ing point of a citation, and citations are segmented in this <lb/>manner. If no reference string markers are found, several <lb/>heuristics are used to decide where individual strings start <lb/>and end based on the length of previous lines (short length <lb/>indicates a possible final line of a reference string), strings <lb/>that appear to be author name lists (usually found at the <lb/>beginning of unmarked citations), and ending punctuation <lb/>(the final line of citation usually will end with a period. <lb/>The list of individual reference strings is then written out <lb/>and the CRF++ model as discussed earlier is applied to the <lb/>data. <lb/> 4. Post-Processing Steps <lb/> Based on the output of running CRF++, several steps are <lb/>necessary to normalize each tagged field into a standard <lb/>representation. Author names may occur in various orders <lb/>and formats in reference strings, such as &quot; M.-Y. Kan and <lb/>I. G. Councill &quot; or &quot; Kan, M.-Y. &amp; Councill, I. G. &quot; . The <lb/>name string must first be segmented into individual names <lb/>based on an analysis of separator locations (e.g., comma or <lb/> semicolon placement). Each name is then normalized to the <lb/>form &quot; M-Y Kan &quot; and &quot; I G Councill &quot; . Number fields such <lb/>as publication volume and number are normalized such that <lb/>only the numeric value is preserved (e.g. &quot; vol. 5 &quot; is nor-<lb/>malized to &quot; 5 &quot; ). Similarly, only the year portion of date <lb/>fields is preserved. Finally, page numbers are normalized <lb/>into the form &quot; start–end &quot; , such that a field &quot; pp. 584-589 &quot; <lb/>becomes &quot; 584–589 &quot; . <lb/> 5. Extracting Citation Contexts <lb/> Based on the reference marker that was discovered dur-<lb/>ing reference segmentation or generated during post-<lb/>processing, one or more regular expressions are generated <lb/>that can be used to scan the body text for citations to a par-<lb/>ticular reference string. These expressions vary based on <lb/>the three types of markers (corresponding to the three cases <lb/>for reference string segmentation above). For markers ex-<lb/>plicitly tagged with square bracket or parenthetical markers <lb/>in the reference section, the markers are converted into reg-<lb/>ular expressions directly. Naked number markers (e.g., &quot; 1 &quot; <lb/>or &quot; 1. &quot; ) are converted into square bracket and parenthetical <lb/>expressions. In the case of naked numbers, priority is given <lb/>to the square bracket representation, and the parenthetical <lb/>expression will not be applied if square bracket matches <lb/>occur in the body text. The marker expressions are flexi-<lb/>ble enough to handle cases where a match occurs in a list <lb/>of references (e.g., &quot; [12, 2, 5] &quot; ) without matching the same <lb/>numbers outside of the reference context (e.g., &quot; see Figure <lb/>(2) &quot; ). <lb/>Finally, markers from unmarked citation lists (such as in <lb/>APA style) will be generated based on the last names of <lb/>the authors and year of publication. Various forms of the <lb/>marker will be created, such that a paper authored by Pol-<lb/>jak, Rendl, and Wolkowicz in 1994 will yield the following <lb/>markers: 1) &quot; Poljak, Rendl, Wolkowicz, 1994 &quot; , 2) &quot; Pol-<lb/>jak, Rendl, and Wolkowicz, 1994 &quot; , 3) &quot; Poljak, Rendl, &amp; <lb/>Wolkowicz, 1994 &quot; , and 4) &quot; Poljak et al., 1994 &quot; . Some <lb/>added flexibility regarding omitted punctuation is built into <lb/>the regular expressions but is not included here for clarity. <lb/>Each regular expression is then applied to the body text to <lb/>generate a list of all context matches. The size of the con-<lb/>text string is configurable, but by default extends to 200 <lb/>characters on either side of the match. For the sake of ef-<lb/>ficiency when faced with long documents, matching will <lb/>cease after a configurable number of matches are found. <lb/> 6. Usage and API <lb/> ParsCit includes command line utilities for extracting ref-<lb/>erence strings from text documents. By default, text files <lb/>are expected to be encoded in UTF-8, but the expected en-<lb/>coding can be adjusted using perl command line switches. <lb/>To run ParsCit on a single document, users simply execute <lb/>a single command: <lb/> citeExtract.pl textfile [outfile] <lb/> If &quot; outfile &quot; is specified, the XML output will be written to <lb/>that file; otherwise, the XML will be printed to standard <lb/>output. <lb/>
			
			There is also a web service interface available, using the <lb/>SOAP::Lite perl module. To start the service, one executes: <lb/> parscit-service.pl <lb/> A Web Service Definition Language (WSDL) file is pro-<lb/>vided with the distribution that outlines the message de-<lb/>tails expected by the ParsCit service for use by developers. <lb/>Expected parameters in the input message are &quot; filePath &quot; <lb/>(a path to the text file to parse) and &quot; repositoryID &quot; . The <lb/>ParsCit service is designed for deployment in an envi-<lb/>ronment where text files may be located on file systems <lb/>mounted from arbitrary machines on the network. Thus, <lb/> &quot; repositoryID &quot; provides a means to map a given shared file <lb/>system to its mount point. Repository mappings are con-<lb/>figurable. The &quot; filePath &quot; parameter provides a path to the <lb/>text file relative to the repository mount point. The local <lb/>file system may be specified using the reserved repository <lb/>ID &quot; LOCAL &quot; . In that case, an absolute path to the text file <lb/>may be specified. <lb/>Both perl and ruby clients are also provided that demon-<lb/>strate how to use the service. For example, one can execute <lb/>the perl client with the following command: <lb/> parscit-client.pl filePath repositoryID <lb/> If the call is successful, the XML output will be printed to <lb/>standard output. <lb/>The ParsCit libraries may be used directly from <lb/>external perl applications through a single interface <lb/>module. <lb/>If XML output is desired (the default), <lb/>the ParsCit::Controller::extractCitations ($filePath) sub-<lb/>routine will suffice. <lb/>If it is desirable to have <lb/>faster, more structured access to citation data from <lb/>the external code, a more convenient implementa-<lb/>tion, ParsCit::Controller::extractCitationsImpl ($filePath), <lb/>is provided. Rather than returning the data in XML repre-<lb/>sentation, the parameters returned are a status code (code ¿ <lb/>0 indicates success), an error message (blank if no error), a <lb/>reference to a list of ParsCit::Citation objects containing the <lb/>parsed citation data, and a reference to the body text identi-<lb/>fied during pre-processing for subsequent context analysis <lb/>or indexing. <lb/> 7. Evaluation <lb/> An evaluation of ParsCit performance can take place at <lb/>two levels: the raw sequence decoding performance of <lb/>the underlying CRF model or the normalized output after <lb/>application-level post-processing. Most previous work cen-<lb/>ters on the core task of reference string parsing, but does <lb/>not include evaluations of field normalizations such as au-<lb/>thor delimitation or retrieval of citations contexts. These <lb/>two latter features are core aspects of ParsCit that make it <lb/>eminently suited for direct incorporation in external digi-<lb/>tal library software and frameworks. However, in order to <lb/>make direct comparisons with other work we limit our dis-<lb/>cussion to published results on reference parsing using pub-<lb/>licly available datasets. This limits us to evaluating ParsCit <lb/>on three different datasets of reference strings available for <lb/> the computer science domain and analyzing performance <lb/>on the CRF sequence decoding alone. <lb/> 7.1. Cora <lb/> The Cora dataset is derived from one of the first stud-<lb/>ies in automated reference string parsing (Seymore et al., <lb/>1999). This dataset created a gold standard for 200 refer-<lb/>ence strings sampled from various computer science publi-<lb/>cations. These citations were segmented into thirteen dif-<lb/>ferent fields – &quot; author &quot; , &quot; booktitle &quot; , &quot; date &quot; , &quot; editor &quot; , &quot; insti-<lb/>tution &quot; , &quot; journal &quot; , &quot; location &quot; , &quot; note &quot; , &quot; pages &quot; , &quot; publisher &quot; , <lb/> &quot; tech &quot; , &quot; title &quot; , and &quot; volume &quot; – reflective of BibTeX fields <lb/>that might be used to generate the references themselves. <lb/>Table 1 gives the field accuracy and F  1  of ParsCit, trained <lb/>using ten-fold cross validation, compared to the original <lb/>CRF-based system (Peng and McCallum, 2004) that in-<lb/>spired our work. Note that the Cora dataset does not fur-<lb/>ther segment the author field into individual authors; so our <lb/>evaluation is done by regarding any contiguous &quot; author &quot; <lb/>fields as a single field. <lb/>Field <lb/>ParsCit <lb/>Peng <lb/>Precision Recall F  1  Acc. F  1 <lb/> Author <lb/>98.7 <lb/>99.3 <lb/>.99 99.9 .99 <lb/>Booktitle <lb/>92.7 <lb/>94.2 <lb/>.93 97.7 .94 <lb/>Date <lb/>100 <lb/>98.4 <lb/>.99 99.8 .99 <lb/>Editor <lb/>92.0 <lb/>81.0 <lb/>.86 99.5 .88 <lb/>Institution <lb/>90.9 <lb/>87.9 <lb/>.89 99.7 .94 <lb/>Journal <lb/>90.8 <lb/>91.2 <lb/>.91 99.1 .91 <lb/>Location <lb/>95.6 <lb/>90.0 <lb/>.93 99.3 .87 <lb/>Note <lb/>74.2 <lb/>59.0 <lb/>.65 99.7 .81 <lb/>Pages <lb/>97.7 <lb/>98.4 <lb/>.98 99.9 .99 <lb/>Publisher <lb/>95.2 <lb/>88.7 <lb/>.92 99.4 .76 <lb/>Tech <lb/>94.0 <lb/>79.6 <lb/>.86 99.4 .87 <lb/>Title <lb/>96.0 <lb/>98.4 <lb/>.97 98.9 .98 <lb/>Volume <lb/>97.3 <lb/>95.5 <lb/>.96 99.9 .98 <lb/>Average* <lb/>95.7 <lb/>95.7 <lb/>.95 <lb/>– <lb/>.91 <lb/>Table 1: Field reference string parsing performance on the <lb/>Cora dataset using 10-fold cross validation. Averages are <lb/>micro averages for ParsCit and macro averages for (Peng <lb/>and McCallum, 2004). <lb/>We follow the the experimental methodology of the orig-<lb/>inal experiments done in (Peng and McCallum, 2004) as <lb/>closely as possible, using ten-fold cross validation with 50-<lb/>line slices of the training data. The results above show that <lb/>the core module of ParsCit that performs reference string <lb/>segmentation performs satisfactorily, and is largely compa-<lb/>rable to Peng and McCallum&apos;s original CRF based system. <lb/>The publicly-available implementation of ParsCit comes <lb/>loaded with a model trained over the full Cora dataset. <lb/> 7.2. CiteSeer  X <lb/> In order to characterize ParsCit&apos;s performance within its <lb/>largest deployment context, CiteSeerX, a separate data set <lb/>was generated by randomly sampling 200 reference strings <lb/>from the approximately 14 million strings within the Cite-<lb/>SeerX system at the time of the evaluation. Each reference <lb/>string was manually labeled in the very same manner as the <lb/>Cora data set. This sample contains reference strings in a <lb/>wide variety of formats. Table 2 shows the results of ap-<lb/>plying ParsCit to the CiteSeerX data set. Interestingly, per-<lb/>formance deteriorates significantly for all fields, indicating <lb/>that the Cora data set may not be representative of the va-<lb/>riety of reference string formats found within the computer <lb/>science domain. However, results are still good for most <lb/>fields and very good for author, title, and date fields, which <lb/>are the most critical fields for citation matching, a hallmark <lb/>feature of the CiteSeerX and Cora systems. <lb/>Further analysis of the mistakes that were made on the Cite-<lb/>SeerX data set reveals that most errors affect only small <lb/>portions of the reference string decoding. Approximately <lb/>51% of the strings were decoded perfectly. For the 49% of <lb/>strings where mistakes were made, Figure 1 shows the dis-<lb/>tribution of the percentage of tokens within the strings that <lb/>were misclassifed, showing that only a small percentage of <lb/>strings were damaged by more than 25%. Figure 2 further <lb/>shows that only 7% of all strings contained misclassifica-<lb/>tions in more than two separate fields. <lb/>Field <lb/>Precision Recall F  1 <lb/> Author <lb/>95.8 <lb/>95.7 <lb/>.96 <lb/>Booktitle <lb/>72.5 <lb/>92.9 <lb/>.81 <lb/>Date <lb/>98.8 <lb/>89.8 <lb/>.94 <lb/>Editor <lb/>95.6 <lb/>51.1 <lb/>.67 <lb/>Institution <lb/>70.9 <lb/>76.7 <lb/>.74 <lb/>Journal <lb/>88.0 <lb/>78.6 <lb/>.83 <lb/>Location <lb/>91.9 <lb/>78.4 <lb/>.85 <lb/>Note <lb/>88.9 <lb/>17.2 <lb/>.29 <lb/>Pages <lb/>90.3 <lb/>91.5 <lb/>.91 <lb/>Publisher <lb/>88.7 <lb/>74.8 <lb/>.81 <lb/>Tech <lb/>76.1 <lb/>70.0 <lb/>.73 <lb/>Title <lb/>91.9 <lb/>93.9 <lb/>.93 <lb/>Volume <lb/>89.3 <lb/>85.0 <lb/>.87 <lb/>Table 2: Field reference string parsing performance on the <lb/>CiteSeer  X  dataset. <lb/> 0 <lb/> 20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>Percent of tokens misclassified <lb/> Reference string number <lb/> Figure 1: On those citation strings where mistakes are made <lb/>(roughly half), this shows the distribution of the percentage <lb/>of tokens misclassified by ParsCit. <lb/> 0 <lb/> 20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>1 <lb/>1.5 <lb/>2 <lb/>2.5 <lb/>3 <lb/>3.5 <lb/>4 <lb/>4.5 <lb/>5 <lb/>5.5 <lb/>6 <lb/>Reference string number <lb/>Number of damaged fields <lb/> Figure 2: On those citation strings where mistakes are <lb/>made, this shows the distribution of the number of fields <lb/>that were damaged by misclassifications. Only about 7% of <lb/>citations in the test set are damaged in more than 2 fields. <lb/> 7.3. FLUX-CiM <lb/> The FLUX-CiM authors (Cortez et al., 2007) use two <lb/>different datasets to evaluate their unsupervised reference <lb/>string parsing system: a health sciences dataset and a com-<lb/>puter science dataset. Unlike FLUX-CiM, ParsCit is a su-<lb/>pervised system and can be re-trained for the particularities <lb/>of the Health Science domain. As we have yet to complete <lb/>the preparation work necessary for re-training, we have <lb/>only compiled the FLUX-CiM results for CS dataset, as <lb/>the domain matches the Cora dataset. FLUX-CiM reports <lb/>accuracies and F  1  scores for each type of field, but addi-<lb/>tionally segments contiguous authors as individual fields. <lb/>
			
			FLUX-CiM annotates ten fields, differing from Cora&apos;s thir-<lb/>teen. A crosswalk to convert Cora annotation to FLUX-<lb/>CiM was generated (collapsing &quot; editors &quot; with &quot; authors &quot; ; <lb/> &quot; institution &quot; with &quot; publisher &quot; ; omitting &quot; note &quot; and &quot; tech &quot; ; <lb/>and expanding &quot; volume &quot; to differentiate between &quot; volume &quot; <lb/>and &quot; number &quot; ). <lb/>The FLUX-CiM CS dataset was &quot; gathered [from] a het-<lb/>erogenous collection composed by assorted references <lb/>from several conferences and journals in [computer sci-<lb/>ence] area. &quot; The dataset has 300 instances, of which 14 are <lb/>duplicates. Since no gold-standard markup was available, <lb/>we retagged the provided raw input strings using the cross-<lb/>walk. We then applied ParsCit (trained on the Cora model) <lb/>to test its performance against FLUX-CiM. We follow their <lb/>evaluation metrics and report field-specific precision, recall <lb/>and F  1  values. <lb/>Statistics for FLUX-CiM are replicated from their pub-<lb/>lished work; ParsCit&apos;s field accuracy is compiled using the <lb/> conlleval.pl script provided by the CoNLL confer-<lb/>ence shared tasks on chunk labeling. Table 3 shows that <lb/>ParsCit compares favorably against FLUX-CiM on the key <lb/>common fields of &quot; author &quot; and &quot; title &quot; , as was seen in the <lb/>CiteSeer  X  dataset. Key errors that the system makes in <lb/>comparison with FLUX-CiM is in not segmenting &quot; vol-<lb/>ume &quot; , &quot; number &quot; and &quot; pages &quot; , as ParsCit currently does not <lb/>further tokenize beyond whitespaces in the reference string <lb/>(e.g., &quot; 11(4):11-22 &quot; versus &quot; 11 (4) 11 -22 &quot; ). FLUX-CiM <lb/>does and is able to distinguish these fields more accurately. <lb/>We plan on incorporating some preprocessing heuristics to <lb/>ParsCit to correct for such errors. <lb/> 8. Related Work <lb/> The problem of citation parsing has been the focus of sev-<lb/>eral research initiatives (Cameron, 1997; Lawrence et al., <lb/>1999). We examine existing citation parsers, which can be <lb/>generally divided into two categories: template matching <lb/>and machine learning based approaches. <lb/>A template matching approach takes an input citation and <lb/>matches its syntactic pattern against known templates. The <lb/>template with the best fit to the input is then used to label <lb/>the citation&apos;s tokens as fields. The canonical example of a <lb/>template based approach is ParaTools (Jewell, 2000), a set <lb/>of Perl modules to perform reference string parsing. Para-<lb/>Tools contains 400 templates to match reference strings to, <lb/>but even this large amount manifests coverage problems. <lb/>While users may choose to add new templates to ParaTools <lb/>manually, the process is cumbersome and unscalable. The <lb/>fact that authors may not strictly adhere to citation styles or <lb/>that text extraction or OCR may produce reference strings <lb/>that do not adhere to the templates also diminishes this util-<lb/>ity. A further weakness of ParaTools is that it tags ambigu-<lb/>ous fields as &quot; Any &quot; , equivalent to not tagging the token at <lb/>all. (Huang et al., 2004) report ParaTool&apos;s precision as ap-<lb/>proximately 30%. This level of performance and lack of <lb/>portability make the approach unsuitable for high volume <lb/>data processing. <lb/>The limitations of the template-based approach have en-<lb/>couraged researchers to try supervised machine learned <lb/>models for citation parsing. Given sufficient training data, <lb/>a machine-learned parser can produce high performance in <lb/>accuracy, regardless of citation styles. We review four sys-<lb/>tems published in recent years that deal with this work. <lb/>Seymore et al. (1999)&apos;s work led to the creation of the <lb/>Cora dataset. Their approach used a Hidden Markov Model <lb/>(HMM) to build a reference string sequence labeler. Un-<lb/>like a standard HMM, they propose and validate perfor-<lb/>mance improvements when using internal states for differ-<lb/>ent parts of the field (similar to IOB encoding on other la-<lb/>beling tasks). In later work by the same group, Peng and <lb/>McCallum (2004) used the reference string parsing task as <lb/>a benchmark for testing Conditional Random Fields (CRF). <lb/>Their work established CRFs as strong learning model for <lb/>this task. This work motivates our choice of a CRF as the <lb/>base learning model for ParsCit. <lb/>The first version of ParsCit used Maximum Entropy (ME) <lb/>training to compute a model (Ng, 2004). Aside from using <lb/>ME, which can be seen as a step towards a discriminative <lb/>version of Hidden Markov Models, this work featured two <lb/>rounds of prediction: a first round to label a reference string <lb/>itself and a second, global round, that takes into account <lb/>how other reference strings nearby (e.g., in a Reference or <lb/>Bibliography section) were labeled by the first round. This <lb/>approach is the only one that tries to take advantage of such <lb/>information, which may prove useful in cases where a spe-<lb/>cific bibliographic style is followed. <lb/>FLUX-CiM (Cortez et al., 2007) features an unsupervised <lb/>approach to the problem that uses a frequency-tuned lexi-<lb/> Field <lb/> ParsCit <lb/>FLUX-CiM <lb/>Precision Recall F  1  Precision Recall <lb/> F  1 <lb/> Author <lb/>98.8 <lb/>99.0 <lb/>.99 <lb/>93.5 <lb/>95.6 <lb/>0.95 <lb/>Title <lb/>98.8 <lb/>98.3 <lb/>.96 <lb/>93.0 <lb/>93.0 <lb/>0.93 <lb/>Journal <lb/>97.1 <lb/>82.9 <lb/>.89 <lb/>95.7 <lb/>97.8 <lb/>0.97 <lb/>Date <lb/>99.8 <lb/>94.5 <lb/>.97 <lb/>97.8 <lb/>97.4 <lb/>0.98 <lb/>Pages <lb/>94.7 <lb/>99.3 <lb/>.97 <lb/>97.0 <lb/>97.8 <lb/>0.97 <lb/>Conference(Booktitle) <lb/>95.7 <lb/>99.3 <lb/>.97 <lb/>97.4 <lb/>95.4 <lb/>0.96 <lb/>Place(Location) <lb/>96.9 <lb/>88.4 <lb/>.89 <lb/>96.8 <lb/>97.6 <lb/>0.97 <lb/>Publisher <lb/>98.8 <lb/>75.9 <lb/>.85 <lb/>100.0 <lb/>100.0 1.00 <lb/>Number <lb/>– <lb/>– <lb/>– <lb/>97.9 <lb/>97.9 <lb/>0.98 <lb/>Volume <lb/>95.3 <lb/>89.7 <lb/>.92 <lb/>100.0 <lb/>98.2 <lb/>0.99 <lb/>Average <lb/>97.4 <lb/>97.4 <lb/>.94 <lb/>96.9 <lb/>97.1 <lb/>0.97 <lb/>Table 3: Field reference string parsing performance on the FLUX-CiM dataset. <lb/>con. The approach takes a four stage approach of blocking, <lb/>matching, binding and joining. The last step is comparable <lb/>to ParsCit&apos;s final step of breaking up a continguous fields <lb/>such as &quot; author &quot; into component fields with the same tag. <lb/>As stated earlier FLUX-CiM performs markedly well with <lb/>respect to journal articles where the fields of &quot; volume &quot; and <lb/> &quot; number &quot; are prevalent. <lb/>Retrieving citation contexts has been a key feature in Cite-<lb/>Seer and nascent digital libraires. Approaches continue to <lb/>be heuristically-driven, in both this system and (Powley and <lb/>Dale, 2007). Work continues in the community to utilize <lb/>citation contexts to discern a citation&apos;s function and com-<lb/>pile a summary of how a work influences or is described by <lb/>others (Teufel et al., 2006; Wu et al., 2006; Schwartz et al., <lb/>2007). <lb/> 9. Conclusion <lb/> We have introduced ParsCit, an open-source package for <lb/>locating reference strings, parsing them and retrieving their <lb/>citation contexts 3 . ParsCit employs state-of-the-art ma-<lb/>chine learning models to achieve its high accuracy in refer-<lb/>ence string segmentation, and heuristic rules to locate and <lb/>delimit the reference strings and to locate citation contexts. <lb/>ParsCit has been successfully deployed within CiteSeer  X 4 , <lb/>a large-scale digital library of computer science publica-<lb/>tions that has recently been released. It is hoped that by <lb/>making the source of the ParsCit package open to all that <lb/>the community at large can benefit from its use in further-<lb/>ing natural language, digital library and scholarly dissemi-<lb/>nation research. ParsCit is one of the tool deliverables as-<lb/>sociated with the ACL ARC project, described in a separate <lb/>LREC paper (Bird et al., 2008). <lb/>In current work, we plan to continue evaluating and tun-<lb/>ing ParsCit by taking advantage of more training data. We <lb/>welcome feedback from the community in using ParsCit. <lb/>
		
		</body>

		<back>
			
			<div type="acknowledgement">10. Acknowledgments <lb/> The authors thank Yong Kiat Ng for his prior work on a <lb/>maximum entropy based precursor to the current ParsCit. <lb/>Min-Yen Kan&apos;s work on the project was generously sup-<lb/>ported by AcRF Tier 1 grant R-252-000-288-112. Isaac <lb/></div>

			<note place="footnote"> 3 Available at http://wing.comp.nus.edu.sg/parsCit/. <lb/> 4 http://citeseerx.ist.psu.edu <lb/></note>

			<listBibl> Councill and C. Lee Giles are partially supported by NSF <lb/>CRI 0454052. <lb/> 11. References <lb/> Steven Bird, Robert Dale, Bonnie J. Dorr, Bryan Gib-<lb/>son, Mark T. Joseph, Min-Yen Kan, Dongwon Lee, <lb/>Brett Powley, Dragomir R. Radev, and Yee Fan Tan. <lb/>2008. The ACL anthology reference corpus: A refer-<lb/>ence dataset for bibliographic research. In Language Re-<lb/>sources and Evaluation Conference. <lb/> Robert D. Cameron. 1997. A universal citation database as <lb/>a catalyst for reform in scholarly communication. First <lb/>Monday, 2(4). <lb/>Eli Cortez, Altigran S. da Silva, Marcos André Gonçalves, <lb/>Filipe Mesquita, and Edleno S. de Moura. 2007. Flux-<lb/>cim: flexible unsupervised extraction of citation meta-<lb/>data. In JCDL &apos;07: Proceedings of the 2007 Confer-<lb/>ence on Digital Libraries, pages 215–224, New York, <lb/>NY, USA. ACM. <lb/>C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. <lb/>1998. Citeseer: an automatic citation indexing system. <lb/>In DL &apos;98: Proceedings of the third ACM conference <lb/>on Digital libraries, pages 89–98, New York, NY, USA. <lb/>ACM Press. <lb/>I-Ane Huang, Jan-Ming Ho, Hung-Yu Kao, and Wen-<lb/>Chang Lin. 2004. Extracting citation metadata from <lb/>online publication lists using BLAST. In Proc. of the <lb/>Eighth Pacific-Asia Conference on Knowledge Discovery <lb/>and Data Mining (PAKDD-04), Sydney, Australia, May. <lb/>Michael Jewell. <lb/>2000. ParaCite: <lb/>An overview. <lb/>http://paracite.eprints.org/docs/overview.html. <lb/>John D. Lafferty, Andrew McCallum, and Fernando C. N. <lb/>Pereira. 2001. Conditional random fields: Probabilistic <lb/>models for segmenting and labeling sequence data. In <lb/> ICML &apos;01: Proceedings of the Eighteenth International <lb/>Conference on Machine Learning, pages 282–289, San <lb/>Francisco, CA, USA. Morgan Kaufmann Publishers Inc. <lb/>Steve Lawrence, C. Lee Giles, and Kurt Bollacker. 1999. <lb/>Digital libraries and autonomous citation indexing. <lb/> IEEE Computer, 32(6):67–71. <lb/>Yong Kiat Ng. 2004. Citation parsing using maximum en-<lb/>tropy and repairs. Undergraduate thesis, National Uni-<lb/>versity of Singapore. <lb/> Fuchun Peng and Andrew McCallum. 2004. Accurate in-<lb/>formation extraction from research papers using condi-<lb/>tional random fields. In In Proceedings of Human Lan-<lb/>guage Technology Conference / North American Chapter <lb/>of the Association for Computational Linguistics annual <lb/>meeting, pages 329–336. <lb/>Brett Powley and Robert Dale. 2007. Evidence-based in-<lb/>formation extraction for high accuracy citation and au-<lb/>thor name identification. Recherche d&apos;Information As-<lb/>sist. <lb/> Ariel Schwartz, Anna Divoli, and Marti Hearst. 2007. <lb/>Multiple alignment of citation sentences with conditional <lb/>random fields and posterior decoding. In Proceedings of <lb/>the 2007 Joint Conference on Empirical Methods in Nat-<lb/>ural Language Processing and Computational Natural <lb/>Language Learning (EMNLP-CoNLL), pages 847–857, <lb/>June. <lb/>Kristie Seymore, Andrew McCallum, and Roni Rosenfeld. <lb/>1999. Learning hidden markov model structure for in-<lb/>formation extraction. In AAAI&apos;99 Workshop on Machine <lb/>Learning for Information Extraction. <lb/> Simone Teufel, Advaith Siddharthan, and Dan Tidhar. <lb/>2006. Automatic classification of citation function. In <lb/> Proceedings of the 2006 Conference on Empirical Meth-<lb/>ods in Natural Language Processing, pages 103–110, <lb/>Sydney, Australia, July. Association for Computational <lb/>Linguistics. <lb/>Jien-Chen Wu, Yu-Chia Chang, Hsien-Chin Liou, and Ja-<lb/>son S. Chang. 2006. Comutational analysis of move <lb/>structures in acdemic abstracts. In COLING/ACL Inter-<lb/>active Presentation Sessions, pages 41–44. </listBibl>

		</back>
	</text>
</tei>
