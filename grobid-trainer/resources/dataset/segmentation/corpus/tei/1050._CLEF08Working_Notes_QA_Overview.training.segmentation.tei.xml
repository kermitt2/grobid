<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>OVERVIEW OF THE CLEF 2008 <lb/>MULTILINGUAL QUESTION ANSWERING <lb/>TRACK <lb/>Pamela Forner 1 , Anselmo Peñas 2 , Eneko Agirre 3 , Iñaki Alegria 4 , Corina <lb/>Forăscu 5 , Nicolas Moreau 6 , Petya Osenova 7 , Prokopis Prokopidis 8 , Paulo Ro-<lb/>cha 9 , Bogdan Sacaleanu 10 , Richard Sutcliffe 11 , and Erik Tjong Kim Sang 12 <lb/>1 CELCT, Trento, Italy (forner@celct.it) <lb/>2 Departamento de Lenguajes y Sistemas Informáticos, UNED, Madrid, Spain <lb/>(anselmo@lsi.uned.es) <lb/>3 Computer Science Department, University of Basque Country, Spain (e.agirre@ehu.es) <lb/>4 <lb/>University of Basque Country, Spain (i.alegria@ehu.es) <lb/>5 UAIC and RACAI, Romania (corinfor@info.uaic.ro) <lb/>6 ELDA/ELRA, Paris, France (moreau@elda.org) <lb/>7 BTB, Bulgaria, (petya@bultreebank.org) <lb/>8 ILSP Greece, Athena Research Center (prokopis@ilsp.gr) <lb/>9 Linguateca, DEI UC, Portugal, (Paulo.Rocha@di.uminho.pt) <lb/>10 DFKI, Germany, (bogdan@dfki.de) <lb/>11 DLTG, University of Limerick, Ireland (richard.sutcliffe@ul.ie) <lb/>12 University of Groningen (e.f.tjong.kim.sang@rug.nl) <lb/>Abstract The QA campaign at CLEF [1], was manly the same as that proposed <lb/>last year. The results and the analyses reported by last year&apos;s participants sug-<lb/>gested that the changes introduced in the previous campaign had led to a drop in <lb/>systems&apos; performance. So for this year&apos;s competition it has been decided to practi-<lb/>cally replicate last year&apos;s exercise. <lb/>Following last year&apos;s experience some QA pairs were grouped in clusters. Every <lb/>cluster was characterized by a topic (not given to participants). The questions from <lb/>a cluster contained co-references between one of them and the others. Moreover, <lb/>as last year, the systems were given the possibility to search for answers in Wiki-<lb/>pedia 1 as document corpus beside the usual newswire collection. <lb/>In addition to the main task, three additional exercises were offered, namely the <lb/>Answer Validation Exercise (AVE), the Question Answering on Speech Tran-<lb/>scriptions (QAST), which continued last year&apos;s successful pilot, and Word Sense <lb/>Disambiguation for Question Answering (QA-WSD). <lb/>As general remark, it must be said that the task still proved to be very challenging <lb/>for participating systems. In comparison with last year&apos;s results the Best Overall <lb/>Accuracy dropped significantly from 41,75% to 19% in the multi-lingual subtasks, <lb/></front>

			<note place="footnote">1 http://wikipedia.org <lb/></note>

			<front>while instead it increased a little in the monolingual sub-tasks, going from 54% to <lb/>63,5%. <lb/></front>

			<body>1 Introduction <lb/>QA@CLEF 2008 was carried out according to the spirit of the campaign, con-<lb/>solidated in previous years. Beside the classical main task, three additional exer-<lb/>cises were proposed: <lb/>• the main task: several monolingual and cross-language sub-tasks, were of-<lb/>fered: Bulgarian, English, French, German, Italian, Portuguese, Romanian, <lb/>Greek, Basque and Spanish were proposed as both query and target languages. <lb/>• the Answer Validation Exercise (AVE) [2]: in its third round was aimed at eva-<lb/>luating answer validation systems based on textual entailment recognition. In <lb/>this task, systems were required to emulate human assessment of QA responses <lb/>and decide whether an Answer to a Question is correct or not according to a <lb/>given Text. Results were evaluated against the QA human assessments. <lb/>• the Question Answering on Speech Transcripts (QAST) [3,14]: which contin-<lb/>ued last year&apos;s successful pilot task, aimed at providing a framework in which <lb/>QA systems could be evaluated when the answers to factual and definition <lb/>questions must be extracted from spontaneous speech transcriptions. <lb/>• the Word Sense Disambiguation for Question Answering (QA-WSD) [4], a pi-<lb/>lot task which provided the questions and collections with already disambi-<lb/>guated Word Senses in order to study their contribution to QA performance. <lb/>As far as the main task is concerned, following last year experience, the exer-<lb/>cise consisted of topic-related questions, i.e. clusters of questions which were re-<lb/>lated to the same topic and contained co-references between one question and the <lb/>others. The requirement for questions related to a topic necessarily implies that the <lb/>questions refer to common concepts and entities within the domain in question. <lb/>This is accomplished either by co-reference or by anaphoric reference to the topic, <lb/>implicit or explicitly expressed in the first question or in its answer. <lb/>Moreover, besides the usual news collections provided by ELRA/ELDA, arti-<lb/>cles from Wikipedia were considered as an answer source. Some questions could <lb/>have answers only in one collection, i.e. either only in the news corpus or in <lb/>Wikipedia. <lb/>As a general remark, this year we had the same number of participants as in <lb/>2007 campaign, but the number of submissions went up. Due to the complexity of <lb/>the innovation introduced in 2007 -the introduction of topics and anaphora, list <lb/>questions, Wikipedia corpus -the questions tended to get a lot more difficult and <lb/>the performance of systems dropped dramatically, so, people were disinclined to <lb/></body>

			<body>continue the following year (i.e. 2008), inverting the positive trend in participation <lb/>registered in the previous campaigns. <lb/>As reflected in the results, the task proved to be even more difficult than ex-<lb/>pected. Results improved in the monolingual subtasks but are still very low in the <lb/>cross-lingual subtasks. <lb/>This paper describes the preparation process and presents the results of the QA <lb/>track at CLEF 2008. In section 2, the tasks of the track are described in detail. The <lb/>results are reported in section 3. In section 4, some final analysis about this cam-<lb/>paign is given. <lb/>2 Task Description <lb/>As far as the main task is concerned, the consolidated procedure was followed, <lb/>capitalizing on the experience of the task proposed in 2007. <lb/>The exercise consisted of topic-related questions, i.e. clusters of questions <lb/>which were related to the same topic and contained co-references between one <lb/>question and the others. Neither the question types (F, D, L) nor the topics were <lb/>given to the participants. <lb/>The systems were fed with a set of 200 questions -which could concern facts or <lb/>events (F-actoid questions), definitions of people, things or organisations (D-<lb/>efinition questions), or lists of people, objects or data (L-ist questions)-and were <lb/>asked to return up to three exact answers per question, where exact meant that nei-<lb/>ther more nor less than the information required was given. <lb/>The answer needed to be supported by the docid of the document in which the <lb/>exact answer was found, and by portion(s) of text, which provided enough context <lb/>to support the correctness of the exact answer. Supporting texts could be taken <lb/>from different sections of the relevant documents, and could sum up to a maxi-<lb/>mum of 700 bytes. There were no particular restrictions on the length of an an-<lb/>swer-string, but unnecessary pieces of information were penalized, since the an-<lb/>swer was marked as ineXact. As in previous years, the exact answer could be <lb/>exactly copied and pasted from the document, even if it was grammatically incor-<lb/>rect (e.g.: inflectional case did not match the one required by the question). Any-<lb/>way, systems were also allowed to use natural language generation in order to cor-<lb/>rect morpho-syntactical inconsistencies (e.g., in German, changing dem <lb/>Presidenten into der President if the question implies that the answer is in nomi-<lb/>native case), and to introduce grammatical and lexical changes (e.g., QUESTION: <lb/>What nationality is X? TEXT: X is from the Netherlands EXACT ANSWER: <lb/>Dutch). <lb/>The subtasks were both: <lb/>• monolingual, where the language of the question (Source language) and <lb/>the language of the news collection (Target language) were the same; <lb/></body>

			<body>• cross-lingual, where the questions were formulated in a language differ-<lb/>ent from that of the news collection. <lb/>Two new languages have been added, i.e. Basque and Greek both as source <lb/>and target languages. In total eleven source languages were considered, <lb/>namely, Basque, Bulgarian, Dutch, English, French, German, Greek, Italian, <lb/>Portuguese, Romanian and Spanish. All these languages were also considered <lb/>as target languages. <lb/>Table 1. Tasks activated in 2008 (coloured cells) <lb/>TARGET LANGUAGES (corpus and answers) <lb/>BG <lb/>DE <lb/>EL <lb/>EN <lb/>ES <lb/>EU FR <lb/>IT <lb/>NL <lb/>PT <lb/>RO <lb/>SOURCE LANGUAGES <lb/>(questions) <lb/>BG <lb/>DE <lb/>EL <lb/>EN <lb/>ES <lb/>EU <lb/>FR <lb/>IT <lb/>NL <lb/>PT <lb/>RO <lb/>As shown in Table 1, 43 tasks were proposed: <lb/>• 10 Monolingual -i.e. Bulgarian (BG), German (DE), Greek (EL), Spanish <lb/>(ES), Basque (EU), French (FR), Italian (IT), Dutch (NL), Portuguese <lb/>(PT) and Romanian (RO); <lb/>• 33 Cross-lingual (as customary in recent campaigns, in order to prepare <lb/>the cross-language subtasks, for which at least one participant had regis-<lb/>tered, some target language question sets were translated into the com-<lb/>bined source languages). <lb/>Anyway, as Table 2 shows, not all the proposed tasks were then carried out by <lb/>the participants. <lb/>Table 2. Tasks chosen by at least 1 participant in QA@CLEF campaigns <lb/>MONOLINGUAL <lb/>CROSS-LINGUAL <lb/>CLEF-2004 <lb/>6 <lb/>13 <lb/>CLEF-2005 <lb/>8 <lb/>15 <lb/>CLEF-2006 <lb/>7 <lb/>17 <lb/>CLEF-2007 <lb/>7 <lb/>11 <lb/>CLEF-2008 <lb/>8 <lb/>12 <lb/>As long-established, the monolingual English (EN) task was not available as it <lb/>seems to have been already thoroughly investigated in TREC campaigns. English <lb/>was still both source and target language in the cross-language tasks. <lb/>2.1 Questions Grouped by Topic <lb/>The procedure followed to prepare the test set was the same as that used in the <lb/>2007 campaign. First of all, each organizing group, responsible for a target lan-<lb/>guage, freely chose a number of topics. For each topic, one to four questions were <lb/>generated. Topics could be not only named entities or events, but also other cate-<lb/>gories such as objects, natural phenomena, etc. (e.g. George W. Bush; Olympic <lb/>Games; notebooks; hurricanes; etc.). The set of ordered questions were related to <lb/>the topic as follows: <lb/>• the topic was named either in the first question or in the first answer <lb/>• the following questions could contain co-references to the topic expressed in <lb/>the first question/answer pair. <lb/>Topics were not given in the test set, but could be inferred from the first ques-<lb/>tion/answer pair. For example, if the topic was George W. Bush, the cluster of <lb/>questions related to it could have been: <lb/>Q1: Who is George W. Bush?; Q2: When was he born?; Q3: Who is his wife? <lb/>The requirement for questions related to a same topic necessarily implies that <lb/>the questions refer to common concepts and entities within the domain. The most <lb/>common form is pronominal anaphoric reference to the topic declared in the first <lb/>question, e.g.: <lb/>Q4: What is a polygraph?; Q5: When was it invented? <lb/>However, other forms of co-reference occurred in the questions. Here is an ex-<lb/>ample: <lb/>Q6: Who wrote the song &quot;Dancing Queen&quot;?; Q7: How many people were in the <lb/>group? <lb/>Here the group refers to an entity expressed not in the question but only in the <lb/>answer. However the QA system does not know this and has to infer it, a task <lb/>which can be very complex, especially if the topic is not provided in the test set. <lb/>2.2 Document collections <lb/>Beside the data collections composed of news articles provided by <lb/>ELRA/ELDA (see Table 3), also Wikipedia was considered. <lb/>The Wikipedia pages in the target languages, as found in the version of No-<lb/>vember 2006, could be used. Romanian had Wikipedia 2 as the only document col-<lb/>lection, because there was no newswire Romanian corpus. The &quot;snapshots&quot; of <lb/>Wikipedia were made available for download both in XML and HTML versions. <lb/>The answers to the questions had to be taken from actual entries or articles of <lb/>Wikipedia pages. Other types of data such as images, discussions, categories, <lb/>templates, revision histories, as well as any files with user information and meta-<lb/>information pages, had to be excluded. <lb/>One of the major reasons for using Wikipedia was to make a first step towards <lb/>web formatted corpora where to search for answers. In fact, as nowadays so large <lb/>information sources are available on the web, this may be considered a desirable <lb/>next level in the evolution of QA systems. An important advantage of Wikipedia <lb/>is that it is freely available for all languages so far considered. Anyway the varia-<lb/>tion in size of Wikipedia, depending on the language, is still problematic. <lb/>2.3 Types of Questions <lb/>As far as the question types are concerned, as in previous campaigns, the three <lb/>following categories were considered: <lb/>1. Factoid questions, fact-based questions, asking for the name of a person, a lo-<lb/>cation, the extent of something, the day on which something happened, etc. We <lb/>consider the following 8 answer types for factoids: <lb/>-PERSON, e.g.: Q8: Who was called the &quot;Iron-Chancellor&quot;? A8: Otto von <lb/>Bismarck. <lb/></body>

			<note place="footnote">2 http://static.wikipedia.org/downloads/November_2006/ro/ <lb/></note>

			<body>-TIME, e.g.: Q9: What year was Martin Luther King murdered? A9: 1968. <lb/>-LOCATION, e.g.: Q10: Which town was Wolfgang Amadeus Mozart born <lb/>in? A10: Salzburg. <lb/>-ORGANIZATION, e.g.: Q11: What party does Tony Blair belong to?: <lb/>A11: Labour Party. <lb/>-MEASURE, e.g.: Q12: How high is Kanchenjunga? A12: 8598m. <lb/>-COUNT, e.g.: Q13: How many people died during the Terror of PoPot? <lb/>A13: 1 million. <lb/>-OBJECT, e.g.: Q14: What does magma consist of? A14: Molten rock. <lb/>-OTHER, i.e. everything that does not fit into the other categories above, <lb/>e.g.: Q15: Which treaty was signed in 1979? A15: Israel-Egyptian peace <lb/>treaty. <lb/>Table 3. Document collections used in QA@CLEF 2008 <lb/>TARGET LANG. COLLECTION <lb/>PERIOD <lb/>SIZE <lb/>[BG] Bulgarian <lb/>Sega <lb/>2002 <lb/>120 MB (33,356 docs) <lb/>Standart <lb/>Novinar <lb/>2002 <lb/>2002 <lb/>93 MB (35,839 docs) <lb/>[DE] German <lb/>[EL] Greek <lb/>Frankfurter Rundschau <lb/>1994 <lb/>320 MB (139,715 docs) <lb/>Der Spiegel <lb/>1994/1995 <lb/>63 MB (13,979 docs) <lb/>German SDA <lb/>1994 <lb/>144 MB (71,677 docs) <lb/>German SDA <lb/>The Southeast European Times <lb/>1995 <lb/>2002 <lb/>141 MB (69,438 docs) <lb/>[EN] English <lb/>Los Angeles Times <lb/>1994 <lb/>425 MB (113,005 docs) <lb/>Glasgow Herald <lb/>1995 <lb/>154 MB (56,472 docs) <lb/>[ES] Spanish <lb/>[EU] Basque <lb/>EFE <lb/>1994 <lb/>509 MB (215,738 docs) <lb/>EFE <lb/>Egunkaria <lb/>1995 <lb/>2001/2003 <lb/>577 MB (238,307 docs) <lb/>[FR] French <lb/>Le Monde <lb/>1994 <lb/>157 MB (44,013 docs) <lb/>Le Monde <lb/>1995 <lb/>156 MB (47,646 docs) <lb/>French SDA <lb/>1994 <lb/>86 MB (43,178 docs) <lb/>French SDA <lb/>1995 <lb/>88 MB (42,615 docs) <lb/>[IT] Italian <lb/>La Stampa <lb/>1994 <lb/>193 MB (58,051 docs) <lb/>Italian SDA <lb/>1994 <lb/>85 MB (50,527 docs) <lb/>Italian SDA <lb/>1995 <lb/>85 MB (50,527 docs) <lb/>[NL] Dutch <lb/>NRC Handelsblad <lb/>1994/1995 <lb/>299 MB (84,121 docs) <lb/>Algemeen Dagblad <lb/>1994/1995 <lb/>241 MB (106,483 docs) <lb/>[PT] Portuguese <lb/>Público <lb/>1994 <lb/>164 MB (51,751 docs) <lb/>Público <lb/>1995 <lb/>176 MB (55,070 docs) <lb/>Folha de São Paulo <lb/>1994 <lb/>108 MB (51,875 docs) <lb/>Folha de São Paulo <lb/>1995 <lb/>116 MB (52,038 docs) <lb/>2. Definition questions, questions such as &quot;What/Who is X?&quot;, and are divided into <lb/>the following subtypes: <lb/>-PERSON, i.e., questions asking for the role/job/important information <lb/>about someone, e.g.: Q16: Who is Robert Altmann? A16: Film maker <lb/>-ORGANIZATION, i.e., questions asking for the mission/full <lb/>name/important information about an organization, e.g.: Q17: What is the <lb/>Knesset? A17: Parliament of Israel. <lb/>-OBJECT, i.e., questions asking for the description/function of objects, e.g.: <lb/>Q18: What is Atlantis? A18: Space Shuttle. <lb/>-OTHER, i.e., question asking for the description of natural phenomena, <lb/>technologies, legal procedures etc., e.g.: Q19: What is Eurovision? A19: <lb/>Song contest. <lb/>3. closed list questions: i.e., questions that require one answer containing a de-<lb/>termined number of items, e.g.: Q20: Name all the airports in London, Eng-<lb/>land. A20: Gatwick, Stansted, Heathrow, Luton and City. <lb/>As only one answer was allowed, all the items had to be present in sequence in <lb/>the document and copied, one next to the other, in the answer slot. <lb/>Besides, all types of questions could contain a temporal restriction, i.e. a tem-<lb/>poral specification that provided important information for the retrieval of the cor-<lb/>rect answer, for example: <lb/>Q21: Who was the Chancellor of Germany from 1974 to 1982? <lb/>A21: Helmut Schmidt. <lb/>Q22: Which book was published by George Orwell in 1945? <lb/>A22: Animal Farm. <lb/>Q23: Which organization did Shimon Perez chair after Isaac Rabin&apos;s <lb/>death? <lb/>A23: Labour Party Central Committee. <lb/>Some questions could have no answer in the document collection, and in that <lb/>
			case the exact answer was &quot;NIL&quot; and the answer and support docid fields were left <lb/>empty. A question was assumed to have no right answer when neither human as-<lb/>sessors nor participating systems could find one. <lb/>The distribution of the questions among these categories is described in Table 4. <lb/>Each question set was then translated into English, which worked as inter-<lb/>language during the translation of the datasets into the other tongues for the acti-<lb/>vated cross-lingual subtasks. <lb/>Table 4. Test set breakdown according to question type, <lb/>number of participants and number of runs <lb/>F <lb/>D <lb/>L <lb/>T <lb/>NIL <lb/># Participants # Runs <lb/>BG <lb/>159 <lb/>24 <lb/>17 <lb/>28 <lb/>9 <lb/>1 <lb/>1 <lb/>DE <lb/>160 <lb/>30 <lb/>10 <lb/>9 <lb/>13 <lb/>3 <lb/>12 <lb/>EL <lb/>163 <lb/>29 <lb/>8 <lb/>31 <lb/>0 <lb/>0 <lb/>0 <lb/>EN <lb/>160 <lb/>30 <lb/>10 <lb/>12 <lb/>0 <lb/>4 <lb/>5 <lb/>ES <lb/>161 <lb/>19 <lb/>20 <lb/>42 <lb/>10 <lb/>4 <lb/>10 <lb/>EU <lb/>145 <lb/>39 <lb/>16 <lb/>23 <lb/>17 <lb/>1 <lb/>4 <lb/>FR <lb/>135 <lb/>30 <lb/>35 <lb/>66 <lb/>10 <lb/>1 <lb/>3 <lb/>IT <lb/>157 <lb/>31 <lb/>12 <lb/>13 <lb/>10 <lb/>0 <lb/>0 <lb/>NL <lb/>151 <lb/>39 <lb/>10 <lb/>13 <lb/>10 <lb/>1 <lb/>4 <lb/>PT <lb/>162 <lb/>28 <lb/>10 <lb/>16 <lb/>11 <lb/>6 <lb/>9 <lb/>RO <lb/>162 <lb/>28 <lb/>10 <lb/>47 <lb/>11 <lb/>2 <lb/>4 <lb/>2.4 Formats <lb/>As the format is concerned, also this year both input and output files were format-<lb/>ted as an XML file. For example, the first four questions in the EN-FR test set, i.e. <lb/>English questions that hit a French document collection -were represented as fol-<lb/>lows: <lb/>&lt;input&gt; <lb/>&lt;q target_lang=&quot;FR&quot; source_lang=&quot;EN&quot; q_id=&quot;0001&quot; <lb/>q_group_id=&quot;1600&quot;&gt;Which is the largest bird in Africa?&lt;/q&gt; <lb/>&lt;q target_lang=&quot;FR&quot; source_lang=&quot;EN&quot; q_id=&quot;0002&quot; <lb/>q_group_id=&quot;1600&quot;&gt;How many species of ostriches are there?&lt;/q&gt; <lb/>&lt;q target_lang=&quot;FR&quot; source_lang=&quot;EN&quot; q_id=&quot;0003&quot; <lb/>q_group_id=&quot;1601&quot;&gt;Who served as a UNICEF goodwill ambassador be-<lb/>tween 1988 and 1992?&lt;/q&gt; <lb/>&lt;q target_lang=&quot;FR&quot; source_lang=&quot;EN&quot; q_id=&quot;0004&quot; <lb/>q_group_id=&quot;1601&quot;&gt;What languages did she speak?&lt;/q&gt; <lb/>... <lb/>&lt;/input&gt; <lb/>An example of system output which answered the above questions was the <lb/>following: <lb/>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; <lb/>&lt;!DOCTYPE output SYSTEM &quot;QA-CLEF-OUTPUT.dtd&quot;&gt; <lb/>&lt;output&gt; <lb/>&lt;a q_id=&quot;0001&quot; q_group_id=&quot;1600&quot; run_id=&quot;syna081enfr&quot; score=&quot;0.000&quot;&gt; <lb/>&lt;answer&gt;version&lt;/answer&gt; <lb/>&lt;docid&gt;Afrique des Grands Lacs&lt;/docid&gt; <lb/>&lt;support&gt; <lb/>&lt;s_id&gt;Afrique des Grands Lacs&lt;/s_id&gt; <lb/>&lt;s_string&gt;Comprendre la crise de l&apos;Afrique des grands lacs -dossier <lb/>RFI (version archivée par Internet Archive).&lt;/s_string&gt; <lb/>&lt;/support&gt; <lb/>&lt;/a&gt; <lb/>&lt;a q_id=&quot;0002&quot; q_group_id=&quot;1600&quot; run_id=&quot;syna081enfr&quot; score=&quot;0.000&quot;&gt; <lb/>&lt;answer&gt;500 000&lt;/answer&gt; <lb/>&lt;docid&gt;ATS.940202.0138&lt;/docid&gt; <lb/>&lt;support&gt; <lb/>&lt;s_id&gt;ATS.940202.0138&lt;/s_id&gt; <lb/>&lt;s_string&gt;Avec une superficie de seulement 51 000 km2, le Costa Rica <lb/>abrite quelque 500 000 espèces végétales et animales. Il compte <lb/>plus d&apos;espèces d&apos;oiseaux et d&apos;arbres qu&apos;il n&apos;y en a sur <lb/>l&apos;ensemble du territoire des Etats-Unis. &lt;/s_string&gt; <lb/>&lt;/support&gt; <lb/>&lt;/a&gt; <lb/>&lt;a q_id=&quot;0003&quot; q_group_id=&quot;1601&quot; run_id=&quot;syna081enfr&quot; score=&quot;0.000&quot;&gt; <lb/>&lt;answer&gt;NIL&lt;/answer&gt; <lb/>&lt;docid/&gt; <lb/>&lt;support&gt; <lb/>&lt;s_id/&gt; <lb/>&lt;s_string/&gt; <lb/>&lt;/support&gt; <lb/>&lt;/a&gt; <lb/>&lt;a q_id=&quot;0004&quot; q_group_id=&quot;1601&quot; run_id=&quot;syna081enfr&quot; score=&quot;0.000&quot;&gt; <lb/>&lt;answer&gt;NIL&lt;/answer&gt; <lb/>&lt;docid/&gt; <lb/>&lt;support&gt; <lb/>&lt;s_id/&gt; <lb/>&lt;s_string/&gt; <lb/>&lt;/support&gt; <lb/>&lt;/a&gt; <lb/>... <lb/>&lt;/output&gt; <lb/>2.5 Evaluation <lb/>As far the evaluation process is concerned, no changes were made with respect to <lb/>the previous campaigns. Human judges assessed the exact answer (i.e. the shortest <lb/>string of words which is supposed to provide the exact amount of information to <lb/>answer the question) as: <lb/>• R (Right) if correct; <lb/>• W (Wrong) if incorrect; <lb/>• X (ineXact) if contained less or more information than that required by <lb/>the query; <lb/>• U (Unsupported) if either the docid was missing or wrong, or the sup-<lb/>porting snippet did not contain the exact answer. <lb/>Most assessor-groups managed to guarantee a second judgement of all the runs. <lb/>As regards the evaluation measures, the main one was accuracy, defined as the <lb/>average of SCORE(q) over all 200 questions q, where SCORE(q) is 1 in the first <lb/>answer to q in the submission file is assessed as R, and 0 otherwise. <lb/>In addition most assessor groups computed the following measures: <lb/>• Confident Weighted Score (CWS). Answers are in a decreasing order of <lb/>confidence and CWS rewards systems that give correct answers at the top <lb/>of the ranking [16] <lb/>• the Mean Reciprocal Rank (MRR) over N assessed answers per question <lb/>(to consider the three answers). That is, the mean of the reciprocal of the <lb/>rank of the first correct label over all questions. If the first correct label is <lb/>ranked as the 3rd label, then the reciprocal rank (RR) is 1/3. If none of <lb/>the first N responses contains a correct label, RR is 0. RR is 1 if the high-<lb/>est ranked label matches the correct label. <lb/>3 Results <lb/>As far as accuracy is concerned, scores were generally far lower than usual, as <lb/>Figure 1 shows. Although comparison between different languages and years is <lb/>not possible, in Figure 1 we can observe some trends which characterized this <lb/>year&apos;s competition: best accuracy in the monolingual task increased with respect <lb/>to last year, going up again to the values recorded in 2006. But systems -even <lb/>those that participated in all previous campaigns -did not achieve a brilliant over-<lb/>all performance. Apparently systems could not manage suitably the new chal-<lb/>lenges, although they improved their performances when tackling issues already <lb/>treated in previous campaigns. <lb/>More in detail, best accuracy in the monolingual task scored 63,5 almost ten <lb/>points up with respect to last year, meanwhile the overall performance of the sys-<lb/>tems was quite low, as average accuracy was 23,63, practically the same as last <lb/>year. On the contrary, the performances in the cross-language tasks recorded a <lb/>drastic drop: best accuracy reached only 19% compared to 41,75% in the previous <lb/>year, which means more than 20 points lower, meanwhile average accuracy was <lb/>more or less the same as in 2007 -13,24 compared to 10,9. <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>Mono <lb/>Bilingual <lb/>Mono <lb/>Bilingual <lb/>Mono <lb/>Bilingual <lb/>Mono <lb/>Bilingual <lb/>Mono <lb/>Biligual <lb/>Mono <lb/>
			Bilingual <lb/>Best Average <lb/>CLEF 03 <lb/>CLEF 04 <lb/>CLEF 05 <lb/>CLEF 06 <lb/>CLEF 07 <lb/>CLEF 08 <lb/>Figure 1. Best and average scores in QA@CLEF campaigns <lb/>On the contrary, Best accuracy over the bilingual tasks, decreased considerably. <lb/>This is also true for average performances. This year a small increase was re-<lb/>corded in the bilingual tasks but it seems that the high level of difficulty of the <lb/>question sets particularly impacted the bilingual tasks and the task proved to be <lb/>still difficult also for veterans. <lb/>3.1 Participation <lb/>Table 5. Number of participants in QA@CLEF <lb/>America Europe Asia Australia TOTAL <lb/>CLEF 2003 <lb/>3 <lb/>5 <lb/>0 <lb/>0 <lb/>8 <lb/>CLEF 2004 <lb/>1 <lb/>17 <lb/>0 <lb/>0 <lb/>18 <lb/>CLEF 2005 <lb/>1 <lb/>22 <lb/>1 <lb/>0 <lb/>24 <lb/>CLEF 2006 <lb/>4 <lb/>24 <lb/>2 <lb/>0 <lb/>30 <lb/>CLEF 2007 <lb/>3 <lb/>16 <lb/>1 <lb/>1 <lb/>21 <lb/>CLEF 2008 <lb/>1 <lb/>20 <lb/>0 <lb/>0 <lb/>21 <lb/>The number of participants has remained almost the same as in 2007 (see Table <lb/>5). As noticed, this is probably the consequence of the new challenges introduced <lb/>last year in the exercise. <lb/>Also the geographical distribution remained almost unchanged, even though <lb/>there was no participation from Australia and Asia. No runs were submitted nei-<lb/>ther for Italian or Greek tasks. <lb/>Anyway, the number of submitted runs, increased from a total of 37 registered last <lb/>year to 51 (see Table 6). The breakdown of participants and runs, according to <lb/>language, is shown in Table 4 (Section 2.3). As in previous campaigns, more par-<lb/>ticipants chose the monolingual tasks, which once again demonstrated to be more <lb/>approachable. <lb/>Table 6. Number of submitted runs <lb/>Submitted runs Monolingual Cross-lingual <lb/>CLEF 2003 <lb/>17 <lb/>6 <lb/>11 <lb/>CLEF 2004 <lb/>48 <lb/>20 <lb/>28 <lb/>CLEF 2005 <lb/>67 <lb/>43 <lb/>24 <lb/>CLEF 2006 <lb/>77 <lb/>42 <lb/>35 <lb/>CLEF 2007 <lb/>37 <lb/>23 <lb/>14 <lb/>CLEF 2008 <lb/>51 <lb/>31 <lb/>20 <lb/>In the following subsections a more detailed analysis of the results in each lan-<lb/>guage follows, giving specific information on the performances of the participat-<lb/>ing systems in the single sub-tasks and on the different types of questions, provid-<lb/>ing the relevant statistics and comments. <lb/>3.2 Basque as target <lb/>In the first year working with Basque as target only a research groups submit-<lb/>ted runs for evaluation in the track having Basque as target language, the Ixa <lb/>group from the University of the Basque Country. They sent four runs: one mono-<lb/>lingual, one English-Basque and two Spanish-Basque. <lb/>The Basque question set consisted of 145 factoid questions, 39 definition ques-<lb/>tions and 16 list questions. 39 questions contained a temporal restriction, and 10 <lb/>had no answer in the Gold Standard. 40 answers were retrieved from Wikipedia, <lb/>the remains from the news collections. Half of the questions were linked to a top-<lb/>ic, so the second (and sometimes the 3rd) question was more difficult to answer. <lb/>The news were from the Egunkaria newspaper during 2000, 2001 and 2002 <lb/>years and the information from Wikipedia was the exportation corresponding to <lb/>the 2006 year. <lb/>Table 7 shows the evaluation results for the four submitted runs (one monolin-<lb/>gual and three cross-lingual). The table shows the number of Right, Wrong, in-<lb/>eXact and Unsupported answers, as well as the percentage of correctly answered <lb/>Factoids, Temporally restricted questions, Definition and List questions. <lb/>Table 7. Evaluation results for the four submitted runs. <lb/>Run <lb/>R <lb/># <lb/>W <lb/># <lb/>X <lb/># <lb/>U <lb/># <lb/>%F <lb/>[145] <lb/>%T <lb/>[23] <lb/>%D <lb/>[39] <lb/>L% <lb/>[16] <lb/>NIL <lb/>CWS <lb/>Over-<lb/>all <lb/>accu-<lb/>racy <lb/># <lb/>% <lb/>[*] <lb/>ixag08 <lb/>1eueu <lb/>26 <lb/>163 <lb/>11 <lb/>0 <lb/>15.9 <lb/>8.7 <lb/>7.7 <lb/>0 <lb/>4 <lb/>7.0 <lb/>0.023 <lb/>13 <lb/>ixag08 <lb/>1eneu <lb/>11 <lb/>182 <lb/>7 <lb/>0 <lb/>5.5 <lb/>4.3 <lb/>7.7 <lb/>0 <lb/>6 <lb/>6.2 <lb/>0.004 <lb/>5.5 <lb/>ixag08 <lb/>1eseu <lb/>11 <lb/>182 <lb/>7 <lb/>0 <lb/>6.9 <lb/>4.3 <lb/>2.6 <lb/>0 <lb/>4 <lb/>4.8 <lb/>0.004 <lb/>5.5 <lb/>ixag08 <lb/>2eseu <lb/>7 <lb/>185 <lb/>8 <lb/>0 <lb/>4.8 <lb/>4.3 <lb/>0 <lb/>0 <lb/>3 <lb/>3.5 <lb/>0.003 <lb/>3.5 <lb/>The monolingual run (ixag081eueu.xml) achieved accuracy of 13%, lower than <lb/>the most systems for other target languages during the evaluation of 2007 but bet-<lb/>ter than some of them. It is necessary to underline that Basque is a highly flexional <lb/>language, doing matching of term and entities more complex, and that ir is the first <lb/>participation. The system achieved better accuracy in factoids questions (15.9%). <lb/>No correct answers was retrieved for list questions. It is necessary to remark that <lb/>57 answers were NIL (only four of them were corrects), perhaps participants can <lb/>improve this aspect. <lb/>Looking to the cross-lingual runs the loss of accuracy respect to the monolin-<lb/>gual system is a bit more than 50% for the two best runs. This percentage is quite <lb/>similar with runs for other target languages in 2007. The overall accuracy is the <lb/>same for both (English and Spanish to Basque) but only they agree in five correct <lb/>answers (each system gives other six correct answers). The second system for <lb/>Spanish-Basque get poorer results and only is slightly better in inexact answers. <lb/>
			These runs get also a lot of NIL answers. <lb/>3.3 Bulgarian as Target <lb/>Table 8. Results for the submitted run for Bulgarian <lb/>Run <lb/>R <lb/>W <lb/>X <lb/>U <lb/>% F % T <lb/>% D <lb/>% L NIL <lb/>CWS <lb/>MRR <lb/>Overall <lb/>accuracy <lb/># <lb/># <lb/># <lb/># <lb/>[*] <lb/>[*] <lb/>[*] <lb/>[*] <lb/># <lb/>% <lb/>[*] <lb/>btb1 <lb/>20 <lb/>173 <lb/>7 <lb/>0 <lb/>8.80 7.14 <lb/>25.00 0.00 -<lb/>0.00 0.01 -<lb/>10 % <lb/>This year, contrary to our optimistic expectations, only one run by one group <lb/>(BTB) was performed for Bulgarian. As the table above shows, the result is far <lb/>from satisfying. Again, the definitions were detected better in comparison to other <lb/>question types. Also, the difference between the detection of factoids and of tem-<lb/>porally restricted questions is negligible. The results from the previous years de-<lb/>creased in both directions -as participating groups and as system performance. <lb/>3.4 Dutch as Target <lb/>The questions for the Dutch subtask of CLEF-QA 2008 were written by four <lb/>native speakers. They selected random articles from either Wikipedia or the news <lb/>collection and composed questions based on the topics of the articles. <lb/>Table 9. Properties of the 200 Dutch questions (134 topics) in the test set <lb/>Question types <lb/>Factoid answer types <lb/>Temporal restric-<lb/>tion <lb/>Definition <lb/>39 <lb/>Count <lb/>20 <lb/>No <lb/>187 <lb/>Factoid <lb/>151 <lb/>Location <lb/>18 <lb/>Yes <lb/>13 <lb/>List <lb/>Measure <lb/>20 <lb/>Question per topic <lb/>Answer source <lb/>Object <lb/>19 <lb/>1 question <lb/>100 <lb/>News <lb/>20 <lb/>Organization <lb/>18 <lb/>2 questions <lb/>15 <lb/>None (NIL answer) <lb/>5 <lb/>Other <lb/>17 <lb/>3 questions <lb/>6 <lb/>Wikipedia <lb/>175 <lb/>Person <lb/>19 <lb/>4 questions <lb/>13 <lb/>Definition answer types <lb/>Time <lb/>20 <lb/>Topic types <lb/>Location <lb/>3 <lb/>List answer types <lb/>Location <lb/>15 <lb/>Object <lb/>6 <lb/>Location <lb/>6 <lb/>Object <lb/>23 <lb/>Organization <lb/>8 <lb/>Other <lb/>1 <lb/>Organization <lb/>14 <lb/>Other <lb/>12 <lb/>Person <lb/>2 <lb/>Other <lb/>50 <lb/>Person <lb/>10 <lb/>Time <lb/>1 <lb/>Person <lb/>32 <lb/>The quartet produced a total of 222 question-answer pairs from which they se-<lb/>lected a set of 200 that satisfied the type distribution requirements of the task or-<lb/>ganizers. An overview of the question types and answer types can be found in Ta-<lb/>ble 9. <lb/>This year, only one team took part in the question answering task with Dutch as <lb/>target language: the University of Groningen. The team submitted two monolin-<lb/>gual runs and two cross-lingual runs (English to Dutch). All runs were assessed <lb/>twice by a single assessor. This resulted in a total of eight conflicts (1%). These <lb/>were corrected. The results of the assessment can be found in Table 10. <lb/>Table 10. Assessment results for the four submitted runs for Dutch. <lb/>Run <lb/>R <lb/># <lb/>W <lb/># <lb/>X <lb/># <lb/>U <lb/># <lb/>%F <lb/>[151] <lb/>%T <lb/>[13] <lb/>%D <lb/>[39] <lb/>L% <lb/>[10] <lb/>NIL <lb/>CWS <lb/>Over-<lb/>all <lb/>accu-<lb/>racy <lb/># <lb/>% <lb/>[*] <lb/>gron0 <lb/>81nlnl <lb/>50 <lb/>138 <lb/>11 <lb/>1 <lb/>24.5 <lb/>15.4 <lb/>33.3 <lb/>0.0 <lb/>19 <lb/>5.3 <lb/>0.342 <lb/>25.0 <lb/>gron0 <lb/>82nlnl <lb/>51 <lb/>136 <lb/>10 <lb/>3 <lb/>24.5 <lb/>15.4 <lb/>35.9 <lb/>0.0 <lb/>15 <lb/>6.7 <lb/>0.331 <lb/>25.5 <lb/>gron0 <lb/>81ennl <lb/>27 <lb/>157 <lb/>10 <lb/>6 <lb/>13.2 <lb/>7.7 <lb/>17.9 <lb/>0.0 <lb/>30 <lb/>3.3 <lb/>0.235 <lb/>13.5 <lb/>gron0 <lb/>82ennl <lb/>27 <lb/>157 <lb/>10 <lb/>6 <lb/>13.2 <lb/>7.7 <lb/>17.9 <lb/>0.0 <lb/>30 <lb/>3.3 <lb/>0.235 <lb/>13.5 <lb/>The two cross-lingual runs gron081ennl andron082ennl produced exactly the <lb/>same answers. <lb/>The best monolingual run (gron082nlnl) achieved exactly the same score as the <lb/>best run of 2007 (25.5%). The same is true for the best monolingual run (13.5%). <lb/>The fact that the two scores are in the same range as last year is no big surprise <lb/>since the task has not changed considerably this year and all scores have been <lb/>achieved by the same system. <lb/>Like in 2007, the system performed better for definition questions than for oth-<lb/>er question types. The definition questions could be divided in two subtypes: those <lb/>that asked for a definition (26) and those that contained a definition and asked for <lb/>the name of the defined object (12). The monolingual runs performed similarly for <lb/>both subtypes but the cross-lingual runs did not contain a correct answer to any <lb/>question of the second subtype. <lb/>None of the runs obtained any points for the list questions. The answers con-<lb/>tained some parts that were correct but none of them were completely correct. We <lb/>were unable to award points for partially correct answers in the current assessment <lb/>scheme. <lb/>All the runs were produced by the same system and the differences between the <lb/>runs are small. The cross-lingual runs contained seven correct answers that were <lb/>not present in any of the monolingual runs (for questions 20, 25, 120, 131, 142, <lb/>150 and 200). Eight questions were only answered correctly in a single monolin-<lb/>gual run (1, 28, 54, 72, 83, 143, 193 and 199). Thirty-five questions were ans-<lb/>wered correctly in two runs, three in three runs and seventeen in all four runs. 137 <lb/>questions failed to receive any correct answer. <lb/>3.5 English as Target <lb/>Table 11. Evaluation results for the English submitted runs. <lb/>Run <lb/>R <lb/>W <lb/>X U % F <lb/>% T % D <lb/>% L NIL <lb/>CWS <lb/>K1 <lb/>Overall <lb/>accuracy <lb/># <lb/># <lb/># <lb/># <lb/>[160] <lb/>[12] [30] <lb/>[10] # <lb/>%[0] <lb/>dcun081deen 16 <lb/>168 <lb/>7 <lb/>9 <lb/>5.00 <lb/>8.33 26.67 <lb/>0.00 0 <lb/>0.00 <lb/>0.00516 <lb/>0.10 <lb/>8.00 <lb/>dcun082deen 1 <lb/>195 <lb/>3 <lb/>1 <lb/>0.63 <lb/>0.00 0.00 <lb/>0.00 0 <lb/>0.00 <lb/>0.00013 <lb/>0.03 <lb/>0.50 <lb/>dfki081deen <lb/>28 <lb/>164 <lb/>5 <lb/>3 <lb/>6.25 <lb/>8.33 60.00 <lb/>0.00 0 <lb/>0.00 <lb/>0.01760 <lb/>N/A <lb/>14.00 <lb/>ilkm081nlen 7 <lb/>182 <lb/>2 <lb/>9 <lb/>4.38 <lb/>0.00 0.00 <lb/>0.00 0 <lb/>0.00 <lb/>0.00175 <lb/>N/A <lb/>3.50 <lb/>wlvs081roen 38 <lb/>155 <lb/>2 <lb/>5 <lb/>11.25 <lb/>0.00 66.67 <lb/>0.00 0 <lb/>0.00 <lb/>0.05436 <lb/>0.13 <lb/>19.00 <lb/>* Total number in the test set. <lb/>Creation of Questions. The task this year was exactly the same as in 2007 and <lb/>moreover the three collections were the same: Glasgow Herald, LA Times and <lb/>Wikipedia. However, given the considerable interest in the Wikipedia which has <lb/>been shown by Question Answering groups generally, it was decided to increase <lb/>the number of questions drawn from it to 75% overall, with just 25% coming from <lb/>the two newspaper collections. This means that 40 of the 160 Factoids came from <lb/>the newspapers, together with seven of the 30 Definitions and two of the ten Lists. <lb/>These questions were divided equally between the Glasgow Herald and LA Times. <lb/>All the remainder we drawn from the Wikipedia. <lb/>Considerable care was taken in the selection of the questions. The distribution <lb/>by answer type was controlled exactly as in previous years. As requested by the <lb/>organisers there were exactly twenty each of Factoid target type PERSON, TIME, <lb/>
			LOCATION, MEASURE, COUNT, ORGANIZATION, OBJECT and OTHER. <lb/>Similarly for Definitions there were eight PERSON, seven ORGANIZATION, <lb/>seven OBJECT and eight OTHER. For Lists there were four OTHER, two each of <lb/>PERSON and ORGANIZATION, and one each of LOCATION and OBJECT. <lb/>In addition to the above distribution, we also controlled the distribution of top-<lb/>ics for the question groups, something which was made practicable by the use of <lb/>the Wikipedia. Questions were drawn from a number of predefined subject fields: <lb/>countries towns, roads and bridges, shops, politicians and politics, sports and <lb/>sports people, foods and vegetables, cars, classical music including instruments, <lb/>popular music, literature poetry and drama, philosophy, films, architecture, lan-<lb/>guages, science, consumer goods, and finally organisations. Questions were distri-<lb/>buted among these topics. The maximum in any topic was twenty (sports) and the <lb/>minimum was two (shops). For the majority there were between four and six ques-<lb/>tion groups. For each such topic, one or more questions were set depending on <lb/>what information the texts contained. As a change from last year, the organisers <lb/>asked us to include 100 singleton topics. This effectively meant that half the ques-<lb/>tions in the overall set of 200 were simple &quot;one-off&quot; queries as were set in CLEF <lb/>prior to 2007 and for the earlier TREC campaigns. <lb/>Questions were entered via a web interface developed by the organisers last <lb/>year. However, this year they improved it considerably, for example allowing <lb/>modifications to be made to existing entries. This was a great help and a com-<lb/>mendable effort on their part. <lb/>Summary Statistics. Five cross-lingual runs with English as target were sub-<lb/>mitted this year, as compared with eight in 2007 and thirteen in 2006. Four groups <lb/>participated in three languages, Dutch, German and Romanian. Each group <lb/>worked with only one source language, and only DCUN submitted two runs. The <lb/>rest submitted only one run. <lb/>Assessment Procedure. Last year we used the excellent Web-based assess-<lb/>ment system developed originally for the QiQA task by University of Amsterdam. <lb/>However, we were asked not to use this in 2008 because it only allows one answer <lb/>per question per system to be assessed and it was required to assess multiple an-<lb/>swers per question per system. For this reason we used a Web-based tool devel-<lb/>oped by UNED in Madrid. <lb/>All answers were double-judged. Where the assessors differed, the case was re-<lb/>viewed and a decision taken. There were 63 judgement differences in total. Three <lb/>of the runs contained multiple answers to individual questions in certain cases, and <lb/>these were all assessed, as per the requirement of the organisers. If we assume that <lb/>the number of judgements was in fact 200 questions * five runs, i.e. 1,000, we can <lb/>compute a lower bound for the agreement level. This gives a figure of (1,000-<lb/>63)/1,000, i.e. 93.7%. The equivalent figure for 2007 (called Agreement Level 2 in <lb/>the Working Notes for last year) was 97.6%. Given that we have computed a low-<lb/>er bound this year (and not therefore the exact figure) this seems acceptable. <lb/>Results Analysis. Of the five runs with English as target, wlvs081roen was the <lb/>best with an accuracy of 19.00% overall. They also did very will on the defini-<lb/>tions, scoring 66.67%. The only source language for which there was more than <lb/>one run was German, for which there were three submissions from two groups. <lb/>dfki081 scored the best with 14.00% and this was followed by dcun081deen with <lb/>8.00% and dcun082deen with 0.50%. dfki also did very well on definitions with <lb/>an accuracy of 60.00. Interestingly, none of the systems answered any of the list <lb/>questions correctly. Only dcun082deen answered one list question inexactly. <lb/>If we compare the results this year with those of last year when the task was <lb/>very similar, performance has improved here. The best score in 2007 was <lb/>wolv071roen with 14.00% (the best score) which has now improved to 19.00%. <lb/>Similarly, dfki071deen scored 7.00% in 2007 but increased this to 14.00% this <lb/>year in dfki081deen. An attempt was made to set easier questions this year, which <lb/>might have affected performance. In addition, many more questions came from <lb/>the Wikipedia in 2008 with only a minority being drawn from the newspaper cor-<lb/>pora. <lb/>3.6 QA-WSD subtask <lb/>The QA-WSD task brings semantic and retrieval evaluation together. The par-<lb/>ticipants were offered the same queries and document collections as for the main <lb/>QA exercise, but with the addition of word sense tags as provided by two automat-<lb/>ic word sense disambiguation (WSD) systems. Contrary to the main QA task, Wi-<lb/>kipedia articles are not included, and thus systems need to reply to the questions <lb/>that have an answer in the news document collection. The goal of the task is to test <lb/>whether WSD can be used beneficially for Question Answering, and is closely re-<lb/>lated to the Robust-WSD subtask of the ad-hoc track in CLEF 2008. <lb/>The exercise scenario is event-targeted QA on a news document collection. In the <lb/>QA-WSD track only English monolingual and Spanish to English bilingual tasks <lb/>are offered, i.e. English is the only target language, and queries are available on <lb/>both English and Spanish. The queries were the same as for the main QA exercise, <lb/>and the participation followed the same process, except for the use of the sense-<lb/>annotated data. <lb/>The goal of this task is to evaluate whether word sense information can help in <lb/>certain queries. For this reason, participants were required to send two runs for <lb/>each of the monolingual/bilingual tasks where they participate: one which does <lb/>not use sense annotations and another one which does use sense annotations. <lb/>Whenever possible, the only difference between the two runs should be solely the <lb/>use or not of the sense information. Participants which send a single run would be <lb/>discarded from the evaluation. <lb/>

			The WSD data is based on WordNet version 1.6 and was supplemented with free-<lb/>ly available data from the English and Spanish WordNets in order to test different <lb/>expansion strategies. Two leading WSD experts run their systems [17][18], and <lb/>provided those WSD results for the participants to use. <lb/>The task website [4] provides additional information on data formats and re-<lb/>sources. <lb/>Results <lb/>From the 200 questions provided to participants, only 49 queries had a correct <lb/>answer in the news collection. The table below provides the results for the partici-<lb/>pant on those 49 questions. <lb/>Table 12. Results of the EN2EN QA-WSD runs on the 49 queries which had replies in the news <lb/>collections <lb/>Run <lb/>R <lb/># <lb/>W <lb/># <lb/>X <lb/># <lb/>U <lb/># <lb/>%F <lb/>[40] <lb/>%T <lb/>[5] <lb/>%D <lb/>[7] <lb/>L% <lb/>[2] <lb/>NIL <lb/>CWS <lb/>Over-<lb/>all <lb/>accu-<lb/>racy <lb/>0 <lb/>% <lb/>[0] <lb/>nlel08 <lb/>1enen <lb/>8 <lb/>41 <lb/>0 <lb/>0 <lb/>17.5 <lb/>0 <lb/>14.2 <lb/>0 <lb/>0 <lb/>0 <lb/>0.03 <lb/>16.32 <lb/>nlel08 <lb/>2enen <lb/>7 <lb/>42 <lb/>0 <lb/>0 <lb/>15.0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0.02 <lb/>14.29 <lb/>The first run does not use WSD, while the second uses the sense tags returned <lb/>by the NUS WSD system. The WSD tags where used in the passage retrieval <lb/>module. The use of WSD does not provide any improvement, and causes one <lb/>more error. For the sake of completeness we also include below the results on all <lb/>200 queries. Surprisingly the participant managed to find two (one in the WSD <lb/>run) correct answer for the Wikipedia questions in the news collection. <lb/>Table 13. Results of the EN2EN QA-WSD runs on all 200 queries, just for the sake of compari-<lb/>son <lb/>Run <lb/>R <lb/># <lb/>W <lb/># <lb/>X <lb/># <lb/>U <lb/># <lb/>%F <lb/>[160] <lb/>%T <lb/>[5] <lb/>%D <lb/>[7] <lb/>L% <lb/>[10] <lb/>NIL <lb/>CWS <lb/>Over-<lb/>all <lb/>accu-<lb/>racy <lb/>0 <lb/>% <lb/>[0] <lb/>nlel08 <lb/>1enen <lb/>10 <lb/>188 <lb/>0 <lb/>2 <lb/>5.6 <lb/>0 <lb/>3.3 <lb/>0 <lb/>0 <lb/>0 <lb/>0.00 <lb/>5.00 <lb/>nlel08 <lb/>2enen <lb/>8 <lb/>189 <lb/>0 <lb/>3 <lb/>4.4 <lb/>0 <lb/>3.3 <lb/>0 <lb/>0 <lb/>0 <lb/>0.00 <lb/>4.00 <lb/>3.7 French as Target <lb/>This year only one group took part in the evaluation tasks using French as a <lb/>target language: the French group Synapse Développement. Last year&apos;s second <lb/>participant, the Language Computer Corporation (LCC, USA) didn&apos;t send any <lb/>submission this time. <lb/>Synapse submitted three runs in total: <lb/>• one monolingual run: French to French (FR-to-FR), <lb/>• two bilingual runs: English-to-French (EN-to-FR) and Portuguese-to-<lb/>French (PT-to-FR). <lb/>In the following, these will be referred to as: <lb/>• syn08frfr <lb/>(for FR-to-FR), <lb/>• syn08enfr (for EN-to-FR), <lb/>• syn08ptfr (for PT-to-FR). <lb/>As last year, three types of questions were proposed: factual, definition and <lb/>closed list questions. Participants could return one exact answer per question and <lb/>up to two runs. Some questions (10%) had no answer in the document collection, <lb/>and in this case the exact answer is &quot;NIL&quot;. <lb/>The French test set consists of 200 questions: <lb/>• 135 Factual (F), <lb/>• 30 Definition (D), <lb/>• 35 closed List questions (L). <lb/>Among these 200 questions, 66 were temporally restricted questions (T) and 12 <lb/>were NIL questions (i.e. a &quot;NIL&quot; answer was expected, meaning that there is no <lb/>valid answer for this question in the document collection). <lb/>Table 14. Results of the monolingual and bilingual French runs. <lb/>Run <lb/>Assessed Answers <lb/>(#) <lb/>R <lb/># <lb/>W <lb/># <lb/>X <lb/># <lb/>U <lb/># <lb/>%F <lb/>[135] <lb/>%T <lb/>[66] <lb/>%D <lb/>[30] <lb/>L% <lb/>[35] <lb/>NIL <lb/>Answers <lb/>CWS <lb/>Overall accuracy <lb/># <lb/>% <lb/>[12] <lb/>syn08frfr <lb/>200 <lb/>131 <lb/>77 <lb/>9 <lb/>1 <lb/>54.8 <lb/>51.5 <lb/>86.7 <lb/>37.1 <lb/>20 <lb/>50.0 <lb/>0.30937 <lb/>56.5 <lb/>syn08enfr <lb/>200 <lb/>36 <lb/>157 <lb/>6 <lb/>1 <lb/>15.6 <lb/>15.1 <lb/>50.0 <lb/>0.0 <lb/>60 <lb/>8.3 <lb/>0.02646 <lb/>18.0 <lb/>syn08ptfr <lb/>200 <lb/>33 <lb/>163 <lb/>4 <lb/>0 <lb/>14.1 <lb/>13.6 <lb/>43.3 <lb/>2.9 <lb/>67 <lb/>11.9 <lb/>0.02387 <lb/>16.5 <lb/>Table 14 shows the final results of the assessment of the 3 runs submitted by <lb/>Synapse. For each run, the following statistics are provided: <lb/>• The number of correct (R), wrong (W), inexact (X) and unsupported <lb/>answers (U), <lb/>• The accuracy calculated within each of the categories of questions: <lb/>F, D, T and L questions, <lb/>• The number of NIL answers and the proportion of correct ones (i.e. <lb/>corresponding to a NIL questions), <lb/>• The Confidence Weighted Score (CWS) measure. <lb/>• The accuracy calculated over all answers. <lb/>Figure 2 shows the best scores for systems using French as target in the last <lb/>five CLEF QA campaigns. <lb/>Figure 2: Best scores for systems using French as target in CLEF QA campaigns <lb/>For the monolingual task, the Synapse system returned 113 correct answers <lb/>(accuracy of 56.5%), slightly more than last year (accuracy of 54.0%). The bilin-<lb/>gual runs performance is quite low, with an accuracy of 18.0% for EN-to-FR and <lb/>16.5% for PT-to-FR. It cannot be fairly compared to the results of CLEF2007, be-<lb/>cause Synapse didn&apos;t submit bilingual runs last year. Last year, LCC obtained an <lb/>accuracy of 41.7% for EN-to-FR, but did not submit anything this year. <lb/>It appears that the level of performance strongly depends on the type of ques-<lb/>tions. The monolingual run scores very high on the definition questions (86.7%). <lb/>The lowest performance is obtained with closed list questions (37.1%). <lb/>It is even more obvious when looking at the bilingual runs. If the systems per-<lb/>formed pretty well on the definition questions (50.0% and 43.3% for EN-to-FR <lb/>and PT-to-FR respectively), they could not cope with the closed list questions. The <lb/>PT-to-FR system could only give one close list correct answer. The EN-to-FR sys-<lb/>tem could not even answer to any of these questions. The bilingual runs did not <lb/>reach high accuracy with factoid and temporally restricted questions (50.0% and <lb/>43.3% for EN-to-FR and PT-to-FR respectively). This year, the complexity of the <lb/>task, in particular regarding closed list questions, seems to have been hard to cope <lb/>with for the bilingual systems. <lb/>The complexity of the task is also reflected by the number of NIL answers. The <lb/>monolingual system returned 20 NIL answers (to be compared with the 12 ex-<lb/>pected). The bilingual systems returned 60 (EN-to-FR) and 67 (EN-to-FR) NIL <lb/>answers, i.e. at least 5 times more as expected. <lb/>

			It is also interesting to look at the results when categorizing questions by the <lb/>size of the topic they belong to. This year, topics could contain from 1 single ques-<lb/>tion to 4 questions. The CLEF 2008 set consists of: <lb/>• 52 single question topics, <lb/>• 33 topics with 2 questions (66 questions in total), <lb/>• 18 topics with 3 questions (54 questions in total), <lb/>• 7 topics with 4 questions (28 questions in total). <lb/>Table 15, Table 16 and Table 17 give the results of each run according to the <lb/>size of the topics. <lb/>Table 15. Results per topic size (FR-to-FR) <lb/>Run <lb/>Size of topic <lb/>Assessed <lb/>Answers # <lb/>Overall accuracy <lb/>(%) <lb/>syn08frfr <lb/>1 <lb/>52 <lb/>55.8 <lb/>syn08frfr <lb/>2 <lb/>66 <lb/>50.0 <lb/>syn08frfr <lb/>3 <lb/>24 <lb/>66.7 <lb/>syn08frfr <lb/>4 <lb/>28 <lb/>53.6 <lb/>Table 16. Results per topic size (EN-to-FR) <lb/>Run <lb/>Size of <lb/>topic <lb/>Assessed An-<lb/>swers # <lb/>Overall ac-<lb/>curacy (%) <lb/>syn08enfr <lb/>1 <lb/>52 <lb/>21.2 <lb/>syn08enfr <lb/>2 <lb/>66 <lb/>22.7 <lb/>syn08enfr <lb/>3 <lb/>24 <lb/>13.0 <lb/>syn08enfr <lb/>4 <lb/>28 <lb/>10.7 <lb/>Table 17. Results per topic size (PT-to-FR) <lb/>Run <lb/>Size of top-<lb/>ic <lb/>Assessed An-<lb/>swers <lb/>Overall accu-<lb/>racy (%) <lb/># <lb/>syn08ptfr <lb/>1 <lb/>52 <lb/>25.0 <lb/>syn08ptfr <lb/>2 <lb/>66 <lb/>18.2 <lb/>syn08ptfr <lb/>3 <lb/>24 <lb/>9.3 <lb/>syn08ptfr <lb/>4 <lb/>28 <lb/>10.7 <lb/>The monolingual system (Table 15) is not sensitive to the size of the topic <lb/>question set. On the opposite, the performances of the bilingual systems (Table 16 <lb/>and Table 17) decrease by a half, when comparing the 1-and 2-question sets to the <lb/>3-and 4-question sets. A possible explanation is that the bilingual systems per-<lb/>form poorly with questions containing anaphoric references (which are more <lb/>likely to occur in the 3-and 4-question sets). <lb/>In conclusion, there was unfortunately only one participant this year. In particu-<lb/>lar; it would have been interesting to see how the LCC group, which submitted a <lb/>bilingual run last year, would have performed this year. <lb/>This decrease in participation can be explained by the discouragement of some <lb/>participants. Some have complained that the task is each year harder (e.g. this <lb/>year, there were more closed list questions and anaphoric references than last <lb/>year) that can result in a decrease in the systems performances. <lb/>This year, the number and complexity of closed list questions was clearly <lb/>higher than the previous year. In the same way, there were more temporally re-<lb/>stricted questions, more topics (comprising from 2 to 4 questions) and more ana-<lb/>phoric references. It seems that this higher level of difficulty particularly impacted <lb/>the bilingual tasks. In spite of this, the monolingual Synapse system performed <lb/>slightly better than last year. <lb/>3.8 German as Target <lb/>Three research groups submitted runs for evaluation in the track having Ger-<lb/>man as target language: The German Research Center for Artificial Intelligence <lb/>(DFKI), the Fern Universität Hagen (FUHA) and the Universität Koblenz-Landau <lb/>(LOGA). All groups provided system runs for the monolingual scenario, DFKI <lb/>and FUHA submitted runs for the cross-language English-German scenario and <lb/>FUHA had also runs for the Spanish-German scenario. <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>2008 <lb/>2007 <lb/>2006 <lb/>2005 <lb/>2004 <lb/>Best Mono <lb/>Aggregated <lb/>Mono <lb/>Figure 3. Results evolution <lb/>Compared to the previous editions of the evaluation forum, this year an increase in <lb/>the accuracy of the best performing system and of an aggregated virtual system for <lb/>monolingual and a decrease in the accuracy of the best performing system and of <lb/>an aggregated virtual system for cross-language tasks was registered. <lb/>Table 18. Topic distribution over data collections <lb/>Topic Size <lb/># Topics / <lb/>CLEF <lb/># Topics / <lb/>WIKI <lb/># Topics <lb/>1 <lb/>39 <lb/>35 <lb/>74 <lb/>2 <lb/>10 <lb/>14 <lb/>24 <lb/>3 <lb/>5 <lb/>5 <lb/>10 <lb/>4 <lb/>3 <lb/>9 <lb/>12 <lb/>Total <lb/>57 <lb/>63 <lb/>120 <lb/>Table 19. Topic type breakdown over data collections <lb/>CLEF <lb/>WIKI <lb/>Topic Type <lb/>Topic Size <lb/>Total <lb/>Topic Size <lb/>Total <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>PERSON <lb/>5 <lb/>2 <lb/>1 <lb/>1 <lb/>9 <lb/>0 <lb/>1 <lb/>0 <lb/>2 <lb/>3 <lb/>OBJECT <lb/>7 <lb/>1 <lb/>0 <lb/>0 <lb/>8 <lb/>16 <lb/>3 <lb/>0 <lb/>2 <lb/>21 <lb/>ORGANIZATION <lb/>9 <lb/>1 <lb/>2 <lb/>1 <lb/>13 <lb/>7 <lb/>2 <lb/>1 <lb/>1 <lb/>11 <lb/>LOCATION <lb/>8 <lb/>2 <lb/>2 <lb/>1 <lb/>13 <lb/>1 <lb/>3 <lb/>2 <lb/>2 <lb/>8 <lb/>EVENT <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>0 <lb/>2 <lb/>OTHER <lb/>9 <lb/>4 <lb/>0 <lb/>1 <lb/>14 <lb/>11 <lb/>3 <lb/>2 <lb/>2 <lb/>18 <lb/>57 <lb/>63 <lb/>The number of topics covered by the test set questions was of 120 distributed as it <lb/>follows: 74 topics consisting of 1 question, 24 topics of 2 related questions, 10 <lb/>topics of 3 related questions, and 12 topics of 4 related questions. The distribution <lb/>of the topics over the document collections (CLEF vs. Wikipedia) is presented in <lb/>Table 18. <lb/>Table 20. Question EAType breakdown over data collections <lb/>EAType <lb/>CLEF <lb/>WIKI <lb/>Total <lb/>PERSON <lb/>15 <lb/>15 <lb/>30 <lb/>LOCATION <lb/>13 <lb/>12 <lb/>25 <lb/>TIME <lb/>13 <lb/>8 <lb/>21 <lb/>COUNT <lb/>13 <lb/>7 <lb/>20 <lb/>OBJECT <lb/>7 <lb/>18 <lb/>25 <lb/>MEASURE <lb/>12 <lb/>8 <lb/>20 <lb/>ORGANIZATION <lb/>15 <lb/>13 <lb/>28 <lb/>OTHER <lb/>9 <lb/>22 <lb/>31 <lb/>Total <lb/>97 <lb/>103 <lb/>200 <lb/>The details of systems&apos; results can be seen in Table 21. <lb/>Table 21. System Performance -Details <lb/>Run <lb/>R <lb/>W X <lb/>U % F <lb/>% T % D % L NIL <lb/>CWS <lb/>MRR <lb/>Overall <lb/>accuracy <lb/># <lb/># <lb/># <lb/># [160] <lb/>[9] <lb/>[30] [10] # <lb/>% <lb/>[10] <lb/>dfki081dede M 73 119 2 <lb/>6 30.62 44.44 80 <lb/>0 <lb/>0 0 <lb/>0.16 0 <lb/>36.5 <lb/>dfki082dede M 74 120 2 <lb/>4 31.25 33.33 80 <lb/>0 <lb/>0 0 <lb/>0.16 0 <lb/>37 <lb/>fuha081dede M 45 141 8 <lb/>6 24.37 44.44 20 <lb/>0 <lb/>1 4.76 0.05 0.29 22.5 <lb/>fuha082dede M 46 139 11 4 25.62 33.33 16.66 0 <lb/>21 4.76 0.048 0.29 23 <lb/>loga081dede M 29 159 11 1 13.75 0 <lb/>20 <lb/>10 <lb/>55 5.45 0.031 0.19 14.5 <lb/>loga082dede M 27 163 9 <lb/>1 13.12 0 <lb/>16.66 10 <lb/>48 4.16 0.029 0.17 13.5 <lb/>dfki081ende C 29 164 2 <lb/>5 10 <lb/>0 <lb/>43.33 0 <lb/>0 0 <lb/>0.038 0 <lb/>14.5 <lb/>fuha081ende C 28 163 6 <lb/>3 15 <lb/>11.11 13.33 0 <lb/>81 7.4 0.023 0.24 14 <lb/>fuha082ende C 28 160 6 <lb/>6 15 <lb/>11.11 13.33 0 <lb/>81 7.4 0.019 0.22 14 <lb/>fuha081esde C 19 169 9 <lb/>2 9.43 <lb/>0 <lb/>13.33 0 <lb/>9 0 <lb/>0.015 0.15 9.54 <lb/>fuha082esde C 17 173 5 <lb/>5 8.12 <lb/>0 <lb/>13.33 0 <lb/>61 3.27 0.007 0.13 8.5 <lb/>

			According to Table 19 the most frequent topic types were OTHER (32), OBJECT <lb/>(29) and ORGANIZATION (24), with first two types more present for the <lb/>Wikipedia collection of documents (WIKI). <lb/>As regards the source of the answers, 97 questions from 57 topics asked for <lb/>information out of the CLEF document collection and the rest of 103 from 63 top-<lb/>ics for information from Wikipedia. Table 20 shows a breakdown of the test set <lb/>questions by the expected answer type (EAType) for each collection of data. <lb/>3.9 Portuguese as Target <lb/>The Portuguese track had six different participants: beside the veteran groups <lb/>of Priberam, Linguateca, Universidade de Évora, INESC and FEUP, we had a new <lb/>participants this year, Universidade Aberta. No bilingual task occurred this year. <lb/>In this fourth year of Portuguese participation, Priberam repeated the top place <lb/>of its previous years, with University of Évora behind. Again we added the classi-<lb/>fication the classification X-, meaning incomplete, keeping the classification X+ <lb/>for answers with extra text or other kinds of inexactness. In Table 22 we present <lb/>the overall results (all tables in these notes refer exclusively to the first answer by <lb/>each system). <lb/>Table 22: Results of the runs with Portuguese as target: all 200 questions (first answers only) <lb/>To provide a more direct comparison with pre-2006 results, in Table 23 we <lb/>present the results both for first question of each topic (which we believe is more <lb/>readily comparable to such results) and for the linked questions. <lb/>On the whole, compared to last year, Priberam and Senso (UE) improved their <lb/>results, which were already the best. INESC system and Esfinge (Linguateca) also <lb/>showed some improvement, at a lower level Raposa (FEUP) showed similar re-<lb/>Run <lb/>Name <lb/>R <lb/>(#) <lb/>W <lb/>(#) <lb/>X+ <lb/>(#) <lb/>X-<lb/>(#) <lb/>U <lb/>(#) <lb/>Overall <lb/>Accuracy <lb/>(%) <lb/>NIL Accuracy <lb/># <lb/>Precision <lb/>(%) <lb/>Recall <lb/>(%) <lb/>diue081 <lb/>93 <lb/>94 <lb/>8 <lb/>1 <lb/>2 <lb/>46.5% <lb/>21 <lb/>9.5 <lb/>20 <lb/>esfi081 <lb/>47 <lb/>134 <lb/>5 <lb/>7 <lb/>5 <lb/>23.5% <lb/>20 <lb/>20.0 <lb/>20 <lb/>esfi082 <lb/>39 <lb/>137 <lb/>7 <lb/>9 <lb/>6 <lb/>19.5% <lb/>20 <lb/>15.0 <lb/>10 <lb/>feup081 <lb/>29 <lb/>165 <lb/>2 <lb/>2 <lb/>2 <lb/>14.5% <lb/>142 <lb/>8.5 <lb/>90 <lb/>feup082 <lb/>25 <lb/>169 <lb/>3 <lb/>1 <lb/>2 <lb/>12.5% <lb/>149 <lb/>8.1 <lb/>90 <lb/>idsa081 <lb/>65 <lb/>119 <lb/>8 <lb/>8 <lb/>32.5% <lb/>12 <lb/>16.7 <lb/>20 <lb/>ines081 <lb/>40 <lb/>150 <lb/>2 <lb/>1 <lb/>5 <lb/>20.0% <lb/>123 <lb/>9.7 <lb/>90 <lb/>ines082 <lb/>40 <lb/>150 <lb/>2 <lb/>1 <lb/>5 <lb/>20.0% <lb/>123 <lb/>9.7 <lb/>90 <lb/>prib081 <lb/>127 55 <lb/>9 <lb/>3 <lb/>4 <lb/>63.5% <lb/>8 <lb/>12.5 <lb/>10 <lb/>sults. The system of Universidade Aberta appeared with good results compared to <lb/>some veteran systems. We leave it to the participants to comment on whether it <lb/>might have been caused by harder questions or changes (or lack thereof) in the <lb/>systems. <lb/>Table 23. Results of the runs with Portuguese as target: answers to linked and unlinked questions <lb/>Run <lb/>Name <lb/>First questions <lb/>(# 151) <lb/>Linked questions <lb/>(# 49) <lb/>R <lb/>(#) <lb/>W <lb/>(#) <lb/>X+ <lb/>(#) <lb/>X-<lb/>(#) <lb/>U <lb/>(#) <lb/>Accuracy <lb/>(%) <lb/>R <lb/>(#) <lb/>Accuracy <lb/>(%) <lb/>diue081 <lb/>82 <lb/>59 <lb/>6 <lb/>3 <lb/>1 <lb/>54.3 <lb/>11 <lb/>22.4 <lb/>esfi081 <lb/>42 <lb/>92 <lb/>5 <lb/>7 <lb/>5 <lb/>27.3 <lb/>7 <lb/>14.3 <lb/>esfi082 <lb/>33 <lb/>97 <lb/>6 <lb/>9 <lb/>6 <lb/>21.9 <lb/>8 <lb/>16.3 <lb/>feup081 <lb/>29 <lb/>116 <lb/>2 <lb/>2 <lb/>2 <lb/>19.2 <lb/>3 <lb/>6.1 <lb/>feup082 <lb/>25 <lb/>120 <lb/>3 <lb/>1 <lb/>2 <lb/>16.6 <lb/>3 <lb/>6.1 <lb/>idsa081 <lb/>54 <lb/>85 <lb/>6 <lb/>6 <lb/>35.8 <lb/>11 <lb/>22.4 <lb/>ines081 <lb/>35 <lb/>106 <lb/>2 <lb/>3 <lb/>5 <lb/>23.2 <lb/>8 <lb/>16.3 <lb/>ines082 <lb/>35 <lb/>106 <lb/>2 <lb/>3 <lb/>5 <lb/>23.2 <lb/>8 <lb/>16.3 <lb/>prib081 <lb/>105 <lb/>32 <lb/>9 <lb/>4 <lb/>1 <lb/>69.5 <lb/>22 <lb/>44.9 <lb/>Table 24. Results of the assessment of the monolingual Portuguese runs: definitions <lb/>Run <lb/>loc <lb/>obj <lb/>org <lb/>oth <lb/>per <lb/>TOT <lb/>% <lb/>1 <lb/>6 <lb/>6 <lb/>8 <lb/>6 <lb/>27 <lb/>diue081 <lb/>5 <lb/>6 <lb/>8 <lb/>5 <lb/>24 <lb/>89% <lb/>esfi081 <lb/>1 <lb/>2 <lb/>4 <lb/>2 <lb/>9 <lb/>33% <lb/>esfi082 <lb/>1 <lb/>1 <lb/>2 <lb/>7% <lb/>feup081 <lb/>1 <lb/>1 <lb/>1 <lb/>1 <lb/>4 <lb/>15% <lb/>feup082 <lb/>1 <lb/>1 <lb/>1 <lb/>1 <lb/>4 <lb/>15% <lb/>idsa081 <lb/>1 <lb/>5 <lb/>1 <lb/>5 <lb/>5 <lb/>17 <lb/>63% <lb/>ines081 <lb/>1 <lb/>5 <lb/>1 <lb/>7 <lb/>3 <lb/>17 <lb/>63% <lb/>ines082 <lb/>1 <lb/>5 <lb/>1 <lb/>7 <lb/>3 <lb/>17 <lb/>63% <lb/>prib081 <lb/>5 <lb/>5 <lb/>6 <lb/>2 <lb/>18 <lb/>67% <lb/>combination <lb/>1 <lb/>6 <lb/>6 <lb/>8 <lb/>6 <lb/>27 100% <lb/>Unlike last year , the results over linked questions are significatively different <lb/>(and below) from those over not-linked. Question 180 was wrongly redacted, re-<lb/>ferring to Aida&apos;s opera Verdi instead of the other way around, which also affected <lb/>two linked questions. Therefore, we accepted both NIL answers to those ques-<lb/>tions, as well as correct ones. <lb/>

			Table 24 shows the results for each answer type of definition questions, while <lb/>Table 25 shows the results for each answer type of factoid questions (including list <lb/>questions). As it can be seen, four out of six systems perform clearly better when <lb/>it comes to definitions than to factoids. Particularly Senso has a high accuracy re-<lb/>garding definitions. <lb/>Table 25. Results of the assessment of the Portuguese runs: factoids, including lists. <lb/>Run <lb/>cou <lb/>loc <lb/>mea <lb/>obj <lb/>org <lb/>oth <lb/>per <lb/>tim <lb/>TOT <lb/>% <lb/>17 <lb/>38 <lb/>16 <lb/>2 <lb/>10 <lb/>33 <lb/>33 <lb/>24 <lb/>173 <lb/>diue081 <lb/>6 <lb/>17 <lb/>8 <lb/>1 <lb/>5 <lb/>13 <lb/>8 <lb/>11 <lb/>69 <lb/>35% <lb/>esfi081 <lb/>8 <lb/>8 <lb/>2 <lb/>2 <lb/>2 <lb/>14 <lb/>4 <lb/>40 <lb/>20% <lb/>esfi082 <lb/>8 <lb/>8 <lb/>2 <lb/>2 <lb/>2 <lb/>13 <lb/>4 <lb/>39 <lb/>20% <lb/>feup081 <lb/>5 <lb/>4 <lb/>4 <lb/>1 <lb/>2 <lb/>8 <lb/>4 <lb/>28 <lb/>14% <lb/>feup082 <lb/>5 <lb/>3 <lb/>4 <lb/>1 <lb/>2 <lb/>6 <lb/>3 <lb/>24 <lb/>12% <lb/>idsa081 <lb/>9 <lb/>9 <lb/>9 <lb/>6 <lb/>8 <lb/>7 <lb/>48 <lb/>24% <lb/>ines081 <lb/>4 <lb/>9 <lb/>2 <lb/>1 <lb/>4 <lb/>6 <lb/>26 <lb/>13% <lb/>ines082 <lb/>4 <lb/>9 <lb/>2 <lb/>1 <lb/>4 <lb/>6 <lb/>26 <lb/>13% <lb/>prib081 <lb/>11 <lb/>21 <lb/>13 <lb/>1 <lb/>7 <lb/>18 <lb/>22 <lb/>16 <lb/>109 <lb/>55% <lb/>combination <lb/>16 <lb/>31 <lb/>15 <lb/>1 <lb/>7 <lb/>23 <lb/>27 <lb/>21 <lb/>141 <lb/>82% <lb/>We included in both Table 24 and Table 25 a virtual run, called combination, in <lb/>which one question is considered correct if at least one participating system found <lb/>a valid answer. The objective of this combination run is to show the potential <lb/>achievement when combining the capacities of all the participants. The combina-<lb/>tion run can be considered, somehow, state-of-the-art in monolingual Portuguese <lb/>question answering. All definition questions were answered by at least one sys-<lb/>tem. <lb/>Table 26. Average size of answers (values in number of words) <lb/>Run name <lb/>Non-NIL <lb/>Answers (#) <lb/>Average an-<lb/>swer size <lb/>Average <lb/>answer <lb/>size (R only) <lb/>Average snip-<lb/>pet size <lb/>Average snippet <lb/>size (R only) <lb/>diue081 <lb/>179 <lb/>2.8 <lb/>3.6 <lb/>25.9 <lb/>26.1 <lb/>esfi081 <lb/>180 <lb/>2.6 <lb/>3.0 <lb/>78.4 <lb/>62.5 <lb/>esfi082 <lb/>180 <lb/>1.8 <lb/>1.7 <lb/>78.2 <lb/>62.4 <lb/>feup081 <lb/>58 <lb/>1.8 <lb/>3.4 <lb/>64.2 <lb/>51.6 <lb/>feup081 <lb/>51 <lb/>1.8 <lb/>3.7 <lb/>63.3 <lb/>51.4 <lb/>idsa081 <lb/>188 <lb/>5.0 <lb/>10.0 <lb/>28.6 <lb/>34.4 <lb/>ines081 <lb/>77 <lb/>3.0 <lb/>7.4 <lb/>79.6 <lb/>36.6 <lb/>ines082 <lb/>77 <lb/>3.0 <lb/>7.4 <lb/>79.6 <lb/>36.6 <lb/>prib081 <lb/>192 <lb/>3.2 <lb/>3.4 <lb/>27.6 <lb/>25.1 <lb/>The system with best results, Priberam, answered correctly 64.8% the questions <lb/>with at least one correct answer. In all, 130 questions were answered by more than <lb/>one system. <lb/>In Table 26, we present some values concerning answer and snippet size. <lb/>Temporally restricted questions: Table 27 presents the results of the 17 tem-<lb/>porally restricted questions. As in previous years, the effectiveness of the systems <lb/>to answer those questions is visibly lower than for non-TRQ questions. <lb/>Table 27. Accuracy of temporally restricted questions. <lb/>Run name <lb/>Correct answers <lb/>(#) <lb/>T.R.Q <lb/>correctness (%) <lb/>Non-T.R.Q <lb/>correctness (%) <lb/>Total <lb/>correctness (%) <lb/>diue081 <lb/>4 <lb/>23.5 <lb/>48..6 <lb/>46.5 <lb/>esfi081 <lb/>3 <lb/>17.6 <lb/>24.0 <lb/>23.5 <lb/>esfi082 <lb/>3 <lb/>17.6 <lb/>19.7 <lb/>19.5 <lb/>feup081 <lb/>1 <lb/>5.9 <lb/>15.3 <lb/>14.5 <lb/>feup082 <lb/>1 <lb/>5.9 <lb/>13.1 <lb/>12.5 <lb/>Idsa081 <lb/>2 <lb/>11.8 <lb/>34.4 <lb/>32.5 <lb/>ines081 <lb/>1 <lb/>5.9 <lb/>21.3 <lb/>20.0 <lb/>ines082 <lb/>1 <lb/>5.9 <lb/>21.3 <lb/>20.0 <lb/>prib081 <lb/>8 <lb/>47.1 <lb/>65.0 <lb/>63.5 <lb/>List questions: ten questions were defined as list questions all closed list facto-<lb/>ids with two to five each 3 . The results haven&apos;t improved with UE getting two cor-<lb/>rect answers. Priberam three and all other system zero. There were however seven <lb/>cases of incomplete answers (i.e.. answering some elements of the list only) al-<lb/>though only two of them with than one element of the answer. <lb/>Table 28. Answers by source and their correctness <lb/>Run <lb/>News <lb/>Wikipedia <lb/>NIL <lb/># <lb/>% correct <lb/># <lb/>% correct <lb/># <lb/>% correct <lb/>Selection <lb/>34 <lb/>-<lb/>144 <lb/>-<lb/>10 <lb/>-<lb/>diue081 <lb/>35 <lb/>40% <lb/>144 <lb/>53% <lb/>21 <lb/>10% <lb/>esfi081 <lb/>85 <lb/>21% <lb/>95 <lb/>28% <lb/>20 <lb/>10% <lb/>esfi082 <lb/>81 <lb/>17% <lb/>99 <lb/>24% <lb/>20 <lb/>5% <lb/>feup081 <lb/>10 <lb/>40% <lb/>48 <lb/>33% <lb/>142 <lb/>6% <lb/>feup082 <lb/>9 <lb/>44% <lb/>42 <lb/>29% <lb/>149 <lb/>6% <lb/>idsa081 <lb/>50 <lb/>28% <lb/>138 <lb/>36% <lb/>12 <lb/>17% <lb/>ines081 <lb/>31 <lb/>23% <lb/>46 <lb/>52% <lb/>123 <lb/>7% <lb/>ines082 <lb/>31 <lb/>23% <lb/>46 <lb/>52% <lb/>123 <lb/>7% <lb/>prib081 <lb/>46 <lb/>63% <lb/>146 <lb/>66% <lb/>8 <lb/>13% <lb/></body>

			<note place="footnote">3 There were some open list questions as well, but they were clas-<lb/>sified and evaluated as ordinary factoids. <lb/></note>

			<body>Answer source: Table 28 presents the distribution of questions by source dur-<lb/>ing their selection. The distribution of sources used by the different runs and their <lb/>correctness. <lb/>3.10 Romanian as Target <lb/>In the third year of Romanian participation in QA@CLEF, and the second one <lb/>with Romanian addressed as a target language, the question generation was based <lb/>on the collection of Wikipedia Romanian pages frozen in November 2006 4 -the <lb/>same corpus as in the previous edition 5 . <lb/>Creation of Questions. The questions were generated starting from the corpus <lb/>and based on the Guidelines for Question Generation 6 , the Guidelines for Partici-<lb/>pants 7 and the final decisions taken after email discussions between the organizers. <lb/>The 200 questions are distributed according to Table 29, where for each type of <lb/>question and expected answer we indicate also the temporally restricted questions <lb/>out of the total number of questions. Without counting the NIL questions, 100% of <lb/>the questions has the answer in Wikipedia collection. <lb/>Table 29. Question &amp; Answer types distribution in Romanian (in brackets the number of tempo-<lb/>rally restricted questions) <lb/>Q <lb/>type <lb/>/expected A <lb/>type <lb/>PER <lb/>SON <lb/>TIM <lb/>E <lb/>LOC. <lb/>ORG. <lb/>MEAS <lb/>URE <lb/>COU <lb/>NT <lb/>OBJE <lb/>CT <lb/>OTH <lb/>ER <lb/>TOTAL <lb/>FACTOID <lb/>20 <lb/>(9) <lb/>23 <lb/>(5) <lb/>26 (4) 20 (10) <lb/>17 (3) <lb/>22 <lb/>(5) <lb/>18 (4) <lb/>16 <lb/>(4) <lb/>162 (44) <lb/>DEF. <lb/>8 <lb/>1 <lb/>6 (2) <lb/>6 <lb/>7 <lb/>28 (2) <lb/>LIST <lb/>3 <lb/>1 (1) <lb/>1 <lb/>2 (1) <lb/>3 <lb/>10 (2) <lb/>NIL <lb/>8 <lb/></body>

			<note place="footnote">4 http://static.wikipedia.org/downloads/November_2006/ro/ <lb/></note>

			<note place="footnote">5 At http://static.wikipedia.org/downloads/ the frozen versions of <lb/>Wikipedia exist for April 2007 and June 2008, for all languages in-<lb/>volved in QA@CLEF. <lb/></note>

			<note place="footnote">6 http://celct.isti.cnr.it/ClefQA/QA@CLEF08_Question_Generation_Gui <lb/>delines.pdf <lb/></note>

			<note place="footnote">7 http://nlp.uned.es/clef-qa/QA@CLEF08_Guidelines-for-<lb/>Participants.pdf <lb/></note>

			<body>As the Guidelines for Question Generation did not change since the previous edi-<lb/>tion, there were no major difficulties in creating the Romanian gold standard for <lb/>the 2008 QA@CLEF. The working version of the GS was uploaded on the ques-<lb/>tion generation interface developed at CELCT (Italy), by filling all the required <lb/>fields. <lb/>For the topic-related questions (clusters of up to four questions, related to one <lb/>same topic) we kept about the same number as in the previous edition: in 2007 we <lb/>had 122 topics and now there are 119 topics. The percentage of topic-linked ques-<lb/>tions is illustrated in Table 30, showing that 127 questions were grouped under 46 <lb/>topics, hence 63.5% out of the total 200 questions were linked in topics with more <lb/>than one question. <lb/>Table 30. Topic-related questions <lb/># of questions <lb/>/ Topic type <lb/>PERSO <lb/>N <lb/>LOC. <lb/>ORG. <lb/>EVEN <lb/>T <lb/>OBJE <lb/>CT <lb/>OTHE <lb/>R <lb/>Total <lb/>topics <lb/>Total <lb/>ques-<lb/>tions <lb/>4 Qs <lb/>5 <lb/>1 <lb/>1 <lb/>5 <lb/>12 <lb/>48 <lb/>3 Qs <lb/>5 <lb/>1 <lb/>1 <lb/>1 <lb/>3 <lb/>11 <lb/>33 <lb/>2 Qs <lb/>5 <lb/>3 <lb/>4 <lb/>2 <lb/>9 <lb/>23 <lb/>46 <lb/>1 Q <lb/>13 <lb/>6 <lb/>19 <lb/>17 <lb/>18 <lb/>73 <lb/>73 <lb/>TOTAL <lb/>28 <lb/>11 <lb/>24 <lb/>1 <lb/>20 <lb/>35 <lb/>119 <lb/>200 <lb/>In fact the questions contain not 127, but only 51 anaphoric elements of various <lb/>types, so that 25.5% of the questions are linked through coreferential relations. <lb/>The personal, possessive or demonstrative pronouns were used in most of the cas-<lb/>es to create anaphoric relations. The antecedents are mainly the focus of the pre-<lb/>vious question, or the previous answer. Few such questions require inference in <lb/>order to be correctly answered. For example in order to correctly answer the F-<lb/>Time question When was the first Esperanto dictionary for Romanian published? <lb/>and then the L-Other Name all the grammatical cases of this artificial language., <lb/>one needs to correctly link the anaphor &quot;artificial language&quot; to its antecedent <lb/>which is &quot;Esperanto&quot; and not &quot;Romanian&quot; (also a language but not artificial); this <lb/>is possible by establishing, based on a text snippet, that Esperanto is an artificial <lb/>language. <lb/>The 8 NIL questions, even though they seem somehow unnatural, were created <lb/>by including questions about facts impossible from a human perception; for ex-<lb/>ample the question In which year did Paul Kline publish his work about the natu-<lb/>ral phenomena called hail? has no answer in any of the articles about the psychol-<lb/>ogist. Another type of NIL questions are those based on inference -the question <lb/>How many bicameral Parliaments are there in Cuba? is a NIL question because <lb/>in all wiki articles one can find that Cuba has a unicameral parliament. Another <lb/>type of NIL questions (with answer in English, but not in Romanian) we have <lb/>created cannot be good items neither in a cross-lingual evaluation where the an-<lb/>swers are to be find in any language, nor in an evaluation based on an open text <lb/>collection such as the web. The question What is a micron? has no answer in the <lb/>Romanian wiki articles from 2006, but it can have an answer in other Romanian <lb/>webpages, and, moreover, in the English wiki articles it has more than a correct <lb/>answer depending on the domain where the term is used (in the metric system or <lb/>in vacuum engineering). <lb/>For the LIST type we created only questions whose answers are to be found in <lb/>one same text section. The 2007 evaluation for Romanian showed that &quot;open list&quot; <lb/>questions (with answers in various sections of an article or even in various ar-<lb/>ticles) are difficult to handle, therefore we made the LIST questions easier. <lb/>Systems&apos; analysis and evaluation. Like in the 2007 edition, this year two Roma-<lb/>nian groups took part in the monolingual task with Romanian as a target language: <lb/>the Faculty of Computer Science from the Al. I. Cuza University of Iasi (UAIC), <lb/>
			and the Research Institute for Artificial Intelligence from the Romanian Academy <lb/>(ICIA), Bucharest. Each group submitted two runs, the four systems having an av-<lb/>erage of 2.4 answers per question for ICIA, and 1.92 for UAIC. The 2008 general <lb/>results are presented in Tables 31 below. <lb/>The statistics includes a system, named combined, obtained through the combina-<lb/>tion of the 4 participating RO-RO systems. Because at the evaluation time we ob-<lb/>served that there are correct answers not only in the first position, but also on the <lb/>second or the third, the combined system considers that an answer is R if there ex-<lb/>ists at least one R answer among all the answers returned by the four systems. If <lb/>there is no R answer, the same strategy is applied to X, U and finally W answers. <lb/>This &quot;ideal&quot; system permits to calculate the percentage of the questions (and their <lb/>type), answered by at least one of the four systems in any of the maximum 3 an-<lb/>swers returned for a question. <lb/>All three systems crashed on the LIST questions. The best results were obtained <lb/>by ICIA for DEFINITION questions, whereas UAIC performed best with the <lb/>FACTOID questions. The combined system suggests that a joint system, devel-<lb/>oped by both groups, would improve substantially the general results for Roma-<lb/>nian. <lb/>Using in a first stage the web interface for assessing the QA runs, developed at <lb/>UNED in Spain, the assessment took into consideration one question with all its <lb/>answers at the time, assuring that the same evaluation criteria are applied to all an-<lb/>swers. The judgment of the answers was based on the same Guidelines as in 2007, <lb/>therefore we kept the same criteria as in 2007, in order to assure consistency in-<lb/>side the Romanian language, which gives also the possibility to evaluate the sys-<lb/>tems in their evolution from one year to another. For example, one could easily <lb/>see that the UAIC systems had most of the answers for the DEFINITION ques-<lb/>tions evaluated as ineXact, because the answers were judged as being &quot;longer than <lb/>the minimum amount of information required&quot; and hence &quot;unnecessary pieces of <lb/>information were penalized&quot;. Since all the 2007 and 2008 answers were evaluated <lb/>this way, we considered it is more important to have uniformly applied rules in-<lb/>side one language than to change the evaluation in order to be consistent across <lb/>languages. On the other hand the ICIA answers judged as ineXact are due to an-<lb/>swers that are too long, snippets shortened as such as they do not contain the an-<lb/>swer, or because the answer and the snippet has no connections. <lb/>Tables 31. Results in the monolingual task, Romanian as target language <lb/>Run <lb/>R <lb/>W <lb/>U F <lb/>T <lb/>D <lb/>L <lb/>NIL <lb/>CWS <lb/>MRR <lb/>Overall <lb/>accuracy <lb/># <lb/># <lb/># [162] [47] [28] <lb/>[10] # <lb/>% [8] <lb/>icia08 <lb/>1roro <lb/>10 179 <lb/>1 <lb/>0 4.938 <lb/>8.51 <lb/>1 <lb/>7.143 0.0 15 6.667 <lb/>0.0081 <lb/>2 <lb/>0.0821 <lb/>7 <lb/>5.0 <lb/>icia08 <lb/>2roro <lb/>21 168 <lb/>1 <lb/>0 6.173 <lb/>8.51 <lb/>1 <lb/>39.286 0.0 15 6.667 <lb/>0.0219 <lb/>1 <lb/>0.1431 <lb/>9 <lb/>10.5 <lb/>uaic08 <lb/>1roro <lb/>41 128 <lb/>7 <lb/>3 <lb/>24.69 <lb/>1 <lb/>25.5 <lb/>32 <lb/>3.571 0.0 65 7.692 <lb/>0.0367 <lb/>9 <lb/>0.3432 <lb/>4 <lb/>20.5 <lb/>uaic08 <lb/>2roro <lb/>45 125 <lb/>6 <lb/>4 <lb/>26.54 <lb/>3 <lb/>27.6 <lb/>60 <lb/>3.571 10.0 64 9.375 <lb/>0.0489 <lb/>2 <lb/>0.3679 <lb/>9 <lb/>22.5 <lb/>Run <lb/>FACTOID QUESTIONS <lb/>LIST QUESTIONS <lb/>DEFINITION QUESTION <lb/>R <lb/>W <lb/>X <lb/>U ACC <lb/>R W X U ACC <lb/>R <lb/>W X <lb/>U ACC <lb/>Combined <lb/>72 75 <lb/>12 3 <lb/>44.444 1 <lb/>9 <lb/>0 <lb/>0 <lb/>10.000 14 5 <lb/>10 0 <lb/>50.000 <lb/>icia081roro <lb/>8 <lb/>144 10 0 <lb/>4.938 <lb/>0 <lb/>10 0 <lb/>0 <lb/>0.000 <lb/>2 <lb/>25 1 <lb/>0 <lb/>7.143 <lb/>icia082roro <lb/>10 143 9 <lb/>0 <lb/>6.173 <lb/>0 <lb/>10 0 <lb/>0 <lb/>0.000 <lb/>11 15 2 <lb/>0 <lb/>39.286 <lb/>uaic081roro 40 113 6 <lb/>3 <lb/>24.691 0 <lb/>9 <lb/>1 <lb/>0 <lb/>0.000 <lb/>1 <lb/>6 <lb/>21 0 <lb/>3.571 <lb/>uaic082roro 43 110 5 <lb/>4 <lb/>26.543 1 <lb/>9 <lb/>0 <lb/>0 <lb/>10.000 1 <lb/>6 <lb/>21 0 <lb/>3.571 <lb/>The evaluation was made more difficult because two of the submitted runs con-<lb/>tain the answers in a totally arbitrary order, with topic-related questions having <lb/>their answers in various parts of the submitted file. If in the first stage the UNED <lb/>interface was of a great help, after the xml file was generated with all the evalua-<lb/>tions, the corrections needed a thorough manual inspection. Anyway it was nice to <lb/>find out that the answer to the question Which terrorist organization does Osama <lb/>bin Laden belong to? is Pentagon. <lb/>
			3.11 Spanish as Target <lb/>The participation at the Spanish as Target subtask has decreased from 5 groups in <lb/>2007 to 4 groups this year. 6 runs were monolingual and 3 runs were crosslingual. <lb/>Table 32 shows the summary of systems results with the number of Right (R), <lb/>Wrong (W), Inexact (X) and Unsupported (U) answers. The table shows also the <lb/>accuracy (in percentage) of factoids (F), factoids with temporal restriction (T), <lb/>definitions (D) and list questions (L). Best values are marked in bold face. <lb/>Table 32. Results for Spanish as target <lb/>Run <lb/>R <lb/># <lb/>W <lb/># <lb/>X <lb/># <lb/>U <lb/># <lb/>% F <lb/>[124] <lb/>% T <lb/>[36] <lb/>% D <lb/>[20] <lb/>% L <lb/>[20] <lb/>NIL <lb/># <lb/>F <lb/>[10] <lb/>CWS <lb/>MRR <lb/>Overall <lb/>accuracy <lb/>prib081eses 86 105 5 4 41,13 41,67 75 20 <lb/>3 <lb/>0,17 0,178 0,4483 42,5 <lb/>inao082eses 44 152 3 1 19,35 8,33 <lb/>80 <lb/>5 <lb/>4 <lb/>0,10 0,068 0,2342 <lb/>22 <lb/>inao081eses 42 156 1 1 15,32 8,33 <lb/>95 <lb/>5 <lb/>3 <lb/>0,13 0,053 0,2375 <lb/>21 <lb/>qaua082eses 39 156 4 1 22,58 13,89 30 <lb/>-<lb/>6 <lb/>0,15 0,041 0,2217 19,5 <lb/>mira081eses 32 156 3 9 12,90 2,78 <lb/>75 <lb/>-<lb/>3 <lb/>0,21 0,032 0,1766 <lb/>16 <lb/>mira082eses 29 159 3 9 11,29 2,78 <lb/>70 <lb/>-<lb/>3 <lb/>0,23 0,026 0,1591 14,50 <lb/>qaua081enes 25 173 -2 11,29 16,67 20 <lb/>5 <lb/>6 <lb/>0,19 0,011 0,1450 12,50 <lb/>qaua082enes 18 176 3 3 9,68 <lb/>8,33 <lb/>15 <lb/>-<lb/>8 <lb/>0,15 0,006 0,1108 <lb/>9 <lb/>mira081fres 10 185 2 3 5,65 <lb/>-<lb/>15 <lb/>-<lb/>3 <lb/>0,12 0,008 0,0533 <lb/>5 <lb/>Table 33. Results for self-contained and linked questions, compared with overall accuracy <lb/>Run <lb/>% Accuracy over <lb/>Self-contained <lb/>questions <lb/>[139] <lb/>% Accuracy <lb/>over <lb/>Linked questions <lb/>[61] <lb/>% Overall <lb/>Accuracy <lb/>[200] <lb/>prib081eses <lb/>53,24 <lb/>18,03 <lb/>42,50 <lb/>inao082eses <lb/>25,18 <lb/>13,11 <lb/>22,00 <lb/>inao081eses <lb/>25,18 <lb/>9,84 <lb/>21,00 <lb/>qaua082eses <lb/>22,30 <lb/>13,11 <lb/>19,50 <lb/>mira081eses <lb/>21,58 <lb/>3,28 <lb/>16,00 <lb/>mira082eses <lb/>21,58 <lb/>3,28 <lb/>14,50 <lb/>qaua081enes <lb/>17,27 <lb/>-<lb/>12,50 <lb/>qaua082enes <lb/>12,23 <lb/>1,64 <lb/>9,00 <lb/>mira081fres <lb/>6,47 <lb/>1,64 <lb/>5,00 <lb/>Table 33 shows that the first question of the topic group is answered much <lb/>more easily than the rest of the questions which need to solve some references to <lb/>previous questions and answers. <lb/>Regarding NIL questions, Table 34 shows the harmonic mean (F) of precision <lb/>and recall for self-contained questions, linked questions and all questions, taking <lb/>into account only the first answer. In most of the systems, NIL is not given as <lb/>second or third candidate answer. <lb/>Table 34. Results for Spanish as target for NIL questions <lb/>F-measure <lb/>(Self-<lb/>contained@1) <lb/>F-measure <lb/>(@1) <lb/>Precision <lb/>(@1) <lb/>Recall <lb/>(@1) <lb/>prib081eses <lb/>0,26 <lb/>0,17 <lb/>0.12 <lb/>0.30 <lb/>inao082eses <lb/>0,14 <lb/>0.10 <lb/>0.06 <lb/>0.40 <lb/>inao081eses <lb/>0,19 <lb/>0.13 <lb/>0.08 <lb/>0.30 <lb/>qaua082eses <lb/>0,27 <lb/>0.15 <lb/>0.09 <lb/>0.60 <lb/>mira081eses <lb/>0,27 <lb/>0.21 <lb/>0.17 <lb/>0.30 <lb/>mira082eses <lb/>0,29 <lb/>0.23 <lb/>0.19 <lb/>0.30 <lb/>qaua081enes <lb/>0,26 <lb/>0.19 <lb/>0.11 <lb/>0.80 <lb/>qaua082enes <lb/>0,20 <lb/>0.15 <lb/>0.09 <lb/>0.60 <lb/>mira081fres <lb/>0,15 <lb/>0.12 <lb/>0.07 <lb/>0.30 <lb/>The correlation coefficient r between the self-score and the correctness of the <lb/>answers (shown in Table 34) has been similar to the obtained last year, being not <lb/>good enough yet, and explaining the low results in CWS and K1 [6] measures. <lb/>Table 35. Answer extraction and correlation coefficient (r) for Spanish as target <lb/>Run <lb/>%Answer Ex-<lb/>traction <lb/>r <lb/>prib081eses <lb/>90,53 <lb/>0,4006 <lb/>mira082eses <lb/>80,56 <lb/>0,0771 <lb/>inao082eses <lb/>80,00 <lb/>0,1593 <lb/>mira081eses <lb/>80,00 <lb/>0,0713 <lb/>qaua082eses <lb/>73,58 <lb/>0,2466 <lb/>inao081eses <lb/>67,74 <lb/>0,1625 <lb/>qaua081enes <lb/>75,76 <lb/>0,0944 <lb/>qaua082enes <lb/>58,06 <lb/>0,0061 <lb/>mira081fres <lb/>55,56 <lb/>0,0552 <lb/>Since a supporting snippet is requested in order to assess the correctness of the <lb/>answer, we have evaluated the systems capability to extract the answer when the <lb/>snippet contains it. The first column of Table 35 shows the percentage of cases <lb/>where the correct answer was present in the snippet and correctly extracted. This <lb/>information is very useful to diagnose if the lack of performance is due to the pas-<lb/>sage retrieval or to the answer extraction process. As shown in the table, the best <lb/>systems are also better in the task of answer extraction. In general, all systems <lb/>have improved their performance in Answer Extraction compared with previous <lb/>editions. <lb/>With respect to the source of the answers, Table 36 shows that in this second <lb/>year of using Wikipedia, this collection is now the main source of correct answers <lb/>for most of the systems (with the exception of U. of Alicante). <lb/>Table 36. Results for questions with answer in Wikipedia and EFE <lb/>Run <lb/>% Of correct answers <lb/>found in EFE <lb/>% Of Correct <lb/>Answers found <lb/>in Wikipedia <lb/>% Of Correct an-<lb/>swers found NIL <lb/>prib081eses <lb/>36,97 <lb/>60,50 <lb/>2,52 <lb/>inao082eses <lb/>24,14 <lb/>68,97 <lb/>6,90 <lb/>inao081eses <lb/>25 <lb/>70 <lb/>5 <lb/>qaua082eses <lb/>48,53 <lb/>42,65 <lb/>8,82 <lb/>mira081eses <lb/>23,26 <lb/>69,77 <lb/>6,98 <lb/>mira082eses <lb/>21,62 <lb/>70,27 <lb/>8,11 <lb/>qaua081enes <lb/>52,27 <lb/>29,55 <lb/>18,18 <lb/>qaua082enes <lb/>48,57 <lb/>34,29 <lb/>17,14 <lb/>mira081fres <lb/>33,33 <lb/>41,67 <lb/>25 <lb/>4 Conclusions <lb/>This year we proposed the same evaluation setting as in 2007 campaign. In <lb/>fact, last year the task was changed considerably and this affected the general level <lb/>of results and also the level of participation in the QA task. This year participation <lb/>increased slightly but the task proved to be still very difficult. Wikipedia increased <lb/>its presence as a source of questions and answers. Following last year&apos;s conclu-<lb/>sions Wikipedia seemed to be a good source for finding answers to simple factoid <lb/>questions. <lb/>Moreover, the overall decrease in accuracy was probably due to linked ques-<lb/>tions. This fact confirms that topic resolution is a weak point for QA systems. <lb/>Only 5 out of 11 target languages had more than one different participating <lb/>group. Thus from the evaluation methodology perspective, a comparison between <lb/>systems working under similar circumstances cannot be accomplished and this <lb/>impedes one of the major goals of campaigns such the QA@CLEF, i.e. the sys-<lb/>tems comparison which could determine an improvement in approaching QA <lb/>problematic issues. <lb/>In six years of QA experimentation, a lot of resources and know-how have <lb/>been accumulated, nevertheless systems do not show a brilliant overall perfor-<lb/>mance, even those that have participated to most QA campaigns, and still seem <lb/>not to manage suitably the different challenges proposed. <lb/>In conclusion, it is clear that a redefinition of the task should be thought in the <lb/>next campaign. This new definition of the task should permit the evaluation and <lb/>comparison of systems even working in different languages. The new setting <lb/>should also take as reference a real user scenario, perhaps in a new document col-<lb/>lection. <lb/></body>

			<div type="acknowledgement">Acknowledgements. A special thank to Danilo Giampiccolo (CELCT, Trento, <lb/>Italy), who has given his precious advise and valuable support at many levels for <lb/>the preparation and realization of the QA track at CLEF 2008. <lb/>Jesús Herrera has been partially supported by the Spanish Ministry of Educa-<lb/>tion and Science (TIN2006-14433-C02-01 project). <lb/>Anselmo Peñas has been partially supported by the Spanish Ministry of Science <lb/>and Technology within the Text-Mess-INES project (TIN2006-15265-C06-02). <lb/>Paulo Rocha was supported by the Linguateca project, jointly funded by the <lb/>Portuguese Government and the European Union (FEDER and FSE), under con-<lb/>tract ref. POSC/339/1.3/C/NAC <lb/></div>

			<listBibl>References <lb/>1. QA@CLEF Website: http://clef-qa.itc.it/ <lb/>2. AVE Website: http://nlp.uned.es/QA/ave/. <lb/>3. QAST Website: http://www.lsi.upc.edu/~qast/ <lb/>4. QA-WSD Website: http://ixa2.si.ehu.es/qawsd/ <lb/>5. QA@CLEF 2007 Organizing Committee. Guidelines 2007. <lb/>http://clef-qa.itc.it/2007/download/QA@CLEF07_Guidelines-for-Participants.pdf <lb/>6. Hartrumpf, S., Glöckner, I., Leveling, J.: University of Hagen at QA@CLEF 2007: <lb/>Coreference Resolution for Questions and Answer Merging. This volume. <lb/>7. Herrera, J., Peñas A., Verdejo, F.: Question Answering Pilot Task at CLEF 2004. In: <lb/>Peters, C., Clough, P., Gonzalo, J., Jones, Gareth J.F., Kluck, M., Magnini, B. (eds.): <lb/>Multilingual Information Access for Text, Speech and Images. Lecture Notes in Com-<lb/>puter Science, Vol. 3491. Springer-Verlag, Berlin Hidelberg New York (2005) 581-<lb/>590 <lb/>8. Ion, R.: Word Sense Disambiguation Methods Applied to English and Romanian. PhD <lb/>thesis, Romanian Academy, Bucharest (2007). <lb/>9. Ion, R., Mititelu, V.B.: Constrained Lexical Attraction Models. In: Nineteenth Interna-<lb/>tional Florida Artificial Intelligence Research Society Conference, pp. 297-302. AAAI <lb/>Press, Menlo Park, California, USA (2006). <lb/>10. Landis, J. R. and Koch, G. G.: The measurements of observer agreement for categori-<lb/>cal data. Biometrics, 33 (1997) 159-174. <lb/>11. Laurent, D., Séguéla, P., Nêgre S.: Cross Lingual Question Answering using <lb/>QRISTAL for CLEF 2007. This volume. <lb/>12. Magnini, B., Giampiccolo, D., Forner, P., Ayache, C., Jijkoun, V., Osenova, P., Peñas, <lb/>A., Rocha, P., Sacaleanu , B., and Sutcliffe, R.: Overview of the CLEF 2006 Multilin-<lb/>gual Question Answering Track. In: Evaluation of Multilingual and Multi-modal In-<lb/>formation Retrieval. Lecture Notes in Computer Science, Vol. 4730. Springer-Verlag, <lb/>Berlin Heidelberg New York (2007) 223-256. <lb/>13. Peñas, A., Rodrigo, Á., Verdejo, F.: Overview of the Answer Validation Exercise <lb/>2007. This volume. <lb/>14. Turmo, J., Comas, P., Ayache, C, Mostefa, D., Rosset, S., Lamel, L.: Overview of <lb/>QAST 2007. <lb/>15. Vallin, A., Magnini, B., Giampiccolo, D., Aunimo, L., Ayache, C.,Osenova, P., Peñas, <lb/>A., de Rijke, M., Sacaleanu , B., Santos, D. and Sutcliffe, R. .: Overview of the CLEF <lb/>2005 Multilingual Question Answering Track. In: Accessing Multilingual Information <lb/>Repositories. Lecture Notes in Computer Science, Vol. 4022. Springer-Verlag, Berlin <lb/>Heidelberg New York (2006) 307-331. <lb/>16. Voorhees, E.: Overview of the TREC 2002 Question Answering Track. In NIST Spe-<lb/>cial Publication 500-251: The Eleventh Text REtrieval Conference (TREC 2002). Na-<lb/>tional Institute of Standards and Technology, USA (2002). <lb/>17. Agirre, Eneko &amp; Lopez de Lacalle, Oier (2007). UBC-ALM: Combining k-NN with <lb/>SVD for WSD. Proceedings of the 4th International Workshop on Semantic Evalua-<lb/>tions (SemEval 2007), pp. 341-345. Prague, Czech Republic. <lb/>18. Chan, Yee Seng, &amp; Ng, Hwee Tou, &amp; Zhong, Zhi (2007). NUS-PT: Exploiting Paral-<lb/>lel Texts for Word Sense Disambiguation in the English All-Words Tasks. Proceed-<lb/>ings of the 4th International Workshop on Semantic Evaluations (SemEval 2007), pp. <lb/>253-256. Prague, Czech Republic. </listBibl>


	</text>
</tei>
