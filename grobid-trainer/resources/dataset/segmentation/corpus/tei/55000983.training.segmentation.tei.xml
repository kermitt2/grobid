<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<titlePage>RC24939 (W1001-093) January 29, 2010 <lb/>Computer Science <lb/>IBM Research Report <lb/>Load Balancing Algorithms for the Session Initiation Protocol <lb/>Hongbo Jiang <lb/>Huazhong University of Science and Technology <lb/>Arun Iyengar, Erich Nahum, Wolfgang Segmuller, <lb/>Asser Tantawi, Charles P. Wright <lb/>IBM Research Division <lb/>Thomas J. Watson Research Center <lb/>P.O. Box 704 <lb/>Yorktown Heights, NY 10598 <lb/>Research Division <lb/>Almaden -Austin -Beijing -Cambridge -Haifa -India -T. J. Watson -Tokyo -Zurich <lb/> </titlePage>

			<front>Load Balancing Algorithms for the Session <lb/>Initiation Protocol <lb/> Hongbo Jiang † , Arun Iyengar ‡ , Erich Nahum ‡ , Wolfgang Segmuller ‡ , Asser Tantawi ‡ , and Charles P. Wright ‡ <lb/>Huazhong University of Science and Technology † <lb/>IBM T.J. Watson Research Center ‡ <lb/>Abstract-This paper introduces several novel load balancing <lb/>algorithms for distributing Session Initiation Protocol (SIP) <lb/>requests to a cluster of SIP servers. Our load balancer improves <lb/>both throughput and response time versus a single node, while <lb/>exposing a single interface to external clients. We present the <lb/>design, implementation, and evaluation of our system using a <lb/>cluster of Intel x86 machines running Linux. We compare our <lb/>algorithms with several well-known approaches and present scal-<lb/>ability results for up to 10 nodes. Our best algorithm, Transaction <lb/>Least-Work-Left (TLWL), achieves its performance by integrat-<lb/>ing several features: knowledge of the SIP protocol; dynamic <lb/>estimates of back-end server load; distinguishing transactions <lb/>from calls; recognizing variability in call length; and exploiting <lb/>differences in processing costs for different SIP transactions. <lb/>By integrating these attributes, our algorithm provides finer-<lb/>grained load balancing than standard approaches, resulting in <lb/>throughput improvements of up to 24 percent and response times <lb/>improvements of up to two orders of magnitude. We present <lb/>a detailed analysis of how our algorithms significantly reduce <lb/>response time. <lb/></front>

			<body>I. INTRODUCTION <lb/>The Session Initiation Protocol (SIP) is a general-purpose <lb/>signaling protocol used to control various types of media <lb/>sessions. SIP is a protocol of growing importance, with uses in <lb/>Voice over IP, Instant Messaging, IPTV, Voice Conferencing, <lb/>and Video Conferencing. Wireless providers are standardizing <lb/>on SIP as the basis for the IP Multimedia System (IMS) <lb/>standard for the Third Generation Partnership Project (3GPP). <lb/>Third-party VoIP providers use SIP (e.g., Vonage, Gizmo), <lb/>as do digital voice offerings from existing legacy Telcos <lb/>(e.g., AT&amp;T, Verizon) as well as their cable competitors (e.g., <lb/>Comcast, Time-Warner). <lb/>While individual servers may be able to support hundreds <lb/>or even thousands of users, large-scale ISPs need to support <lb/>customers in the millions. A central component to providing <lb/>any large-scale service is the ability to scale that service with <lb/>increasing load and customer demands. A frequent mechanism <lb/>to scale a service is to use some form of a load-balancing <lb/>dispatcher that distributes requests across a cluster of servers. <lb/>However, almost all research in this space has been in the <lb/>context of either the Web (e.g., HTTP [25]) or file service (e.g., <lb/>NFS [1]). This paper presents and evaluates several algorithms <lb/>for balancing load across multiple SIP servers. We introduce <lb/>new algorithms which outperform existing ones. Our work is <lb/>relevant not just to SIP but also for other systems where it <lb/>is advantageous for the load balancer to maintain sessions in <lb/>which requests corresponding to the same session are sent by <lb/>the load balancer to the same server. <lb/>SIP has a number of features which distinguish it from <lb/>protocols such as HTTP. SIP is a transaction-based protocol <lb/>designed to establish and tear down media sessions, frequently <lb/>referred to as calls. Two types of state exist in SIP. The first, <lb/>session state, is created by the INVITE transaction and is <lb/>destroyed by the BYE transaction. Each SIP transaction also <lb/>creates state that exists for the duration of that transaction. SIP <lb/>thus has overheads that are associated both with sessions and <lb/>with transactions, and taking advantage of this fact can result <lb/>in more optimized SIP load balancing. <lb/>The session-oriented nature of SIP has important implica-<lb/>tions for load balancing. Transactions corresponding to the <lb/>same call must be routed to the same server; otherwise, <lb/>the server will not recognize the call. Session-aware request <lb/>assignment (SARA) is the process where a system assigns <lb/>requests to servers such that sessions are properly recognized <lb/>by that server, and subsequent requests corresponding to that <lb/>same session are assigned to the same server. In contrast, <lb/>sessions are less significant in HTTP. While SARA can be <lb/>done in HTTP for performance reasons (e.g., routing SSL <lb/>sessions to the same back end to encourage session reuse and <lb/>minimize key exchange), it is not necessary for correctness. <lb/>Many HTTP load balancers do not take sessions into account <lb/>in making load balancing decisions. <lb/>Another key aspect of the SIP protocol is that different <lb/>transaction types, most notably the INVITE and BYE trans-<lb/>actions, can incur significantly different overheads: On our <lb/>systems, INVITE transactions are about 75 percent more <lb/>expensive than BYE transactions. A load balancer could make <lb/>use of this information to make better load balancing decisions <lb/>which improve both response time and throughput. Our work <lb/>is the first to demonstrate how load balancing can be improved <lb/>by combining SARA with estimates of relative overhead for <lb/>different requests. <lb/>This paper introduces and evaluates several novel algorithms <lb/>for balancing load across SIP servers. Each algorithm com-<lb/>bines knowledge of the SIP protocol, dynamic estimates of <lb/>server load, and Session-Aware Request Assignment (SARA). <lb/>In addition, the best-performing algorithm takes into account <lb/>the variability of call lengths, distinguishing transactions from <lb/>calls, and the difference in relative processing costs for differ-<lb/>ent SIP transactions. <lb/>1) Call-Join-Shortest-Queue (CJSQ) tracks the number of <lb/>calls (throughout the remainder of this paper, the terms <lb/></body>

			<page>2 <lb/></page>

			<body>call and session are used interchangeably) allocated to <lb/>each back-end server and routes new SIP calls to the <lb/>node with the least number of active calls. <lb/>2) Transaction-Join-Shortest-Queue (TJSQ) routes a new <lb/>call to the server that has the fewest active transactions, <lb/>rather than the fewest calls. This algorithm improves on <lb/>CJSQ by recognizing that calls in SIP are composed <lb/>of the two transactions, INVITE and BYE, and that <lb/>by tracking their completion separately, finer-grained <lb/>estimates of server load can be maintained. This leads <lb/>to better load balancing, particularly since calls have <lb/>variable length and thus do not have a unit cost. <lb/>3) Transaction-Least-Work-Left (TLWL) routes a new call <lb/>to the server that has the least work, where work (i.e., <lb/>load) is based on relative estimates of transaction costs. <lb/>TLWL takes advantage of the observation that INVITE <lb/>transactions are more expensive than BYE transactions. <lb/>We have found that a 1.75:1 cost ratio between INVITE <lb/>and BYE results in the best performance. <lb/>We implement these algorithms in software by adding them <lb/>to the OpenSER open-source SIP server configured as a load <lb/>balancer. Our evaluation is done using the SIPp open-source <lb/>workload generator driving traffic through the load balancer <lb/>to a cluster of servers running commercially available SIP <lb/>servers. The experiments are conducted on a dedicated testbed <lb/>of Intel x86-based servers connected via Gigabit Ethernet. This <lb/>paper makes the following contributions: <lb/>• We show that two of our new algorithms, TLWL and <lb/>TJSQ, scale better, provide higher throughputs and ex-<lb/>hibit lower response times than any of the other ap-<lb/>proaches we tested. The differences in response times are <lb/>particularly significant. For low to moderate workloads, <lb/>TLWL and TJSQ provide response times for INVITE <lb/>transactions that are an order of magnitude lower than <lb/>that of any of the other approaches. Under high loads, the <lb/>improvement in response time increases to two orders of <lb/>magnitude. <lb/>• We present the design and implementation of a load <lb/>balancer for SIP servers, and demonstrate throughput of <lb/>up to 5500 calls per second and scalability of up to 10 <lb/>nodes. Our measurements show that the dispatcher intro-<lb/>duces minimal overhead to a SIP request. We extensively <lb/>evaluate several approaches for balancing SIP load across <lb/>servers including the three novel algorithms described <lb/>above as well as standard distribution policies such as <lb/>round-robin or hashing based on the SIP Call-ID. <lb/>• We present a detailed analysis of why TLWL and TJSQ <lb/>provide substantially better response times than the other <lb/>algorithms. Occupancy has a significant effect on re-<lb/>sponse times, where the occupancy for a transaction T <lb/>assigned to a server S is the number of transactions <lb/>already being handled by S when T is assigned to it. As <lb/>described in detail in Section VI, by allocating load more <lb/>evenly across nodes, the distributions of occupancy across <lb/>the cluster are balanced, resulting in greatly improved <lb/>response times. The naive approaches, in contrast, lead <lb/>to imbalances in load. These imbalances result in the <lb/>distributions of occupancy that exhibit large tails, which <lb/>contribute significantly to response time as seen by that <lb/>request. To our knowledge, we are the first to observe <lb/>this phenomenon experimentally. <lb/>• We show how our load balancing algorithms perform <lb/>with heterogeneous back ends. With no knowledge of <lb/>the server capacities, our approaches adapt naturally <lb/>to variations in back-end server processing power and <lb/>achieve considerably better performance than previous <lb/>approaches. <lb/>• Our work has implications for load balancing in general. <lb/>The results suggest that load balancers should keep track <lb/>of the number of uncompleted requests assigned to each <lb/>server in order to make better load balancing decisions. <lb/>If the load balancer can reliably estimate the relative <lb/>overhead for requests that it receives, this can further <lb/>improve performance. <lb/>These results show that our load balancer can effectively <lb/>scale SIP server throughput and provide significantly lower <lb/>response times without becoming a bottleneck. The dramatic <lb/>response time reductions that we achieve with TLWL and <lb/>TJSQ suggest that these algorithms should be adapted for other <lb/>applications, particularly when response time is crucial. <lb/>The remainder of this paper is organized as follows: Section <lb/>II provides a brief background on SIP. Section III presents <lb/>the design of our load balancing algorithms, and Section <lb/>IV describes their implementation. Section V overviews our <lb/>experimental software and hardware, and Section VI shows <lb/>our results in detail. Section VII discusses related work. Sec-<lb/>tion VIII presents our summary and conclusions, and briefly <lb/>mentions plans for future work. <lb/>Fig. 1. SIP Trapezoid <lb/>II. BACKGROUND <lb/>This section presents a brief overview of SIP. Readers <lb/>familiar with SIP may prefer to continue to Section III. <lb/>A. Overview of the Protocol <lb/>SIP is a control-plane protocol designed to establish, alter, <lb/>and terminate media sessions between two or more parties. <lb/></body>

			<page>3 <lb/></page>

			<body>INVITE sip:voicemail@us.ibm.com SIP/2.0 <lb/>Via: SIP/2.0/UDP sip-proxy.us.ibm.com:5060;branch=z9hG4bK74bf9 <lb/>Max-Forwards: 70 <lb/>From: Hongbo &lt;sip:hongbo@us.ibm.com&gt;;tag=9fxced76sl <lb/>To: VoiceMail Server &lt;sip:voicemail@us.ibm.com&gt; <lb/>Call-ID: 3848276298220188511@hongbo-thinkpad.watson.ibm.com <lb/>CSeq: 1 INVITE <lb/>Contact: &lt;sip:hongbo@hongbo-thinkpad.watson.ibm.com;transport=udp&gt; <lb/>Content-Type: application/sdp <lb/>Content-Length: 151 <lb/>v=0 <lb/>o=hongbo 2890844526 2890844526 IN IP4 hongbo-thinkpad.watson.ibm.com <lb/>s=-<lb/>c=IN IP4 9.2.2.101 <lb/>t=0 0 <lb/>m=audio 49172 RTP/AVP 0 <lb/>a=rtpmap:0 PCMU/8000 <lb/>TABLE I <lb/>AN EXAMPLE SIP MESSAGE <lb/>The core IETF SIP specification is given in RFC 3261 [27], <lb/>although there are many additional RFCs that enhance and <lb/>refine the protocol. Several kinds of sessions can be used, <lb/>including voice, text, and video, which are transported over <lb/>a separate data-plane protocol. SIP is a text-based protocol <lb/>that derives much of its syntax from HTTP [12]. Messages <lb/>contain headers and additionally bodies, depending on the type <lb/>of message. <lb/>The separation of the data plane from the control plane is <lb/>one of the key features of SIP and contributes to its flexibility. <lb/>SIP was designed with extensibility in mind; for example, <lb/>the SIP protocol requires that proxies forward and preserve <lb/>headers that they do not understand. As another example, SIP <lb/>can runs over many protocols such as UDP, TCP, SSL, SCTP, <lb/>IPv4 and IPv6. <lb/>SIP does not allocate and manage network bandwidth <lb/>as does a network resource reservation protocol such as <lb/>RSVP [34]; that is considered outside the scope of the proto-<lb/>col. <lb/>In VoIP, SIP messages contain an additional protocol, the <lb/>Session Description Protocol (SDP) [28], which negotiates <lb/>session parameters (e.g., which voice codec to use) between <lb/>end points using an offer/answer model. Once the end hosts <lb/>agree to the session characteristics, the Real-time Transport <lb/>Protocol (RTP) is typically used to carry voice data [30]. After <lb/>session setup, endpoints usually send media packets directly <lb/>to each other in a peer-to-peer fashion, although this can be <lb/>complex if network middleboxes such as Network Address <lb/>Translation (NAT) or firewalls are present. <lb/>SIP is composed of four layers, which define how the <lb/>protocol is conceptually and functionally designed, but not <lb/>necessarily implemented. The bottom layer is called the syn-<lb/>tax/encoding layer, which defines message construction. This <lb/>layer sits above the IP transport layer, e.g., UDP or TCP. SIP <lb/>syntax is specified using an augmented Backus-Naur Form <lb/>grammar (ABNF). The next layer is called (somewhat con-<lb/>fusingly) the transport layer. This layer determines how a SIP <lb/>client sends requests and handles responses, and how a server <lb/>receives requests and sends responses. The third layer is called <lb/>the transaction layer. This layer matches responses to requests, <lb/>manages SIP application-layer timeouts, and retransmissions. <lb/>The fourth layer is called the transaction user (TU) layer, <lb/>which may be thought of as the application layer in SIP. <lb/>The TU creates an instance of a client request transaction and <lb/>passes it to the transaction layer. <lb/>B. SIP Users and User Agents <lb/>A SIP Uniform Resource Identifier (URI) uniquely iden-<lb/>tifies a SIP user, e.g., sip:hongbo@us.ibm.com. This layer of <lb/>indirection enables features such as location-independence and <lb/>mobility. <lb/>SIP users employ end points known as user agents. These <lb/>entities initiate and receive sessions. They can be either hard-<lb/>ware (e.g., cell phones, pages, hard VoIP phones) or software <lb/>(e.g., media mixers, IM clients, soft phones). User agents are <lb/>further decomposed into User Agent Clients (UAC) and User <lb/>Agent Servers (UAS), depending on whether they act as a <lb/>client in a transaction (UAC) or a server (UAS). Most call <lb/>flows for SIP messages thus display how the UAC and UAS <lb/>behave for that situation. <lb/>SIP uses HTTP-like request/response transactions. A trans-<lb/>action consists of a request to perform a particular method <lb/>(e.g., INVITE, BYE, CANCEL, etc.) and at least one response <lb/>to that request. Responses may be provisional, namely, that <lb/>they provide some short term feedback to the user (e.g., <lb/>TRYING, RINGING) to indicate progress, or they can be <lb/>final (e.g., OK, 407 UNAUTHORIZED). The transaction is <lb/>completed when a final response is received, but not with only <lb/>a provisional response. <lb/>A SIP session is a relationship in SIP between two user <lb/>agents that lasts for some time period; in VoIP, a session cor-<lb/>responds to a phone call. For example, an INVITE message <lb/>not only creates a transaction (the sequence of messages for <lb/>completing the INVITE), but also a session if the transactions <lb/></body>

			<page>4 <lb/></page>

			<body>completes successfully. A BYE message creates a new trans-<lb/>action and, when the transaction completes, ends the session. <lb/>Figure 1 illustrates a typical SIP VoIP scenario, known as <lb/>the &quot;SIP Trapezoid.&quot; Note the separation between control and <lb/>data paths: SIP messages traverse the SIP overlay network, <lb/>routed by proxies, to find the eventual destinations. Once <lb/>endpoints are found, communication is typically performed <lb/>directly in a peer-to-peer fashion. In this case, the UAS <lb/>is another IP phone; however, the UAS can also be a full <lb/>server providing services such as voicemail, firewalling, voice <lb/>conferencing, etc. This work focuses on scaling the UAS, <lb/>rather than the proxy. Thus, for example, we are not concerned <lb/>about registration traffic, which is handled by the proxy. <lb/>C. The SIP Message Header <lb/>Table I shows a sample SIP message. In this message, <lb/>the user hongbo@us.ibm.com is contacting the voicemail <lb/>server to check his voicemail. This message is the initial <lb/>INVITE request to establish a media session with the voice-<lb/>mail server. An important line to notice is the Call-ID: <lb/>header, which is a globally unique identifier for the session <lb/>that is to be created. Subsequent SIP messages must refer to <lb/>that Call-ID to look up the established session state. If the <lb/>voicemail server is provided by a cluster, the initial INVITE <lb/>request will be routed to one back-end node, which will create <lb/>the session state. Barring some form of distributed shared <lb/>memory in the cluster, subsequent packets for that session <lb/>must also be routed to the same back-end node, otherwise <lb/>the packet will be erroneously rejected. Thus, many SIP load <lb/>balancing approaches use the Call-ID as hashing value in <lb/>order to route the message to the proper node. For example, <lb/>Nortel&apos;s Layer 4-7 switch product [22] uses this approach. <lb/>III. LOAD BALANCING ALGORITHMS <lb/>This section presents the design of our load balancing <lb/>algorithms. Figure 2 depicts our overall system. User Agent <lb/>Clients send SIP requests (e.g., INVITE, BYE) to our load <lb/>balancer which then selects a SIP server to handle each <lb/>request. The distinction between the various load balancing <lb/>algorithms presented in this paper are how they choose which <lb/>SIP server to handle a request. Servers send SIP responses <lb/>(e.g., 180 TRYING or 200 OK) to the load balancer which <lb/>then forwards the response to the client. <lb/>Note that SIP is used to establish, alter, or terminate media <lb/>sessions. Once a session has been established, the parties <lb/>participating in the session would typically communicate di-<lb/>rectly with each other using a different protocol for the media <lb/>transfer which would not go through our SIP load balancer. <lb/>A. Novel Algorithms <lb/>A key aspect of our load balancer is that requests corre-<lb/>sponding to the same session (call) are routed to the same <lb/>server. The load balancer only has the freedom to pick a <lb/>server to handle the first request of a call. All subsequent <lb/>requests corresponding to the call must go to the same server. <lb/>Fig. 2. System Architecture <lb/>This allows all requests corresponding to the same session to <lb/>efficiently access state corresponding to the session. <lb/>Our new load balancing algorithms, including the best <lb/>performing one (Transaction-Least-Work-Left (TLWL)), are <lb/>based on assigning calls to servers by picking the server with <lb/>the (estimated) least amount of work assigned but not yet <lb/>completed. While the concept of assigning work to servers <lb/>with the least amount of work left to do has been applied <lb/>in other contexts [16], [29], the specifics of how to do this <lb/>efficiently for a real application are often not at all obvious. <lb/>The system needs some method to reliably estimate the amount <lb/>of work that a server has left to do at the time load balancing <lb/>decisions are made. <lb/>In our system, the load balancer can estimate the work <lb/>assigned to a server based on the requests it has assigned to <lb/>the server and the responses it has received from the server. <lb/>All responses from servers to clients first go through the <lb/>load balancer which forwards the responses to the appropriate <lb/>clients. By monitoring these responses, the load balancer can <lb/>determine when a server has finished processing a request or <lb/>call and update the estimates it is maintaining for the work <lb/>assigned to the server. <lb/>1) Call-Join-Shortest-Queue: <lb/>The Call-Join-Shortest-<lb/>Queue (CJSQ) algorithm estimates the amount of work a <lb/>server has left to do based on the number of calls (sessions) <lb/>assigned to the server. Counters are maintained by the load <lb/>balancer indicating the number of calls assigned to each <lb/>server. When a new INVITE request is received (which <lb/>corresponds to a new call), the request is assigned to the <lb/>server with the lowest counter, and the counter for the server <lb/>is incremented by one. When the load balancer receives a <lb/>200 OK response to the BYE corresponding to the call, it <lb/>knows that the server has finished processing the call and <lb/>decrements the counter for the server. <lb/>A limitation of this approach is that the number of calls <lb/>assigned to a server is not always an accurate measure of the <lb/>load on a server. There may be long idle periods between the <lb/>transactions in a call. In addition, different calls may consist of <lb/>different numbers of transactions and may consume different <lb/>amounts of server resources. An advantage of CJSQ is that <lb/>it can be used in environments in which the load balancer is <lb/>aware of the calls assigned to servers but does not have an <lb/>accurate estimate of the transactions assigned to servers. <lb/></body>

			<page>5 <lb/></page>

			<body>2) Transaction-Join-Shortest-Queue: <lb/>An <lb/>alternative <lb/>method is to estimate server load based on the number <lb/>of transactions (requests) assigned to the servers. The <lb/>Transaction-Join-Shortest-Queue (TJSQ) algorithm estimates <lb/>the amount of work a server has left to do based on the <lb/>number of transactions (requests) assigned to the server. <lb/>Counters are maintained by the load balancer indicating the <lb/>number of transactions assigned to each server. When a new <lb/>INVITE request is received (which corresponds to a new <lb/>call), the request is assigned to the server with the lowest <lb/>counter, and the counter for the server is incremented by one. <lb/>When the load balancer receives a request corresponding to <lb/>an existing call, the request is sent to the server handling the <lb/>call, and the counter for the server is incremented. When the <lb/>load balancer receives a 200 OK response for a transaction, <lb/>it knows that the server has finished processing the transaction <lb/>and decrements the counter for the server. <lb/>A limitation of this approach is that all transactions are <lb/>weighted equally. In the SIP protocol, INVITE requests <lb/>are more expensive than BYE requests, since the INVITE <lb/>transaction state machine is more complex than the one for <lb/>non-INVITE transactions (such as BYE). This difference in <lb/>processing cost should ideally be taken into account in making <lb/>load balancing decisions. <lb/>3) Transaction-Least-Work-Left: The Transaction-Least-<lb/>Work-Left (TLWL) algorithm addresses this issue by assigning <lb/>different weights to different transactions depending on their <lb/>relative costs. It is similar to TJSQ with the enhancement that <lb/>transactions are weighted by relative overhead; in the special <lb/>case that all transactions have the same expected overhead, <lb/>TLWL and TJSQ are the same. Counters are maintained by the <lb/>load balancer indicating the weighted number of transactions <lb/>assigned to each server. New calls are assigned to the server <lb/>with the lowest counter. A ratio is defined in terms of relative <lb/>cost of INVITE to BYE transactions. We experimented with <lb/>several values for this ratio of relative cost. TLWL-2 assumes <lb/>INVITE transactions are twice as expensive as BYE trans-<lb/>actions and are indicated in our graphs as TLWL-2. We found <lb/>the best performing estimate of relative costs was 1.75; these <lb/>are indicated in our graphs as TLWL-1.75. <lb/>Note that if it is not feasible to determine the relative <lb/>overheads of different transaction types, TJSQ can be used <lb/>which results in almost as good performance as TLWL-1.75 <lb/>as will be shown in the results section. <lb/>Thus far, our presentation of the load balancing algorithms <lb/>assumes that the servers have similar processing capacities. <lb/>However, this may not always be the case. Some servers <lb/>may be more powerful than others; other servers may have <lb/>substantial background jobs that consume cycles. In these <lb/>situations, the load balancer could assign a new call to the <lb/>server with the lowest value of estimated work left to do (as <lb/>determined by the counters) divided by the capacity of the <lb/>server; this applies to CJSQ, TJSQ, and TLWL. <lb/>In some cases, though, the load balancer might not know <lb/>the capacity of the servers. For these situations, our new al-<lb/>gorithms have the robustness to automatically adapt to hetero-<lb/>geneous back-end servers with over 60% higher throughputs <lb/>than the previous algorithms we tested as we will show in the <lb/>results section. <lb/>CJSQ, TJSQ, and TLWL are all novel load balancing <lb/>algorithms. In addition, we are not aware of any previous <lb/>work which has successfully adapted least work left <lb/>algorithms for load balancing with SARA. <lb/>4) Adapting TJSQ and TLWL to an HTTP Environment <lb/>Without SARA: Note that a simpler form of TJSQ could <lb/>be deployed for applications in which SARA is not needed. <lb/>For example, consider a Web-based system communicating <lb/>over HTTP. The load balancer would have the flexibility to <lb/>assign requests to servers without regard for sessions. It would <lb/>maintain information about the number of requests assigned <lb/>to each server. The key support that the load balancer would <lb/>need from the server would be a notification of when a request <lb/>has completed. In systems for which all responses from the <lb/>server first go back to the load balancer which then forwards <lb/>the responses to the client, a response from the server would <lb/>serve as the desired notification, so no further support from <lb/>the server would be needed. <lb/>This system could further be adapted to a version of TLWL <lb/>without SARA if the load balancer is a content-aware layer <lb/>7 switch [25]. In this case, the load balancer has the ability <lb/>to examine the request and also receives responses from the <lb/>server; no additional server support would be required for the <lb/>load balancer to keep track of the number of requests assigned <lb/>to each server. Based on the contents of the request, the load <lb/>balancer could assign relative weights to the requests. For <lb/>example, a request for a dynamic page requiring invocation <lb/>of a server program could be assigned a higher weight than a <lb/>request for a file. The load balancer could use its knowledge of <lb/>the application to assign different weights to different requests. <lb/>B. Comparison Algorithms <lb/>We also implemented several standard load balancing algo-<lb/>rithms for comparison. These algorithms are not novel but are <lb/>described for completeness. <lb/>1) Response-time Weighted Moving Average: Another <lb/>method is to make load balancing decisions based on server <lb/>response times. The Response-time Weighted Moving Average <lb/>(RWMA) algorithm assigns calls to the server with the lowest <lb/>weighted moving average response time of the last n (20 in <lb/>our implementation) response time samples. The formula for <lb/>computing the RWMA linearly weights the measurements so <lb/>that the load balancer is responsive to dynamically changing <lb/>loads, but does not overreact if the most recent response time <lb/>measurement is highly anomalous. The most recent sample has <lb/>a weight of n, the second most recent a weight of n − 1, and <lb/>the oldest a weight of one. The load balancer determines the <lb/>response time for a request based on the time when the request <lb/>is forwarded to the server and the time the load balancer <lb/>receives a 200 OK reply from the server for the request. <lb/>2) Hash and FNVHash: We have also implemented a <lb/>couple of simple load balancing algorithms which do not <lb/></body>

			<page>6 <lb/></page>

			<body>require the load balancer to estimate server load, response <lb/>times, or work remaining to be done. <lb/>The Hash algorithm is a static approach for assigning calls <lb/>to servers based on Call-ID which is a string contained in <lb/>the header of a SIP message identifying the call to which the <lb/>message belongs. A new INVITE transaction with Call-ID x <lb/>is assigned to server (Hash(x)modN ), where Hash(x) is a <lb/>hash function and N is the number of servers. We have used <lb/>both the original hash function provided by OpenSER and <lb/>FNV hash [23]. In the performance graphs, Hash indicates <lb/>that the OpenSER hash function was used while FNVHash <lb/>indicates that FNV hash was used. <lb/>3) Round Robin: The hash algorithm is not guaranteed to <lb/>assign the same number of calls to each server. The Round <lb/>Robin (RR) algorithm guarantees a more equal distribution of <lb/>calls to servers. If the previous call was assigned to server M , <lb/>the next call is assigned to server (M + 1)modN , where N <lb/>is again the number of servers in the cluster. <lb/>IV. LOAD BALANCER IMPLEMENTATION <lb/>This section describes the implementation of the load bal-<lb/>ancer. Figure 3 illustrates the structure of the load balancer. <lb/>The rectangles represent key functional modules of the load <lb/>balancer, while the irregular shaped boxes represent state <lb/>information that is maintained. The arrows represent commu-<lb/>nication flows. <lb/>The Receiver receives requests which are then parsed by <lb/>the Parser. The Session Recognition module determines if the <lb/>request corresponds to an already existing session by querying <lb/>the Session State which is implemented as a hash table as <lb/>described below. If so, the request is forwarded to the server to <lb/>which the session was previously assigned. If not, the Server <lb/>Selection module assigns the new session to a server using <lb/>one of the algorithms described earlier. For several of the load <lb/>balancing algorithms we have implemented, these assignments <lb/>may be based on Load Estimates maintained for each of the <lb/>servers. The Sender forwards requests to servers and updates <lb/>Load Estimates and Session State as needed. <lb/>Session <lb/>Session <lb/>State <lb/>Receiver <lb/>Parser <lb/>Recognition <lb/>Incoming Packets <lb/>Selection <lb/>Server <lb/>Estimates <lb/>Load <lb/>New Sessions <lb/>Trigger <lb/>Existing Sessions <lb/>Sender Outgoing Packets <lb/>Fig. 3. Load Balancer Architecture <lb/>The Receiver also receives responses sent by servers. The <lb/>client to receive the response is identified by the Session <lb/>Recognition module which obtains this information by query-<lb/>ing the Session State. The Sender then sends the response to <lb/>the client and updates Load Estimates and Session State as <lb/>needed. <lb/>The Trigger module updates Session State and Load Esti-<lb/>mates after a session has expired. <lb/>Figure 4 shows the pseudocode for the main loop of the load <lb/>balancer. The pseudocode is intended to convey the general <lb/>approach of the load balancer; it omits certain corner cases <lb/>and error handling (for example, for duplicate packets). The <lb/>essential approach is to identify SIP packets by their Call-ID <lb/>and use that as a key for identifying calls. Our load balancer <lb/>selects the appropriate server to handle the first request of <lb/>a call. It also maintains mappings between calls and servers <lb/>using two hash tables which are indexed by call ID. That way, <lb/>when a new transaction corresponding to the call is received, <lb/>it will be routed to the correct server. <lb/>01: h = hash call-id <lb/>02: look up session in active table <lb/>03: if not found <lb/>04: <lb/>/* don&apos;t know this session */ <lb/>05: <lb/>if INVITE <lb/>06: <lb/>/* new session */ <lb/>07: <lb/>select one node d using algorithm <lb/>08: <lb/>(TLWL, TJSQ, RR, Hash, etc) <lb/>09: <lb/>add entry (s,d,ts) to active table <lb/>10: <lb/>s = STATUS_INV <lb/>11: <lb/>node_counter[d] += w inv <lb/>12: <lb/>/* non-invites omitted for clarity */ <lb/>13: else /* this is an existing session */ <lb/>14: <lb/>if 200 response for INVITE <lb/>15: <lb/>s = STATUS_INV_200 <lb/>16: <lb/>record response time for INVITE <lb/>17: <lb/>node_counter[d] -= w inv <lb/>18: <lb/>else if ACK request <lb/>19: <lb/>s = STATUS_ACK <lb/>20: <lb/>else if BYE request <lb/>21: <lb/>s = STATUS_BYE <lb/>22: <lb/>node_counter[d] += w bye <lb/>23: <lb/>else if 200 response for BYE <lb/>24: <lb/>s = STATUS_BYE_200 <lb/>25: <lb/>record response time for BYE <lb/>26: <lb/>node_counter[d] -= w bye <lb/>27: <lb/>move entry to expired table <lb/>28: /* end session lookup check */ <lb/>29: if request (INVITE , BYE etc.) <lb/>30: <lb/>forward to d <lb/>31: else if response (200/100/180/481) <lb/>32: <lb/>forward to client <lb/>Fig. 4. Load Balancing Pseudocode <lb/>The active hash table maintains state information on calls <lb/>and transactions that the system is currently handling, and an <lb/>expired hash table is used for routing stray duplicate packets <lb/>for requests that have already completed. This is analogous <lb/></body>

			<page>7 <lb/></page>

			<body>to the handling of old duplicate packets in TCP when the <lb/>protocol state machine is in the TIME-WAIT state [2]. When <lb/>sessions are completed, their state is moved into the expired <lb/>hash table. After the load balancer receives a 200 status <lb/>message from a server in response to a BYE message from <lb/>a client, the load balancer moves the call information from <lb/>the active hash table to the expired hash table so that the <lb/>call information is maintained long enough for the client to <lb/>receive the 200 status message that the BYE request has been <lb/>processed by the server. Information in the expired hash table <lb/>is periodically reclaimed by garbage collection. Both hash <lb/>tables store multiple entities which hash to the same bucket <lb/>in a linked list. <lb/>We found that the choice of hash function affects the <lb/>efficiency of the load balancer. The hash function used by <lb/>OpenSER did not do a very good job of distributing call IDs <lb/>across hash buckets. Given a sample test with 300,000 calls, <lb/>OpenSER&apos;s hash function distributed the calls to about 88,000 <lb/>distinct buckets. This resulted in a high percentage of buckets <lb/>containing several call ID records; searching these buckets <lb/>adds overhead. <lb/>We experimented with several different hash functions and <lb/>found FNV hash [23] to be the best one. For that same test <lb/>of 300,000 calls, FNV Hash mapped these calls to about <lb/>228,000 distinct buckets. The average length of searches was <lb/>thus reduced by a factor of almost three. <lb/>When an INVITE request arrives corresponding to a new <lb/>call, the call is assigned to a server using one of the algorithms <lb/>described earlier. Subsequent requests corresponding to the <lb/>call are always sent to the same machine where the original <lb/>INVITE was assigned to. For algorithms that use response <lb/>time, the response time of the individual INVITE and BYE <lb/>requests are recorded when they are completed. An array of <lb/>node counter values are kept that track the number of of <lb/>INVITE and BYE requests. <lb/>V. EXPERIMENTAL ENVIRONMENT <lb/>In this section we describe the hardware and software that <lb/>we use for our experiments, our experimental methodology, <lb/>and metrics we measure. <lb/>SIP Software. For client-side workload generation, we <lb/>use the the open source SIPp [13] tool, which is the de <lb/>facto standard for generating SIP load. SIPp is a configurable <lb/>packet generator, extensible via a simple XML configuration <lb/>language. It uses an efficient event-driven architecture but <lb/>is not fully RFC compliant (e.g., it does not do full packet <lb/>parsing). It can thus emulate either a client (UAC) or server <lb/>(UAS), but at many times the capacity of a standard SIP end-<lb/>host. We take advantage of this feature in Section VI-E, using <lb/>SIPp as both a client and server in order to stress the load <lb/>balancer. We use the Subversion revision 311 version of SIPp. <lb/>We use a commercially available SIP server. <lb/>Hardware and System Software. We conduct experiments <lb/>using two different types of machines, both of which are <lb/>IBM x-Series rack-mounted servers. Table II summarizes the <lb/>hardware and software configuration for our testbed. Eight of <lb/>Feature <lb/>Machine Type A Machine Type B <lb/>Quantity <lb/>11 <lb/>3 <lb/>CPU <lb/>3.06 GHz <lb/>2.8 GHz <lb/>RAM <lb/>4 GB <lb/>2 GB <lb/>Kernel <lb/>2.6.9-55.0.6 <lb/>2.6.9-11 <lb/>Distro <lb/>RedHat AS 4.5 <lb/>RedHat AS 4.5 <lb/>Roles <lb/>Back-End Server, <lb/>Workload <lb/>Load Balancer <lb/>Generation <lb/>TABLE II <lb/>HARDWARE TESTBED CHARACTERISTICS <lb/>the servers have two processors; however, we use only one pro-<lb/>cessor for our experiments. All machines are interconnected <lb/>using a gigabit Ethernet switch. <lb/>To obtain CPU utilization and network I/O rates, we use <lb/>nmon [15], a free performance monitoring tool from IBM for <lb/>AIX and Linux environments. For application and kernel pro-<lb/>filing, we use the open-source OProfile [24] tool. OProfile is <lb/>configured to report the default GLOBAL POWER EVENT, <lb/>which reports time in which the processor is not stopped (i.e. <lb/>non-idle profile events). <lb/>Workload. The workload we use is SIPp&apos;s simple SIP <lb/>UAC call model consisting of an INVITE, which the server <lb/>responds to with 100 TRYING, 180 RINGING, and 200 <lb/>OK responses. The client then sends an ACK request which <lb/>creates the session. After a variable pause to model call hold <lb/>times, the client closes the session using a BYE which the <lb/>server responds to with a 200 OK response. Calls may or <lb/>may not have pause times associated with them, intended <lb/>to capture the variable call duration of SIP sessions. In our <lb/>experiments, pause times are normally distributed with a mean <lb/>of one minute and a variance of 30 seconds. <lb/>While simple, this is a common configuration used in <lb/>SIP performance testing. Currently no standard SIP workload <lb/>model exists, although SPEC is attempting to define one [32]. <lb/>1 <lb/>10 <lb/>100 <lb/>1000 <lb/>500 <lb/>1000 <lb/>1500 <lb/>2000 <lb/>2500 <lb/>3000 <lb/>Response Time (ms) <lb/>Offered Load (Calls/sec) <lb/>CJSQ <lb/>RWMA <lb/>Hash <lb/>FNVHash <lb/>RR <lb/>TJSQ <lb/>TLWL-2 <lb/>TLWL-1.75 <lb/>Fig. 5. Average Response Time for INVITE <lb/>Methodology. Each run lasts for 3 minutes after a warm-up <lb/>period of 10 minutes. For experiments using the commercially <lb/></body>

			<page>8 <lb/></page>

			<body>available SIP server as the back-end server, there is also a <lb/>ramp-up phase until the experimental rate is reached. The <lb/>request rate starts at 1 cps and increases by x cps every second, <lb/>where x is the number of back-end nodes. Thus, if there are <lb/>8 servers, after 5 seconds, the request rate will be 41 cps. <lb/>If load is evenly distributed, each node will see an increase <lb/>in the rate of received calls of one additional cps until the <lb/>experimental rate is reached. After the experimental rate is <lb/>reached, it is sustained. SIPp is used in open-loop mode; calls <lb/>are generated at the configured rate regardless of whether the <lb/>other end responds to them. <lb/>Metrics. We measure both throughput and response time. <lb/>We define throughput as the number of completed requests <lb/>per second. The peak throughput is defined as the maximum <lb/>throughput which can be sustained while successfully handling <lb/>more than 99.99% of all requests. Response time is defined <lb/>as the length of time between when a request (INVITE or <lb/>BYE) is sent and the successful 200 OK is received. <lb/>Component Performance. We have measured the through-<lb/>put of a single SIPp node in our system to be 2925 calls per <lb/>second (cps) without pause times and 2098 cps with pause <lb/>times. The peak throughput for the commercially available SIP <lb/>server is about 300 cps in our system; this figure varies slightly <lb/>depending on the workload. Surprisingly, peak throughput is <lb/>not affected much by pause times. We have observed that <lb/>servers can be adversely affected by pause times. Since the <lb/>SIP server is java-based, we believe other overheads dominate <lb/>and obscure this effect. <lb/>VI. RESULTS <lb/>In this section, we present in detail the experimental results <lb/>of the load balancing algorithms defined in Section III. <lb/>A. Response Time <lb/>We observe significant differences in the response times of <lb/>the different load balancing algorithms. Figure 5 shows the <lb/>average response time for each algorithm versus offered load <lb/>measured for the INVITE transaction. In this experiment, the <lb/>load balancer distributes requests across 8 SIP server nodes. <lb/>The maximum throughput for such a system is around 2400 <lb/>cps. Note especially that the Y axis is in logarithmic scale. <lb/>Two versions of Transaction-Least-Work-Left are used. For <lb/>the curve labeled TLWL-1.75, INVITE transactions are 1.75 <lb/>times the weight of BYE transactions. In the curve labeled <lb/>TLWL-2, the weight is 2:1. The curve labeled Hash uses the <lb/>standard OpenSER hash function, whereas FNV hash is used <lb/>for the curve labeled FNVHash. <lb/>The algorithms cluster into three groups: TLWL-1.75, <lb/>TLWL-2, and TJSQ, which offer the best performance; CJSQ, <lb/>Hash, FNVHash, and Round Robin in the middle; and RWMA <lb/>which results in the worst performance. The differences in <lb/>response times are significant even when the system is not <lb/>heavily loaded. For example, at 200 cps, which is less than <lb/>10% of peak throughput, the average response time is about <lb/>2 ms for the algorithms in the first group, about 15 ms <lb/>for algorithms in the middle group, and about 65 ms for <lb/>1 <lb/>10 <lb/>100 <lb/>1000 <lb/>10000 <lb/>500 <lb/>1000 <lb/>1500 <lb/>2000 <lb/>2500 <lb/>3000 <lb/>Response Time (ms) <lb/>Offered Load (Calls/sec) <lb/>CJSQ <lb/>RWMA <lb/>Hash <lb/>FNVHash <lb/>RR <lb/>TJSQ <lb/>TLWL-2 <lb/>TLWL-1.75 <lb/>Fig. 6. Average Response Time for BYE <lb/>RWMA. These trends continue as load increases, with TLWL-<lb/>1.75, TLWL-2, and TJSQ resulting in response times 5-10 <lb/>smaller less than those for algorithms in the middle group. <lb/>As the system approaches peak throughput, the performance <lb/>advantage of the first group of algorithms increases to two <lb/>orders of magnitude. <lb/>1 <lb/>10 <lb/>100 <lb/>1000 <lb/>500 <lb/>1000 <lb/>1500 <lb/>2000 <lb/>2500 <lb/>3000 <lb/>3500 <lb/>Response Time (ms) <lb/>Offered Load (Calls/sec) <lb/>2 nodes <lb/>4 nodes <lb/>6 nodes <lb/>8 nodes <lb/>10 nodes <lb/>Fig. 7. Average response time for TLWL-1.75 <lb/>Hash and Round Robin are commonly used algorithms. <lb/>As described earlier, Hash is what is used by OpenSER. <lb/>The Nortel Networks Layer 2-7 Gigabit Ethernet Switch <lb/>Module also uses hashing for load balancing SIP requests. <lb/>The significant improvements in response time that TLWL and <lb/>TJSQ provide present a compelling reason for systems such as <lb/>these to use our algorithms. Section VI-C provides a detailed <lb/>analysis of the reasons for the large differences in response <lb/>times that we observe. <lb/>Similar trends are seen in Figure 6, which shows average <lb/>response time for each algorithm vs. offered load for the BYE <lb/>transaction, again using 8 SIP server nodes. BYE transactions <lb/>consume fewer resources than INVITE transactions resulting <lb/></body>

			<page>9 <lb/></page>

			<body>0 <lb/>500 <lb/>1000 <lb/>1500 <lb/>2000 <lb/>2500 <lb/>TLWL-1.75 <lb/>TLWL-2 <lb/>TJSQ <lb/>RR <lb/>FNVHash <lb/>Hash <lb/>RWMA <lb/>CJSQ <lb/>Throughput (calls/sec) <lb/>1515 <lb/>1854 <lb/>1855 <lb/>1954 <lb/>2135 <lb/>2272 <lb/>2317 <lb/>2439 <lb/>Fig. 8. Peak Throughput of Various Algorithms with 8 SIP Servers <lb/>in lower average response times. TLWL-1.75, TLWL-2, and <lb/>TJSQ provide the lowest average response times for BYE <lb/>transactions. However, the differences in response times for the <lb/>various algorithms are smaller than is the case with INVITE <lb/>transactions. This is largely because of SARA. The load <lb/>balancer has freedom to pick the least loaded server for the first <lb/>INVITE transaction of a call. However, a BYE transaction <lb/>must be sent to the server which is already handling the call. <lb/>Since TLWL-1.75 results in excellent response time and the <lb/>highest throughput of any of the algorithms we tested, it is the <lb/>preferred algorithm to use. We therefore provide additional <lb/>data on the response time of TLWL-1.75. Figure 7 shows <lb/>the average response time versus load for the Transaction-<lb/>LWL algorithm using different numbers of back-end SIP <lb/>server nodes. Observe that additional numbers of nodes give <lb/>significantly improved response time at each load level. SIP <lb/>applications tend to be near real-time, and thus improved <lb/>responsiveness has value to them. Even if a cluster is not fully <lb/>utilized, employing multiple nodes can improve the quality of <lb/>service at a given load level. <lb/>This graph illustrates that when the call rate significantly <lb/>exceeds the peak throughput, the system enters overload, and <lb/>the response time can go up significantly. Proper techniques <lb/>for handling overload are beyond the scope of this paper; we <lb/>hope to address it in future work. <lb/>B. Throughput <lb/>We now examine how well the throughput of our load <lb/>balancing algorithms scale with increasing load. Figure 8 <lb/>shows the peak throughputs vs. offered load for the various <lb/>algorithms using 8 back-end nodes. Several interesting results <lb/>are illustrated in this graph. <lb/>In the ideal case, we would hope to see 8 nodes provide <lb/>8 times the single-node performance. Recall that the peak <lb/>throughput is the maximum throughput which can be sustained <lb/>while successfully handling more than 99.99% of all requests <lb/>and is approximately 300 cps for a SIP server node. Therefore, <lb/>linear scalability suggests a maximum possible throughput of <lb/>about 2400 cps for 8 nodes. <lb/>TLWL-1.75 achieves linear scalability and results in the <lb/>highest peak throughput of 2439 cps. TLWL-2 comes close <lb/>to TLWL-1.75, but TLWL-1.75 does better due to its bet-<lb/>ter estimate of the cost ratio between INVITE and BYE <lb/>transactions. The same three algorithms resulted in the best <lb/>response times and peak throughput. However, the differences <lb/>in throughput between these algorithms and the other ones are <lb/>not as high as the differences in response time. <lb/>The standard algorithm used in OpenSER, Hash, achieves <lb/>1954 cps. Despite being a static approach with no dynamic <lb/>allocation at all, one could consider hashing doing relatively <lb/>well at about 80% of TLWL-1.75. Round-robin (RR in the <lb/>Figure) does somewhat better at 2135 cps, or 88% percent of <lb/>TLWL-1.75, illustrating that even very simple approaches to <lb/>balancing load across a cluster are better than none at all. <lb/>0 <lb/>500 <lb/>1000 <lb/>1500 <lb/>2000 <lb/>2500 <lb/>3000 <lb/>10 nodes <lb/>8 nodess <lb/>6 nodes <lb/>4 nodes <lb/>2 nodes <lb/>Throughput (calls/sec) <lb/>583 <lb/>1267 <lb/>1854 <lb/>2439 <lb/>2928 <lb/>Fig. 9. Peak throughput vs. # of nodes (TLWL-1.75) <lb/>Response-time Weighted Moving Average (RWMA) re-<lb/>sulted in the second lowest peak throughput. We did not <lb/>obtain good performance from this algorithm. It resulted in the <lb/>highest response times. Response times may not be the most <lb/>reliable measure of load on the servers. If the load balancer <lb/>weights the most recent response time(s) too heavily, this <lb/>might provide insufficient information to determine the least <lb/>loaded server. On the other hand, if the load balancer gives <lb/>significant weight to response times in the past, this makes the <lb/>algorithm too slow to respond to changing load conditions. <lb/>A server having the lowest weighted average response time <lb/>might have several new calls assigned to it resulting in too <lb/>much load on the server before the load balancer determines <lb/>that it is no longer the least loaded server. By contrast, when <lb/>a call is assigned to a server using TLWL-1.75 or TJSQ, the <lb/>load balancer takes this into account right away in making <lb/>future load balancing decisions. Therefore, TLWL-1.75 and <lb/>TJSQ would not encounter this problem. <lb/>Calls-Join-Shortest-Queue (CJSQ) is significantly worse <lb/>than the others, since it does not distinguish call hold times in <lb/></body>

			<page>10 <lb/></page>

			<body>the way that the transaction-based algorithms do. Experiments <lb/>we ran that did not include pause times (not shown due to <lb/>space limitations) showed CJSQ providing very good perfor-<lb/>mance, comparable to TJSQ. This is perhaps not surprising <lb/>since, when there are no pause times, the algorithms are <lb/>effectively equivalent. However, the presence of pause times <lb/>can lead CJSQ to misjudgments about allocation that end <lb/>up being worse than a static allocation such as Hash. TJSQ <lb/>does better than most of the other algorithms. This shows that <lb/>knowledge of SIP transactions and paying attention to the call <lb/>hold time can make a significant difference, particularly in <lb/>contrast to CJSQ. <lb/>0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>Cumulative Distribution Function <lb/>Occupancy (at Arrival) <lb/>FNVHash <lb/>RR <lb/>TJSQ <lb/>TLWL-1.75 <lb/>Fig. 10. CDF: Occupancy at one node xd017 <lb/>C. Occupancy and Response Time <lb/>Given the substantial improvements in response time shown <lb/>in Section VI-A, we believe it is worth explaining in depth how <lb/>certain load balancing algorithms can reduce response time <lb/>versus others. We show this in two steps: First, we demonstrate <lb/>how the different algorithms behave in terms of occupancy, <lb/>namely, the number of requests allocated to the system. The <lb/>occupancy for a transaction T assigned to a server S is the <lb/>number of transactions already being handled by S when T <lb/>is assigned to it. Then, we show how occupancy has a direct <lb/>influence on response time. In the experiments described in <lb/>this section, requests were distributed among four servers at a <lb/>rate of 600 cps. Experiments were run for one minute; thus, <lb/>each experiment results in 36,000 calls. <lb/>Figure 10 shows the cumulative distribution frequency <lb/>(CDF) of the occupancy as seen by a request at arrival time <lb/>for one back-end node for four algorithms: FNVHash, Round-<lb/>Robin, TJSQ, and TLWL-1.75. This shows how many requests <lb/>are effectively &quot;ahead in line&quot; of the arriving request. A point <lb/>(5, y) would indicate that y is the proportion of requests with <lb/>occupancy no more than 5. Intuitively, it is clear that the <lb/>more requests there are in service when a new request arrives, <lb/>the longer that new request will have to wait for service. <lb/>One can observe that the two Transaction-based algorithms <lb/>0.001 <lb/>0.01 <lb/>0.1 <lb/>1 <lb/>0.1 <lb/>1 <lb/>10 <lb/>100 <lb/>Complementary Cumulative Distribution Function <lb/>Occupancy (at Arrival) <lb/>FNVHash <lb/>RR <lb/>TJSQ <lb/>TLWL-1.75 <lb/>Fig. 11. CCDF: Occupancy at one node xd017 <lb/>see lower occupancies for the full range of the distribution, <lb/>where 90 percent see fewer than two requests, and in the <lb/>worst case never see more than 20 requests. Round-Robin and <lb/>Hash, however, have a much more significant proportion of <lb/>their distributions with higher occupancy values; 10 percent <lb/>of requests see 5 or more requests upon arrival. This is <lb/>particularly visible when looking at the complement of the <lb/>CDF, as shown in Figure 11: Round-robin and Hash have much <lb/>more significant tails than do TJSQ or TLWL-1.75. While the <lb/>medians of the occupancy values for the different algorithms <lb/>are the same (note that over 60% of the transactions for all <lb/>of the algorithms in Figure 10 have an occupancy of 0), the <lb/>tails are not, which influences the average response time, as <lb/>we show next. <lb/>Recall that average response time is the sum of all the <lb/>response times seen by individual requests divided by the <lb/>number of requests. Given a test run over a period at a fixed <lb/>load rate, all the algorithms have the same total number of <lb/>requests over the run. Thus by looking at contribution to total <lb/>response time we can see how occupancy affects average <lb/>response time. <lb/>Figure 12 shows the contribution of each request to the <lb/>total response time for the four algorithms in Figure 10, where <lb/>requests are grouped by the occupancy they observe when they <lb/>arrive in the system. In this graph, a point (5, y) would indicate <lb/>that y is the sum of response times for all requests arriving at a <lb/>system with 5 requests assigned to it. One can see that Round-<lb/>Robin and Hash have many more requests in the tail beyond <lb/>an observed occupancy of 20. However, this graph does not <lb/>give us a sense of how much these observations contribute <lb/>to the sum of all the response times (and thus the average <lb/>response time). This sum is shown in Figure 20, which is the <lb/>accumulation of the contributions based on occupancy. <lb/>In this graph, a point (5, y) would indicate that y is the <lb/>sum of response times for all requests with an occupancy <lb/>up to 5. Each curve accumulates the components of response <lb/>time (the corresponding points in Figure 12) until the total <lb/></body>

			<page>11 <lb/></page>

			<body>1 <lb/>4 <lb/>16 <lb/>64 <lb/>256 <lb/>1024 <lb/>4096 <lb/>16384 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>Response Time Contribution (ms) <lb/>Occupancy (at Arrival) <lb/>FNVHash <lb/>RR <lb/>TJSQ <lb/>TLWL-1.75 <lb/>Fig. 12. Response Time Contribution <lb/>sum of response times is given at the top right of the curve. <lb/>For example, in the Hash algorithm, approximately 12,000 <lb/>requests see an occupancy of zero, and contribute about <lb/>25,000 milliseconds towards the total response time. 4,000 <lb/>requests see an occupancy of one and contribute about 17,000 <lb/>milliseconds of response time to the total. Since the graph is <lb/>cumulative, the Y value for x = 1 is the sum of the two oc-<lb/>cupancy values, about 42,000 milliseconds. By accumulating <lb/>all the sums, one sees how large numbers of instances where <lb/>requests arrive at a system with high occupancy can add to <lb/>the average response time. <lb/>Figure 20 shows that TLWL-1.75 has a higher sum of <lb/>response times (40,761 milliseconds) than does TJSQ (34304 <lb/>ms), a difference of about 18 percent. This is because TJSQ is <lb/>singularly focused on minimizing occupancy, whereas TLWL-<lb/>1.75 minimizes work. Thus TJSQ has a smaller response time <lb/>at this low load (600 cps), but at higher loads, TLWL-1.75&apos;s <lb/>better load balancing allows it to provide higher throughput. <lb/>To summarize, by balancing load more evenly across a <lb/>cluster, the transaction-based algorithms improve response <lb/>time by minimizing the number of requests a new arrival must <lb/>wait behind before receiving service. This obviously depends <lb/>on the scheduling algorithm used by the server in the back end; <lb/>however, Linux systems like ours effectively have a scheduling <lb/>policy which is a hybrid between first-in-first-out (FIFO) and <lb/>processor sharing (PS) [11]. Thus the number of requests in <lb/>the system has a strong correlation with the response time seen <lb/>by an arriving request. <lb/>D. Heterogeneous Back Ends <lb/>In many deployments, it is not realistic to expect that <lb/>all nodes of a cluster have the same server capacity. Some <lb/>servers may be more powerful than others. Other servers may <lb/>be running background tasks which limit the CPU resources <lb/>which can be devoted to SIP. In this section, we look at how <lb/>our load balancing algorithms perform when the back end <lb/>servers have different capabilities. In these experiments, the <lb/>0 <lb/>1000 <lb/>2000 <lb/>3000 <lb/>4000 <lb/>5000 <lb/>6000 <lb/>1000 <lb/>2000 <lb/>3000 <lb/>4000 <lb/>5000 <lb/>6000 <lb/>7000 <lb/>8000 <lb/>Throughput (Calls/sec) <lb/>Offered Load <lb/>nopause <lb/>pause <lb/>Fig. 13. Load Balancer Throughput vs. Offered Load <lb/>0 <lb/>20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>1000 <lb/>2000 <lb/>3000 <lb/>4000 <lb/>5000 <lb/>6000 <lb/>7000 <lb/>8000 <lb/>0 <lb/>50 <lb/>100 <lb/>150 <lb/>200 <lb/>250 <lb/>CPU utilization (%) <lb/>Network I/O speed (Mbps) <lb/>Offered Load (Calls/sec) <lb/>nopause (CPUUtilization) <lb/>nopause (NetworkSpeed) <lb/>pause (CPUUtilization) <lb/>pause (NetworkSpeed) <lb/>Fig. 14. Load Balancer CPU Utilization (left scale) and Network Bandwidth <lb/>Consumed (right scale) <lb/>0 <lb/>2e+06 <lb/>4e+06 <lb/>6e+06 <lb/>8e+06 <lb/>1e+07 <lb/>1.2e+07 <lb/>8000 <lb/>6000 <lb/>5500 <lb/>5000 <lb/>4000 <lb/>2000 <lb/>1000 <lb/>CPU Events <lb/>2908790 <lb/>5147155 <lb/>9127360 <lb/>10891027 <lb/>11607917 <lb/>12102605 <lb/>11920234 <lb/>Other <lb/>CLibrary <lb/>Dispatcher <lb/>OpenSERCore <lb/>Kernel <lb/>Fig. 15. Load Balancer CPU Profile <lb/></body>

			<page>12 <lb/></page>

			<body>load balancer is routing requests to two different nodes. One <lb/>of the nodes is running another task which is consuming about <lb/>50% of its CPU capacity. The other node is purely dedicated <lb/>to handling SIP 300 cps. Ideally, the load balancing algorithm <lb/>in this heterogeneous system should result in a throughput of <lb/>about one and a half times this <lb/>0 <lb/>50 <lb/>100 <lb/>150 <lb/>200 <lb/>250 <lb/>300 <lb/>350 <lb/>400 <lb/>450 <lb/>TLWL-1.75 <lb/>TJSQ <lb/>RR <lb/>FNVHash <lb/>Throughput (calls/sec) <lb/>267 <lb/>267 <lb/>411 <lb/>438 <lb/>Fig. 16. Peak Throughput (Heterogeneous Backends) <lb/>Figure 16 shows the peak throughputs of four of the <lb/>load balancing algorithms. TLWL-1.75 achieves the highest <lb/>throughput of 438 cps, which is very close to optimal. TJSQ <lb/>is next at 411 CPS. Hash and RR provide significantly lower <lb/>peak throughputs. <lb/>1 <lb/>10 <lb/>100 <lb/>1000 <lb/>10000 <lb/>200 <lb/>250 <lb/>300 <lb/>350 <lb/>400 <lb/>450 <lb/>500 <lb/>550 <lb/>600 <lb/>Response Time (ms) <lb/>Offered Load (Calls/sec) <lb/>FNVHash <lb/>RR <lb/>TJSQ <lb/>TLWL-1.75 <lb/>Fig. 17. Average Response Time <lb/>Response times are shown in Figure 17. TLWL-1.75 offers <lb/>the lowest response times followed by TJSQ. The response <lb/>times for RR and Hash are considerably worse, with Hash <lb/>resulting in the longest response times. These results clearly <lb/>demonstrate that TLWL-1.75 and TJSQ are much better at <lb/>adapting to heterogeneous environments than RR and Hash. <lb/>Unlike those two, the dynamic algorithms track the number <lb/>of calls or transactions assigned to the back ends and attempt to <lb/>keep them balanced. Since the faster machine satisfies requests <lb/>twice as quickly, twice as many calls are allocated to it. Note <lb/>that that is done automatically without the dispatcher having <lb/>any notion of the disparity in processing power of the back-end <lb/>machines. <lb/>E. Load Balancer Capacity <lb/>In this section we evaluate the performance of the load <lb/>balancer itself, to see how much load it can support before it <lb/>becomes a bottleneck for the cluster. We use 5 SIPp nodes as <lb/>clients and 5 SIPp nodes as servers which allow us to generate <lb/>around 10,000 cps without SIPp becoming a bottleneck. Recall <lb/>from Section V that SIPp can be used in this fashion to emulate <lb/>both a client and a server, with a load balancer in between. <lb/>Figure 13 shows observed throughput vs. offered load <lb/>for the dispatcher using TLWL-1.75. The load balancer can <lb/>support up to about 5500 cps before succumbing to overload <lb/>when no pause times are used, and about 5400 cps when <lb/>pauses are introduced. Given that the peak throughput of the <lb/>SIP server is about 300 cps, the prototype should be able to <lb/>support about 17 SIP servers. <lb/>Figure 14 shows CPU utilization and network bandwidth <lb/>consumed versus offered load for the load balancer. The <lb/>graph confirms that the CPU is fully utilized at around 5500 <lb/>cps. We see that the bandwidth consumed never exceeds <lb/>300 megabits per second (Mbps) in our gigabit testbed; thus, <lb/>network bandwidth is not a bottleneck. <lb/>Figure 15 shows the CPU profiling results for the load <lb/>balancer obtained via oprofile for various load levels. As can <lb/>be seen, roughly half the time is spent in the Linux kernel, <lb/>and half the time in the core OpenSER functions. The load <lb/>balancing module, marked &quot;dispatcher&quot; in the graph, is a very <lb/>small component consuming fewer than 10 percent of cycles. <lb/>This suggests that if even higher performance is required from <lb/>the load balancer, several opportunities for improvement are <lb/>available. For example, further OpenSER optimizations could <lb/>be pursued, or the load balancer could be moved into the kernel <lb/>in a fashion similar to the IP Virtual Services (IPVS) [26] <lb/>subsystem. Since we are currently unable to fully saturate the <lb/>load balancer on our testbed, we leave this as future work. <lb/>F. Baseline SIPp and SIP Server Performance <lb/>This section presents the performance of our individual <lb/>components. These are not research results per se but instead <lb/>supplement the research results presented earlier. These results <lb/>also demonstrate that the systems we are using are not, by <lb/>themselves, bottlenecks that interfere with our evaluation. <lb/>Component <lb/>No Pause <lb/>Pause <lb/>SIPp <lb/>2925 <lb/>2098 <lb/>SIP Server <lb/>286 <lb/>290 <lb/>TABLE III <lb/>COMPONENT PERFORMANCE (CALLS PER SECOND) <lb/>These experiments show the load that an individual SIPp <lb/>client instance is capable of generating in isolation. Here, SIPp <lb/></body>

			<page>13 <lb/></page>

			<body>1 <lb/>10 <lb/>100 <lb/>1000 <lb/>500 <lb/>1000 <lb/>1500 <lb/>2000 <lb/>2500 <lb/>3000 <lb/>3500 <lb/>Response Time (ms) <lb/>Offered Load (Calls/sec) <lb/>nopause <lb/>pause <lb/>Fig. 18. SIPp Response Time <lb/>0 <lb/>20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>100 <lb/>150 <lb/>200 <lb/>250 <lb/>300 <lb/>350 <lb/>CPU utilization (%) <lb/>Offered Load (Calls/sec) <lb/>nopause <lb/>pause <lb/>Fig. 19. CPU utilization for a single SIP server node <lb/>is used in a back-to-back fashion, as both the client and the <lb/>server, with no load balancing intermediary in between them. <lb/>Table III shows the peak throughput that we obtained for SIPp <lb/>on our testbed for two configurations: with and without pause <lb/>times. Pause time is intended to capture the call duration that a <lb/>SIP session can last. Here, pause time is normally distributed <lb/>with a mean of 1 minute and a variance of 30 seconds. Observe <lb/>that introducing a pause reduces the available capacity of a <lb/>SIPp instance from 2925 calls/second (cps) to 2098 cps, a <lb/>reduction of nearly a third. <lb/>Figure 18 shows the average response time vs. load of a <lb/>call generated by SIPp. Note the log scale of the Y-axis in <lb/>the graph. SIPp uses millisecond granularity for timing; thus <lb/>calls completing in under 1 millisecond effectively appear as <lb/>zero. We observe that response times appear and increase <lb/>significantly at 2000 cps when pauses are used and 2400 <lb/>cps when pauses are excluded. At these load values, SIPp <lb/>itself starts becoming a bottleneck and a potential factor in <lb/>performance measurements. To ensure this does not happen, <lb/>we limit the load requested from a single SIPp box to 2000 <lb/>cps with pauses and 2400 without pauses. Thus, our two SIPp <lb/>client workload generators can produce an aggregate request <lb/>rate of 4000 or 4800 cps with and without pauses, respectively. <lb/>Table III also shows peak throughput observed for the <lb/>commercially available SIP server running on one of our back-<lb/>end nodes. Here, one SIPp client generates load to the SIP <lb/>server, again with no load balancer between them. <lb/>Again two configurations are shown: with and without pause <lb/>times. We see that the SIP server can support about 286 cps <lb/>with pause times, and 290 cps without pauses. Figure 19 shows <lb/>CPU utilization vs. offered load. This graph confirms that <lb/>the SIP server supports about 290 cps at 100 percent CPU <lb/>utilization. <lb/>VII. RELATED WORK <lb/>A load balancer for SIP is presented in [31]. In this paper, <lb/>requests are routed to servers based on the receiver of the call. <lb/>A hash function is used to assign receivers of calls to servers. <lb/>A key problem with this approach is that it is difficult to come <lb/>up with an assignment of receivers to servers which results in <lb/>even load balancing. This approach also does not adapt itself <lb/>well to changing distributions of calls to receivers. Our study <lb/>considers a wider variety of load balancing algorithms and <lb/>shows scalability to a larger number of nodes. It should be <lb/>noted that [31] is concerned not just with scalability. The paper <lb/>also addresses high availability and how to handle failures. <lb/>A number of products are advertising support for SIP <lb/>load balancing including Nortel Networks&apos; Layer 2-7 Gigabit <lb/>Ethernet Switch Module for IBM BladeCenter [17], Foundry <lb/>Networks&apos; ServerIron [21], and F5&apos;s BIG-IP [9]. Publicly <lb/>available information on these products does not reveal the <lb/>specific load balancing algorithms that they employ. <lb/>16384 <lb/>32768 <lb/>65536 <lb/>131072 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>Cumulative Response Time Contribution (ms) <lb/>Occupancy (at Arrival) <lb/>FNVHash <lb/>RR <lb/>TJSQ <lb/>TLWL-1.75 <lb/>Fig. 20. Response Time Cumulative Contribution <lb/>A considerable amount of work has been done in the area <lb/>of load balancing for HTTP requests [5]. One of the earliest <lb/>papers in this area describes how NCSA&apos;s Web site was scaled <lb/>using round-robin DNS [19]. Advantages of using an explicit <lb/></body>

			<page>14 <lb/></page>

			<body>load balancer over round-robin DNS were demonstrated in [8]. <lb/>The load balancer described in [8] is content unaware because <lb/>it does not examine the contents of a request. Content-aware <lb/>load balancing, in which the load balancer examines the <lb/>request itself to make routing decisions, is described in [25], <lb/>[3], [4]. Routing multiple requests from the same client to the <lb/>same server for improving the performance of SSL in clusters <lb/>is described in [14]. Load balancing at highly accessed real <lb/>Web sites is described in [6], [18]. Client-side techniques for <lb/>load balancing and assigning requests to servers are presented <lb/>in [20], [10]. A method for load balancing in clustered Web <lb/>servers in which request size is taken into account in assigning <lb/>requests to servers is presented in [7]. <lb/>Least-work-left (LWL) and join-shortest-queue (JSQ) have <lb/>been applied to assigning tasks to servers in other do-<lb/>mains [16], [29]. While conceptually TLWL, TJSQ, and CJSQ <lb/>use similar principles for assigning sessions to servers, there <lb/>are considerable differences in our work. Past work in this area <lb/>has not considered SARA. In SARA, only the first request in <lb/>a session can be assigned to a server using LWL or JSQ. <lb/>Subsequent requests from the session must be assigned to the <lb/>server handling the first request; load balancing using LWL <lb/>and JSQ as defined in these papers is thus not possible. In <lb/>addition, these papers do not reveal how a load balancer can <lb/>reliably estimate the least work left for a SIP server which is <lb/>an essential feature of our load balancer. <lb/>VIII. SUMMARY AND CONCLUSIONS <lb/>This paper introduces three novel approaches to load bal-<lb/>ancing in SIP server clusters. We have presented the design, <lb/>implementation, and evaluation of a load balancer for cluster-<lb/>based SIP servers. Our load balancer performs session-aware <lb/>request assignment (SARA) to ensure that SIP transactions <lb/>are routed to the proper back-end node that contains the <lb/>appropriate session state. We presented three novel algorithms: <lb/>Call Join Shortest Queue (CJSQ), Transaction Join Shortest <lb/>Queue (TJSQ), and Transaction Least-Work-Left (TLWL). <lb/>The TLWL algorithms result in the best performance both <lb/>in response time and throughput followed by TJSQ. TJSQ <lb/>has the advantage that knowledge of relative overheads of <lb/>different transaction types is not needed. The most significant <lb/>differences were in response time. Under light to moderate <lb/>loads, TLWL-1.75, TLWL-2, and TJSQ achieved response <lb/>times for INVITE transactions which were at least 5 times <lb/>smaller than the other algorithms we tested. Under heavy <lb/>loads, TLWL-1.75, TLWL-2, and TJSQ have response times <lb/>for INVITE transactions two orders of magnitude smaller <lb/>than the other approaches. For SIP applications that require <lb/>good quality of service, these dramatically lower response <lb/>times are significant. We showed that these algorithms provide <lb/>significantly better response time by distributing requests <lb/>across the cluster more evenly, thus minimizing occupancy <lb/>and correspondingly the amount of time a particular request <lb/>waits behind others for service. TLWL-1.75 provides 25% <lb/>better throughput than a standard hash-based algorithm and <lb/>14% better throughput than a dynamic round-robin algorithm. <lb/>TJSQ provides nearly the same level of performance, but CJSQ <lb/>performs poorly, since it does not distinguish transactions from <lb/>calls and does not consider variable call hold times. <lb/>Our results show that by combining knowledge of the <lb/>SIP protocol, recognizing the variability of call lengths, dis-<lb/>tinguishing transactions from calls, and accounting for the <lb/>difference in processing costs for different SIP transaction <lb/>types, load balancing for SIP servers can be significantly <lb/>improved. <lb/>The dramatic reduction in response times that we achieved <lb/>for both TLWL and TJSQ, compared to other approaches, <lb/>suggest that they should be applied to other domains besides <lb/>SIP, particularly if response time is crucial. Our results are <lb/>influenced by the fact that SIP requires SARA. However, even <lb/>in environments where SARA is not needed, variants of TLWL <lb/>and TJSQ without SARA could be deployed and may offer <lb/>significant benefits over commonly deployed load balancing <lb/>algorithms based on round robin, hashing, or response times. <lb/>A key aspect of TJSQ and TLWL which can be applied to load <lb/>balancing systems in general is that load balancers should keep <lb/>track of the number of uncompleted requests assigned to each <lb/>server in order to make better load balancing decisions. In <lb/>addition, if the load balancer can reliably estimate the relative <lb/>overhead for requests that it receives, this can further improve <lb/>performance. <lb/>Several opportunities exist for potential future work. These <lb/>include: evaluating our algorithms on larger clusters to further <lb/>test their scalability; adding a fail-over mechanism to ensure <lb/>that the load balancer is not a single point of failure; and <lb/>looking at other SIP workloads such as instant messaging or <lb/>presence. <lb/></body>

			<listBibl>REFERENCES <lb/>[1] Darrell C. Anderson, Jeffrey S. Chase, and Amin Vahdat. Interposed <lb/>request routing for scalable network storage. In OSDI, pages 259-272, <lb/>2000. <lb/>[2] Mohit Aron and Peter Druschel. TCP implementation enhancements for <lb/>improving Webserver performance. Technical Report TR99-335, Rice <lb/>University Computer Science Dept., July 1999. <lb/>[3] Mohit Aron, Peter Druschel, and Willy Zwaenepoel. Efficient support <lb/>for P-HTTP in cluster-based Web servers. In Proceedings of the USENIX <lb/>1999 Annual Technical Conference, Monterey, CA, June 1999. <lb/>[4] Mohit Aron, Darren Sanders, Peter Druschel, and Willy Zwaenepoel. <lb/>Scalable content-aware request distribution in cluster-based network <lb/>servers. In Proceedings of the USENIX 2000 Annual Technical Con-<lb/>ference, San Diego, CA, June 2000. <lb/>[5] Valeria Cardellini, Emiliano Casalicchio, Michele Colajanni, and <lb/>Philip S. Yu. The state of the art in locally distributed Web-server <lb/>systems. ACM Computing Surveys, 34(2):263-311, June 2002. <lb/>[6] Jim Challenger, Paul Dantzig, and Arun Iyengar. A scalable and highly <lb/>available system for serving dynamic data at frequently accessed Web <lb/>sites. In Proceedings of ACM/IEEE SC98, November 1998. <lb/>[7] Gianfranco Ciardo, Alma Riska, and Evgenia Smirni. EQUILOAD: <lb/>A load balancing policy for clustered Web servers. Performance <lb/>Evaluation, 46(2-3):101-124, 2001. <lb/>[8] Dan Dias, William Kish, Rajat Mukherjee, and Renu Tewari. A scalable <lb/>and highly available Web server. In Proceedings of the 1996 IEEE <lb/>Computer Conference (COMPCON), February 1996. <lb/>[9] F5. F5 introduces intelligent traffic management solution to power <lb/>service providers&apos; rollout of multimedia services. http://www.f5.com/ <lb/>news-press-events/press/2007/20070924.html. <lb/>[10] Zongming Fei, Samrat Bhattacharjee, Ellen Zegura, and Mustapha <lb/>Ammar. A novel server selection technique for improving the response <lb/>time of a replicated service. In Proceedings of IEEE INFOCOM, 1998. <lb/></listBibl>

			<page>15 <lb/></page>

			<listBibl>[11] Hanhua Feng, Vishal Misra, and Dan Rubenstein. PBS: A unified <lb/>priority-based scheduler. In Proceedings of ACM Sigmetrics, San Diego, <lb/>CA, June 2007. <lb/>[12] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee. <lb/>Hypertext transfer protocol -HTTP/1.1. RFC 2068, Internet Engineering <lb/>Task Force, January 1997. <lb/>[13] Richard Gayraud and Olivier Jacques. SIPp. http://sipp.sourceforge.net. <lb/>[14] G. Goldszmidt, G. Hunt, R. King, and R. Mukherjee. Network <lb/>dispatcher: A connection router for scalable Internet services. In <lb/>Proceedings of the 7th International World Wide Web Conference, <lb/>Brisbane, Australia, April 1998. <lb/>[15] Nigel Griffiths. nmon: A free tool to analyze AIX and Linux perfor-<lb/>mance. http://www.ibm.com/developerworks/aix/library/au-analyze aix/ <lb/>index.html. <lb/>[16] Mor Harchol-Balter, Mark Crovella, and Cristina D. Murta. On choosing <lb/>a task assignment policy for a distributed server system. Journal of <lb/>Parallel and Distributed Computing, 59(2):204-228, 1999. <lb/>[17] IBM. Application switching with Nortel Networks layer 2-7 gigabit <lb/>ethernet switch module for IBM BladeCenter. http://www.redbooks. <lb/>ibm.com/abstracts/redp3589.html?Open. <lb/>[18] Arun Iyengar, Jim Challenger, Daniel Dias, and Paul Dantzig. High-<lb/>performance Web site design techniques. IEEE Internet Computing, <lb/>4(2), March/April 2000. <lb/>[19] Thomas T. Kwan, Robert E. McGrath, and Daniel A. Reed. NCSA&apos;s <lb/>World Wide Web server: Design and performance. IEEE Computer, <lb/>28(11):68-74, November 1995. <lb/>[20] Dan Mosedale, William Foss, and Rob McCool. Lessons learned admin-<lb/>istering Netscape&apos;s Internet site. IEEE Internet Computing, 1(2):28-35, <lb/>March/April 1997. <lb/>[21] Foundry Networks. ServerIron switches support SIP load balancing <lb/>VoIP/SIP traffic management solutions. http://www.foundrynet.com/ <lb/>solutions/sol-app-switch/sol-voip-sip/. <lb/>[22] Nortel Networks. Layer 2-7 GbE switch module for IBM BladeCenter. <lb/>http://www-132.ibm.com/webapp/wcs/stores/servlet/ProductDisplay? <lb/>productId=4611686018425170446&amp;storeId=1&amp;langId=-1&amp;catalogId= <lb/>-840. <lb/>[23] Landon Curt Noll. Fowler/Noll/Vo (FNV) hash. http://isthe.com/chongo/ <lb/>tech/comp/fnv/. <lb/>[24] OProfile. A system profiler for Linux. http://oprofile.sourceforge.net/. <lb/>[25] Vivek S. Pai, Mohit Aron, Gaurav Banga, Michael Svendsen, Peter <lb/>Druschel, Willy Zwaenepoel, and Erich M. Nahum. Locality-aware <lb/>request distribution in cluster-based network servers. In Architectural <lb/>Support for Programming Languages and Operating Systems, pages <lb/>205-216, 1998. <lb/>[26] Linux Virtual Server Project. IP virtual server (IPVS). http://www. <lb/>linuxvirtualserver.org/software/ipvs.html. <lb/>[27] J. Rosenberg, H. Schulzrinne, G. Camarillo, A. Johnston, J. Peterson, <lb/>R. Sparks, M. Handley, and E. Schooler. SIP: session initiation protocol. <lb/>RFC 3261, Internet Engineering Task Force, June 2002. <lb/>[28] J. Rosenberg and Henning Schulzrinne. An offer/answer model with <lb/>session description protocol (SDP). RFC 3264, Internet Engineering <lb/>Task Force, June 2002. <lb/>[29] Bianca Schroeder and Mor Harchol-Balter. Evaluation of task assign-<lb/>ment policies for supercomputing servers: The case for load unbalancing <lb/>and fairness. Cluster Computing, 7(2):151-161, 2004. <lb/>[30] Henning Schulzrinne, Stephen Casner, Ron Frederick, and Van Jacobson. <lb/>RTP: a transport protocol for real-time applications. RFC 3550, Internet <lb/>Engineering Task Force, July 2003. <lb/>[31] Kundan Singh and Henning Schulzrinne. Failover and load sharing in <lb/>SIP telephony. In Proceedings of the 2005 International Symposium on <lb/>Performance Evaluation of Computer and Telecommunication Systems <lb/>(SPECTS&apos;05), July 2005. <lb/>[32] Systems Performance Evaluation Corporation (SPEC). SPEC SIP <lb/>subcommittee. http://www.spec.org/specsip/. <lb/>[33] www.openser.org. The open SIP express router (OpenSER). http://www. <lb/>openser.org. <lb/>[34] Lixia Zhang, Steve Deering, Deborah Estrin, Scott Shenker, and David <lb/>Zappola. RSVP: a new resource reservation protocol. IEEE Communi-<lb/>cations Magazine, 40(5):116-127, May 2002. </listBibl>


	</text>
</tei>
