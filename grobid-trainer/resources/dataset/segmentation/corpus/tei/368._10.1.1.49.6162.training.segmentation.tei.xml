<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Evaluation and Ordering of Rules Extracted from Feedforward <lb/>Networks <lb/>Ismail Taha and Joydeep Ghosh <lb/>Laboratory of Arti cial Neural Systems <lb/>University of Texas <lb/>Austin, TX. 78712-1084 <lb/>fismail,ghoshg@pine.ece.utexas.edu <lb/>Abstract <lb/>Rules extracted from trained feedforward networks <lb/>can be used for explanation, validation, and cross-<lb/>referencing of network output decisions. This paper <lb/>introduces a rule evaluation and ordering mechanism <lb/>that orders rules extracted from feedforward networks <lb/>based on three performance measures. Detailed experi-<lb/>ments using three rule extraction techniques as applied <lb/>to the Wisconsin breast cancer database, illustrate the <lb/>power of the proposed methods. Moreover, a method <lb/>of integrating the output decisions of both the extracted <lb/>rule-based system and the corresponding trained net-<lb/>work is proposed. The integrated system provides fur-<lb/>ther improvements. <lb/></front>

			<body>1. Introduction <lb/>Symbolic rules extracted from trained networks help <lb/>in: (i) alleviating the knowledge acquisition problem <lb/>and re ning initial domain knowledge; (ii) providing <lb/>reasoning and explanation capabilities; (iii) support-<lb/>ing cross-referencing and veri cation capabilities; and <lb/>(iv) alleviating the catastrophic interference problem of <lb/>ANNs. KT 2], Subset 10], MofN 10], and NeuroRule <lb/>6] are some of the notable algorithms for rule extrac-<lb/>tion from feedforward ANNs. A very rich source of lit-<lb/>erature review of di erent rule extraction approaches <lb/>is a technical report written by Andrews et al. 1]. We <lb/>note that none of these works present a way of order-<lb/>ing the extracted rules based on an evaluation criteria, <lb/>even though such an ordering is needed by any generic <lb/></body>

			<front>This research was supported in part by ARO contract <lb/>DAAH04-94-G-0417, DAAH04-95-10494, and ATP grant #442. <lb/>Ismail Taha was also supported by the Egyptian Government <lb/>Ph.D. Fellowship in Electrical and Computer Engineering. <lb/></front>

			<body>inference engine that uses these rules. In this paper we <lb/>introduce a method for evaluating and ordering rules <lb/>extracted by three di erent techniques. The Wisconsin <lb/>Breast Cancer database is used to illustrate the power <lb/>of these methods, as well as the usefulness of integrat-<lb/>ing the network output with that of the derived expert <lb/>system, based on an approach introduced in section 4. <lb/>2 Three Rule Extraction Techniques <lb/>In this section we summarize three recent techniques <lb/>for extracting rules from trained feedforward ANNs 9]. <lb/>The rst approach is a binary Black-box Rule Extrac-<lb/>tion technique. The second and the third approaches <lb/>belong to the Link Rule Extraction category 8]. <lb/>2.1 First Technique (BIO-RE) <lb/>The rst approach is named Binarized Input-<lb/>Output Rule Extraction (BIO-RE) because it ex-<lb/>tracts binary rules from any neural network trained <lb/>with \binary&quot; inputs, based on its input-output map-<lb/>ping. It is surprisingly e ective within its domain of <lb/>applicability. The idea underlying BIO-RE is to con-<lb/>struct a truth table that represents all valid input-<lb/>output mappings of the trained network. BIO-RE is <lb/>then applies a logic minimization tool, Espresso 5], to <lb/>this truth table to generate a set of optimal binary <lb/>rules that represent the behavior of the trained net-<lb/>works. For example, an extracted rule: \IF Y 1 AND <lb/>NOT Y 2 ?! O 1 &quot;, is rewritten as \IF X 1 &gt; 1 AND <lb/>X 2 <lb/>2 ?! O 1 &quot;, where i is set to be the threshold <lb/>of X i (see Table 1 for examples). The BIO-RE ap-<lb/>proach is suitable when the input/output variables are <lb/>naturally binary or when binarization does not signif-<lb/>icantly degrade the performance. Also the input size <lb/>(n) should be small. <lb/>2.2 Second Technique (Partial-RE) <lb/>The idea underlying Partial-RE algorithm is that it <lb/>rst sorts both positive and negative incoming links for <lb/>each hidden and output node in descending order into <lb/>two di erent sets based on their weight values. Start-<lb/>ing from the highest positive weight (say i), it searches <lb/>for individual incoming links that can cause a node j <lb/>(hidden/output) to be active regardless of other input <lb/>links to this node. If such a link exists, it generates a <lb/>rule: \IF Node i <lb/>cf <lb/>?! Node j &quot;, where cf represents the <lb/>measure of belief in the extracted rule and is equal to <lb/>the activation value of node j with this current combi-<lb/>nation of inputs. If a node i was found strong enough <lb/>to activate a node j, then this node is marked and can-<lb/>not be used in any further combinations when checking <lb/>the same node j. Partial-RE continues checking subse-<lb/>quent weights in the positive set until it nds one that <lb/>cannot activate the current node j by itself. Partial-<lb/>RE performs the same procedure for negative links and <lb/>small combinations of both positive and negative links <lb/>if the required number of premises in a rule is &gt; 1. <lb/>Partial-RE algorithm is suitable for large size prob-<lb/>lems, since extracting all possible rules is NP-hard and <lb/>extracting only the most e ective rules is a practical <lb/>alternative. See Table 2 for examples. <lb/>2.3 Third Technique (Full-RE) <lb/>Full-RE rst generates intermediate rules in the for-<lb/>mat: <lb/>IF (c 1 X 1 + c 2 X 2 + + c n X n ) &gt;= j ] cf <lb/>?! <lb/>Consequent j , where: c i is a constant representing the <lb/>e ect of the i th input (X i ) on Consequent j and j is a <lb/>constant determined based on the activation function <lb/>of node j to make it active. If node j is in the layer <lb/>above node i then c i represents the weight value w ji <lb/>of the link between these two nodes. In cases where <lb/>the neural network inputs (X i s) are continuous valued <lb/>inputs, then a range of X i values may satisfy an in-<lb/>termediate rule, and one would want to determine a <lb/>suitable extremum value in such a range. To make <lb/>this tractable, each input range has to be discretized <lb/>into a small number of values that can be subsequently <lb/>examined. Thus, each input feature X i 2 (a i ; b i ) is <lb/>discretized into k intervals 4]. When Full-RE nds <lb/>more than one discretization value of an input X i that <lb/>can satisfy the intermediate rule (i.e., the rule has more <lb/>than one feasible solution) then it chooses the minimum <lb/>or the maximum of these values based on the sign of <lb/>the corresponding e ect parameter c i . If c i is negative <lb/>then Full-RE chooses the minimum discretization value <lb/>of X i , otherwise it chooses the maximum value. How-<lb/>ever, all selected discretization values should satisfy the <lb/>left hand side (the inequality) of the intermediate rule <lb/>and the boundary constraints of all input features of <lb/>this inequality. Final rules extracted by Full-RE are <lb/>represented in the same format of Partial-RE expect <lb/>that each i is replaced by one of the discretization <lb/>boundaries (say d i;l ) selected by Full-RE as described <lb/>earlier. See Table 3 for examples. <lb/>3 Rule Evaluation and Ordering Proce-<lb/>dure <lb/>To evaluate the performance of rules extracted from <lb/>trained networks by any of the three presented tech-<lb/>niques (or by any other rule extraction approach), a <lb/>simple rule evaluation procedure which attaches three <lb/>performance measures to each extracted rule is devel-<lb/>oped. The three performance measures used to de-<lb/>termine the order of the extracted rules are: (i)The <lb/>soundness measure: it measures how many times <lb/>each rule is correctly red. (ii)The completeness <lb/>measure: a completeness measure attached to a rule <lb/>represents how many unique patterns are correctly <lb/>identi ed/classi ed by this rule and not by any other <lb/>extracted rule that is inspected by the inference engine <lb/>before this rule. For each extracted set of rules with <lb/>the same consequent, if the sum of the completeness <lb/>measures of all rules in this set equals the total num-<lb/>ber of input patterns having the corresponding output <lb/>then this set of extracted rules is 100% complete with <lb/>respect to that consequent. An extracted rule with zero <lb/>completeness measure but having a soundness mea-<lb/>sure &gt; 0 means that there is a preceding rule(s), in <lb/>the order of rule application, that covers the same in-<lb/>put patterns that this rule covers. Such a rule may <lb/>be removed. (iii)The false-alarm measure: it mea-<lb/>sures how many times a rule is mis red over the avail-<lb/>able data set. While the values of both the complete-<lb/>ness and false-alarm measures depend on the order of <lb/>rule application and the inference engine the soundness <lb/>measure does not. <lb/>3.1 The Rule Ordering Procedure <lb/>Finding the optimal ordering of extracted rules is <lb/>a combinatorial problem. So the following &quot;greedy&quot; <lb/>algorithm to order any set of extracted rules, based on <lb/>the three performance measures is developed. The rule <lb/>ordering algorithm rst creates a list L that contains <lb/>all extracted rules. Assume that the list L is divided <lb/>into two lists, a head list (L h ) and a tail list (L t ), where <lb/>L h is the list of all ordered rules and L t is the list of <lb/>all remaining (unordered) rules 1 . Initially, L h is empty <lb/>and L t includes all the extracted rules. A performance <lb/>criteria is used to select one rule from L t to be moved <lb/>to the end of L h , and the the process continues till L t <lb/>is null. <lb/>The steps of the rule ordering algorithm are as follows: <lb/>1. Initialize L h = f g, L t = fall extracted rulesg. <lb/>2. WHILE L t 6 = f g, DO <lb/>(a) Fire all rules in L h in order. <lb/>(b) Compute the completeness and false-alarm <lb/>measures for each rule in L t using the avail-<lb/>able data set. <lb/>(c) IF 9 a rule with zero false-alarm <lb/>THEN this rule is moved from L t to the end <lb/>of L h 2 . <lb/>ELSE Among all rules in L t select the one <lb/>with the highest <lb/>(Completeness -False-alarm) mea-<lb/>sure; add this rule to <lb/>the end of L h , delete it form L t . <lb/>(d) IF 9 any rule in L t with a zero completeness <lb/>measure then remove this rule from L t . This <lb/>means that the rules in L h cover this rule. <lb/>3. END DO. <lb/>In this paper, all rules extracted by our approaches <lb/>are ordered using the above rule ordering algorithm. <lb/>Also, the measures attached to all extracted rules as-<lb/>sume that an inference engine that res only one rule <lb/>per input (namely, the rst reable rule) is used. <lb/>4 Output Integration <lb/>The main objective of combining or integrating dif-<lb/>ferent learning modules is to increase the overall gener-<lb/>alization capability. Since the set of extracted rules is <lb/>an \approximated symbolic representation&quot; of the em-<lb/>bedded knowledge in the internal structure of the corre-<lb/>sponding trained network, it is expected that when an <lb/>input is applied to the extracted rules and the trained <lb/>network, they will usually both provide the same out-<lb/>put decision (see Table 5 for examples). The integra-<lb/>tion module should be able to choose the \better&quot; out-<lb/>put decision when the two decisions di er, and to com-<lb/>pute the certainty factor of the nal output decision. <lb/></body>

			<note place="footnote">1 i.e., the ordering of rules in Lt has no e ect. <lb/>2 If 9 more than one rule with zero false-alarm THEN select <lb/>the one with the highest completeness measure out of these rules <lb/>to be moved from Lt to the end of L h . <lb/></note>

			<body>When the two output decisions are di erent, the inte-<lb/>gration module can use the following selection criteria <lb/>to select a suitable decision. <lb/>1. Select the sub-system (i.e., the set of extracted <lb/>rules or the trained ANN) with the highest overall <lb/>performance if none of the following conditions are <lb/>satis ed: <lb/>2. For any mismatched pair of output decisions, <lb/>check the value of the neural network output deci-<lb/>sion (i.e., the activation value of the corresponding <lb/>output node of the neural network before thresh-<lb/>olding) <lb/>(a) If the extracted rule-base is indicated by Rule <lb/>1, but the neural network output is signi -<lb/>cantly high, then choose the neural network <lb/>instead to provide the nal decision. Also, <lb/>report that the extracted rule-base was not <lb/>able to identify this case, so that a new rule <lb/>can be asserted in the current knowledge base <lb/>to handle such cases in the future. <lb/>(b) If the neural network is indicated by Rule 1, <lb/>but the network output is signi cantly low, <lb/>then choose the extracted rule-base instead to <lb/>provide the nal output of this case. Also, re-<lb/>port that the neural network was not able to <lb/>identify this case, so that it can be retrained. <lb/>This case can also be applied if the di erence <lb/>between the two highest activation values of <lb/>the neural network output nodes is not sig-<lb/>ni cant. <lb/>This simple heuristic criteria of selecting one of the <lb/>two mismatched output decisions was applied for all <lb/>the three architectures and their corresponding set of <lb/>extracted rules using the breast cancer problem. The <lb/>implementation results are given in Table 5. <lb/>5 Implementation Results <lb/>The Wisconsin breast cancer data set has nine in-<lb/>puts (X 1 X 9 ) and two output classes (Benign or <lb/>Malignant). The available 683 instances were divided <lb/>randomly into a training set of size 341 and a test set of <lb/>size 342. In all experiments, an MLP network is trained <lb/>using the backpropagation algorithm with momentum <lb/>as well as a regularization term 3]. The dimension-<lb/>ality of the breast-cancer input space is reduced from <lb/>9 to 6 inputs using PCA 7, 9]. BIO-RE, Partial-RE, <lb/>and Full-RE are used to extract rules from Cancer-Bin, <lb/>Cancer-Norm, and Cancer-Cont networks respectively, <lb/>where the rst network is trained with a binarized <lb/>version of the available data, Cancer-Norm is trained <lb/>with a normalized input patterns, and Cancer-Cont is <lb/>trained with the original data set after dimensionality <lb/>reduction. Table 1, 2, and 3 present three sets of or-<lb/>dered rules extracted by the three rule extraction tech-<lb/>niques, along with the corresponding performance mea-<lb/>sures. Table 4 provides an overall comparison between <lb/>the performance of the extracted rules and their cor-<lb/>responding trained networks. It shows that the three <lb/>techniques were successfully used with approximately <lb/>the same performance regardless of the nature of the <lb/>training and testing data sets used for each network. <lb/>Also, it shows that binarizing and scaling the breast <lb/>cancer data set did not degrade the performance of <lb/>the trained networks or of the rules extracted by BIO-<lb/>RE and Partial-RE from these networks (\Cancer-Bin&quot; <lb/>and \Cancer-Norm&quot; respectively). This is due to the <lb/>fact that the original input features of the breast cancer <lb/>problem have the same range (1,10). Table 5 shows the <lb/>impact of the integration method. It is important to <lb/>mention that the limited gains due to the integration <lb/>is because of the high degree of agreement between the <lb/>two modules. Only 22, 20, and 14 out of 683 outcomes <lb/>were di erent respectively for the three experiments. <lb/>The integration mechanism was able to select correctly <lb/>20, 17, and 14 of these mismatches respectively. <lb/>6 Conclusions <lb/>Knowledge Based Neural Network researchers have <lb/>not reported on the aspect of how to order extracted <lb/>rules from trained ANNs. In this paper we introduced <lb/>a simple greedy rule evaluation algorithm that can or-<lb/>der rules extracted by any algorithm, with a goal of <lb/>improving the performance rate of the extracted rules <lb/>over available data. We also presented an integration <lb/>mechanism that can be used to select the nal out-<lb/>put decision if the output from the extracted rule-base <lb/>is di erent from that of the trained network. Exper-<lb/>imental results illustrated the power of the proposed <lb/>methods for extracting, evaluating and ordering the ex-<lb/>tracted rules as well as integrating them with the cor-<lb/>responding network output decisions. The extracted <lb/>rules compare favorably with other reported implemen-<lb/>tation results such as NeuroRule 6]. <lb/></body>

			<listBibl>References <lb/>1] R. Andrews, J. Diederich, and A. Tickle. A survey and <lb/>critique of techniques for extracting rules from trained <lb/>arti cial neural networks. Knowledge-Based Systems, <lb/>8(6):373{389, December 1995. <lb/>2] L. Fu. Rule learning by searching on adapted nets. <lb/>In Proceedings of the Ninth National Conference on <lb/>Arti cial Intelligence (Anaheim CA), pages 590{595, <lb/>1991. <lb/>3] J. Ghosh and K. Tumer. Structural adaptation and <lb/>generalization in supervised feed-forward networks. <lb/>Journal of Arti cial Neural Networks, 1(4):431{458, <lb/>1994. <lb/>4] H. Liu and R. Setiono. Chi2: Feature selection and <lb/>discretization of numeric attributes. In Proceedings <lb/>of the Seventh International Conference on Tools with <lb/>Arti cial Intelligence, pages 388{391, November 1995. <lb/>5] R. Ruddel and A. Sangiovanni-Vincentelli. Espresso-<lb/>MV: Algorithms for multiple-Valued logic minimiza-<lb/>tion. In Proceedings of Cust. Int. Circ. Conf. Portland, <lb/>May 1985. <lb/>6] R. Setiono and H. Liu. Symbolic representation of <lb/>neural networks. IEEE Computer, pages 71{77, March <lb/>1996. <lb/>7] I. Taha and J. Ghosh. A hybrid intelligent architecture <lb/>for re ning input characterization and domain knowl-<lb/>edge. In Proceedings of World Congress on Neural <lb/>Networks, (WCNN), volume II, pages 284{287, July <lb/>1995. <lb/>8] I. Taha and J. Ghosh. Symbolic interpretation of <lb/>arti cial neural networks. Technical Report TR-97-<lb/>01-106, The Computer and Vision Research Center, <lb/>University of Texas, Austin, 1996. (Available from <lb/>URL http://www.lans.ece.utexas.edu), to IEEE Trans-<lb/>actions on Knowledge and Data Engineering, Septem-<lb/>ber, 1996. <lb/>9] I. Taha and J. Ghosh. Three techniques for extracting <lb/>rules from feedforward networks. In Intelligent Engi-<lb/>neering Systems Through Arti cial Neural Networks, <lb/>volume 6. ASME Press, November 1996. <lb/>10] G. Towell and J. Shavlik. The extraction of re ned <lb/>rules from knowledge-based neural networks. Machine <lb/>Learning, 13(1):71{101, 1993. <lb/></listBibl>

			<body>Table 1. Rules extracted from network \Cancer-Bin&quot; by BIO-RE technique. <lb/>Cancer <lb/>Performance Measures <lb/>No. <lb/>Rule Body <lb/>Class <lb/>Sound-Comple-False <lb/>ness <lb/>teness Alarm <lb/>R 1 <lb/>If X 3 3:0 and X 7 3:3 <lb/>and X 8 2:7 and X 9 1:5 Benign <lb/>391/444 391/444 2/683 <lb/>R 2 <lb/>If X 1 4:1 and X 3 3:0 <lb/>and X 7 3:3 and X 9 1:5 Benign <lb/>317/444 8/444 0/683 <lb/>R 3 <lb/>If X 1 4:1 and X 3 3:0 <lb/>and X 8 2:7 and X 9 1:5 Benign <lb/>316/444 7/444 0/683 <lb/>R 4 <lb/>If X 1 4:1 and X 3 3:0 <lb/>and X 7 3:3 and X 8 2:7 Benign <lb/>316/444 7/444 0/683 <lb/>R 5 <lb/>If X 1 4:1 and X 7 3:3 <lb/>and X 8 2:7 and X 9 1:5 Benign <lb/>314/444 5/444 0/683 <lb/>R 6 <lb/>If X 1 4:1 and X 3 3:0 Malignant 200/239 199/239 15/683 <lb/>R 7 <lb/>If X 3 3:0 and X 7 3:3 Malignant 187/239 27/239 2/683 <lb/>R 8 <lb/>If X 3 3:0 and X 8 2:7 Malignant 187/239 3/239 0/683 <lb/>R 9 <lb/>If X 1 4:1 and X 7 3:3 Malignant 167/239 7/239 1/683 <lb/>R 10 <lb/>If X 1 4:1 and X 9 1:5 Malignant 100/239 <lb/>1 <lb/>3 <lb/>R 11 <lb/>Default Class <lb/>Benign <lb/>5/444 <lb/>5/444 0/239 <lb/>Total For Benign Rules <lb/>423/444 2/683 <lb/>Total For Malignant Rules <lb/>237/239 21/683 <lb/>Overall Performance% <lb/>96:63% 3:37% <lb/>Table 2. Rules extracted from network \Cancer-Norm&quot; by Partial-RE technique. <lb/>Cancer <lb/>Performance Measures <lb/>No. <lb/>Rule Body <lb/>Class <lb/>CF Sound-Comple-False <lb/>ness <lb/>teness Alarm <lb/>R 1 If X 2 3:0 and X 3 3:0 and X 7 3:3 Benign 0.99 412/444 412/444 6/683 <lb/>R 2 If X 1 4:1 and X 3 3:0 and X 7 3:3 Benign 0.99 324/444 1/444 0/683 <lb/>R 3 <lb/>If X 2 3:0 and X 3 3:0 <lb/>Malignant 0.99 222/239 219/239 15/683 <lb/>R 4 If X 1 4:1 and X 7 3:3 and X 8 2:7 Malignant 0.99 137/239 8/239 0/683 <lb/>R 5 If X 1 4:1 and X 2 3:0 and X 7 3:3 Benign 0.84 327/444 4/444 0/683 <lb/>R 6 <lb/>If X 1 4:1 and X 2 3:0 <lb/>Malignant 0.99 198/239 2/239 0/683 <lb/>R 7 If X 1 4:1 and X 2 3:0 and X 3 3:0 Benign 0.84 333/444 9/444 1/683 <lb/>R 8 <lb/>If X 1 4:1 and X 3 3:0 <lb/>Malignant 0.99 200/239 3/239 2/683 <lb/>R 9 If X 2 3:0 and X 3 3:0 and X 8 2:7 Benign 0.99 409/444 1/444 0/683 <lb/>Total For Benign Rules <lb/>427/444 7/683 <lb/>Total For Malignant Rules <lb/>232/239 17/683 <lb/>Overall Performance% <lb/>96:49% 3:51% <lb/>Table 3. Rules extracted from network \Cancer-Cont&quot; by Full-RE technique. <lb/>Cancer <lb/>Performance Measures <lb/>No. <lb/>Rule Body <lb/>Class <lb/>CF Sound-Comple-False <lb/>ness <lb/>teness Alarm <lb/>R 1 If X 1 &lt; 8 and X 3 &lt; 3 Benign 0.96 394/444 394/444 5/683 <lb/>R 2 If X 2 2 and X 7 3 Malignant 0.83 227/239 223/239 18/683 <lb/>R 3 If X 1 &lt; 8 and X 7 &lt; 3 Benign 0.75 300/444 27/444 1/683 <lb/>R 4 <lb/>If X 1 8 <lb/>Malignant 0.89 123/239 9/239 1/683 <lb/>R 5 If X 1 &lt; 8 and X 2 &lt; 2 Benign 0.79 369/444 4/444 1/683 <lb/>Total For Benign Rules <lb/>425/444 7/683 <lb/>Total For Malignant Rules <lb/>232/239 19/683 <lb/>Overall Performance% <lb/>96:19% 3:80% <lb/>Table 4. Performance comparison between the sets of extracted rules and their corresponding trained <lb/>networks for the breast-cancer problem. <lb/>Neural Network <lb/>Extracted Rules <lb/>ratio % match <lb/>ratio % match <lb/>Binarized <lb/>Training 333/341 97.65 <lb/>331/341 97.07 <lb/>Network <lb/>Testing 317/342 92.69 <lb/>329/342 96.20 <lb/>(Cancer-Bin) Overall 650/683 95.17 <lb/>660/683 96.63 <lb/>Normalized Training 329/341 96.48 <lb/>331/341 97.07 <lb/>Network <lb/>Testing 325/342 95.03 <lb/>328/342 95.91 <lb/>(Cancer-Norm) Overall 654/683 95.75 <lb/>659/683 96.49 <lb/>Continuous Training 334/341 97.95 <lb/>330/341 96.77 <lb/>Network <lb/>Testing 331/342 96.78 <lb/>327/342 95.61 <lb/>(Cancer-Cont) Overall 665/683 97.36 <lb/>657/683 96.19 <lb/>Table 5. Overall performance of HIA after applying the integration mechanism using the breast-cancer <lb/>database. <lb/>#both #both #disagreed <lb/>#correct <lb/>Overall Performance <lb/>correct wrong <lb/>on <lb/>decisions on <lb/>ratio % correct <lb/>mismatches <lb/>Cancer-Bin <lb/>647 <lb/>14 <lb/>22 <lb/>20/22 <lb/>667/683 <lb/>97.77 <lb/>Cancer-Norm <lb/>647 <lb/>16 <lb/>20 <lb/>17/20 <lb/>664/683 <lb/>97.22 <lb/>Cancer-Cont <lb/>653 <lb/>16 <lb/>14 <lb/>14/14 <lb/>667/683 <lb/>97.77 </body>


	</text>
</tei>
