<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>SLAC-PUB-16069 <lb/>Work supported in part by US Department of Energy under contract DE-AC02-76SF00515. <lb/>SLAC National Accelerator Laboratory, Stanford University, Stanford, California 94039, USA <lb/>High Energy Colliding Beams; What Is Their Future? 1 <lb/>Burton Richter <lb/>Stanford University and SLAC National Accelerator Laboratory <lb/>brichter@slac.stanford.edu <lb/>Abstract <lb/>The success of the first few years of LHC operations at CERN, and the expectation of <lb/>more to come as the LHC&apos;s performance improves, are already leading to discussions <lb/>of what should be next for both proton-proton and electron-positron colliders. In this <lb/>discussion I see too much theoretical desperation caused by the so far unsuccessful <lb/>hunt for what is beyond the Standard Model, and too little of the necessary interaction of <lb/>the accelerator, experimenter, and theory communities necessary for a scientific and <lb/>engineering success. Here, I give my impressions of the problem, its possible solution, <lb/>and what is needed to have both a scientifically productive and financially viable future. <lb/></front>

			<body>1: Introduction <lb/>I have been asked to introduce this issue on what might be beyond the frontier <lb/>colliding-beam machines of today; the proton-proton Large Hadron Collider (LHC) <lb/>currently operating at CERN; and the electron-positron International Linear Collider <lb/>(ILC), which though fully designed, is still only a dream in the minds of its proponents. <lb/>I think I may be the last still around of the first generation of pioneers that brought <lb/>colliding beam machines to reality. I have been personally involved in building and <lb/>using such machines since 1957 when I became part of the very small group that <lb/>started to build the first of the colliders 2 . While the decisions on what to do next belong <lb/>to the younger generation, the perspective of one of the old guys might be useful. I see <lb/>too little effort going into long range accelerator R&amp;D, and too little interaction of the <lb/>three communities needed to choose the next step, the theorists, the experimenters, <lb/>and the accelerator people. Without some transformational developments to reduce the <lb/>cost of the machines of the future, there is a danger that we will price ourselves out of <lb/>the market. <lb/></body>

			<front>1 <lb/>To be published in Review of Accel. Sci. &amp; Technology, Vol. 7 (2014), Editors Alexander W. Chao and <lb/>Weiren Chou,, World Scientific Pub., Singapore. <lb/></front>

			<note place="footnote">2 If you are interested in the history of the development of the colliders from the beginning to today, in <lb/>1992 I wrote a long article titled &quot;The Rise of Colliding Beams.&quot; It can be found in &quot;The Rise of the <lb/>Standard Model&quot;, Lillian Hoddeson et al., Cambridge University Press, 1997, and as a SLAC publication <lb/>SLAC-PUB-6023. It has detail on who really did what. <lb/></note>

			<page>2 <lb/></page>

			<body>In section 2 below I will give a short review of how we have come to where we <lb/>are in the 58 years since the first discussion of the potential of colliding beam machines. <lb/>The development of that technology is what has allowed the field to reach the energies <lb/>of today&apos;s frontier facilities. <lb/>In section 3 I go on to discuss the next step in proton colliders and find that there <lb/>seems to be a great danger of setting the luminosity of the 100 TeV example being <lb/>discussed today too low and so severely limiting its discovery potential. <lb/>Section 4 takes a look at the future of electron colliders. The linear collider <lb/>technology being proposed for the next facility seems to me to allow for the construction <lb/>of the first and last machine of this type because, while the cost of the first seems <lb/>affordable, the cost of the next which has to have much higher energy will not be using <lb/>current technology. <lb/>Section 5 gives some personal thoughts on what needs to be done if accelerator <lb/>based particle physics is to have a long term future. <lb/>Section 6 concludes the paper with some personal thoughts on theory, <lb/>experiment, science politics, and problems looming for large international <lb/>collaborations. <lb/>2: A Short Look Back <lb/>The beginning of colliders came in a paper by D.W. Kerst and his Midwest <lb/>University Research Association (MURA) group, published in 1956 3 followed by a <lb/>longer paper in the Proceedings of the CERN Accelerator Conference 4 , also in 1956. At <lb/>the time the highest energy proton accelerators were the 3.3-GeV Cosmotron at <lb/>Brookhaven National Laboratory, and the 6.2-GeV Bevatron at the Lawrence Berkeley <lb/>National Laboratory. It seemed as if new baryon and meson states were being <lb/>discovered almost as fast as data could be collected and analyzed, and both <lb/>Brookhaven and CERN were discussing building accelerators with an energy of about <lb/>25 GeV. Kerst wrote that it might be possible to go far beyond those energies with a <lb/>new accelerator technology, pointing out that making the beams from two 21.6-GeV <lb/>accelerators collide head-on would give a center-of-mass energy equivalent to that of <lb/>one accelerator of 1000 GeV colliding with a proton fixed target. <lb/></body>

			<listBibl>3 D.W. Kerst et al., Phys. Rev. 102, 590 (1956) <lb/>4 CERN Symposium on High Energy Accelerators and Pion Physics (Geneva: CERN, 1956), <lb/>p.36. <lb/></listBibl>

			<page>3 <lb/></page>

			<body>At this same symposium a new actor came on stage, G. K. O&apos;Neill of Princeton <lb/>University. He too was interested in proton-proton collisions at very high center-of-mass <lb/>energies, and he introduced the notion of the accelerator-storage ring complex. Beams <lb/>would be accelerated to some high energy in a synchrotron and then transferred into <lb/>two storage rings with a common straight section where the beams would interact. <lb/>Since the beams at high energy need much less space in an accelerator vacuum <lb/>chamber than is required for beams at injection, the high-energy storage rings would <lb/>have smaller-cross-section magnets and vacuum chambers, thus adding little to the <lb/>cost of the complex, but at the same time enormously increasing the scientific potential. <lb/>He also observed, &quot;The use of storage rings on electron synchrotrons in the GeV range <lb/>would allow the measurement of the electron-electron interaction at center-of-mass <lb/>energies of about 100 times as great as are now available. The natural beam damping <lb/>in such machines might make beam capture somewhat easier than in the case of <lb/>protons.&quot; That observation had a profound effect on O&apos;Neill&apos;s career and mine, as well <lb/>as on particle physics. <lb/>How to realize a colliding-beam machine was the question. The MURA FFAG <lb/>accelerators discussed by Kerst were enormously complex, and none had ever been <lb/>built at that time (only one small one has been built since). There was considerable <lb/>concern about whether FFAG machines would actually work as well as their proponents <lb/>claimed. <lb/>At the same time, the problem of injection into the proton-synchrotron storage-<lb/>ring complex that O&apos;Neill and others discussed was thought to be very difficult. Indeed, <lb/>O&apos;Neill&apos;s original idea of using a scattering foil for injection was soon proved to be <lb/>impossible. On the other hand, injection and beam stacking in an electron storage ring <lb/>looked easy because of synchrotron-radiation damping. An electron beam could be <lb/>injected off-axis into a storage ring and would perform betatron oscillations around the <lb/>equilibrium orbit. These oscillations would decrease exponentially over time in a <lb/>properly designed magnet lattice because of the emission of synchrotron radiation while <lb/>the energy loss is compensated with RF acceleration. When these oscillations had <lb/>damped sufficiently, another bunch could be injected into the storage ring and would <lb/>damp down on top of the first one. Since phase-space was not conserved in the <lb/>presence of radiation, there was, in principle, no stacking problem. Thus, the collider <lb/>story began with electron machines because they were much easier to inject into, were <lb/>smaller, and were much less expensive than the proton machines that were the dream <lb/>of those who first began the move toward colliders. <lb/>In 1957 O&apos;Neill visited Stanford&apos;s High Energy Physics Lab (HEPL) to discuss the <lb/>possibility of building an electron-electron collider with W.K.H. Panofsky, its Director. <lb/>O&apos;Neill wanted to test the technology and to test Quantum Electrodynamics (QED) to a <lb/>level far beyond anything that had been done before. Panofsky was very interested and <lb/></body>

			<page>4 <lb/></page>

			<body>began helping to put together a Princeton-Stanford collaboration to make a real <lb/>proposal for such a facility, which is where I came in. I was a post-doc interested in <lb/>testing quantum electrodynamics and happily joined the adventure to bring to life a new <lb/>tool for high-energy physics research, and to allow a test of QED to a point far beyond <lb/>what had been done earlier by me. <lb/>The group became O&apos;Neill; Bernard Gittelman, a Princeton post doc; W.C. (Carl) <lb/>Barber, a Stanford senior staff member; and me. Panofsky convinced the Office of <lb/>Naval Research to fund the project and in December of 1958 the construction of the first <lb/>colliding beam machine (CBX) began, launching the collider era. The first beam was <lb/>stored on March 28, 1962; the first physics results testing QED were presented in 1963; <lb/>and the facility was finally shut down in 1968. <lb/>It took longer for the proton colliders to begin. The information on the beam-<lb/>beam interaction from the electron collider was important, but even more important was <lb/>a great deal of theoretical and experimental work on how to inject and stack beams in a <lb/>proton ring. CERN began what was to be the ISR in 1966 and first collisions began in <lb/>1971. The colliding-beam era was fully launched. <lb/>In the nearly 60 years since the first suggestions on the potential of colliding <lb/>beams were made, enormous progress has occurred in both accelerator center-of-mass <lb/>energies (to 200 GeV in the e+e-system and 8 TeV in the p-p system), and in our <lb/>understanding of matter, energy and the structure of our universe. Colliders have <lb/>played a major role, but fixed-target machines have been important too (neutrino <lb/>physics, for example), as have ground-and space-based instruments (dark matter and <lb/>dark energy, for example). <lb/>We have our Standard Model which explains a lot, but not all. It is like a beautiful <lb/>manuscript with Post-it notes stuck on pages here and there. CP violation is allowed, <lb/>but not required; neutrino masses and oscillations are allowed but not explained; dark <lb/>matter is allowed, but what it is is a mystery; the particle-antiparticle asymmetry of the <lb/>universe is allowed but not explained; etc. Taking down some of those notes and <lb/>gaining a deeper understanding of our universe is the object of higher-energy colliding-<lb/>beam systems. <lb/>In Asia, Europe and the U.S. scientists and funding agencies have been trying to <lb/>set priorities on next steps for what has come to be called the energy, intensity, and <lb/>cosmic frontiers. Progress on all is needed to deepen our understanding. This journal <lb/>issue is about proceeding on the energy frontier, but it is important not to forget that with <lb/>what we know today, there are other areas of importance as well. The energy frontier <lb/>will not, and should not, get all the money. <lb/></body>

			<page>5 <lb/></page>

			<body>3: A Look Ahead to Beyond LHC-2014 <lb/>In the early 1980s the U.S. was beginning to develop the ideas for what became <lb/>known as the Superconducting Super Collider (SSC), a 40-TeV center-of-mass (CM) <lb/>superconducting p-p colliding-beam machine. The first mention I know of what became <lb/>the LHC is in an internal report from the LEP group (LEP note 440, 1983) by Stephen <lb/>Myers and Wolfgang Schnell about putting a superconducting p-p collider in the LEP <lb/>tunnel after the LEP e+e-collider had done its job. The LEP tunnel was only roughly <lb/>one-third the circumference of the SSC tunnel, but that could be partly made up for with <lb/>higher field superconducting magnets (eventually the LHC magnets ran at about 30% <lb/>higher field than the design field of the SSC). <lb/>The Myers/Schnell paper started informal discussions at CERN that became <lb/>more serious when the SSC was initially approved by the U.S. Congress, and turned <lb/>into a major design effort when the SSC was canceled by the U.S. Congress in 1993. 5 <lb/>Construction of the LHC began in 1998 and first operational trials began in 2008. <lb/>Unfortunately, a magnet short and massive quench of the superconducting systems <lb/>damaged the machine and the real start of operations at about only half the design <lb/>energy began in early 2010. In early 2015 the LHC will begin operations again at about <lb/>13 TeV compared to the 8-TeV operations before its recent shutdown for upgrading. <lb/>The LHC itself is an evolving machine. Its energy at its restart next year will be <lb/>13 TeV, slowly creeping up to its design energy of 14 TeV. It will shut down in 2018 for <lb/>some upgrades to detectors, and shut down again in 2022 to increase the luminosity. It <lb/>is this high-luminosity version (HL-LHC) that has to be compared to the potential of new <lb/>facilities. There has been some talk of doubling the energy of the LHC (HE-LHC) by <lb/>replacing the 8-tesla magnets of the present machine with 16-tesla magnets, which <lb/>would be relatively easy compared to the even more talked about bolder step to 100 <lb/>TeV for the next project. It is not clear to me why 30-TeV LHC excites so little interest, <lb/>but that is the case. <lb/>A large fraction of the 100 TeV talk (wishes?) comes from the theoretical <lb/>community which is disappointed at only finding the Higgs boson at LHC and is looking <lb/>for something that will be real evidence for what is actually beyond the standard model. <lb/>Regrettably, there has been little talk so far among the three communities, <lb/>experimenters, theorists, and accelerator scientists, on what constraints on the next <lb/>generation are imposed by the requirement that the experiments actually produce <lb/>analyzable data. I will give an example of an LHC-like 100-TeV machine (LHC-100) to <lb/>show some of the design problems. <lb/></body>

			<note place="footnote">5 <lb/>If you want to know more about the rise and fall of the SSC see Michael Riordan, Lillian Hoddeson and <lb/>Adrienne W. Kolb, Tunnel Visions: The Rise and Fall of the Superconducting Super Collider, (University <lb/>of Chicago Press, to be published in 2015) <lb/></note>

			<page>6 <lb/></page>

			<body>The most important choice for a new, higher energy collider is its luminosity, <lb/>which determines its discovery potential. If a new facility is to have the same potential <lb/>for discovery of any kind of new particles as had the old one, the new luminosity <lb/>required is very roughly proportional to the square of the energy because cross sections <lb/>typically drop as E -2 . A seven-fold increase in energy from that of HL-LHC to a 100-TeV <lb/>collider therefore requires a fifty-fold increase in luminosity. If the luminosity is not <lb/>increased, save money by building a lower-energy machine where the discovery <lb/>potential matches the luminosity. <lb/>All the studies that I have seen so far on the 100TeV collider have much lower <lb/>luminosities, more like that of HL-LHC, and limit themselves to events per beam <lb/>crossing about the same as that machine will have. Even at only the luminosity of HL-<lb/>LHC there are some new things that can be found at the new very high-energy facility. <lb/>Examples are new gauge bosons like a Z&apos;, or Gluino where particles with masses up to <lb/>30-TeV for the weak boson, or 10-TeV -15-TeV for the Gluino could be found (if any <lb/>were there). Of course, restricting the luminosity to what will be achieved at HL-LHC <lb/>gives the new machine a limited vision, and will (and should) seriously lower the <lb/>likelihood that it will be funded 6 . <lb/>Table 1 below gives key parameters for my examples scaling up to both high <lb/>luminosity and energy from HL-LHC (24 July, 2014 parameter list) with two versions of <lb/>the LHC-100, one with 8-tesla magnets and the other with 16-tesla magnets (16 tesla <lb/>can be gotten with niobium-tin conductor or with high-Tc magnets and that choice has <lb/>big implications for the cryogenic system that I will not go into here). The parameters <lb/>for the high-energy machines are workable, but not optimized. An optimized machine <lb/>will surely be different, and that optimization will be needed to make the machine more <lb/>affordable and the experiments more likely to succeed. <lb/>All other parameters of importance to the accelerator and experimenter <lb/>communities are linked to the luminosity number. I have kept the bunch spacing of the <lb/>LHC. Double the spacing and the number of particles per bunch has to go up, as does <lb/>the number of interactions per beam crossing, while the beam current and the <lb/>synchrotron radiation power goes down. Halve the bunch spacing and the opposite <lb/>occurs; particles per bunch and interactions per crossing go down while beam current <lb/>and synchrotron radiation go up. <lb/>I use the same value of β* as in HL-LHC, though it may be difficult to make it that <lb/>small at the much higher energy. I also assume crab crossing is used. Also, using an <lb/>injector chain similar to the LHC&apos;s, seven times as many bunches have to be stacked, <lb/></body>

			<note place="footnote">6 There are some advocating scaling the luminosity of a 100-TeV machine from the design luminosity of <lb/>the SSC. I would remind those so advocating, that the SSC was not built, the LHC was. <lb/></note>

			<page>7 <lb/></page>

			<body>though adiabatic damping going to higher energy matches the increase in phase space <lb/>giving the same size beam. <lb/>Table 1: Examples of 100 TeV colliders scaled from HL-LHC <lb/>The events per beam crossing and per unit length along the collision region are <lb/>going to make serious problems for the detectors. Having 50 times the events per <lb/>beam crossing will require something new in detectors. Having the mean spacing <lb/>between vertices go from 1.3 mm to 2.5 microns will probably also require something <lb/>new in detector technology. Getting the experimenters involved in setting parameters is <lb/>necessary in building something that can really do the physics 7 . <lb/>I understand that CERN is setting up such a group. It is about time someone did <lb/>so, and a serious discussion on its physics reach should be the first order of business. <lb/>Detector R&amp;D will be as important in the long run as machine R&amp;D. <lb/>4: A Look Ahead to the ILC and Beyond <lb/>Though the first storage ring electron collider started operations about ten years <lb/>before the first proton collider, the proton machines have gone far beyond their electron <lb/>cousins in energy. The reason is the same one that made electron colliders easier to <lb/>build initially, synchrotron radiation. While synchrotron radiation made it easy to inject <lb/>into an electron collider, it made it much more expensive to go to very high energy. The <lb/>problem was that the minimum-cost machine increases in circumference and cost as <lb/>the square of the energy. 8 I used to use a slide based on a scale-up of the 100-GeV <lb/></body>

			<note place="footnote">7 The summary talk by Weiren Chou at the Fermilab Hadron Collider Workshop, August 25-28, 2014, is <lb/>an excellent introduction to the issues; see <lb/>https://indico.fnal.gov/conferenceOtherViews.py?view=standard&amp;confId=7864 <lb/></note>

			<listBibl>8 Burton Richter, Very High Energy Electron-Positron Colliding Beams for the Study of Weak Interactions, <lb/>Nuclear instruments and Methods, 136 (I976) 47-60 <lb/></listBibl>

			<body>Parameter <lb/>HL-LHC LHC-100 8T <lb/>LHC-100 16T <lb/>Beam Energy (TeV) <lb/>7 <lb/>100 <lb/>100 <lb/>Circumference (km) <lb/>27 <lb/>190 <lb/>95 <lb/>L (cm -2 sec -1 ) <lb/>5x10 34 <lb/>2.5x10 36 <lb/>2.5x10 36 <lb/>Bunch Spacing (ns) <lb/>25 <lb/>25 <lb/>25 <lb/>Beam Current (Amp) <lb/>1.09 <lb/>7.7 <lb/>7.7 <lb/>Synchrotron Rad Power (Mw) 0.0075 <lb/>2.6 <lb/>10.3 <lb/>β * (cm) <lb/>15 <lb/>15 <lb/>15 <lb/>ε n (micron) <lb/>2.5 <lb/>2.5 <lb/>2.5 <lb/>Particles per Bunch <lb/>2.2x10 11 <lb/>1.5x10 12 <lb/>1.5x10 12 <lb/>Events per bunch collision <lb/>140 <lb/>7000 <lb/>7000 <lb/>Events per mm <lb/>1.3 <lb/>0.0025 <lb/>0.0025 <lb/></body>

			<page>8 <lb/></page>

			<body>version of LEP to 1-TeV energy that I called LEP 1,000. It had a 2700-km <lb/>circumference with one interaction region in Geneva and the other in London. <lb/>I started thinking about an alternative, and in 1978 at an accelerator conference <lb/>at Fermilab I found that A. N. Skrinsky (Novosibirsk), M. Tigner (Cornell), and I had <lb/>been thinking about the same issue. 9 We worked out the limitations (primarily from the <lb/>very strong beam-beam interaction that came to be called beamstrahlung), and when I <lb/>returned to SLAC began the design of a system to test the concept that used only one <lb/>existing linac and so would be affordable. Both electron and positron bunches would be <lb/>accelerated in the same pulse of the SLAC linac, and brought them into collision using a <lb/>system of magnets. The entire thing was shaped something like a tennis racquet in <lb/>which both electron and positron bunches would be accelerated in the same linac pulse, <lb/>split at the end of the linac and send each on its way to the collision point around the <lb/>head of the racquet. An imaginative Department of Energy invented a budget category <lb/>they called an R&amp;D construction project to fund it. First collisions were in 1989 and by <lb/>1992 with longitudinally polarized beams the SLAC Linear Collider (SLC) was producing <lb/>real physics, and the accelerator physics community began to think about big linear <lb/>colliders. <lb/>In 1993 the SSC project was canceled by the U.S. Congress. One of the <lb/>reasons was that no other country had agreed to contribute to the construction of the <lb/>project. Hirokata Sugawara (Director of KEK), Bjorn Wiik (Director of DESY), and I <lb/>(Director of SLAC) discussed how to move linear colliders along in the light of the crash <lb/>of the SSC and concluded that one of the SSC&apos;s problems was that potential <lb/>collaborators were not part of the group that decided on the parameters. We thought <lb/>we might do better if we worked together rather that separately and thereby came up <lb/>with a machine design that had international backing from the very beginning. It would <lb/>not guarantee collaborations, but would eliminate one of the barriers to such that <lb/>affected the SSC. <lb/>This worked for a while, but broke down when it came to deciding on the <lb/>technology for the design study. We had let technologies become associated with <lb/>laboratories so choosing the better option between superconducting and room-<lb/>temperature RF was not as easy as it would have been if each of the three labs had <lb/>been involved with both of the technologies. The same mistake is being made again: <lb/>CERN works on CLIC while the other labs work on superconducting linac systems. <lb/>An international group was created to design and estimate the cost of a <lb/>superconducting linear collider with initial center-of-mass energy of 500 GeV, <lb/></body>

			<listBibl>9 L. E. Augustin et al, Limitation of Performance of e+e-Storage Rings and Linear Colliders Systems at <lb/>High energy, Proceedings of the ICFA Workshop on Possibilities and Limitations of Accelerators and <lb/>Detectors, Fermi National Accelerator Laboratory, 1978 <lb/></listBibl>

			<page>9 <lb/></page>

			<body>expandable to 1 TeV at a later date. The Reference Design Report with cost estimate <lb/>was delivered in 2007. In 2007 U.S. dollars the cost was estimated to be $6.6 billion <lb/>including site specific costs, plus 24 million person hours of labor. Accounting was of <lb/>the kind used in Europe and Japan where only the costs of contracts is normally <lb/>included. U.S. practice includes much more such as R&amp;D before and during <lb/>construction, detectors, escalation, contingency, and all labor including that at the <lb/>laboratories where the work was to be done and managed. Typically this increases <lb/>costs over the accounting method used at the ILC by a factor of 2.5 to 3, resulting in a <lb/>cost estimate of $16 to $20 billion for the project in U.S. terms. This was about the <lb/>inflation-adjusted cost estimate for the SSC when it was canceled in 1993. Further, in <lb/>2008 the financial crash made it even more unlikely that a project that large would be <lb/>funded even as an international effort. Work on the ILC slowed dramatically. <lb/>The discovery of the Higgs boson at the LHC in 2012 raised interest in a <lb/>somewhat lower energy e+e-collider to be used as a kind of Higgs factory to produce <lb/>lots of them through the Z-Higgs channel. The scientific advances would come from <lb/>precision measurement of Higgs branching fractions. The standard model Higgs <lb/>couples to leptons proportional to the square of their masses. Deviations from the <lb/>standard predictions give clues to what might be waiting to be discovered through <lb/>internal loop diagrams if masses are above what can be seen in Higgs decays, or even <lb/>directly if the mass of something new is low enough (not likely since if so, it should have <lb/>been found at the LHC). <lb/>The precision being discussed is at about the 1% level for each of the Higgs <lb/>branching fractions being investigated. This is very difficult because the Z-Higgs <lb/>channel has to be tagged by finding the Z decay and then finding the tau pair decays of <lb/>the higgs, and may be impossible for the more complex bottom and charm quark <lb/>decays of the higgs. The problem will be backgrounds as well as statistics. If the <lb/>theorists really need 1% branching fraction, do not go ahead until the experimenters say <lb/>if it can be done to that precision in the real world of many open channels and only a <lb/>relatively small Higgs production. Also, check the luminosity needed. <lb/>5: The Long Term Future of Accelerator Based Physics <lb/>When I was much younger I was a fan of science fiction books. I have never <lb/>forgotten the start of one, though I don&apos;t remember the name of the book or its author. It <lb/>began by saying that high-energy physics&apos; and optical astronomy&apos;s instruments had <lb/>gotten so expensive that the fields were no longer funded. That is something that we <lb/>need to think about. Once before we were confronted with a cost curve that said we <lb/>could never afford to go to very high energy, and colliding beams were invented and <lb/>saved us from the fate given in my science fiction book. We really need to worry about <lb/>that once more. <lb/></body>

			<page>10 <lb/></page>

			<body>In the proton collider world, building the LHC, its detectors, and the repairs <lb/>required to get it to its design energy will cost something like $10 billion. The next step <lb/>being discussed entirely too casually by some of the theorists would take us to 100 TeV <lb/>(CM) at what should be a luminosity 50 times that of the still non-existing HL-LHC. If <lb/>the cost of the next-generation proton collider is really linear with energy, I doubt that a <lb/>100-TeV machine will ever be funded, and the science fiction story of my youth will be <lb/>the real story of our field. <lb/>I see no well-focused R&amp;D program looking to make the next generation of <lb/>proton colliders more cost effective. I do not understand why there is as yet no program <lb/>underway to try to develop lower cost, high-Tc superconducting magnets done on the <lb/>scale of R.R. Wilson&apos;s efforts at Fermilab to successfully develop the first generation of <lb/>commercially viable superconducting magnet that led to the Tevatron, Hera and LHC. <lb/>The only place that might do it today is CERN. I hope they try. <lb/>I am both more optimistic and more pessimistic about e+e-colliders. More <lb/>optimistic because accelerating gradients of more than 50 GeV per meter (50 TeV per <lb/>kilometer sounds even more exciting) have already been demonstrated in plasma-<lb/>wakefield acceleration and of several GeV per meter in laser acceleration, though both <lb/>have now poor 6-dimensional phase space; more pessimistic because I don&apos;t see a <lb/>push to develop these technologies for use in real machines. <lb/>The e+e-colliders have two advantages over the proton colliders. The cross <lb/>sections of interest are all of comparable orders of magnitude. The background of 10 <lb/>billion or more uninteresting events for each interesting one, the problem of proton <lb/>colliders, does not really exist for the electron colliders. There is a low transverse <lb/>momentum fizz that is confined to small angle, but the interesting events are much <lb/>easier to get at. In addition the equivalent mass reach in the electron colliders only <lb/>requires 10% to 20% of the energy of the proton collider with the same mass reach. <lb/>The 100-TeV p-p collider is matched by a 10-to 20-TeV electron collider. <lb/>My challenge to the electron accelerator community is to produce a cost effective <lb/>system with an acceleration gradient of at least 1-GeV per meter with reasonable <lb/>transverse phase space and an energy spread of no more than 10% to 20%. Because <lb/>of the parton distribution in the proton, the effective energy spread in p-p collisions is <lb/>more like 100%. You have about 15 years to do it since that is the time to when HL-<lb/>LHC will start to operate. <lb/></body>

			<page>11 <lb/></page>

			<body>6: A Few Final Thoughts <lb/>The usual back and forth between theory and experiment; sometimes one <lb/>leading, sometimes the other leading; has stalled. The experiments and theory of the <lb/>1960s and 1970s gave us today&apos;s Standard Model that I characterized earlier as a <lb/>beautiful manuscript with some unfortunate Post-it notes stuck here and there with <lb/>unanswered questions written on them. The last 40 years of effort has not removed <lb/>even one of those Post-it notes. The accelerator builders and the experimenters have <lb/>built ever bigger machines and detectors, while the theorists have kept inventing <lb/>extensions to the model. <lb/>If you have seen the movie Particle Fever about the discovery of the Higgs <lb/>boson, you have heard the theorists saying that the only choices today are between <lb/>Super-symmetry and the Landscape. Don&apos;t believe them. Super-symmetry says that <lb/>every fermion has a boson partner and vice versa. That potentially introduces a huge <lb/>number of new arbitrary constants which does not seem like much progress to me. <lb/>However, in its simpler variants the number of new constants is small and a problem at <lb/>high energy is solved. But, experiments at the LHC already seem to have ruled out the <lb/>simplest variants. <lb/>The Landscape surrenders to perpetual ignorance. It says that our universe is <lb/>only one of a near infinity of disconnected universes, each with its own random <lb/>collection of force strengths and constants, and we can never observe or communicate <lb/>with the others. We can never go further in understanding because there is no natural <lb/>law that relates the different universes. The old dream of deriving everything from one <lb/>constant and one equation is dead. There are two problems with the landscape idea. <lb/>The first is a logic one. You cannot prove a negative, so you cannot say that there is no <lb/>more to learn. The second is practical. If it is all random there is no point in funding <lb/>theorists, experimenters, or accelerator builders. We don&apos;t have to wait until we are <lb/>priced out of the market, there is no reason to go on. <lb/>There is a problem here that is new, caused by the ever-increasing mathematical <lb/>complexity of today&apos;s theory. When I received my PhD in the 1950s it was possible for <lb/>an experimenter to know enough theory to do her/his own calculations and to <lb/>understand much of what the theorists were doing, thereby being able to choose what <lb/>was most important to work on. Today it is nearly impossible for an experimenter to do <lb/>what many of yesterday&apos;s experimenters could do, build apparatus while doing their own <lb/>calculations on the significance of what they were working on. Nonetheless, it is <lb/>necessary for experimenters and accelerator physicists to have some understanding of <lb/>where theory is, and where it is going. Not to do so makes most of us nothing but <lb/>technicians for the theorists. Perhaps only the theory phenomenologists should be <lb/>allowed to publish in general readership journals or to comment in movies. <lb/></body>

			<page>12 <lb/></page>

			<body>The ever rising cost of the ITER fusion-energy project has increased skepticism <lb/>that any big international project can be brought in on budget. I have had to deal with <lb/>more than one official skeptic in Washington on the potential of large-scale international <lb/>collaborations. I have pointed to the LHC as a counter-example. ITER was to be built <lb/>with the nations involved contributing components that each country committed to build. <lb/>Its central management had no money and no authority, so could not bring additional <lb/>resources to bear when one important part of the project fell behind, and could do <lb/>nothing to move components from one country to another to solve production problems. <lb/>The result has been a more than tripling of the ITER cost estimate and a delay of more <lb/>than a decade in completion. <lb/>At LHC almost all the funds were managed centrally with the understanding <lb/>(never formally stated) that each country that contributed to the project would see <lb/>contracts roughly in proportion to its contribution. The LHC model worked while the <lb/>ITER model has not. When we think of a next very large international accelerator <lb/>project we have to think of a management system that will not result in an ITER like <lb/>financial and scheduling disaster. <lb/>I end with best wishes to the younger generation. May you make real progress <lb/>to the one constant, one equation solution to the question that brought me to HEP: how <lb/>does the universe work? </body>


	</text>
</tei>
