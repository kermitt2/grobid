<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>OPTIMAL PROCESSOR ASSIGNMENT FOR PARALLEL <lb/>DATABASE DESIGN <lb/>SHAHRAM GHANDEHARIZADEH y , ROBERT R. MEYER z , GARY L. SCHULTZ z AND <lb/>JONATHAN YACKEL z <lb/>Abstract. The computing time bene ts of parallelism in database systems (achieved by us-<lb/>ing multiple processors to execute a query) must be weighed against communication, startup, and <lb/>termination overhead costs that increase as a function of the number of processors used. We con-<lb/>sider problems of minimizing overhead subject to allocating data among the processors according <lb/>to speci ed loads. We present lower bounds for these combinatorial problems and demonstrate how <lb/>processors may be optimally assigned for some problem classes. <lb/></front>

			<body>1. Introduction. In highly-parallel database machines (e.g., Gamma 2], Bubba <lb/>1], Non-Stop SQL 12], XPRS 11] and Volcano 6]) relations are partitioned across <lb/>multiple processors. (Livny et al 9] and Ries and Epstein 10] introduced the related <lb/>concept of \horizontal&quot; partitioning.) This allows each processor to execute a portion <lb/>of a query in parallel with the other processors, resulting in a lower response time <lb/>for the query. However, there is communication overhead associated with initiating <lb/>and terminating a query on multiple processors, and this overhead increases as a <lb/>function of the number of processors used to execute a query 1 . In order to minimize <lb/>overhead while balancing the workload among the processors, Multi-Attribute GrId <lb/>deClustering (MAGIC) introduced by Ghandeharizadeh 3] partitions a relation by <lb/>assigning ranges of several attribute values to each processor in the system. To il-<lb/>lustrate MAGIC consider the partitioning of the Employee relation EMP in gure 1. <lb/>For parallel computation, MAGIC partitions the EMP relation by establishing ranges <lb/>of Salary and Age attribute values. Each cell of this grid corresponds to a fragment <lb/></body>

			<front>This research was partially supported by the Air Force O ce of Scienti c Research under grant <lb/>89-0410, the Defense Advanced Research Projects Agency under contract N00039-86-C-0578, and <lb/>by the National Science Foundation under grants CCR-8907671 and DCR-8512862. <lb/>y Computer Science Department, University of Southern California, Los Angeles, California <lb/>90089-0782. <lb/>z Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706. <lb/></front>

			<note place="footnote">1 This overhead is primarily in the form of additional messages to control the execution of the <lb/>query on additional processors and, in the Gamma database machine 2], increases linearly with the <lb/>number of employed processors. <lb/></note>

			<page>1 <lb/></page>

			<body>Age <lb/>in <lb/>Years <lb/>Salary in $K <lb/>0-20 20-50 &gt; 50 <lb/>0-25 1 <lb/>1 <lb/>2 <lb/>26-50 1 <lb/>3 <lb/>3 <lb/>&gt; 50 2 <lb/>3 <lb/>2 <lb/>Fig. 1. Processor assignment for the EMP relation <lb/>of the relation and must be assigned to some processor. For example, the cell which <lb/>contains records with Salary attribute values that range from 0 to 20 and Age at-<lb/>tribute values that range from 26 to 50 is assigned to processor 1. Given a query on <lb/>either the Age or Salary attribute, the predicate of the query maps to either a row <lb/>or a column (termed a \slice&quot;) of the grid and the corresponding processors are used <lb/>to execute it. Note that, for the assignment depicted by gure 1, every processor is <lb/>assigned three cells, and every query requires two processors. <lb/>Although we concentrate on the limiting case in which overhead is minimized, <lb/>the optimal processor assignments that we obtain below have properties that suggest <lb/>that they may also be good approximations to assignments that would minimize other <lb/>response time functions. For example, suppose the response time r for an average <lb/>query as a function of the number of processors used by the query is modeled by <lb/>r( ) = O + Q= , where O is the overhead per processor and Q is the processing <lb/>time for a query on a single processor, i.e. the overhead increases proportionally with <lb/>the number of processors, while the processing time is inversely proportional to the <lb/>number of processors 2 . In the absence of any constraints, r is minimized when the <lb/>number of processors per query is = <lb/>q <lb/>Q=O. We shall see in (x3) that for our <lb/>version of the problem, which in some sense minimizes the number of processors used <lb/>per query (subject to a load balancing constraint), optimal assignments nevertheless <lb/>have <lb/>p <lb/>N processors assigned to each query, where N is the number of processors <lb/>in the system. If <lb/>q <lb/>Q=O <lb/>p <lb/>N , which is the case if the communication overhead <lb/>for using all N processors dominates the processing time for a single query, then <lb/>our optimal solution comes as close as possible (among assignments that balance <lb/>the workload among processors) to the unconstrained minimum of the alternative <lb/>objective r. <lb/>2. Basic Mathematical Problem Statement. Suppose that we wish to as-<lb/>sign the cells of a D-dimensional grid to N processors, and that the size of the <lb/>grid is M 1 M 2 . . . M D (i.e., the dth attribute is partitioned into M d ranges). <lb/>Let V := Q <lb/>d M d denote the number of cells (volume) of the grid. A \slice&quot; is a <lb/>(D ? 1-dimensional) subgrid containing all the cells with a common value for a given <lb/>coordinate (this corresponds to a query). For example, in an M 1 M 2 grid the slices <lb/>are the M 1 rows and the M 2 columns, and in an M 1 M 2 M 3 grid the slices are <lb/>the M 1 + M 2 + M 3 two-dimensional subgrids. Let S denote the collection of slices. <lb/>Given an assignment of cells to processors and an arbitrary slice s of the grid , let <lb/>s denote the number of distinct processors in the slice s. Given a processor p, let load p <lb/>denote the number of cells assigned to p. The objective function for the optimization <lb/></body>

			<note place="footnote">2 The linear speedup results presented in 2] justify this assertion. <lb/>2 <lb/></note>

			<body>problem that we develop measures total or worst case overhead. total := P <lb/>s2S s <lb/>and max := max s2S s . Note that if each slice has the same frequency of access and <lb/>we are interested in minimizing the average query overhead, then we should minimize <lb/>total . If, on the other hand, we are interested in minimizing the worst case overhead <lb/>incurred by any query, then max should be minimized. The balancing constraints <lb/>are de ned by specifying a load for each processor. (In typical applications, we require <lb/>that the loads are equal or di er by at most 1.) The problem may be formally stated <lb/>as follows: <lb/>Let the following data be given: a dimension D, a number of processors N , the <lb/>cardinality vector M of the partitions in each dimension, and a load for each processor. <lb/>Find an assignment that <lb/>minimizes <lb/>(where is chosen as total or max ) <lb/>s.t. processor p is assigned load p cells (p = 1; 2; . . . ; N ). <lb/>The number of assignments satisfying the balancing constraint is <lb/>V ! <lb/>Q N <lb/>p=1 (loadp!) . <lb/>Complete enumeration of these assignments is not feasible even for relatively small <lb/>problems. For example, given a 5 5 grid, 5 processors, and a load of 5 for each <lb/>processor, there are 623; 360; 743; 125; 120 assignments that satisfy the balancing con-<lb/>straint. <lb/>A similar class of data aggregation problems was studied by Helman 8]. Suppose <lb/>that we replace our notion of \slice&quot; by \arbitrary subset&quot;, i.e., the problem data <lb/>consist of the grid plus a collection of subsets of cells. Then the problem of minimizing <lb/>the total or maximum number of distinct processors in the subsets (with equal loads <lb/>for all processors) corresponds to one of Helman&apos;s K-size aggregation problems, (with <lb/>K = V =N ) which he shows to be NP-complete. <lb/>3. Lower Bounds. In this section we will develop lower bounds on the measures <lb/>total and max . Throughout this section, the following notation is used. p s indicates <lb/>whether processor p appears in slice s: p <lb/>s = 1 if p appears in slice s and p <lb/>s = <lb/>0 otherwise. (Notice that s , the number of distinct processors in slice s, can be <lb/>represented in terms of p <lb/>s : s = P <lb/>p <lb/>p <lb/>s .) p denotes the number of slices containing <lb/>processor p: p = P <lb/>s <lb/>p <lb/>s , and p;d is the number of slices in dimension d containing <lb/>processor p. <lb/>A key relationship in the development of the lower bounds is <lb/>X <lb/>slices in dim d <lb/>s = <lb/>N <lb/>X <lb/>p=1 <lb/>p;d <lb/>(1) <lb/>i.e. the sum over the slices in a particular dimension of the numbers of distinct <lb/>processors in the slices is the same as the sum over all processors of the numbers of <lb/>slices in that dimension in which the processors appear. <lb/>3.1. Lower bounds on total . It is useful to consider the overhead measure <lb/>total from two di erent points of view: total = P <lb/>s s = P <lb/>p p . We rst derive a <lb/>lower bound on p . <lb/>Lemma 3.1. For any processor p (assigned to load p cells), D load <lb/>1 <lb/>D <lb/>p <lb/>p : <lb/>Proof. In each dimension, permute the slices in that dimension so that the slices <lb/>containing p come rst. Notice that the rst p;d slices in dimension d will contain <lb/></body>

			<page>3 <lb/></page>

			<body>p. Notice also that rearranging the order of the slices does not alter any of the p;d . <lb/>Then the p;1 p;2 . . . p;D box in the upper corner of the grid contains all of <lb/>the cells assigned to p. Therefore the \volume&quot; Q D <lb/>d=1 p;d of this box is at least load p , <lb/>i.e., <lb/>load p <lb/>D <lb/>Y <lb/>d=1 <lb/>p;d : <lb/>(2) <lb/>Taking the Dth root of both sides of (2) and applying the arithmetic mean/geometric <lb/>mean inequality (see Hardy et al 7]) we obtain (load p ) <lb/>1 <lb/>D <lb/>Q D <lb/>d=1 p;d <lb/>1 <lb/>D <lb/>1 <lb/>D <lb/>P D <lb/>d=1 p;d = <lb/>p =D , whence D (load p ) <lb/>1 <lb/>D <lb/>p : Since the RHS of the last inequality is integral, we <lb/>may take the ceiling of the LHS. <lb/>Theorem 3.2. <lb/>X <lb/>p <lb/>D load <lb/>1 <lb/>D <lb/>p <lb/>total . <lb/>Proof. Use lemma 3.1 and the fact that total = P <lb/>p p . <lb/>In x4 we give cases in which the lower bound of theorem 3.2 is tight. <lb/>3.2. Lower Bounds on max . At this point we prove a lemma that is used <lb/>in deriving lower bounds on max . Let~ d be the average of the &apos;s for the slices in <lb/>dimension d i.e.~ d := <lb/>X <lb/>slices in dim d <lb/>s =M d . <lb/>Lemma 3.3. <lb/>Q <lb/>d~ d <lb/>N D Q <lb/>p load 1 <lb/>N <lb/>p <lb/>V <lb/>: <lb/>Proof. Using inequality (2) and the arithmetic mean-geometric mean inequality, <lb/>Q <lb/>p load <lb/>1 <lb/>N <lb/>p <lb/>Q <lb/>d <lb/>Q <lb/>p p;d <lb/>1 <lb/>N <lb/>Q <lb/>d <lb/>P <lb/>p <lb/>p;d <lb/>N <lb/>: By equation (1), N D Q <lb/>p load 1 <lb/>N <lb/>p <lb/>V <lb/>Q <lb/>d~ d : <lb/>Theorem 3.4. <lb/> &amp; <lb/> N <lb/>Q <lb/>p load <lb/>1 <lb/>N <lb/>p =V <lb/>1 <lb/>D <lb/>&apos; <lb/>max : <lb/>Proof. Lemma 3.3 gives a bound on a product of averages of &apos;s, so one must <lb/>be at least as big as the ceiling of the Dth root of the bound. <lb/>4. Achieving the Bounds. In order to achieve the lower bound for total , each <lb/>processor must contribute exactly D load <lb/>1 <lb/>D <lb/>p to the objective function. For example, <lb/>this will occur if processor p occupies a hypercubical block of cells with sides of length <lb/>= load <lb/>1 <lb/>D <lb/>p . Note that since interchanging slices in the grid does not a ect the objective <lb/>function, the slices of a processor&apos;s block do not have to be contiguous. For example <lb/>both placements of processor p with a load of 9 below are equivalent and optimal for <lb/>processor p. <lb/>p <lb/>p <lb/>p <lb/>p <lb/>p <lb/>p <lb/>p <lb/>p <lb/>p <lb/>p p p <lb/>p p p <lb/>p p p <lb/></body>

			<page>4 <lb/></page>

			<body>However, the block occupied by a processor may be irregular and still be optimal. <lb/>Consider a 2-dimensional grid where a processor&apos;s load is 7. Then the lower bound <lb/>is <lb/>l 2 <lb/>p <lb/>7 <lb/>m = 6. The following non-square blocks (and obvious variants) have height + <lb/>width = 6, and are therefore optimal: <lb/>If optimal blocks for each processor can be interleaved so as to cover the grid then the <lb/>lower bound on total can be met. One class of problems that have easily obtainable <lb/>optimal solutions are instances in which hyper-rectangular blocks are optimal for <lb/>each processor and the grid can be covered by these blocks. Below we demonstrate <lb/>an optimal assignment for such an instance: a 6 15 grid with 6 processors, each of <lb/>which has a load of 15. (See Ghandeharizadeh et al 5] for a collection of classes for <lb/>which optimal solutions are developed.) <lb/>1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 <lb/>1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 <lb/>1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 <lb/>4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 <lb/>4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 <lb/>4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 <lb/>Another class of problems for which it is possible to construct optimal assignments <lb/>for both total and max is the class of N N grids with N processors, where <lb/>each processor has a load of N . We have developed an algorithm that constructs <lb/>optimal assignments for such problem instances 5]. Figure 1 is an example of such <lb/>an assignment. Note that for any processor the slices in gure 1 may be permuted so <lb/>that the set of cells occupied has the following optimal shape: <lb/>. <lb/>5. Conclusions and Future Work. We have formalized the problem of par-<lb/>titioning data on a parallel database machine in order to minimize overhead. Lower <lb/>bounds on the objective functions have been developed and we have demonstrated <lb/>how the bounds can be attained in many cases. We would like to explore other ap-<lb/>proaches to the data partitioning problem. A branch and bound type of approach in <lb/>a suitably restricted search space seems promising. We also have a nonconvex nonlin-<lb/>ear programming formulation of the problem that suggests other solution techniques. <lb/>Extending the work already done, the square grid assignment algorithm may general-<lb/>ize to more than 2 dimensions. In addition, we would like to deal with more general <lb/>objective functions and load balancing constraints. Finally, it would be interesting <lb/>to consider other applications that t into the task assignment/parallel computing <lb/>framework developed here (see, e.g., Ghandeharizadeh et al 4]). <lb/></body>

			<page>5 <lb/></page>

			<listBibl>REFERENCES <lb/>1] H. Boral, W. Alexander, L. Clay, G. Copeland, S. Danforth, M. Franklin, <lb/>B. Hart, M. Smith, and P. Valduriez, Prototyping Bubba, a highly parallel database <lb/>system, IEEE Transactions on Knowledge and Data Engineering, 2 (1990). <lb/>2] D. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H. Hsiao, and R. Ras-<lb/>mussen, The Gamma database machine project, IEEE Transactions on Knowledge and <lb/>Data Engineering, 2 (1990). <lb/>3] S. Ghandeharizadeh, Physical Database Design in Multiprocessor Systems, PhD thesis, Uni-<lb/>versity of Wisconsin -Madison, 1990. <lb/>4] S. Ghandeharizadeh, L. Ramos, Z. Asad, and W. Qureshi, Object placement in parallel <lb/>hypermedia systems. to appear in the proceedings of the 1991 VLDB Conference. <lb/>5] S. Ghandeharizadeh, G. L. Schultz, R. R. Meyer, and J. Yackel, Optimal balanced <lb/>assignments and a parallel database application, Computer Sciences Technical Report 986, <lb/>University of Wisconsin -Madison, Madison, WI, December 1990. <lb/>6] G. Graefe, Volcano: An extensible and parallel data ow query processing system, Computer <lb/>Science Technical Report, Oregon Graduate Center, Beaverton, OR, June 1989. <lb/>7] G. Hardy, J. Littlewood, and G. Polya, Inequalities, Cambridge, 1959. <lb/>8] P. Helman, A family of NP-complete data aggregation problems, Acta Informatica, 26 (1989), <lb/>pp. 485{499. <lb/>9] M. Livny, S. Khoshafian, and H. Boral, Multi-disk management algorithms, in Pro-<lb/>ceedings of the 1987 ACM SIGMETRICS Int&apos;l Conf. on Measurement and Modeling of <lb/>Computer Systems, May 1987. <lb/>10] D. Ries and R. Epstein, Evaluation of distribution criteria for distributed database systems, <lb/>UCB/ERL Technical Report M78/22, UC Berkeley, May 1987. <lb/>11] M. Stonebraker, D. Patterson, and J. Ousterhout, The design of XPRS, in Proceed-<lb/>ings of the 1988 VLDB Conference, Los Angeles, CA, September 1988. <lb/>12] Tandem Performance Group, A benchmark non-stop SQL on the debit credit transaction, <lb/>in Proceedings of the 1988 SIGMOD Conference, Chicago, IL, June 1988. <lb/></listBibl>

			<page>6 </page>


	</text>
</tei>
