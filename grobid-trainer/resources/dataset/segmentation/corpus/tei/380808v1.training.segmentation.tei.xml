<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>SciPipe -A work ow library for agile development <lb/>of complex and dynamic bioinformatics pipelines <lb/>Samuel Lampa , , * <lb/>Martin Dahl√∂ <lb/>Jonathan Alvarsson <lb/>Ola Spjuth <lb/>Department of Pharmaceutical Biosciences, Uppsala University, Uppsala, Sweden <lb/>Department of Biochemistry and Biophysics, National Bioinformatics Infrastructure Sweden, Science for Life Laboratory, Stockholm <lb/>University, Stockholm, Sweden <lb/>Abstract <lb/>Background: The complex nature of biological data has driven the development of specialized sof ware <lb/>tools. Scienti c work ow management systems simplify the assembly of such tools into pipelines and assist <lb/>with job automation and aids reproducibility of analyses. Many contemporary work ow tools are special-<lb/>ized and not designed for highly complex work ows, such as with nested loops, dynamic scheduling and <lb/>parametrization, which is common in e.g. machine learning. <lb/>Findings: SciPipe is a work ow programming library implemented in the programming language Go, for <lb/>managing complex and dynamic pipelines in bioinformatics, cheminformatics and other elds. SciPipe helps <lb/>in particular with work ow constructs common in machine learning, such as extensive branching, parameter <lb/>sweeps and dynamic scheduling and parametrization of downstream tasks. SciPipe builds on Flow-based <lb/>programming principles to support agile development of work ows based on a library of self-contained, <lb/>re-usable components. It supports running subsets of work ows for improved iterative development, and <lb/>provides a data-centric audit logging feature that saves a full audit trace for every output le of a work ow, <lb/>which can be converted to other formats such as HTML, TeX and PDF on-demand. The utility of SciPipe <lb/>is demonstrated with a machine learning pipeline, a genomics, and a transcriptomics pipeline. <lb/>Conclusions: SciPipe provides a solution for agile development of complex and dynamic pipelines, espe-<lb/>cially in machine leaning, through a exible programming API suitable for scientists used to programming <lb/>or scripting. <lb/></front>

			<body>Findings <lb/>Driven by the highly complex and heterogeneous nature of biological data [ , ], computational biology is <lb/>characterized by an extensive ecosystem of command-line tools, each specialized on one or a few of the many <lb/>aspects of biological data. Because of their specialized nature these tools generally need to be assembled into <lb/>sequences of processing steps, of en called &quot;pipelines&quot;, to produce meaningful results from raw data. Due to <lb/>the increasingly large sizes of biological data sets [ , ], such pipelines of en require integration with High-<lb/>Performance Computing (HPC) infrastructures or cloud computing resources to complete in an acceptable <lb/>time. This has created a need for tools to coordinate the execution of such pipelines in an e cient, robust and <lb/>reproducible manner. This coordination can in principle be done with simple scripts in languages like Bash, <lb/>Python or Perl, but such scripts can quickly become fragile. <lb/>When the number of tasks becomes su ciently large, and the execution time su ciently long, the risk for <lb/>failures during the execution of such scripts increases almost linearly with time, and simple scripts are not a good <lb/>strategy for when large jobs need to be restarted from a failure. They lack the ability to distinguish between <lb/>nished and half-nished les, and can not by default detect if intermediate output les are already created and <lb/>can be re-used to save computing time and resources. These limits with simple scripts calls for a strategy with <lb/>a higher level of automation. This need is addressed by a class of sof ware commonly referred to as &quot;scienti c <lb/>work ow management systems&quot; or simply &quot;work ow tools&quot;. Through a more automated way of handling the <lb/>execution, work ow tools can improve the robustness, reproducibility and understandability of computational <lb/>analyses. In concrete terms, work ow tools of en achieves this by providing means for handling of atomic writes <lb/>(making sure nished and half-nished les can be separated af er a crashed or stopped work ow), caching of <lb/>intermediate results, distribution of tasks to the available computing resources and automatically keeping or <lb/>generating records of exactly what was run, to make analyses reproducible. <lb/>It is widely agreed upon that work ow tools generally make it easier to develop automated, reproducible <lb/>and fault-tolerant pipelines, although many challenges and potential areas for improvement still do exist with <lb/>existing tools [ ]. This has made scienti c work ow systems a highly active area of research, and numerous <lb/>work ow tools have been developed, and many new ones are continuously being developed. <lb/>The work ow tools developed di fer quite widely in terms of how work ows are being de ned, and what <lb/>features are included out-of-the box. This fact probably re ects the fact that di ferent types of work ow tools <lb/>can be suited for di ferent types of users and use cases. Graphical tools like Galaxy [ , , ] and Yabi [ ] provide <lb/>easy to use environments especially well-suited for scientists without scripting experience. Text-based tools <lb/>like Snakemake [ ], Next ow [ ], BPipe [ ], Cuneiform [ ] and Pachyderm [ ] on the other hand, are <lb/>implemented as Domain Speci c Languages (DSLs), that can of en provide a higher level of exibility, at the <lb/>expense of the ease of use of a graphical user interface. They can thus be well suited for &quot;power users&quot; with <lb/>experience in scripting or programming. <lb/>Even more power and exibility can be gained from work ow tools implemented as programming libraries, <lb/>which provide their functionality through a programming API accessed from an existing programming language <lb/>such as Python, Perl or Bash. By implementing the API in an existing language, users get access to the full power <lb/>of the implementation language when writing work ows, as well as the existing tooling around the language. <lb/>One example of a work ow system implemented in this way is Luigi [ ]. <lb/>As reported in [ ], although many users nd important bene ts in using work ow tools, many also ex-<lb/>perience limitations and challenges with existing work ow tools, especially regarding the ability to express <lb/>complex work ow constructs such as branching and iteration, as well as limitations in terms of audit logging <lb/>and reproducibility. Below we will brie y review a few of existing, popular systems, and highlight areas where we <lb/>found that the development of a new approach and tool was desirable for use cases that includes very complex <lb/>work ow constructs. <lb/>Firstly, graphical tools like Galaxy and Yabi, although being easy-to-use even for users without programming <lb/>experience, is of en perceived to be limited in their exibility due to the need to install and run a web server in <lb/>order to use them, which is not always permitted, or practical, on HPC systems. <lb/>Text-based tools implemented as DSLs, such as Snakemake, Next ow, BPipe, Pachyderm and Cuneiform do <lb/>not have this limitation, but have other characteristics which might be problematic for for complex work ows in <lb/>some cases. For example, BPipe does not allow full control of le naming of intermediate output les, which can <lb/>make it harder to nd and access them for manual sanity checking. Snakemake is to the contrary dependent on <lb/>le naming strategies for de ning dependencies, which can in some situations be limiting. Also, Snakemake uses <lb/>a &quot;pull-based&quot; scheduling strategy (the work ow is invoked by asking for a speci c output le, where af er all <lb/>tasks required for reproducing the le will be executed). While this makes it very easy to reproduce speci c les, <lb/>it can make the system hard to use for work ows involving complex constructs such as nested parameter sweeps <lb/>and cross-validation fold generation, where the nal le names might be hard, if at all possible, to foresee. <lb/>Common to Snakemake and BPipe is that they do scheduling and execution of the work ow graph in <lb/>separate stages, meaning that they do not support dynamic scheduling (on-line scheduling during the work ow <lb/>execution) [ ]. Dynamic scheduling is useful both where the number of tasks is unknown before the work ow is <lb/>executed, and where a task needs to be scheduled with a parameter value obtained during the work ow execution. <lb/>An example of the former is reading row by row from a database, splitting a le of unknown size into chunks, or <lb/>processing a continuous stream of data from an external process such as an automated laboratory instrument. <lb/>An example of the latter is training a machine learning model with hyper parameters obtained from a parameter <lb/>optimization step prior to the nal training step. <lb/>Next ow, which has push-based scheduling and supports dynamic scheduling via the data ow paradigm, <lb/>does not have this limitation. It does not, however, support creating a library of re-usable work ow components, <lb/>
			because of its use of data ow variables shared across component de nitions, as this requires processes and the <lb/>work ow dependency graph to be de ned together. <lb/>Pachyderm is a container-based work ow system which uses a JSON and YAML-based DSL to de ne <lb/>pipelines. It has a set of innovative features including a version-controlled data management component with <lb/>Git-like semantics and support for triggering of pipelines based on data updates, among others. These in <lb/>combination can provide some aspects of dynamic scheduling. On the other hand, the more static nature of the <lb/>JSON/YAML-based DSL might not be optimal for really complex setups such as creating loops or branches <lb/>based on parameter values obtained during the execution of the work ow. The requirement of Pachyderm to be <lb/>run on a Kubernetes [ ] cluster can also make it less suitable for some academic environments where ability to <lb/>run pipelines also on traditional HPC clusters is required. On the other hand, because of the easy incorporation <lb/>of existing tools, it is possible to provide such more complex behavior by including a more dynamic work ow <lb/>tool as a work ow step in Pachyderm instead. We thus primarily see Pachyderm as a complement to other <lb/>light-weight work ow systems, rather necessarily than an alternative. <lb/>The usefulness of such an approach where an over-arching frameworks provide primarily an orchestration <lb/>role while calling out to other systems for the actual work ows, is demonstrated by the Arteria project [ ]. <lb/>Arteria builds on the event-based StackStorm framework to allow triggering of external work ows based on any <lb/>type of event, providing a exible automation framework for sequencing core facilities. <lb/>Going back to traditional work ow systems, Cuneiform takes a di ferent approach compared to most <lb/>work ow tools by wrapping shell commands in functions in a exible functional language (described in [ ]), <lb/>which allows leveraging common bene ts in functional programming languages such as side-e fect free functions, <lb/>to de ne work ows. It also leverages the distributed programming capabilities of the Erlang Virtual Machine <lb/>(EVM), to provide automatic distribution of workloads. It is still a new, domain speci c language though, which <lb/>means that tooling and editor support might not be as extensive as for an established programming language. <lb/>Luigi is a work ow library developed by Spotify, which provides a high degree of exibility due to its <lb/>implementation as a programming library, Python. For example, the programming API exposes full control over <lb/>le name generation. Luigi also provides integration with many Big Data systems such as Hadoop and Spark, <lb/>and cloud-centric storage systems like HDFS and S . <lb/>SciLuigi [ ] is a wrapper library for Luigi, previously developed by the authors, which introduces a number <lb/>of bene ts for scienti c work ows by leveraging selected principles from Flow-based programming (FBP) <lb/>(named ports and separate network de nition) to achieve an API that makes iteratively changing the work ow <lb/>connectivity easier than in vanilla Luigi. <lb/>While Luigi and SciLuigi were shown to be a helpful solution for complex work ows in drug discovery, <lb/>they also have a number of limitations for highly complex and dynamic work ows. Firstly, since Python is an <lb/>untyped, interpreted language, certain sof ware bugs are discovered only far into a work ow run, rather than <lb/>while compiling the program. Secondly, the fact that Luigi creates separate processes for each worker, which <lb/>communicate with the central Luigi scheduler via HTTP requests over the network, can lead to robustness <lb/>problems when going over a certain number of workers (around in the authors&apos; experience) leading to HTTP <lb/>connection time-outs. <lb/>The mentioned limitations for complex work ows in existing tools is the background and motivation for <lb/>developing the SciPipe library. <lb/>The SciPipe work ow library <lb/>SciPipe is a work ow library based on Flow-Based Programming principles, implemented as a library in the Go <lb/>programming language. The library is freely available as open source on GitHub [ ]. All releases of GitHub are <lb/>also arhived on Zenodo [ ]. Similarly to Next ow, SciPipe leverages the data ow paradigm to achieve dynamic <lb/>scheduling of tasks based on input data, allowing many work ow constructs not easily coded in many other <lb/>tools. <lb/>Combined with design principles from Flow-based programming such as separate network de nition and <lb/>named ports bound to processes, this has resulted in a productive and discoverable API that enables agile <lb/>
			1 <lb/>package main <lb/>2 <lb/>3 <lb/>import ( <lb/>4 <lb/>&quot;github.com/scipipe/scipipe&quot; <lb/>5 <lb/>) <lb/>6 <lb/>7 <lb/>const dna = &quot;AAAGCCCGTGGGGGACCTGTTC&quot; <lb/>8 <lb/>9 <lb/>func main() { <lb/>10 <lb/>// Initialize workflow, using max 4 CPU cores <lb/>11 <lb/>wf := scipipe.NewWorkflow(&quot;DNA Base Complement Workflow&quot;, 4) <lb/>12 <lb/>13 <lb/>// Initialize processes based on shell commands: <lb/>14 <lb/>15 <lb/>// makeDNA writes a DNA string to a file <lb/>16 <lb/>makeDNA := wf.NewProc(&quot;Make DNA&quot;, &quot;echo &quot;+dna+&quot; &gt; {o:dna}&quot;) <lb/>17 <lb/>makeDNA.SetOut(&quot;dna&quot;, &quot;dna.txt&quot;) <lb/>18 <lb/>19 <lb/>// complmt computes the base complement of a DNA string <lb/>20 <lb/>complmt := wf.NewProc(&quot;Base Complement&quot;, &quot;cat {i:in} | tr ATCG TAGC &gt; {o:compl}&quot;) <lb/>21 <lb/>complmt.SetOut(&quot;compl&quot;, &quot;{i:in|%.txt}.compl.txt&quot;) <lb/>22 <lb/>23 <lb/>// reverse reverses the input DNA string <lb/>24 <lb/>reverse := wf.NewProc(&quot;Reverse&quot;, &quot;cat {i:in} | rev &gt; {o:rev}&quot;) <lb/>25 <lb/>reverse.SetOut(&quot;rev&quot;, &quot;{i:in|%.txt}.rev.txt&quot;) <lb/>26 <lb/>27 <lb/>// Connect data dependencies between out-and in-ports <lb/>28 <lb/>complmt.In(&quot;in&quot;).From(makeDNA.Out(&quot;dna&quot;)) <lb/>29 <lb/>reverse.In(&quot;in&quot;).From(complmt.Out(&quot;compl&quot;)) <lb/>30 <lb/>31 <lb/>// Run the workflow <lb/>32 <lb/>wf.Run() <lb/>33 <lb/>} <lb/>Figure : A simple example work ow implemented with SciPipe. The work ow computes the reverse base <lb/>complement of a string of DNA, using standard UNIX tools. The work ow is a Go program and is supposed to <lb/>be saved in a le with the .go extension and executed with the go run command. On line , the SciPipe library <lb/>is imported, to be later accessed as scipipe. On line , a short string of DNA is de ned. On line -, the full <lb/>work ow is implemented in the program&apos;s main() function, meaning that it will be executed when the resulting <lb/>program is executed. On line , a new work ow object (or &quot;struct&quot; in Go terms) is initiated with a name and <lb/>the maximum number of cores to use. On lines -, the work ow components, or processes, are initiated, <lb/>each with a name and a shell command pattern. Input le names are de ned with a placeholder on the form <lb/>{i:INPORTNAME} and outputs on the form {o:OUTPORTNAME}. The port-name will be used later to access <lb/>the corresponding ports for setting up data dependencies. On line , a component that writes the previously <lb/>de ned DNA string to a le is initiated, and on line , the le path pattern for the out-port dna is de ned <lb/>(in this case a static le name). On line , a component that translates each DNA base to its complementary <lb/>counterpart is initiated. On line , the le path pattern for its only out-port is de ned. In this case, re-using the <lb/>le path of the le it will receive on its in-port named in, thus the {i:in} part. The %.txt part removes .txt <lb/>from the input path. On line , a component that will reverse the DNA string is initiated. On lines -, data <lb/>dependencies are de ned via the in-and out-ports de ned earlier as part of the shell command patterns. On line <lb/>, the work ow is being run. <lb/>authoring of complex and dynamic work ows. The fact that the work ow network is de ned separately from <lb/>processes, enables building work ows based on a library of reusable components, although the creation of ad-hoc <lb/>shell-command based components is also supported. <lb/>SciPipe provides a provenance tracking feature that associates provenance information with data rather than <lb/>with a virtual concept such as a work ow run, which means that it is always easy to verify exactly how each <lb/>output of a work ow was created. <lb/>SciPipe also provides a few features which are not very common among existing tools, or which are not <lb/>commonly occurring together in one system. These include support for streaming via Unix named pipes, ability <lb/>to run push-based work ows up to a speci c stage of the work ow, and exible support for le naming of <lb/>intermediate data les generated by work ows. <lb/>By implementing SciPipe as a library in an existing language, the language&apos;s ecosystem of tooling, editor <lb/>support and third-party libraries can be directly used to avoid &quot;reinventing the wheel&quot; in these areas. By leveraging <lb/>the built-in concurrency features of Go, such as go-routines and channels, the developed code base has been kept <lb/>small compared with similar tools, and also does not require external dependencies for basic usage (some external <lb/>tools are used for optional features like PDF generation and graph plotting). This means that the code base <lb/>should be possible to maintain for a single developer or small team, and that the code base is practical to include <lb/>in work ow developers&apos; own source code repositories, in order to future-proof the functionality of work ows. <lb/>Below, we rst brie y describe how SciPipe work ows are created. We then describe in some detail the features <lb/>of SciPipe that are the most novel or improves most upon existing tools, followed by a few more commonplace <lb/>technical considerations. We nally demonstrate the usefulness of SciPipe by applying it to a set of case study <lb/>work ows in machine learning for drug discovery and next-generation sequencing genomics and transcriptomics. <lb/>Writing work ows with SciPipe <lb/>SciPipe work ows are written as Go programs, in les ending with the .go extension. As such, they require <lb/>the Go tool chain to be installed for compiling and running them. The Go programs can be either compiled to <lb/>self-contained executable les with the go build command, or run directly, using the go run command. <lb/>The simplest way to write a SciPipe program is to write the work ow de nition in the program&apos;s main() <lb/>
			function, which is executed when running the compiled executable le, or running the le as script with go run. <lb/>An example work ow written in this way is shown in in gure , which provides a simple example work ow <lb/>consisting of three processes, demonstrating a few of the basic features of SciPipe. The rst process writes a <lb/>string of DNA to a le, the second computes the base complement, and the last process reverses the string. All in <lb/>all, the work ow computes the reverse base complement of the initial string. <lb/>As can be seen in gure on line , a work ow object (or struct, in Go terminology) is rst initialized, with a <lb/>name and a setting for the maximum number of tasks to run at a time. Furthermore, on line -, processes are <lb/>de ned with the Workflow.NewProc() method on the work ow struct, with name and a command pattern <lb/>which is very similar to the Bash shell command that would be used to run a command manually, but where <lb/>concrete le names have been replaced with placeholders, on the form {i:INPORTNAME}, {o:OUTPORTNAME} <lb/>or {p:PARAMETERNAME}. These placeholders de ne input and output les, as well as parameter values, and <lb/>works as a sort of templates, that will be replaced with concrete values as concrete tasks are scheduled and <lb/>executed. <lb/>As can be seen on lines , and , output paths to use for output les are de ned using the Process.SetOut() <lb/>method, taking an out-port name and a pattern for how to generate the path. For simple work ows this can be <lb/>just a static le name, but for more complex work ows with processes that produce more than one output on <lb/>the same port, it is of en best to re-use some of the input paths and parameter values con gured earlier in the <lb/>command pattern, in combination with some unique string for this particular process, to generate a unique <lb/>path for each output. <lb/>Finally, on lines -, we see how in-ports and out-ports are connected in order to de ne the data depen-<lb/>dencies between tasks. Here, the in-port and out-port names used in the placeholders in the command pattern <lb/>described above, are used to access the corresponding in-ports and out-ports, and making connections between <lb/>them, with a syntax on the general form of InPort.From(OutPort). <lb/>1 <lb/>{ <lb/>2 <lb/>&quot;ID&quot;: &quot;tuir75c24kxe4rrqmm2p&quot;, <lb/>3 <lb/>&quot;ProcessName&quot;: &quot;Reverse&quot;, <lb/>4 <lb/>&quot;Command&quot;: &quot;cat ../dna.compl.txt | rev \u003e dna.compl.rev.txt&quot;, <lb/>5 <lb/>&quot;Params&quot;: {}, <lb/>6 <lb/>&quot;Tags&quot;: {}, <lb/>7 <lb/>&quot;StartTime&quot;: &quot;2018-07-26T13:02:16.855172344+02:00&quot;, <lb/>8 <lb/>&quot;FinishTime&quot;: &quot;2018-07-26T13:02:16.863536059+02:00&quot;, <lb/>9 <lb/>&quot;ExecTimeNS&quot;: 8363715, <lb/>10 <lb/>&quot;OutFiles&quot;: { <lb/>11 <lb/>&quot;rev&quot;: &quot;dna.compl.rev.txt&quot; <lb/>12 <lb/>}, <lb/>13 <lb/>&quot;Upstream&quot;: { <lb/>14 <lb/>&quot;dna.compl.txt&quot;: { <lb/>15 <lb/>&quot;ID&quot;: &quot;2g7tr2trhu9zubovwlua&quot;, <lb/>16 <lb/>&quot;ProcessName&quot;: &quot;Base Complement&quot;, <lb/>17 <lb/>&quot;Command&quot;: &quot;cat ../dna.txt | tr ATCG TAGC \u003e dna.compl.txt&quot;, <lb/>18 <lb/>&quot;Params&quot;: {}, <lb/>19 <lb/>&quot;Tags&quot;: {}, <lb/>20 <lb/>&quot;StartTime&quot;: &quot;2018-07-26T13:02:16.845769702+02:00&quot;, <lb/>21 <lb/>&quot;FinishTime&quot;: &quot;2018-07-26T13:02:16.854035213+02:00&quot;, <lb/>22 <lb/>&quot;ExecTimeNS&quot;: 8265532, <lb/>23 <lb/>&quot;OutFiles&quot;: { <lb/>24 <lb/>&quot;compl&quot;: &quot;dna.compl.txt&quot; <lb/>25 <lb/>}, <lb/>26 <lb/>&quot;Upstream&quot;: { <lb/>27 <lb/>&quot;dna.txt&quot;: { <lb/>28 <lb/>&quot;ID&quot;: &quot;vu8ltmoujzo3vn2b39pr&quot;, <lb/>29 <lb/>&quot;ProcessName&quot;: &quot;Make DNA&quot;, <lb/>30 <lb/>&quot;Command&quot;: &quot;echo AAAGCCCGTGGGGGACCTGTTC \u003e dna.txt&quot;, <lb/>31 <lb/>&quot;Params&quot;: {}, <lb/>32 <lb/>&quot;Tags&quot;: {}, <lb/>33 <lb/>&quot;StartTime&quot;: &quot;2018-07-26T13:02:16.842112643+02:00&quot;, <lb/>34 <lb/>&quot;FinishTime&quot;: &quot;2018-07-26T13:02:16.84486747+02:00&quot;, <lb/>35 <lb/>&quot;ExecTimeNS&quot;: 2754810, <lb/>36 <lb/>&quot;OutFiles&quot;: { <lb/>37 <lb/>&quot;dna&quot;: &quot;dna.txt&quot; <lb/>38 <lb/>}, <lb/>39 <lb/>&quot;Upstream&quot;: {} <lb/>40 <lb/>} <lb/>41 <lb/>} <lb/>42 <lb/>} <lb/>43 <lb/>} <lb/>44 <lb/>} <lb/>Figure : Example audit log le in JSON format [ ], for a le produced by a SciPipe work ow. The work ow <lb/>used to produce this audit log in particular, is the one in gure . The audit information is hierarchical, with <lb/>each level representing a step in the work ow. The rst level contains meta-data about the task executed last, <lb/>to produce the output le that this audit log refers to. The eld Upstream on each level, contains a list of <lb/>all upstream task of the current task, indexed by the le paths that each of the upstream tasks did produce, <lb/>and which was subsequently used by the current task. Each task is given a globally unique ID, which helps to <lb/>deduplicate any duplicate occurrences of tasks, when converting the log to other representations. Execution <lb/>time is given in nanoseconds. Note that input paths in the command eld, is prepended with ../, compared <lb/>to how they appear in the Upstream eld. This is because each task is executed in a temporary directory created <lb/>directly under the work ow&apos;s main execution directory, meaning that to access existing data les, it has to rst <lb/>navigate up one step out of this temporary directory. <lb/>The last thing needed to do to run the work ow, is seen on line , where the Workflow.Run() method is <lb/>executed. Provided that the work ow code in gure is saved in a le named workflow.go, it can be run using <lb/>the go run command, like so: <lb/>$ go run workflow.go <lb/>This will then produce three output les and one accompanying audit log for each le, which we can be seen <lb/>
			by listing the les in a terminal: <lb/>dna.txt <lb/>dna.txt.audit.json <lb/>dna.compl.txt <lb/>dna.compl.txt.audit.json <lb/>dna.compl.rev.txt <lb/>dna.compl.rev.txt.audit.json <lb/>The le dna.txt should now contain the string AAAGCCCGTGGGGGACCTGTTC, and dna.compl.rev.txt <lb/>should contain GAACAGGTCCCCCACGGGCTTT, which is the reverse base complement of the rst string. In the <lb/>last le above, the full audit log for this minimal work ow can be found. An example content of this le is shown <lb/>in gure . <lb/>In this code example, it can be seen that both of the commands we executed are available, and also that the <lb/>Reverse process lists its &quot;upstream&quot; processes, which are indexed by the input le names in its command. Thus, <lb/>under the dna.compl.txt input le, we nd the Base Complement process together with its meta-data, and <lb/>one further upstream process (the Make DNA process). This hierarchic structure of the audit log ensures that <lb/>the complete audit trace, including all commands contributing to the production of an output le, is available <lb/>for each output le from the work ow. <lb/>More information about how to write work ows with SciPipe is available on the documentation website [ ]. <lb/>Note that the full documentation on this website is also available in a folder named docs inside the SciPipe Git <lb/>repository, which ensures that documentation for the version currently used, is always available. <lb/>Dynamic scheduling <lb/>Since SciPipe is built on the principles from Flow-based programming (see the methods section for more details), <lb/>a SciPipe program consists of independently and concurrently running processes, which schedule new tasks <lb/>continually during the work ow run. This is here referred to as dynamic scheduling. This means that it is possible <lb/>to create a process that obtains a value and passes it on to a downstream process as a parameter, so that new <lb/>tasks can be scheduled with it. This feature is important in machine learning work ows, where hyper parameter <lb/>tuning is of en employed to nd an optimal value of a parameter, such as cost for Support Vector Machines <lb/>(SVM), which is then used to parametrize the nal training part of the work ow. <lb/>Re-usable components <lb/>Based on principles from Flow-based programming, the work ow graph in SciPipe is de ned by making connec-<lb/>tions between port objects bound to processes. This enables to keep the dependency graph de nition separate <lb/>from the process de nitions. This is in contrast to other ways of connecting data ow processes, such as with <lb/>data ow variables, which are shared between process de nitions. This makes processes in ow-based program-<lb/>ming fully self-contained, meaning that libraries of re-usable components can be created and that components <lb/>can be loaded on-demand when creating new work ows. A graphical comparison between dependencies de ned <lb/>with data ow variables and ow-based programming ports, is shown in gure . <lb/>Figure : Comparison between data ow variables and Flow-based programming ports in terms of dependency <lb/>de nition. a) shows how data ow variables (blue and green) shared between processes (in gray) make the pro-<lb/>cesses tightly coupled. In other words, process-and network de nitions get intermixed. b) shows how ports (in <lb/>orange) bound to processes in Flow-based programming allows keeping the network de nition separate from <lb/>process de nitions. This enables processes to be reconnected freely without changing their internals. <lb/>Running subsets of work ows <lb/>With pull-based work ow tools like Snakemake or Luigi, it is easy to on-demand reproduce a particular output <lb/>le, since the scheduling mechanism is optimized for the use case of asking for a speci c le and calculating all <lb/>the tasks required to be executed based on that. <lb/>With push-based work ow tools though, reproducing a speci c set of les without running the full work ow <lb/>is not always straight-forward. This is a natural consequence of the push-based scheduling strategy, and data ow <lb/>in particular, as the identities and quantities of output les might not be known before the work ow is run. <lb/>SciPipe provides a mechanism for partly solving this lack of &quot;on demand le generation&quot; in push-based <lb/>data ow tools, by allowing to reproduce all les of a speci ed process, on-demand. That is, the user can tell the <lb/>work ow to run all processes in the work ow upstream of, and including, a speci ed process, while skipping <lb/>processes downstream of it. <lb/>This has turned out very useful when iteratively refactoring or developing new pipelines. When a part in the <lb/>middle of a long sequence of processes need to be changed, it is helpful to be able to test-run the work ow up to <lb/>that particular process only, not the whole work ow, to speed up the development iteration cycle. <lb/>Other characteristics <lb/>Below are a few technical characteristics and considerations that are not necessarily unique to SciPipe, but could <lb/>be of interest to potential users assessing whether SciPipe ts their use cases. <lb/>Data centric audit log <lb/>The audit log feature in SciPipe collects meta data about every executed task (concrete shell command invocation) <lb/>which is passed along with every le that is processed in the work ow. It writes a le in the ubiquitous JSON <lb/>format, with the full trace of tasks executed for every output in the work ow, with the same name as the output <lb/>le in question but with the additional le extension .audit.json. Thus, for every output in the work ow, it <lb/>is possible to check the full record of shell commands used to produce it. An example audit log le can be seen in <lb/>gure . <lb/>This data-oriented provenance reporting contrasts to provenance reports common in many work ow tools, <lb/>which of en provide one report per work ow run only, meaning that the link between data and provenance <lb/>report is not as direct. <lb/>The audit log feature in SciPipe in many aspects re ects the recommendations in [ ] for developing <lb/>provenance reporting in work ows, such as producing a coherent, accurate, inspectable record for every output <lb/>data item from the work ow. By producing provenance records for data output rather than for the full <lb/>work ow only, SciPipe could provide a basis for the problem of iteratively developing work ow variants, as <lb/>outlined in [ ]. <lb/>SciPipe also loads any existing provenance reports for existing les that it uses, and merges these with the <lb/>provenance information from its own execution. This means that even if a chain of processing is spread over <lb/>
			multiple SciPipe work ow scripts, and executed at di ferent times by di ferent users, the full provenance record is <lb/>still being kept and assembled, as long as all work ow steps were executed using SciPipe shell command processes. <lb/>The main limitation to this &quot;follow the data&quot; approach, is for data generated externally to the work ow, or by <lb/>SciPipe components implemented in Go. For external processes, it is up to the external process to generate any <lb/>reporting. For Go-based components in SciPipe, these can not currently dump a textual version of the Go code <lb/>executed. This constitutes an area of future development. <lb/>SciPipe provides experimental support for converting the JSON-structure into reports in HTML and TeX <lb/>format, or into executable Bash scripts that can reproduce the le which the audit report describes from available <lb/>inputs or from scratch. These tools are available in the scipipe helper command. The TeX report can be easily <lb/>further converted to PDF using the pdflatex command of the pdfTex sof ware [ ]. An example of such a <lb/>PDF report, is shown in gure , which was generated from the audit report for the last le generated by the <lb/>code example in gure . <lb/>Atomic writes <lb/>SciPipe ensures that cancelled work ow runs do not result in half-written output les being mistaken for nished <lb/>ones. It does this by executing each task in a temporary folder, and moving all newly created les into their nal <lb/>location a er the task is nished. By using a folder for the execution, any extra les created by a tool that are not <lb/>explicitly con gured by the work ow system, are captured and treated in an atomic way. Examples of where this <lb/>is needed is for the ve extra les created by bwa index [ ], when indexing a reference genome in FASTA <lb/>format. <lb/>Streaming support <lb/>In data intensive elds like Next-Generation Sequencing, it is common that intermediate steps of pipelines <lb/>produce large amounts of intermediate data, of en multiplying the storage requirements considerably compared <lb/>to the raw data from sequencing machines [ ]. To help ease these storage requirements, SciPipe provides the <lb/>ability to optionally stream data between two tasks via Random Access Memory (RAM) instead of saving to <lb/>disk between task executions. This approach has two bene ts. Firstly, the data does not need to be stored on <lb/>disk, which can lessen the storage requirements considerably. Secondly, it enables the downstream task to start <lb/>processing the data from the upstream task immediately as soon as the rst task has started to produce partial <lb/>output. It thus enables to achieve pipeline parallelism in addition to data parallelism, and can thereby shorten <lb/>the total execution time of the pipeline. <lb/>Flexible file naming and data &quot;caching&quot; <lb/>SciPipe allows exible naming of the le path of every intermediate output in the work ow, based on input le <lb/>names and parameter values passed to a process. This enables creating meaningful le naming schemes, to make <lb/>it easy to manually explore and sanity-check outputs from work ows. <lb/>Con guring a custom le naming scheme is not required though. If no le path is con gured, SciPipe will <lb/>automatically create a le path that ensures that two tasks with di ferent parameters or input data will never <lb/>Figure : Audit report for the last le generated by the code example in gure , converted to TeX with SciPipe&apos;s <lb/>experimental audit2tex feature and then converted to PDF with pdfTeX. In the top, the PDF le includes <lb/>summary information about the SciPipe version used and the total execution time. Af er this follows an exe-<lb/>cution time line, in a gantt-chart style, that shows the relative execution times of individual tasks in a graphical <lb/>way. Af er this follows a comprehensive list tables with information for each task executed towards produc-<lb/>ing the le for which the audit report belongs. The task boxes are color coded and ordered in the same way that <lb/>the tasks appear in the timeline. <lb/>clash, and that two tasks with the same command signature, parameters and input-les, will re-use the same <lb/>cached data. <lb/>Known limitations <lb/>Below we list some design decisions and known limitations of SciPipe that might a fect the decision whether to <lb/>use SciPipe for a particular use case or not. <lb/>Firstly, the fact that writing SciPipe work ows requires some basic knowledge of the Go programming <lb/>language, can be o f-putting to users who are not well acquainted with programming. Go code, although having <lb/>taken inspiration from scripting languages, is still markedly more verbose and low-level in nature than Python, <lb/>and can take a little longer to get used to. <lb/>Secondly, the level of integration with HPC resource managers is currently quite basic compared to some <lb/>other work ow tools. The SLURM resource manager can readily be used by using the Prepend eld on <lb/>processes to add a string with a call to the salloc SLURM command, but more advanced HPC integration is <lb/>planned to be addressed in upcoming versions. <lb/>Furthermore, reproducing speci c output les is not as natural and easy as with pull-based tools like Snake-<lb/>make, although SciPipe provides a mechanism to partly resolve this problem. <lb/>Finally, SciPipe does not yet support integration with the Common Work ow Language [ ], for interoper-<lb/>ability of work ows. This is a prioritized area for future development. <lb/>

			Case Studies <lb/>To demonstrate the usefulness of SciPipe, we have used it to implement a number of representative pipelines <lb/>from drug discovery and bioinformatics with di ferent characteristics and hence requirements on the work ow <lb/>system. These work ows are available in a dedicated git repository on GitHub [ ]. <lb/>
			Machine learning pipeline in drug discovery <lb/>Figure : Directed graph of the machine learning drug discovery case study work ow, plotted with SciPipe&apos;s <lb/>work ow plotting function. The graph has been modi ed for clarity by collapsing the individual branches of the <lb/>parameter sweeps and cross validation fold-generation. The layout has also been manually made more compact <lb/>to be viewable in print. The collapsed branches are indicated by intervals in the box labels. tr{500-8000} <lb/>represent branching into training dataset sizes <lb/>, <lb/>, <lb/>, <lb/>, <lb/>. c{0.0001-5.0000} represent <lb/>cost values . <lb/>, . <lb/>, . , . , . , . , . , . , . , . , , , , and , while fld{1-10} <lb/>represent cross validation folds -. Nodes represent processes, while edges represent data dependencies. The <lb/>labels on the edge heads and tails represent ports. Solid lines represent data dependencies via les, while dashed <lb/>lines represent data dependencies via parameters, which are not persisted to le, only transmitted via RAM. <lb/>The initial motivation for building SciPipe stemmed from problems encountered with complex dynamic <lb/>work ows in machine learning for drug discovery applications. It was thus quite natural to implement an <lb/>example of such a work ow in SciPipe. To this end we re-implemented a work ow implemented previously for <lb/>the SciLuigi library [ ], which was itself based on an earlier study [ ]. <lb/>In short, this work ow trains predictive models using the LIBLINEAR sof ware [ ] with molecules <lb/>represented by the signature descriptor [ ]. For linear SVM a cost parameter needs to be tuned, and we tested <lb/>values ( . <lb/>, . <lb/>, . , . , . , . , . , . , . , . , , , , , ) in a -fold cross-validated <lb/>parameter sweep. Five di ferent training set sizes ( , <lb/>, <lb/>, <lb/>, <lb/>) were tested and evaluated with a <lb/>test set size of <lb/>. The raw data set consists of , <lb/>logarithmic solubility values chosen randomly from <lb/>a dataset extracted from PubChem [ ] according to details in [ ]. The work ow is schematically shown in <lb/>gure and was plotted using SciPipe&apos;s built-in plotting function. The gure has been modi ed for clarity by <lb/>collapsing the individual branches of the parameter sweeps and cross validation folds, as well as by manually <lb/>making the layout more compact. <lb/>The implementation in SciPipe was done by creating components which are de ned in separate les (named <lb/>comp COMPONENTNAME in the repository), which can thus be re-used in other work ows. This shows how <lb/>SciPipe can successfully be used to create work ows based on re-usable, externally de ned components. <lb/>The fact that SciPipe supports parametrization of work ow steps with values obtained during the work ow <lb/>run, meant that the full work ow could be kept in a single work ow de nition, in one le. This also made <lb/>it possible to create audit logs for the full work ow execution for the nal output les, and to create the <lb/>automatically plotted work ow graph shown in gure . This is in contrast to the SciLuigi implementation, <lb/>where the parameter sweep to nd the optimal cost, and the nal training, had to be implemented in separate <lb/>work ow les (wffindcost.py and wfmm.py in [ ]), and executed as a large number of completely separate <lb/>work ow runs (one for each dataset size) which meant that logging became fragmented into a large number of <lb/>disparate log les. <lb/>Genomics cancer-analysis pipeline <lb/>reads_fastq2_normal_idx4 <lb/>align_samples_normal_idx4 <lb/>reads2 <lb/>out <lb/>reads_fastq1_tumor_idx6 <lb/>align_samples_tumor_idx6 <lb/>reads1 <lb/>out <lb/>stream_to_substream_tumor <lb/>in <lb/>bam <lb/>realign_create_targets <lb/>realign_indels <lb/>intervals <lb/>intervals <lb/>align_samples_tumor_idx7 <lb/>in <lb/>bam <lb/>merge_bams_tumor <lb/>mark_dupes_tumor <lb/>bam <lb/>mergedbam <lb/>recalibrate_tumor <lb/>print_reads_tumor <lb/>recaltable <lb/>recaltable <lb/>align_samples_normal_idx1 <lb/>stream_to_substream_normal <lb/>in <lb/>bam <lb/>reads_fastq1_normal_idx2 <lb/>align_samples_normal_idx2 <lb/>reads1 <lb/>out <lb/>reads_fastq2_normal_idx7 <lb/>align_samples_normal_idx7 <lb/>reads2 <lb/>out <lb/>reads_fastq2_tumor_idx5 <lb/>align_samples_tumor_idx5 <lb/>reads2 <lb/>out <lb/>reads_fastq1_normal_idx4 <lb/>reads1 <lb/>out <lb/>reads_fastq2_tumor_idx1 <lb/>align_samples_tumor_idx1 <lb/>reads2 <lb/>out <lb/>reads_fastq2_tumor_idx3 <lb/>align_samples_tumor_idx3 <lb/>reads2 <lb/>out <lb/>reads_fastq2_tumor_idx7 <lb/>reads2 <lb/>out <lb/>mark_dupes_normal <lb/>bamnormal <lb/>bam <lb/>bamnormal <lb/>bam <lb/>reads_fastq1_tumor_idx1 <lb/>reads1 <lb/>out <lb/>in <lb/>bam <lb/>in <lb/>bam <lb/>download_apps <lb/>untgz_apps <lb/>tgz <lb/>apps <lb/>reads_fastq2_normal_idx2 <lb/>reads2 <lb/>out <lb/>in <lb/>bam <lb/>reads_fastq2_normal_idx8 <lb/>align_samples_normal_idx8 <lb/>reads2 <lb/>out <lb/>in <lb/>bam <lb/>bamtumor <lb/>bam <lb/>bamtumor <lb/>bam <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>align_samples_tumor_idx2 <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>recalibrate_normal <lb/>print_reads_normal <lb/>recaltable <lb/>recaltable <lb/>bams <lb/>substream <lb/>reads_fastq1_tumor_idx2 <lb/>reads1 <lb/>out <lb/>in <lb/>bam <lb/>reads_fastq1_tumor_idx3 <lb/>reads1 <lb/>out <lb/>merge_bams_normal <lb/>bams <lb/>substream <lb/>in <lb/>bam <lb/>reads_fastq1_normal_idx8 <lb/>reads1 <lb/>out <lb/>bam <lb/>mergedbam <lb/>reads_fastq2_tumor_idx6 <lb/>reads2 <lb/>out <lb/>reads_fastq1_tumor_idx7 <lb/>reads1 <lb/>out <lb/>realbam <lb/>realbamtumor <lb/>realbam <lb/>realbamtumor <lb/>realbam <lb/>realbamnormal <lb/>realbam <lb/>realbamnormal <lb/>in <lb/>bam <lb/>reads_fastq1_normal_idx7 reads1 <lb/>out <lb/>in <lb/>bam <lb/>reads_fastq1_tumor_idx5 <lb/>reads1 <lb/>out <lb/>reads_fastq1_normal_idx1 reads1 <lb/>out <lb/>reads_fastq2_normal_idx1 <lb/>reads2 <lb/>out <lb/>reads_fastq2_tumor_idx2 <lb/>reads2 <lb/>out <lb/>Figure : Directed graph of work ow processes in the Genomics / Cancer Analysis pre-processing pipline, plot-<lb/>ted with SciPipe&apos;s work ow plotting function. Nodes represent processes, while edges represent data depen-<lb/>dencies. The labels on the edge heads and tails represent ports. <lb/>Sarek [ ] is an open-source analysis pipeline to detect germ-line or somatic variants from whole genome <lb/>sequencing, developed by the National Genomics Infrastructure and National Bioinformatics Infrastructure <lb/>Sweden which are both platforms at Science for Life Laboratory. <lb/>To test and demonstrate the applicability of SciPipe to genomics use cases the pre-processing part of the <lb/>Sarek pipeline was implemented in SciPipe. See gure for a directed process graph of the work ow, plotted <lb/>with SciPipe&apos;s work ow plotting function. <lb/>The test data in the test work ow consists of multiple samples of normal and tumor pairs. The work ow starts <lb/>with aligning each sample to a reference genome using BWA [ ] and forwarding the results to Samtools [ ] <lb/>which saves the result as a sorted BAM le. Af er each sample has been aligned, Samtools is again used, to merge <lb/>the normal-and tumor samples into a one BAM [ ] le for tumor samples, and one for normal. Picard [ ] is <lb/>then used to mark duplicate reads in both the normal-and tumor sample BAM les, whereaf er GATK [ ] is <lb/>used to re-calibrate the quality scores of all reads. The outcome of the work ow is two BAM les; one containing <lb/>all the normal samples and one containing all the tumor samples. <lb/>Genomics tools and pipelines have their own set of requirements, which was shown by the fact that some <lb/>aspects of SciPipe had to be modi ed in order to ease development of this pipeline. In particular, many genomics <lb/>tools produce additional output les apart from those speci ed on the command-line. One example of this is the <lb/>extra les produced by BWA when indexing a reference genome in FASTA format. The bwa index command <lb/>produces some ve les, which are not explicitly de ned on the command-line (with the extensions of .bwt, <lb/>.pac, .ann, .amb and .sa). Based on this realization, SciPipe was amended with a folder-based execution <lb/>mechanism which executes each task in a temporary folder, that keeps all output les separate from the main <lb/>output directory until the whole task has completed. This ensures that also les that are not explicitly de ned <lb/>and handled by SciPipe, are also captured and handled in an atomic manner, so that nished and un nished <lb/>output les are always properly separated. <lb/>Furthermore, agile development of genomic tools of en requires being able to see the full command that <lb/>is used to execute a tool, because of the many options that are available to many bioinformatics tools. This <lb/>work ow was thus implemented with ad-hoc commands, which are de ned in-line in the work ow. The ability <lb/>to do this shows that SciPipe supports di ferent ways of de ning components, depending on what ts the use <lb/>case best. <lb/>The successful implementation of this genomics pipeline in SciPipe, thus both ensures and shows that <lb/>SciPipe is works well for tools common in genomics. <lb/>RNA-seq / transcriptomics pipeline <lb/>align_samples_SRR3222409 <lb/>collect_substream_SRR3222409 <lb/>fastqc <lb/>substream <lb/>download_apps <lb/>untgz_apps <lb/>tgz <lb/>apps <lb/>fastqFile_align_SRR3222409_1.chr11.fq.gz <lb/>reads1 <lb/>out <lb/>fastqFile_align_SRR3222409_2.chr11.fq.gz <lb/>reads2 <lb/>out <lb/>fastqFile_fastqc_SRR3222409_1 <lb/>fastqc_sample_SRR3222409_1 <lb/>reads <lb/>out <lb/>fastqFile_fastqc_SRR3222409_2 <lb/>fastqc_sample_SRR3222409_2 <lb/>reads <lb/>out <lb/>in <lb/>done <lb/>in <lb/>done <lb/>untardone <lb/>done <lb/>untardone <lb/>done <lb/>
			Figure : Directed graph of work ow processes in the RNA-Seq Pre-processing work ow, plotted with SciPipe&apos;s <lb/>work ow plotting function. Nodes represent processes, while edges represent data dependencies. The labels <lb/>on the edge heads and tails represent ports. <lb/>To test the ability of SciPipe to work with sof ware used in transcriptomics, some of the initial steps of <lb/>a generic RNA-sequencing work ow were also implemented in SciPipe. Common steps that are needed in <lb/>transcriptomics is to run quality controls and generate reports of the analysis steps. <lb/>The RNA-seq case study pipeline implemented for this paper uses FastQC [ ] to evaluate the quality of <lb/>the raw data being used in the analysis before aligning the data using STAR [ ]. Af er the alignment is done it <lb/>is evaluated using QualiMap [ ], while the Subread package [ ] is used to do a feature counting. <lb/>The nal step of the work ow is to combine all the previous steps for a composite analysis using Mul-<lb/>tiQC [ ], which will summarize the quality of both the raw data and the result of the alignment into a single <lb/>quality report. See gure for a directed process graph of the work ow, plotted with SciPipe&apos;s work ow plotting <lb/>function. <lb/>The successful implementation of this transcriptomics work ow in SciPipe ensures that SciPipe works well <lb/>for di ferent types of bioinformatics work ows and is not limited to one speci c sub-eld of bioinformatics. <lb/>Conclusions <lb/>SciPipe is a programming library that provides a way to write complex and dynamic pipelines in bioinformatics, <lb/>cheminformatics, and more generally in data science and machine learning pipelines involving command-line <lb/>applications. <lb/>Dynamic scheduling allows parametrizing new tasks with values obtained during the work ow run, and the <lb/>Flow-based programming principles of separate network de nition and named ports allow creating a library of <lb/>re-usable components. By having access to the full power of the Go programming language to de ne work ows, <lb/>existing tooling is leveraged. <lb/>Scipipe adopts state-of-the art strategies for achieving atomic writes, caching of intermediate les and a <lb/>data-centric audit log feature that allows identifying the full execution trace for each output, that can be exported <lb/>into either human-readable HTML or TeX/PDF formats, or executable Bash-scripts. <lb/>SciPipe also provides some features not commonly found in many tools such as support for streaming via <lb/>Unix named pipes, ability to run push-based work ows up to a speci c stage of the work ow, and exible support <lb/>for le naming of intermediate data les generated by work ows. SciPipe work ows can also be compiled into <lb/>standalone executables, making deployment of pipelines maximally easy, requiring only Bash and any external <lb/>command-line tools used, to be present on the target machine. <lb/>By being a small library without required external dependencies apart from the Go tool chain and Bash, <lb/>SciPipe is expected to be possible to be maintained and developed in the future even without a large team or <lb/>organization backing it. <lb/>The applicability of SciPipe for cheminformatics, genomics and transcriptomics pipelines has been demon-<lb/>strated with case study work ows in these elds. <lb/>Methods <lb/>The Go Programming Language <lb/>The Go Programming Language (referred to as just &quot;Go&quot;) was developed by Robert Griesemer, Rob Pike and <lb/>Ken Thompson at Google, to provide a statically typed and compiled language that makes it easier to build highly <lb/>concurrent programs, that can also make good use of multiple CPU cores (i.e. &quot;parallel program&quot;), than what is <lb/>the case in widespread compiled languages like C++ [ ]. It tries to provide this by providing a small, simple <lb/>language, with concurrency primitives -go-routines and channels -built-in to the language. Go-routines, <lb/>which are so called light-weight threads, are automatically mapped, or multiplexed, onto physical threads in the <lb/>operating system. This means that very large numbers of go-routines can be created while maintaining a low <lb/>number of operating system threads, such as one per CPU core on the computer at hand. This makes Go an <lb/>ideal choice for problems where many asynchronously running processes need to be handled at the same time, <lb/>or &quot;concurrently&quot;, and for making e cient use of multi-core CPUs. <lb/>The Go compiler is statically linking all its code as part of the compilation. This means that all dependent <lb/>libraries are compiled into the executable le. Because of this, SciPipe work ows can be compiled into self-<lb/>contained executable les without external dependencies apart from the Bash shell and any external command <lb/>line tools used by the work ow. This makes deploying Go programs (and SciPipe work ows) to production <lb/>very easy. <lb/>Go programs are very performant, of en an order of magnitude faster than interpreted languages like Python, <lb/>and in the same order of magnitude as the fastest languages, like C, C++ and Java [ ]. <lb/>
			Data ow and Flow-based programming <lb/>Data ow is a programming paradigm oriented around the idea of independent, asynchronously running pro-<lb/>cesses, that only talk to each other by passing data between each other. This data passing can happen in di ferent <lb/>ways, such as via data ow variables, or via rst-in-rst-out channels. <lb/>Flow-Based Programming (FBP) [ ] is a paradigm for programming developed by John Paul Morrison <lb/>at IBM in the late s / early s, to provide a composable way to assemble programs to be run at mainframe <lb/>computers at customers such as large banks. <lb/>It is a specialized version of data ow, adding the ideas of separate network de nition, named ports, channels <lb/>with bounded bu fers and information packets (representing the data) with de ned lifetimes. Just as in data ow, <lb/>the idea is to divide a program into independent processing units called &quot;processes&quot;, which are allowed to <lb/>communicate with the outside world and other processes solely via message passing. In FBP, this is always done <lb/>over channels with bounded bu fers which are connected to named ports on the processes. Importantly, the <lb/>network of processes and channels is in FBP described &quot;separate&quot; from the process implementations, meaning <lb/>that the network of processes and channels can be reconnected freely without changing the internals of processes. <lb/>This strict separation of the processes, the separation of network structure from processing units, and the <lb/>loosely-coupled nature of its only way of communication with the outside world (message passing over channels) <lb/>makes ow-based programs extremely composable, and naturally component-oriented. Any process can always <lb/>be replaced with any other process that supports the same format of the information packets on its in-ports and <lb/>out-ports. <lb/>Furthermore, since the processes run asynchronously, FBP is, just like Go, very well suited to make e cient <lb/>use of multi-core CPUs, where each processing unit can suitably be placed in its own thread or co-routine to <lb/>spread out on the available CPU-cores on the computer. FBP has a natural connection to work ow systems, <lb/>where the computing network in an FBP program can be likened to the network of dependencies between data <lb/>and processing components in a work ow [ ]. SciPipe leverages the principles of separate network de nition <lb/>and named ports on processes. SciPipe has also taken some inspiration for its API design from the GoFlow [ ] <lb/>Go-based ow-based programming framework. <lb/></body>

			<div type="availability">Availability of supporting source code and requirements <lb/>‚Ä¢ Project name: SciPipe <lb/>‚Ä¢ Documentation and project home page: http://scipipe.org <lb/>‚Ä¢ Source code repository: https://github.com/scipipe/scipipe <lb/>‚Ä¢ Persistent source code archive: https://doi.org/ . <lb/>/zenodo. <lb/>‚Ä¢ Case study work ows: https://github.com/pharmbio/scipipe-demo <lb/>‚Ä¢ Operating system(s): Linux, Unix, Mac <lb/>‚Ä¢ Other requirements: Bash, GraphViz (for work ow graph plotting), LaTeX (for PDF generation) <lb/>‚Ä¢ License: MIT <lb/></div>

			<div type="availability">Availability of supporting data <lb/>‚Ä¢ The raw data for the machine learning cheminformatics demonstration pipeline is available at: <lb/>https://doi.org/ . <lb/>/zenodo. <lb/>‚Ä¢ The applications for the machine learning in drug discovery case study is available at: <lb/>https://doi.org/ . <lb/>/m . gshare. <lb/>.v <lb/>‚Ä¢ The raw data and tools for the genomics and transcriptomics work ows are available at: <lb/>https://doi.org/ . <lb/>/zenodo. <lb/></div>

			<div type="annex">Declarations <lb/>List of abbreviations <lb/>‚Ä¢ API: Application programming interface <lb/>‚Ä¢ CPU: Central processing unit (the core part of every computer) <lb/>‚Ä¢ DSL: Domain-speci c language <lb/>‚Ä¢ FBP: Flow-based programming <lb/>‚Ä¢ HPC: High-performance computing <lb/>‚Ä¢ RAM: Random access memory <lb/>‚Ä¢ SVM: Support vector machine <lb/></div>

			<div type="annex">Ethics approval and consent to participate <lb/>Not applicable. <lb/></div>

			<div type="annex">Consent for publication <lb/>Not applicable. <lb/></div>

			<div type="annex">Competing interests <lb/>The authors declare that they have no competing interests. <lb/></div>

			<div type="funding">Funding <lb/>This work has been supported by the Swedish strategic research programme eSSENCE, the Swedish e-Science <lb/>Research Centre (SeRC), National Bioinformatics Infrastructure Sweden (NBIS), and the European Union&apos;s <lb/>Horizon <lb/>research and innovation programme under grant agreement No <lb/>for the PhenoMeNal <lb/>project. <lb/></div>

			<div type="annex">Authors&apos; Contributions <lb/>OS and SL conceived the project and the idea of component-based work ow design. SL came up with the <lb/>idea of using Go and Flow-based programming principles to implement a work ow library, designed and <lb/>implemented the SciPipe library, and implemented case study work ows. MD contributed to the API design <lb/>and implemented case study work ows. JA implemented the TeX/PDF reporting function and contributed to <lb/>case study work ows. OS supervised the project. All authors read and approved the manuscript. <lb/></div>

			<div type="acknowledgement">Acknowledgments <lb/>We thank Egon Elbre, Johan Dahlberg and Johan Viklund for valuable feedback and suggestions regarding <lb/>the work ow API. We thank Rolf Lampa for valuable suggestions on the audit log feature. We also thank <lb/>colleagues at pharmb.io and on the SciLifeLab slack for helpful feedback on the API, and users on the Flow-<lb/>based programming mailing list for encouraging feedback. <lb/></div>

			<listBibl>Bibliography <lb/>[ ] Nils Gehlenborg, Se√°n I O&apos;donoghue, Nitin S Baliga, Alexander Goesmann, Matthew A Hibbs, Hiroaki <lb/>Kitano, Oliver Kohlbacher, Heiko Neuweger, Reinhard Schneider, Dan Tenenbaum, et al. Visualization <lb/>of omics data for systems biology. Nature methods, ( s):S , <lb/>. <lb/>[ ] Marylyn D Ritchie, Emily R Holzinger, Ruowang Li, Sarah A Pendergrass, and Dokyoon Kim. Methods <lb/>of integrating data to uncover genotype-phenotype interactions. Nature Reviews Genetics, ( ): , <lb/>. <lb/>[ ] Vivien Marx. Biology: The big challenges of big data. Nature, <lb/>( <lb/>): -, <lb/>. <lb/>[ ] Zachary D. Stephens, Skylar Y. Lee, Faraz Faghri, Roy H. Campbell, Chengxiang Zhai, Miles J. Efron, <lb/>Ravishankar Iyer, Michael C. Schatz, Saurabh Sinha, and Gene E. Robinson. Big data: Astronomical or <lb/>genomical? PLoS Biolo , ( ): -, <lb/>. <lb/>[ ] O. Spjuth, E. Bongcam-Rudlo f, G.C. Hern?ndez, L. Forer, M. Giovacchini, R.V. Guimera, A. Kallio, <lb/>E. Korpelainen, M.M. Ka?dula, M. Krachunov, D.P. Kreil, O. Kulev, P.P. ?abaj, S. Lampa, L. Pireddu, <lb/>S. Sch?nherr, A. Siretskiy, and D. Vassilev. Experiences with work ows for automating data-intensive <lb/>bioinformatics. Biolo Direct, ( ), <lb/>. <lb/>[ ] Daniel Blankenberg, Gregory Von Kuster, Nathaniel Coraor, Guruprasad Ananda, Ross Lazarus, Mary <lb/>Mangan, Anton Nekrutenko, and James Taylor. Galaxy: A Web-Based Genome Analysis Tool for Experi-<lb/>mentalists. John Wiley &amp; Sons, Inc., Hoboken, <lb/>. <lb/>[ ] Belinda Giardine, Cathy Riemer, Ross C. Hardison, Richard Burhans, Laura Elnitski, Prachi Shah, <lb/>Yi Zhang, Daniel Blankenberg, Istvan Albert, James Taylor, Webb Miller, W. James Kent, and Anton <lb/>Nekrutenko. Galaxy: A platform for interactive large-scale genome analysis. Genome Res., ( ): <lb/>-<lb/>, <lb/>. <lb/>[ ] B. Giardine, C. Riemer, R. C. Hardison, R. Burhans, L. Elnitski, P. Shah, Y. Zhang, D. Blankenberg, <lb/>I. Albert, J. Taylor, W. Miller, W. J. Kent, and A. Nekrutenko. Galaxy: a platform for interactive large-scale <lb/>genome analysis. Genome Res., , <lb/>. <lb/>[ ] Adam A. Hunter, Andrew B. Macgregor, Tamas O. Szabo, Crispin A. Wellington, and Matthew I. Bellgard. <lb/>Yabi: An online research environment for grid, high performance and cloud computing. Source Code Biol. <lb/>Med., ( ): -, <lb/>. <lb/>[ ] Johannes K√∂ster and Sven Rahmann. Snakemake-a scalable bioinformatics work ow engine. Bioinfor-<lb/>matics, ( ): <lb/>-<lb/>, <lb/>. <lb/>[ ] Paolo Di Tommaso, Maria Chatzou, Evan W Floden, Pablo Prieto Barja, Emilio Palumbo, and Cedric <lb/>Notredame. Next ow enables reproducible computational work ows. Nature Biotech, ( ): -, <lb/>. <lb/>[ ] Simon P. Sadedin, Bernard Pope, and Alicia Oshlack. Bpipe: a tool for running and managing bioinfor-<lb/>matics pipelines. Bioinformatics, ( ): <lb/>-<lb/>, <lb/>. <lb/>[ ] J√∂rgen Brandt, Marc Bux, and Ulf Leser. Cuneiform: a functional language for large scale scienti c data <lb/>analysis. In EDBT/ICDT Workshops, pages -, <lb/>. <lb/>[ ] Jon Ander Novella, Payam Emami Khoonsari, Stephanie Herman, Daniel Whitenack, Marco Capuccini, <lb/>Joachim Burman, Kim Kultima, and Ola Spjuth. Container-based bioinformatics with pachyderm. <lb/>. <lb/>[ ] Erik Bernhardsson, Elias Freider, and Arash Rouhani. spotify/luigi -GitHub. https://github.com/ <lb/>spotify/luigi. [Online; Accessed -July-<lb/>]. <lb/>[ ] Yolanda Gil and Varun Ratnakar. Dynamically generated metadata and replanning by interleaving work ow <lb/>generation and execution. In Semantic Computing (ICSC), <lb/>IEEE Tenth International Conference on, <lb/>pages <lb/>-. IEEE, <lb/>. <lb/>[ ] David K Rensin. Kubernetes-scheduling the future at cloud scale. <lb/>. <lb/>[ ] Johan Dahlberg, Johan Hermansson, Steinar Sturlaugsson, and Pontus Larsson. Arteria: An automation <lb/>system for a sequencing core facility. <lb/>. <lb/>[ ] J√∂rgen Brandt, Wolfgang Reisig, and Ulf Leser. Computation semantics of the functional scienti c <lb/>work ow language cuneiform. Journal of Functional Programming, :e , <lb/>. <lb/>[ ] Samuel Lampa, Jonathan Alvarsson, and Ola Spjuth. Towards agile large-scale predictive modelling in <lb/>drug discovery with ow-based programming design principles. Journal of Cheminformatics, ( ): , <lb/>. <lb/>[ ] Samuel Lampa. SciPipe source code repository at GitHub. https://github.com/scipipe/scipipe. [Online; <lb/>Accessed -July-<lb/>]. <lb/>[ ] Samuel Lampa, Martin Czugan, and Jonathan Alvarsson. scipipe/scipipe latest version -zenodo. https:// <lb/>doi.org/10.5281/zenodo.1157941, July <lb/>. <lb/>[ ] Douglas Crockford. JSON website. http://json.org/. [Online; Accessed -July-<lb/>]. <lb/>[ ] Samuel Lampa. SciPipe documentation. http://scipipe.org. [Online; Accessed -July-<lb/>]. <lb/>[ ] Yolanda Gil and Daniel Garijo. Towards Automating Data Narratives. Proceedings of the nd International <lb/>Conference on Intelligent User Interfaces -IUI &apos; , (February): -. <lb/>[ ] Lucas A M C Carvalho, Bakinam T Essawy, Daniel Garijo, Claudia Bauzer Medeiros, and Yolanda Gil. <lb/>Requirements for Supporting the Iterative Exploration of Scienti c Work ow Variants. <lb/>Workshop on <lb/>Capturing Scientific Knowledge (SciKnow), <lb/>. <lb/>[ ] Peter Breitenlohner and Han The Thanh. pdfTeX. http://www.tug.org/applications/pdftex. [Online; <lb/>Accessed -July-<lb/>]. <lb/>[ ] Heng Li and Richard Durbin. Fast and accurate short read alignment with Burrows-Wheeler transform. <lb/>Bioinformatics, ( ): <lb/>-<lb/>, July <lb/>. <lb/>[ ] Martin Dahl√∂, Douglas G Sco eld, Wesley Schaal, and Ola Spjuth. Tracking the ngs revolution: managing <lb/>life science research on shared high-performance computing clusters. GigaScience, ( ):giy , <lb/>. <lb/>[ ] Peter Amstutz, Michael R. Crusoe, Neboj≈°a Tijaniƒá, Brad Chapman, John Chilton, Michael Heuer, Andrey <lb/>Kartashov, Dan Leehr, Herv√© M√©nager, Maya Nedeljkovich, Matt Scales, Stian Soiland-Reyes, and Luka <lb/>Stojanovic. Common Work ow Language, v . . <lb/>. <lb/>[ ] Samuel Lampa, Martin Dahl√∂, Jonathan Alvarsson, and Ola Spjuth. SciPipe Demonstration work ows <lb/>source code repository at GitHub. https://github.com/pharmbio/scipipe-demo. [Online; Accessed <lb/>-July-<lb/>]. <lb/>[ ] Jonathan Alvarsson, Samuel Lampa, Wesley Schaal, Claes Andersson, Jarl E S Wikberg, and Ola Spjuth. <lb/>Large-scale ligand-based predictive modelling using support vector machines. Journal of Cheminformatics, <lb/>, <lb/>. <lb/>[ ] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A <lb/>Library for Large Linear Classi cation. Journal of Machine Learning Research, ( <lb/>): <lb/>-<lb/>, <lb/>. <lb/>[ ] Jean-Loup Faulon, Donald P Visco, and Ramdas S Pophale. The signature molecular descriptor. . using <lb/>extended valence sequences in qsar and qspr studies. Journal of chemical information and computer sciences, <lb/>( ): -, <lb/>. <lb/>[ ] National Center for Biotechnology Information. PubChem BioAssay Database. <lb/>. <lb/>[ ] Samuel Lampa, Jonathan Alvarsson, and Ola Spjuth. SciLuigi Case study work ow -GitHub. https:// <lb/>github.com/pharmbio/bioimg-sciluigi-casestudy. [Online; Accessed -July-<lb/>]. <lb/>[ ] Science for Life Laboratory. Sarek -an open-source analysis pipeline to detect germline or somatic <lb/>variants from whole genome sequencing. http://opensource.scilifelab.se/projects/sarek/, (Accessed: <lb/>/ / ). <lb/>[ ] Heng Li, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo <lb/>Abecasis, Richard Durbin, and <lb/>Genome Project Data Processing Subgroup. The sequence align-<lb/>ment/map format and samtools. Bioinformatics, ( ): <lb/>-<lb/>, <lb/>. <lb/>[ ] Broad Institute. Picard tools. http://broadinstitute.github.io/picard/, (Accessed: <lb/>/ / ). <lb/>[ ] Aaron McKenna, Matthew Hanna, Eric Banks, Andrey Sivachenko, Kristian Cibulskis, Andrew Kernytsky, <lb/>Kiran Garimella, David Altshuler, Stacey Gabriel, Mark Daly, and Mark A DePristo. The Genome Analysis <lb/>Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data. Genome Res., <lb/>( ): <lb/>-<lb/>, September <lb/>. <lb/>[ ] Simon Andrews. Fastqc -a quality control tool for high throughput sequence data. https://www. <lb/>bioinformatics.babraham.ac.uk/projects/fastqc/, (Accessed: <lb/>/ / ). <lb/>[ ] Alexander Dobin, Carrie A. Davis, Felix Schlesinger, Jorg Drenkow, Chris Zaleski, Sonali Jha, Philippe <lb/>Batut, Mark Chaisson, and Thomas R. Gingeras. Star: ultrafast universal rna-seq aligner. Bioinformatics, <lb/>( ): -, <lb/>. <lb/>[ ] Konstantin Okonechnikov, Ana Conesa, and Fernando Garc√≠a-Alcalde. Qualimap : advanced multi-<lb/>sample quality control for high-throughput sequencing data. Bioinformatics, ( ): -, <lb/>. <lb/>[ ] Yang Liao, Gordon K. Smyth, and Wei Shi. featurecounts: an e cient general purpose program for <lb/>assigning sequence reads to genomic features. Bioinformatics, ( ): -, <lb/>. <lb/>[ ] Philip Ewels, M√•ns Magnusson, Sverker Lundin, and Max K√§ller. Multiqc: summarize analysis results for <lb/>multiple tools and samples in a single report. Bioinformatics, ( ): <lb/>-<lb/>, <lb/>. <lb/>[ ] Go development team. Go FAQ: History of the project. https://golang.org/doc/faq#history. <lb/>[ ] Go development team. Go FAQ: Performance. https://golang.org/doc/faq#Performance. <lb/>[ ] J Paul Morrison. Flow-Based Programming: A new approach to application development. Self-published <lb/>via CreateSpace, Charleston, nd edition, May <lb/>. <lb/>[ ] Vladimir Sibirov. GoFlow source code repository at GitHub. https://github.com/trustmaster/goflow. <lb/>[Online; Accessed -July-<lb/>]. </listBibl>


	</text>
</tei>
