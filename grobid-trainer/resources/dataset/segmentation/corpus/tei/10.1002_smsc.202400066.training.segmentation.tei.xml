<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title level="a">Efficient Nebulization and Pulmonary Biodistribution of Polymeric Nanocarriers in an Acute Lung Injury Preclinical Model</title>
        <author>
          <persName>
            <forename>Anna</forename>
            <surname>Solé‐Porta</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Aina</forename>
            <surname>Areny‐Balagueró</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Marta</forename>
            <surname>Camprubí‐Rimblas</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Elena</forename>
            <surname>Fernández Fernández</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Andrew</forename>
            <surname>O’Sullivan</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Rossella</forename>
            <surname>Giannoccari</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Ronan</forename>
            <surname>MacLoughlin</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Daniel</forename>
            <surname>Closa</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Antonio</forename>
            <surname>Artigas</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Anna</forename>
            <surname>Roig</surname>
          </persName>
        </author>
      </titleStmt>
      <editionStmt>
        <edition>
          <date when="2025-10-25T18:12:49.702443Z">25.10.2025 18:12:49</date>
          <title>grobid.training.segmentation [default]</title>
          <idno type="fileref">10.1002$1$smsc.202400066</idno>
        </edition>
      </editionStmt>
      <publicationStmt>
        <publisher>Wiley</publisher>
        <availability>
          <licence target="https://creativecommons.org/licenses/by/4.0/"/>
        </availability>
        <date type="publication">2024</date>
        <idno type="DOI">10.1002/smsc.202400066</idno>
      </publicationStmt>
      <sourceDesc>
        <bibl>Anna Solé‐Porta, Aina Areny‐Balagueró, Marta Camprubí‐Rimblas, Elena Fernández Fernández, Andrew O’Sullivan, Rossella Giannoccari, Ronan MacLoughlin, Daniel Closa, Antonio Artigas, Anna Roig. (2024). Efficient Nebulization and Pulmonary Biodistribution of Polymeric Nanocarriers in an Acute Lung Injury Preclinical Model. Small Science, None(None), None. DOI: 10.1002/smsc.202400066</bibl>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <appInfo>
        <application version="1.0" ident="pdf-tei-editor" type="editor">
          <ref target="https://github.com/mpilhlt/pdf-tei-editor"/>
        </application>
        <application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-10-25T18:12:49.702443Z" type="extractor">
          <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
          <label type="revision">eb7768b</label>
          <label type="flavor">default</label>
          <label type="variant-id">grobid.training.segmentation</label>
          <ref target="https://github.com/kermitt2/grobid"/>
        </application>
      </appInfo>
    </encodingDesc>
    <revisionDesc>
      <change when="2025-10-25T18:12:49.702443Z" status="draft">
        <desc>Generated with createTraining API</desc>
      </change>
    </revisionDesc>
  </teiHeader>
  <text xmlns="http://www.tei-c.org/ns/1.0" xml:lang="en">
        <front>RESEARCH ARTICLE <lb/>Highly precise community science annotations of video <lb/>camera-trapped fauna in challenging environments <lb/>Mimi Arandjelovic 1,2 , Colleen R. Stephens 3 , Paula Dieguez 2,3 , Nuria Maldonado 3 , <lb/>Ga€ elle Bocksberger 3,4 , Marie-Lyne Despr es-Einspenner 5 , Benjamin Debetencourt 3,6 , <lb/>Vittoria Estienne 3,7 , Ammie K. Kalan 8 , Maureen S. McCarthy 3,9 , Anne-C eline Granjon 2,3 , <lb/>Veronika St€ adele 3,10,11 , Briana Harder 12 , Lucia Hacker 12 , Anja Landsmann 12,13 , Laura K. Lynn 12 , <lb/>Heidi Pfund 12 , Zuzana Ro ckaiov a 12 , Kristeena Sigler 12 , Jane Widness 12,14 , Heike Wilken 12 , <lb/>Antonio Buzharevski 3 , Adeelia S. Goffe 15 , Kristin Havercamp 16 , Lydia L. Luncz 17 , Giulia Sirianni 18 , <lb/>Erin G. Wessling 19,20 , Roman M. Wittig 3,21,22 , Christophe Boesch 3,6 &amp; Hjalmar S. K€ uhl 2,3,23,24 <lb/>1 <lb/>Department of Primate Behavior and Evolution, Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, Leipzig 04103, Germany <lb/>2 <lb/>German Centre for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Puschstrasse 4, Leipzig 04103, Germany <lb/>3 <lb/>Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, Leipzig 04103, Germany <lb/>4 <lb/>Senckenberg Biodiversity and Climate Research Centre (SBiK-F), Senckenberganlage 25, Frankfurt am Main 60325, Germany <lb/>5 Eco-corridors laurentiens, 517 rue Saint-Georges, Saint-J erôme, Qu ebec J7Z 5B6, Canada <lb/>6 <lb/>Wild Chimpanzee Foundation, Bleichertstr. 2, 04155, Leipzig, Germany <lb/>7 <lb/>Wildlife Conservation Society -Congo Program, 151 Avenue Charles de Gaulle, Brazzaville BP 14537, Republic of Congo <lb/>8 <lb/>Department of Anthropology, University of Victoria, Cornett Building, Victoria British Columbia, V8P 5C2, Canada <lb/>9 <lb/>Department of Biological Sciences, University of Southern California, 3616 Trousdale Parkway, Los Angeles California, 90089, USA <lb/>10 <lb/>School of Human Evolution and Social Change, Arizona State University, 900 S Cady Mall, Tempe Arizona, 85281, USA <lb/>11 <lb/>Institute of Human Origins, Arizona State University, 951 S Cady Mall, Tempe AZ, 85281, USA <lb/>12 <lb/>Zooniverse Community Scientist, c/o Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, Leipzig 04103, Germany <lb/>13 <lb/>Faculty of Medicine, Institute for Drug Discovery, Leipzig University, Bruederstrasse 34, Leipzig 04103, Germany <lb/>14 <lb/>Department of Anthropology, Yale University, New Haven, CT, USA <lb/>15 <lb/>School of Medicine Trinity College Dublin, Public Health and Primary Care, Dublin, Ireland <lb/>16 <lb/>Kyoto University, Wildlife Research Institute, 2-24 Tanaka-Sekiden-cho, Sakyo, Kyoto 606-8203, Japan <lb/>17 <lb/>Technological Primates Research Group, Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, Leipzig 04103, Germany <lb/>18 <lb/>Sapienza University of Rome, Piazzale Aldo Moro, 5, Rome 00185, Italy <lb/>19 <lb/>Department of Human Evolutionary Biology, Harvard University, 11 Divinity Avenue, Cambridge Massachusetts, 02138, USA <lb/>20 <lb/>Cognitive Ethology Laboratory, German Primate Center, Leibniz Institute for Primate Research, Kellnerweg 4, Goettingen 37077, Germany <lb/>21 <lb/>Ta€ ı Chimpanzee Project, CSRS, Abidjan 01 BP 1303, Côte d&apos;Ivoire <lb/>22 <lb/>Ape Social Mind Lab, Institute for Cognitive Sciences Marc Jeannerod, CNRS UMR 5229, 67 bd Pinel, Bron Cedex 69675, France <lb/>23 <lb/>Senckenberg Museum f€ ur Naturkunde G€ orlitz, Am Museum 1, G€ orlitz 02826, Germany <lb/>24 <lb/>International Institute Zittau, Technische Universit€ at Dresden, Markt 23, Zittau 02763, Germany <lb/>Keywords <lb/>Camera trap, citizen science, community <lb/>science, video, wildlife monitoring <lb/>Correspondence <lb/>Mimi Arandjelovic, Department of Primate <lb/>Behavior and Evolution, Max Planck Institute <lb/>for Evolutionary Anthropology, Deutscher <lb/>Platz 6, Leipzig 04103, Germany. Tel: +49 <lb/>341 3550; E-mail: arandjel@eva.mpg.de <lb/>Funding Information <lb/>We thank the Heinz L. Krekeler Foundation, <lb/>the Max Planck Society Innovation Fund, the <lb/>Max Planck Society, the Robert Bosch <lb/>Foundation, the German Centre for <lb/>Integrative Biodiversity Research (iDiv) Halle-<lb/>Abstract <lb/>As camera trapping grows in popularity and application, some analytical limita-<lb/>tions persist including processing time and accuracy of data annotation. Typi-<lb/>cally images are recorded by camera traps although videos are becoming <lb/>increasingly collected even though they require much more time for annotation. <lb/>To overcome limitations with image annotation, camera trap studies are <lb/>increasingly linked to community science (CS) platforms. Here, we extend pre-<lb/>vious work on CS image annotations to camera trap videos from a challenging <lb/>environment; a dense tropical forest with low visibility and high occlusion due <lb/>to thick canopy cover and bushy undergrowth at the camera level. Using the <lb/>CS platform Chimp&amp;See, established for classification of 599 956 video clips <lb/>from tropical Africa, we assess annotation precision and accuracy by comparing <lb/>classification of 13 531 1-min video clips by a professional ecologist (PE) with <lb/>output from 1744 registered, as well as unregistered, Chimp&amp;See community <lb/>scientists. We considered 29 classification categories, including 17 species and <lb/>12 higher-level categories, in which phenotypically similar species were grouped. <lb/>702 <lb/>ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/>This is an open access article under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and <lb/>distribution in any medium, provided the original work is properly cited, the use is non-commercial and no modifications or adaptations are made. <lb/>Jena-Leipzig, the iDiv Data &amp; Code Unit for <lb/>assistance with curating and archiving the <lb/>camera trap videos, the Deutsche <lb/>Forschungsgemeinschaft (DFG; FZT-118), the <lb/>Centre Suisse de Recherches Scientifiques, <lb/>and the Centre for Forest Research-Fonds de <lb/>Recherche Qu ebec Nature et Technologies <lb/>International internship program, for financial <lb/>support. <lb/>Editor: Dr. Marcus Rowcliffe <lb/>Associate Editor: Dr. Francesco Rovero <lb/>Received: 29 July 2023; Revised: 27 February <lb/>2024; Accepted: 15 April 2024 <lb/>doi: 10.1002/rse2.402 <lb/>Remote Sensing in Ecology and <lb/>Conservation 2024;10 (6):702-724 <lb/>Overall, annotation precision was 95.4%, which increased to 98.2% when aggre-<lb/>gating similar species groups together. Our findings demonstrate the compe-<lb/>tence of community scientists working with camera trap videos from even <lb/>challenging environments and hold great promise for future studies on animal <lb/>behaviour, species interaction dynamics and population monitoring. <lb/></front>

        <body>Introduction <lb/>Photo and video camera trapping is a widely used <lb/>approach to assess the diversity and abundance (Bessone <lb/>et al., 2020; Gogarten et al., 2020; Mattioli et al., 2018), <lb/>interactions (Ahumada et al., 2011; Droissart et al., 2021; <lb/>Unger et al., 2019), demography (Galvis et al., 2014; Head <lb/>et al., 2013), social structure (Maselli et al., 2014; Tichon <lb/>et al., 2020), health status (Hockings et al., 2021; Muneza <lb/>et al., 2019), and behaviour (Caravaggi et al., 2020; K€ uhl <lb/>et al., 2016; Palmer &amp; Packer, 2021) of species. Even small <lb/>arrays of camera traps quickly generate large amounts of <lb/>footage, which frequently exceed data processing capacity <lb/>(Burton et al., 2015; Swinnen et al., 2014). To overcome <lb/>this limitation, camera trap studies are increasingly linked <lb/>to community science (CS) platforms for data annotation <lb/>(Magle et al., 2019; Pardo et al., 2021; Swanson <lb/>et al., 2015; Swanson et al., 2016). CS is broadly defined <lb/>as <lb/>scientific <lb/>studies <lb/>which <lb/>collaborate <lb/>with <lb/>non-professional researchers who engage in data collec-<lb/>tion, curation, analyses or study design, and can take <lb/>many forms (Balazs &amp; Morello-Frosch, 2013; Fraisl <lb/>et al., 2022). CS has been identified as a critical compo-<lb/>nent of open science, providing opportunities for equita-<lb/>ble and inclusive participation in knowledge creation <lb/>(Frigerio et al., 2018; Kosmala et al., 2016; UNESCO Dig-<lb/>ital Library, 2021; Wehn, 2020). One form of CS involves <lb/>harnessing the power of &apos;crowd wisdom&apos;, where multiple <lb/>annotators evaluate the same subject and may even be <lb/>preferable to single &apos;expert&apos; annotators (Debetencourt <lb/>et al., 2023; Zett et al., 2022). This crowdsourcing of CS <lb/>work is sometimes done voluntarily by participants, <lb/>meaning project leaders need only invest time and <lb/>finances into CS project infrastructure and oversight and <lb/>not the many months required for data acquisition, anno-<lb/>tation and/or analysis. It is important to note that CS <lb/>project organizers should exercise sensitivity however in <lb/>avoiding an extractive relationship with participants <lb/>(Cooper et al., 2021; Eitzel et al., 2017; Ottinger, 2017). <lb/>Global communities of engaged community scientists <lb/>have demonstrated high-quality performance in image <lb/>annotation, including identification of species and behav-<lb/>iour, assignment of age and sex categories or counts of <lb/>individuals (Debetencourt et al., 2023; Egna et al., 2020; <lb/>Kosmala et al., 2016; McCarthy et al., 2021; Swanson <lb/>et al., 2016; Thel et al., 2021). Variation in annotation <lb/>performance among CS projects can relate to habitat dif-<lb/>ferences, partial occlusion of animals, number of images <lb/>in a series, and sensitivity of the triggering device (Egna <lb/>et al., 2020). Most camera trap projects record still images <lb/>and thus most studies on annotation performance of <lb/>associated CS projects are therefore based on still images <lb/>rather than video clips. <lb/>Videos (and images) from challenging habitats, such as <lb/>rainforests, can provide several obstacles to CS engage-<lb/>ment (Egna et al., 2020). Videos are often dark and <lb/>grainy, false triggers from dense foliage at a site result in <lb/>many &apos;boring&apos; blank videos, complex scenes may make <lb/>detecting species more difficult and high occlusion often <lb/>means species can only be reliably identified when close <lb/>to the camera which may increase participant frustration. <lb/>Further, videos require increased technical requirements, <lb/>such as increased power consumption, memory space and <lb/>duration for data processing as compared to still images. <lb/>Yet, despite these hurdles, camera trap videos offer a <lb/>number of advantages over still images including <lb/>increased temporal resolution and audio-visual record-<lb/>ings. This format offers advantages for studies on species <lb/></body>
    
        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

        <page>703 <lb/></page>

        <note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

        <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

        <body>interactions and behaviour (Kalan et al., 2019), or when <lb/>deriving key parameters for estimating animal abundance, <lb/>such as movement speed (Rowcliffe et al., 2016) or obser-<lb/>vation distance (Howe et al., 2017). Human eyes are <lb/>adapted to perceive movement (Walls, 1942) so videos <lb/>should facilitate animal detections in general. Further, <lb/>videos allow many views of an animal, often from multi-<lb/>ple angles, and detection of species-characteristic behav-<lb/>iours (e.g. tail flick direction in different duiker species <lb/>Dubost, 1980) as well as more opportunity to view <lb/>unique traits for individual identification (Debetencourt <lb/>et al., 2023; McCarthy et al., 2021) for tracking and <lb/>mark-recapture <lb/>population <lb/>assessment <lb/>(Head <lb/>et al., 2013). The trade-off between increased temporal <lb/>resolution and technical demands also requires adaptation <lb/>of workflows when annotating camera trap videos with <lb/>CS. For example, the required observer attention span is <lb/>different when annotating single images compared to <lb/>video clips of longer duration. Similarly, bandwidth may <lb/>impose restrictions on data transfer with the consequence <lb/>of limiting the CS community to those who are able to <lb/>access the videos. <lb/>Here, we assess annotation performance of community <lb/>scientists on the CS platform Chimp&amp;See (www.chim-<lb/>pandsee.org) hosted by the Zooniverse (www.zooniverse.-<lb/>org) and present an open subset of our growing annotated <lb/>video camera trap data. Chimp&amp;See was established to <lb/>process 599 956 1-min video clips from a large research <lb/>project, The Pan African Programme: The Cultured Chim-<lb/>panzee (PanAf) (Vaidyanathan, 2011), which aims to <lb/>describe diversity patterns and understand diversifying <lb/>mechanisms among populations of African great apes <lb/>(http://panafrican.eva.mpg.de/). As part of this project, <lb/>more than 1569 camera traps were installed at 3288 loca-<lb/>tions at 46 temporary research sites across the equatorial <lb/>rainforest-savanna gradient in 17 countries to record visit-<lb/>ing wildlife species. Since April 2015, over 31 500 regis-<lb/>tered community scientists have participated in the <lb/>annotation of video clips and work is still ongoing. <lb/>Twenty-nine categories are available for classification of <lb/>species (n = 17) or species groups (n = 12). Species groups <lb/>are defined for phenotypically similar species, for example, <lb/>Ogilby&apos;s (Cephalophus ogilbyi) and bay (Cephalophus dorsa-<lb/>lis) duikers are combined into the category &apos;red duiker&apos;. <lb/>To assess the accuracy of CS classifications, we used a <lb/>subset of 13 531 videos collected as part of a previous <lb/>study at 83 camera trap locations between June 2014 and <lb/>March 2015 in Taї National Park, across an area of 40 km 2 <lb/>and fully annotated by a professional ecologist (Despr es-<lb/>Einspenner et al., 2017) All 1-min videos were segmented <lb/>into 15-s clips to facilitate on-line transfer times and avoid <lb/>decreasing annotator attention. Each clip was shown to up <lb/>to 15 different community scientists. Annotations were <lb/>aggregated into a consensus dataset which was compared <lb/>to the professional ecologist (PE) dataset. <lb/>Materials and Methods <lb/>Chimp&amp;See platform <lb/>Chimp&amp;See (chimpandsee.org) is an online CS platform <lb/>designed and hosted by The Zooniverse (zooniverse.org) <lb/>for annotating camera trap videos. Chimp&amp;See was con-<lb/>ceived for annotating the PanAf video collection of <lb/>599 956 1-min long videos from 46 locations (Fig. 1) and <lb/>is therefore focused on species living across the chimpan-<lb/>zee range. Videos from each PanAf location are uploaded <lb/>in a unique workflow exclusively for that site. In general, <lb/>locations are not referred to by their true name but are <lb/>given a two-word randomized code name with unique <lb/>initials (e.g. Restless Star) in order to assuage concerns <lb/>regarding geotagged photos and videos being used for <lb/>directing poaching activities (Marshall et al., 2020). Com-<lb/>munity scientists are told from which of the four chim-<lb/>panzee subspecies ranges (Fig. 1) the videos originate. <lb/>Data collection at Chimp&amp;See began on April 22nd, 2015 <lb/>and ran continuously until October 22, 2018 on the Zoo-<lb/>niverse Ouroboros platform. The code used to create the <lb/>original Chimp&amp;See platform is available at https:// <lb/>github.com/zooniverse/chimpandsee. The site was then <lb/>redeveloped with the Zooniverse project builder, relocated <lb/>to a new Zooniverse platform (Panoptes), and relaunched <lb/>on July 15, 2019 (see details in Data S1 &apos;Zooniverse pro-<lb/>ject design interface and data management&apos;). Collection <lb/>of annotation data is currently ongoing for the foreseeable <lb/>future. As of June 2023, 31 518 registered community sci-<lb/>entists, along with unregistered individuals, have accom-<lb/>plished 6 918 425 classifications and thereby have <lb/>annotated 300 466 videos. Access to this evolving, larger <lb/>annotated dataset can be provided upon request. <lb/>Classification interface <lb/>One-minute is the standard recording time for camera <lb/>trap videos; however, 15-s clips were assumed to be easier <lb/>for community scientists to engage with and watch in <lb/>their entirety. Therefore, each 1-min video is cut into <lb/>four 15-s clips, each with its own identification code. <lb/>Fifteen-second clips are compressed to a maximum file <lb/>size of 1 MB prior to upload such that the resolution for <lb/>the 15-s clips is generally lower than for the original <lb/>videos. Furthermore, three sets of nine video stills are <lb/>randomly generated from every 15-s clip. Each set of nine <lb/>stills is then made into a 3 9 3 image collage and pre-<lb/>sented as a preview before the clip plays. The intent here <lb/>was to aid viewers to quickly detect blank videos based <lb/></body>
    
        <page>704 <lb/></page>
    
        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

        <note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

        <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

        <body>on these nine stills and to move on to the next annota-<lb/>tion if a video appeared blank. The still preview was a <lb/>feature community scientists could shut off, therefore not <lb/>all annotators chose to see the preview images. <lb/>Community scientists are then shown a 15-s clip and <lb/>asked to annotate which animals are present or whether <lb/>the clip was blank (&apos;nothing here&apos; classification category). <lb/>To obtain a balance between specificity (annotating to the <lb/>species level) and keeping the task accessible and interest-<lb/>ing to a broad group of people (fewer categories), we pre-<lb/>sented community scientists with a choice of 29 <lb/>classification categories. We gave unique-looking species <lb/>their own category and aggregated species that were more <lb/>difficult to differentiate phenotypically (Table 1). Com-<lb/>munity scientists are not given any prior training with <lb/>regard to animal or behavioural identification. There are <lb/>however descriptions with example photos shown with <lb/>each selection, as well as a field guide and other resources <lb/>for animal and behavioural identification on the platform. <lb/>If animals are detected, community scientists are also <lb/>asked to report the number of individuals present (1, 2, <lb/>3, 4 or 5+), the behaviour(s) exhibited (Table S1), and, <lb/>for chimpanzees only, individual age and sex (McCarthy <lb/>et al., 2021). This information, date and time of the clas-<lb/>sification, video clip ID, and the community scientist&apos;s <lb/>login name (or automatically generated ID based on <lb/>unique IP addresses if the community scientist was not <lb/>logged in) was recorded in a mongoDB database. Clips <lb/>were not shown to the same login credentials more than <lb/>once (but see &apos;Data cleaning&apos;). We refer to this informa-<lb/>tion as the classification data. <lb/>Clips are in circulation and presented to viewers until <lb/>one of the following retirement criteria is met: the first <lb/>three classifications are blank, the first seven classifications <lb/>agree on the same animal category (perfect agreement), <lb/>or the clip reaches 15 classifications total. <lb/>Professional ecologist (PE) dataset <lb/>For this study, to evaluate the precision and accuracy of <lb/>annotations by community scientists, we uploaded a set <lb/>of 13 531 camera trap videos from 80 cameras (Bushnell <lb/>Trophy Cam HD Max 8MP Trail Camera model <lb/>119576c) to the Chimp&amp;See platform that were recorded <lb/>in the same format as the PanAf Chimp&amp;See dataset but <lb/>from the Ta€ ı Chimpanzee Project, Ta€ ı National Park, <lb/>Côte d&apos;Ivoire. These videos had been previously anno-<lb/>tated by a professional ecologist (PE) in another study <lb/>(Despr es-Einspenner et al., 2017) (PE dataset). Although <lb/>PE datasets can contain errors (Zett et al., 2022) (further <lb/>discussed below), we benchmark our results against this <lb/>traditionally used standard method of annotation. The <lb/>site is located in a challenging environment: a dense trop-<lb/>ical forest with low light and visibility and high occlusion <lb/>due to dense canopy cover and bushy undergrowth at the <lb/>camera level (1 m above ground). <lb/>A total of 1744 registered community scientists and <lb/>11 167 different unregistered IP addresses provided <lb/>Figure 1. Map of PanAf sites where camera trap videos were collected. Circles are locations where all videos have been annotated on <lb/>Chimp&amp;See, and triangles are sites that still require annotation. The star shows the Ta€ ı Chimpanzee Project (TCP) from which the professional <lb/>ecologist (PE) dataset of videos was recorded. <lb/></body>

        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

        <page>705 <lb/></page>

        <note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

        <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

        <body>Table 1. Original classification categories. Descriptions of each species originally provided to Chimp&amp;See community scientists. <lb/>Chimp&amp;See <lb/>category <lb/>Taxonomy <lb/>Description <lb/>Bird <lb/>Aves spp. <lb/>All birds can be classified using this option. Birds have wings, feathers, and beaks; most are <lb/>capable of flight. A wide variety of birds can be found throughout Africa. Examples of birds <lb/>seen on Chimp &amp; See include guineafowl, ibis, hornbills, and rails. Usually seen during the <lb/>day or at dawn/dusk. <lb/>Cattle <lb/>Bos taurus &amp; B.indicus <lb/>Domestic cattle (cows, bulls, and steers) can sometimes be seen in these videos. Cattle are <lb/>large ungulates typically raised as livestock. Often horned, they can vary in colour, but are <lb/>most often brown, black, tan, and/or white. Branding may be visible on side/flank. Usually <lb/>seen during the day. <lb/>Chimpanzee <lb/>Pan troglodytes <lb/>This large primate, a close relative of humans, has no tail and is usually seen on the ground. <lb/>The hair is most often black, though it can appear grey or yellow-grey, especially on the <lb/>lower back. The face, ears, palms, and soles are hairless and skin colour varies from <lb/>peachy-pink to black. They most often travel by knuckle-walking on all fours and are <lb/>occasionally seen in trees. Males are slightly larger than females; infants have a white spot <lb/>on rear. Almost always seen during the day. <lb/>Dark duiker <lb/>Cephalophus spp. <lb/>Use this option to mark any duikers that are dark grey, black, or dark brown in colour. Dark <lb/>duikers include the yellow-backed duiker, a large duiker notable for a bright yellowish stripe <lb/>on the back of a brown coat with a partially yellow muzzle, and the black duiker, which is <lb/>medium-sized, solid black on the body, fading into red on the head, and with a white tail <lb/>tip. Like other duikers, they have arched backs, stocky bodies and slender legs. <lb/>Yellow-backed duikers are more often seen at night, while black duikers are seen during the <lb/>day or at dawn/dusk. <lb/>Elephant <lb/>Loxodonta spp. <lb/>This massive, grey, thick-skinned animal is famous for its very large ears, long trunk, and ivory <lb/>tusks. When not fully in frame, it can still be identified by its powerful, vertically positioned <lb/>legs and its leathery, wrinkled skin. Seen both at night and during the day.&apos; <lb/>Forest buffalo Syncerus caffer <lb/>Smaller (250-320 kg) subspecies of the African buffalo. Reddish-brown hide darkens to black <lb/>around the face and lower legs, with a black dorsal stripe. Horns curl straight backwards in <lb/>a C shape, with large, sometimes tufted ears. Solid, robust build with relatively short and <lb/>thickset legs; typically carries head low. More often seen at night, but sometimes active <lb/>during the day. <lb/>Giant forest <lb/>hog <lb/>Hylochoerus meinertzhageni <lb/>The largest species of wild pig. Most identifiable by its size, coat of very long black hairs <lb/>(thinner in older hogs), and upward-curved tusks that are proportionally smaller than a <lb/>warthog&apos;s. Skin colour is dark brown. Males have large protruding swellings under each eye. <lb/>Almost always seen during the day or at dawn/dusk. <lb/>Gorilla <lb/>Gorilla spp. <lb/>Like chimpanzees, gorillas are apes, but much bigger and more powerfully built. Gorillas also <lb/>have black skin and faces throughout their lives, while chimpanzees are born with pink skin. <lb/>Gorillas have black/brown coats, extremely muscular arms, and large heads. Males are <lb/>larger, have silver-coloured backs, and large domed crests on top of their heads. Almost <lb/>always seen during the day. Gorillas are not found at any sites in Region A (West Africa). <lb/>Hippopotamus Hippopotamus amphibius &amp; <lb/>Choeropsis liberiensis <lb/>Large and round with short legs and smooth, shiny skin that appears dark grey to pink. Small <lb/>ears and a massive, wide mouth. Short, thick tail is trimmed with black bristles. Mostly seen <lb/>at night. <lb/>Human <lb/>Homo sapiens <lb/>Human beings may occasionally be seen in the videos: researchers, local residents, or even <lb/>poachers. Mark any humans with this tag. When the camera is being methodically adjusted <lb/>by the field team, but they are not in view, you can mark human too.&apos; <lb/>Hyena <lb/>Hyaenidae <lb/>Looks dog-like. Broad head, with large pointed ears; body slopes dramatically from shoulder <lb/>to hip. Two species in study range: spotted and striped. Spotted hyenas have speckled <lb/>grey-red coats. Striped hyenas are slightly smaller, with dirty-grey, striped coats. <lb/>Jentink&apos;s <lb/>duiker <lb/>Cephalophus jentinki <lb/>Duiker with unique coloration: black head and shoulders, thin white band behind shoulders, <lb/>and grey rest of body. Longer horns angling straight back from head. One of the largest <lb/>species of duikers, with an extremely solid body. Almost always seen at night. <lb/>Large <lb/>ungulate <lb/>Ungulata spp. <lb/>Use this option to mark any large hoofed mammal other than those with separate categories; <lb/>for instance, sitatungas, bongos, okapi, roan antelope, etc. Water chevrotains are medium-<lb/>(Continued) <lb/></body>

        <page>706 <lb/></page>

        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

        <note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

        <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

        <body>Table 1. Continued. <lb/>Chimp&amp;See <lb/>category <lb/>Taxonomy <lb/>Description <lb/>sized, but get marked in this category. Some species are more likely to be seen during the <lb/>day, and others at night. <lb/>Leopard <lb/>Panthera pardus <lb/>Muscular golden big cat with black rosettes. Spotted face, no black lines, with small, round <lb/>ears. Long, spotted tail has bright white fur underneath the tip, which is easy to see when <lb/>they curl their tails upward. Melanistic variant has mostly (or fully) black coat. Seen both at <lb/>night and during the day.&apos; <lb/>Lion <lb/>Panthera leo <lb/>Massive, muscular cats. They are tawny coloured with paler underparts; cubs show some <lb/>spots, especially on their bellies and legs. They have a long tail with smooth fur and a dark <lb/>tuft on its tip. Males have manes that get darker and thicker with age.&apos; <lb/>Other <lb/>(primate) <lb/>Cercopithecidae &amp; Lorisoidea <lb/>spp <lb/>Non-ape primates are different from apes in several ways. They typically are smaller, with tails, <lb/>less broad chests, and less upright posture. African monkeys have non-prehensile tails, hind <lb/>legs longer than forearms, and downward-pointing nostrils. Coloration varies between <lb/>species. Africa is also home to galagos (sometimes called bushbabies) and pottos, two kinds <lb/>of small primitive primate. Monkeys are frequently seen in groups and during the day or at <lb/>dawn/dusk. Galagos and pottos are usually seen alone and at night. <lb/>Other (non-<lb/>primate) <lb/>n/a <lb/>Mark any animal that does not fall into the other categories as &apos;other non-primate&apos;. This <lb/>includes cat-like viverrids like the civet and genet (almost always seen at night), as well as <lb/>honey badgers (night and day), hyrax (night and day), hares (night), and bats (night). Hyrax <lb/>can be distinguished from rodents by their lack of a tail. Domestic animals other than cattle <lb/>(e.g. dogs, goats, sheep) should be marked in this category as well. Please mark &apos;Nothing <lb/>here&apos; for insects and fires, but feel free to tag them on the talk page! <lb/>Pangolin <lb/>Manidae spp. <lb/>Also called &apos;scaly anteater&apos;, this unique creature&apos;s body is covered from snout to long tail in <lb/>overlapping earth-toned plates. Multiple species of pangolin are native to Africa, ranging in <lb/>size from around 2 kg to over 30 kg. Almost always seen at night.&apos; <lb/>Porcupine <lb/>Hystricidae spp. <lb/>Porcupines are short, rounded creatures covered from head to tail with long quills. Two <lb/>species of porcupine are found in the study area: the crested porcupine with long quills on <lb/>the back and sides that are raised into a crest, and the smaller brush-tailed porcupine, which <lb/>has a small tuft of quills at the end of its thin tail. Both species are almost always seen at <lb/>night. <lb/>Red duiker <lb/>Sylvicapra spp. &amp; some <lb/>Cephalophus spp. <lb/>Use this option to mark small to medium duikers with chestnut-red fur; for example: the bush <lb/>duiker which is small and reddish-brown, or the bay duiker, notable for its red body colour <lb/>and black stripe down its back. Like other duikers, they have arched backs, stocky bodies <lb/>and slender legs. Some species are seen mainly at night, and others during the day. <lb/>Red river hog <lb/>Potamochoerus spp. <lb/>Pudgy pig-like animal notable for its bright red fur and its curled, pointed, elfin ears with <lb/>white ear-tufts. The large muzzle is black with two small tusks and white markings around <lb/>the eyes. Longer fur on flanks and underbelly. Has a line of spiky blonde hair down the <lb/>spine. Seen both at night and during the day. Bushpigs, close relatives of red river hogs, <lb/>should also be marked using this classification. <lb/>Reptile <lb/>Reptilia spp. <lb/>Reptiles that may be found in Africa include lizards, snakes, turtles, and crocodiles. Reptiles <lb/>typically have shells or scales and are often coloured in earthtones (though some snakes may <lb/>have vibrant coloration). Mark all reptiles as &apos;reptile&apos;. <lb/>Rodent <lb/>Rodentia <lb/>Rodents of Africa include mice, squirrels, gerbils, and rats, but not hares, which should be <lb/>marked as other (non-primate). These animals are typically small, with short limbs, thick <lb/>bodies, and long tails. Rats and mice are almost always seen at night, while squirrels are <lb/>almost always seen during the day or at dawn/dusk. (Note: please mark porcupines <lb/>separately.) <lb/>Small <lb/>antelope <lb/>Bovidae spp. <lb/>Use this option to mark any small antelope other than a listed type of duiker; for instance: <lb/>bushbuck, royal antelope, dik-dik, oribi, pygmy antelope, reedbuck, etc. <lb/>Small cat <lb/>Felinae spp. <lb/>Several species of small felines can be found in Africa, including the caracal, the serval, the <lb/>African golden cat, and the African wildcat. Any cat smaller than a leopard or a cheetah can <lb/>be classified as a &apos;small cat&apos;. Please note that viverrids, like civets and genets, are not cats, <lb/>and should be marked as &apos;other (non-primate)&apos;. <lb/>(Continued) <lb/></body>

        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

        <page>707 <lb/></page>

        <note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

        <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

        <body>equivalent annotations on the videos represented in the <lb/>PE dataset. <lb/>The 13 531 1-min camera trap videos at their original <lb/>1280 9 720 p resolution are available for download <lb/>through the German Centre for Integrative Biodiversity <lb/>Research (iDiv) Data Portal at https://idata.idiv.de/ (Data-<lb/>set id: 3507 and accession number: 10.25829/idiv.3507-<lb/>yo9f8q). Data Structure is explained in Data Accessibility. <lb/>The PE data and all associated CS data (classification <lb/>data, consensus data, hashtag data, and links to the 15-s <lb/>clips, see below) are available at https://github.com/ <lb/>Chimp-and-See/ChimpandSee_Species_ID_MS_2024 and <lb/>archived at https://zenodo.org/records/7046745 (accession <lb/>number: 10.5281/zenodo.7046745). <lb/>Data cleaning, video consensus, precision <lb/>and accuracy <lb/>Data was cleaned and aggregated as the PE dataset anno-<lb/>tations were made on full 1-min videos and community <lb/>science classifications on 15-s clips (Fig. 2a, and see &apos;Clip <lb/>consensus&apos; in Data S1 for details). That is, to make the <lb/>datasets comparable, the responses from the four clips per <lb/>video in the classification dataset were aggregated into a <lb/>single result and subsequently compared to the PE dataset <lb/>classifications. <lb/>Consensus video animal classifications were then com-<lb/>pared to the PE dataset classifications to determine the <lb/>overall and category-specific accuracy of annotations by <lb/>community scientists. Accuracy was calculated as the <lb/>number of videos with matching annotations divided by <lb/>the total number of videos. Precision was calculated for <lb/>each animal category as the number of consensus video <lb/>classifications with a proportion agreement to the PE <lb/>classifications of 50% or more divided by the total num-<lb/>ber of consensus videos in that animal category. That is, <lb/>for all values of proportion consensus agreement above <lb/>50% the community scientists are considered to have <lb/>identified the correct species. For values below 50%, the <lb/>proportion of votes that identified the correct category <lb/>irrespective of the final consensus represents the propor-<lb/>tion consensus agreement. For example, if the video con-<lb/>tained a chimpanzee, but 80% of the classifications were <lb/>for bird and 20% for chimpanzee, the video would be <lb/>considered a false positive for bird with 20% proportion <lb/>consensus agreement to the correct classification <lb/>(chimpanzee). <lb/>The false negative rate for an animal category is calcu-<lb/>lated as the proportion of videos truly containing that <lb/>animal that was incorrectly annotated as blank, non-<lb/>consensus, or a wrong consensus by the community sci-<lb/>entists. The false positive rate for an animal category is <lb/>calculated as the total number of videos incorrectly anno-<lb/>tated as that category by the community scientists divided <lb/>by the total number of videos not containing that cate-<lb/>gory. To determine whether categories that were more <lb/>common were more likely to have incorrect classifica-<lb/>tions, we correlated the number of false positive videos <lb/>per category with that category&apos;s commonness, defined as <lb/>the number of videos in which that category was detected <lb/>in the PE dataset, using a Kendall&apos;s tau-b correlation <lb/>(Kendall, 1945). <lb/>We similarly also aggregated and analysed the accuracy <lb/>of community scientists at determining the number of <lb/>individuals of a species present in a video (Supplemental <lb/>Methods &apos;Analysis of counts&apos;). <lb/>Table 1. Continued. <lb/>Chimp&amp;See <lb/>category <lb/>Taxonomy <lb/>Description <lb/>Small grey <lb/>duiker <lb/>Philantomba maxwellii &amp; <lb/>monticola <lb/>Some of the smallest antelopes. Coat ranges from light brown to blue-grey with paler chest <lb/>and underbelly. Small spiky horns in most males and some females. Stocky body, arched <lb/>back, large hindquarters and thin legs. Usually seen during the day or at dawn/dusk. <lb/>Warthog <lb/>Phacochoerus africanus <lb/>This pig-like animal has a grey body covered sparsely with darker hairs, and mane of long, <lb/>wiry hairs along its neck and back. Its tail is thick with a black tassel. It has tusks that curve <lb/>up around its snout. More often seen during the day, but sometimes seen at night. <lb/>Wild dog <lb/>Lycaon pictus <lb/>Social pack canine with tall, solid build and mottled coat of blacks, browns, reds, and whites. <lb/>Muzzle is black and tail is typically white-tipped. Build is similar to domestic dogs and lacks <lb/>hyenas&apos; sloped backs. <lb/>Zebra duiker <lb/>Cephalophus zebra <lb/>Easily identifiable by the unique dark zebra-like stripes that cover its chestnut-coloured back. <lb/>Like other duikers, it has an arched back and stocky body with slender legs. The head has <lb/>conical horns, the muzzle is black, and the lower jaw and undersides are white. Usually seen <lb/>during the day or at dawn/dusk. <lb/>Nothing here <lb/>Blank videos <lb/>Choose this option if you see only insects, fire or truly nothing at all. <lb/></body>

        <page>708 <lb/></page>

        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

        <note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

        <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

        <body>no <lb/>no <lb/>13,531 videos <lb/>11,894 videos <lb/>11,260 videos <lb/>PE data set <lb/>Blank Non-blank Total <lb/>Chimp&amp;See data set <lb/>Blank <lb/>2559 <lb/>1,106 <lb/>(12.8%) <lb/>3,665 <lb/>Non-blank <lb/>51 <lb/>(2.0%) <lb/>7,544 7,595* <lb/>Total <lb/>2,610 <lb/>8,650 11,260 <lb/>C&amp;S animal categories <lb/>615 <lb/>19 <lb/>634 <lb/>PE data set <lb/>Blank Non-blank Total <lb/>Chimp&amp;See data set <lb/>Blank <lb/>2,559 <lb/>1,106 <lb/>(12.1%) <lb/>3,665 <lb/>Non-blank <lb/>54 <lb/>(2.1%) <lb/>8,067 8,121* <lb/>Total <lb/>2,613 <lb/>9,173 11,786 <lb/>&quot;ungulate&quot; <lb/>93 <lb/>15 <lb/>108 <lb/>11,786 videos <lb/>11,772 videos <lb/>PE data set <lb/>Blank Non-blank Total <lb/>Chimp&amp;See data set <lb/>Blank <lb/>2,559 <lb/>1,106 <lb/>(12.1%) <lb/>3,665 <lb/>Non-blank <lb/>52 <lb/>(2.0%) <lb/>8,055 8,107* <lb/>Total <lb/>2,611 <lb/>9,161 11,772 <lb/>&quot;duiker&quot;,&quot;hoofed&quot; <lb/>107 <lb/>15 <lb/>122 <lb/>Clip and video consensus <lb/>No consensus in any clip: <lb/>2+ consensus categories: <lb/>Total videos removed: <lb/>Umbrella categories <lb/>*Precision 95.4% (N=7,243) <lb/>*Precision 98.1% (N=7,949) <lb/>*Precision 98.2% (N=7,976) <lb/>&quot;hog&quot; <lb/>Data cleaning PE data set, remove videos: <lb/>Expert annotated as unidenƟfiable (N=55) <lb/>(N= 436 videos) <lb/>Expert annotated as mulƟspecies (N=264) <lb/>Expert annotated duiker category (N=117) <lb/>Debugging, remove clips: <lb/>16+ classificaƟons (N=156 videos) <lb/>(N= 1,138 videos) <lb/>Remove mulƟple classificaƟon &amp; aggregate mulƟple lines from single CS annotators <lb/>Only 1 or 2 blank classificaƟons (N=982) <lb/>Data cleaning CS data, remove clips: <lb/>Clips with median number of species &gt;1 (N=61 videos) <lb/>(N=63 videos) <lb/>Clips with &quot;presence undetermined&quot; for 4/4 clips (N=2 videos) <lb/>yes <lb/>classificaƟon data <lb/>hashtag consensus matches <lb/>video consensus? <lb/>consensus classificaƟon videos <lb/>no consensus clips (0/4) <lb/>hashtag consensus across clips? <lb/>non-consensus classificaƟon videos <lb/>non-consensus <lb/>video <lb/>consensus clips but differ <lb/>on consensus category <lb/>no <lb/>non-consensus <lb/>video <lb/>hashtag consensus matches <lb/>any clip consensus? <lb/>non-consensus video <lb/>video consensus <lb/>= <lb/>hashtag consensus <lb/>yes <lb/>hashtag for any of the 4 clips in the video? <lb/>yes <lb/>no <lb/>video consensus = classificaƟon consensus <lb/>classificaƟon consensus <lb/>is blank? <lb/>yes <lb/>(A) <lb/>(B) <lb/>no <lb/>yes <lb/></body>

        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

        <page>709 <lb/></page>

        <note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

        <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

        <body>Umbrella categories <lb/>There are several categories of animals that could easily <lb/>be misclassified as one another. We would expect com-<lb/>munity scientists to incorrectly classify these videos at the <lb/>animal category level, but correctly classify the videos if <lb/>we broadened the category definitions. For instance, com-<lb/>munity scientists could have difficulty distinguishing the <lb/>various duiker species (e.g. Cephalophus ogilbyi and C. <lb/>niger), but consistently agree on &apos;duiker&apos;. Therefore, we <lb/>repeated the above summarization process for two addi-<lb/>tional sets of designations we call &apos;umbrella categories&apos;. <lb/>The first set grouped all duiker categories together as <lb/>&apos;duiker&apos;, all hog categories as &apos;hog&apos;, and the large ungulate <lb/>and small antelope categories together as &apos;hoofed animal&apos;. <lb/>We then grouped the duikers and hoofed animal catego-<lb/>ries into an &apos;ungulate&apos; category. <lb/>Consensus threshold <lb/>To determine whether using a consensus threshold higher <lb/>than 50% would influence the precision or accuracy of <lb/>our results, we repeated the aggregation procedure using <lb/>threshold values of 0.65 and 0.75. We performed this pro-<lb/>cess for the umbrella categories as well. <lb/>Hashtags <lb/>Community scientists can also provide more specific clas-<lb/>sification information in the form of hashtags (#). This is <lb/>not a mandatory part of the workflow, but rather a sepa-<lb/>rate option after submitting standard classification infor-<lb/>mation. If interested, the community scientist is taken to <lb/>the &apos;Talk&apos; section of Chimp&amp;See where they can further <lb/>discuss the clips with each other, as well as annotate clips <lb/>with additional information. For example, while they <lb/>could hashtag a clip with the same annotation(s) they <lb/>used during the classification workflow (e.g. #red_duiker), <lb/>they could also specify species-level hashtags (e.g. specify-<lb/>ing which red duiker species was present: #bay_duiker or <lb/>#brookes_duiker). Unlike classifications, hashtags are visi-<lb/>ble to all participants under each video in the Talk inter-<lb/>face. Errors in hashtags can therefore be detected and <lb/>corrections are sometimes requested by both community <lb/>scientists and moderators. Community scientists also use <lb/>hashtags for behaviours not listed in the classification <lb/>interface, types of vocalizations and individual chimpan-<lb/>zee identification (McCarthy et al., 2021). A standardized <lb/>list of hashtags is curated by the community scientists <lb/>with new ones being established when interesting behav-<lb/>iours or new species are discovered in the videos. An <lb/>evolving list of hashtags and associated videos is curated <lb/>at: https://www.zooniverse.org/projects/sassydumbledore/ <lb/>chimp-and-see/talk/2278/899975. <lb/>We performed the data summarization process once <lb/>more, incorporating the hashtag data to increase the <lb/>number of videos that reach consensus as well as improve <lb/>their precision and accuracy. The workflow for incorpo-<lb/>rating the hashtag into the classification data (original <lb/>and umbrella categories) is summarized in Figure 2b and <lb/>detailed in Appendix S1. We further determined the pre-<lb/>cision of hashtags by collecting all species-and <lb/>genus-level hashtags for all four clips and comparing <lb/>them to the species annotation by the PE for the entire <lb/>video. We examined matches at the species level but also <lb/>when the species hashtag was not present (e.g. #red_colo-<lb/>bus) but a genus-level hashtag was (e.g. #colobus) as this <lb/>was still more informative data than the general classifica-<lb/>tion category of &apos;other primate&apos;. Again, videos that con-<lb/>tained the &apos;vocal-only&apos; or &apos;off_vocalization&apos; hashtags were <lb/>omitted (n = 22), as were those that contained a &apos;multi-<lb/>species&apos; hashtag (n = 25). <lb/>Figure 2. Data cleaning and analysis flow charts for (A) classification data. The original dataset comprised 13 531 videos. Data cleaning of the <lb/>professional ecologist (PE) dataset involved removing videos annotated by the PE as including multiple species and non-specific categories. <lb/>Debugging of the community scientist (CS) dataset included removing clips with 16 or more classifications, removing duplicate classifications of <lb/>the same clip by the same community scientist, as well as aggregating multiple annotations of the same species by the same community scientist <lb/>for a single clip. When this resulted in only one or two blank classifications we removed the video from the dataset. We further removed possible <lb/>multiple species videos as identified by the community scientists. Finally, we removed videos with four &apos;presence undetermined&apos; clips, meaning <lb/>that fewer than 3 non-blank classifications had been made on each clip. Because each 1-min video was annotated at the minute level by PEs, but <lb/>at the level of 15-s clips by community scientists, the four 15-s clips needed to be combined to determine the consensus animal category they <lb/>represented. &apos;No consensus in any clip&apos; indicates that none of the clips that make up the video reached a consensus on any category. &apos;2+ <lb/>consensus categories&apos; indicates that the majority of clips did not agree on the same classification category, so aggregation of consensus clips into <lb/>a consensus video was not possible. The data were then analysed in three ways: Using the 29 original Chimp&amp;See (C&amp;S) animal categories, <lb/>lumping all hogs together and then either lumping all duikers together and small and large ungulates together into a hoofed umbrella category <lb/>or by lumping all duikers and the hoofed animal category into an ungulate umbrella category. (B) hashtag analysis. In an attempt to improve the <lb/>total number of videos analysed and increase precision and accuracy, hashtag data was combined with the classification data according to the <lb/>rules outlined herein. <lb/></body>
    
    <page>710 <lb/></page>
    
    <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>
    
    <note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>
    
    <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>
    
    <body>Retirement criteria <lb/>We investigated whether we could lower the retirement <lb/>criteria while retaining a similar level of precision and <lb/>number of consensus videos. For videos that did not <lb/>reach perfect agreement and were retired with 15 classifi-<lb/>cations, we repeated the consensus clip and consensus <lb/>video analyses such that only the first X number of classi-<lb/>fications were used, where X ranged from eight to 14. As <lb/>videos with 7/7 perfect agreement would also have had 4/ <lb/>4 perfect agreement, we used the dataset of videos that <lb/>were retired with 15 classifications to determine if we <lb/>could lower the perfect agreement retirement criteria. We <lb/>assessed whether having limited those videos to between <lb/>four and six classifications, when those classifications were <lb/>in perfect agreement, would have resulted in a different <lb/>consensus classification, to determine if lowering the per-<lb/>fect agreement criteria (7/7) was possible. <lb/>Results <lb/>Data cleaning, video consensus, precision <lb/>and accuracy <lb/>Of the 13 531 videos, 11 894 remained after data cleaning <lb/>and processing (Fig. 2a). 2617 of these were PE-annotated <lb/>as blank and 9277 were PE-annotated as non-blank. <lb/>Figure 2a summarizes the number of consensus videos <lb/>obtained when using and omitting the umbrella groups. <lb/>When considering only whether species were found in <lb/>videos (i.e. blank versus non-blank detection), the false neg-<lb/>ative rate ranged from 12.1 to 2.8%, the false positive rate <lb/>from 2.0 to 2.1%, with 99.3 to 99.4% precision and 89.7 to <lb/>90.2% accuracy (Fig. 2a). When considering the videos in <lb/>which species were found, the accuracy (i.e. number of <lb/>videos with the correct species) ranged from 95.4 to 98.2%. <lb/>Category-specific precision, false positives <lb/>and negatives <lb/>Of the 29 animal categories available for annotation on <lb/>Chimp&amp;See, only 19 are for species present in the Ta€ ı <lb/>National Park, although 22 animal categories were identi-<lb/>fied by consensus by community scientists. For the origi-<lb/>nal categories, the precision was 95.4% and most <lb/>categories had accuracies above 97% (Fig. 3). Due to a <lb/>handful of outliers, the bird category had a precision of <lb/>79.6%, although the median percentage agreement was <lb/>100%. Three categories (small cat, small antelope, and <lb/>warthog) were annotated by community scientists even <lb/>though species in those categories were not present in the <lb/>Ta€ ı National Park video dataset. <lb/>Combining all duikers into the umbrella duiker cate-<lb/>gory resulted in 98.8% precision and increased the total <lb/>number of duiker videos with correct consensus catego-<lb/>ries from 5435 to 5957. Two of the three hogs (giant for-<lb/>est hog and red river hog) showed high precision when <lb/>considered separately, while warthog (a species not pre-<lb/>sent at Ta€ ı) had two false positive annotations. When the <lb/>hog species were combined, community scientists reached <lb/>99.5% precision. Combining large ungulate and small <lb/>antelope into the hoofed animal category resulted in <lb/>66.7% precision, though there were very few videos of <lb/>large ungulates (N = 4) and no videos of small antelopes <lb/>in the dataset, so the rate is most probably not represen-<lb/>tative. The combined ungulate umbrella category was <lb/>98.9% accurate which is comparable to when only duikers <lb/>were combined but allowed for an additional 26 videos to <lb/>be correctly classified. <lb/>We note that the categories other (primate), hippopot-<lb/>amus, rodent, bird, and other (non-primate) showed a <lb/>slight decrease in consensus video number (range: 1-7) <lb/>when re-analysing the dataset with the umbrella catego-<lb/>ries. This suggests that when the duikers and hogs were <lb/>considered together, they collectively equalled the amount <lb/>of support for these other categories, resulting in the <lb/>videos being classified as non-consensus. At least one <lb/>video per category of those dropped when combining <lb/>species categories had reached an incorrect consensus in <lb/>the original analysis. A maximum decrease of two correct <lb/>videos was obtained for any given category. In all cases, <lb/>however, the precision increased between 0.2 and 4.2%. <lb/>For simplicity, in all figures and analyses, we present the <lb/>original category data without the umbrella analysis, and <lb/>the results from the umbrella categories (without showing <lb/>the above-noted changes to the original categories). <lb/>False positives occurred when a video reached a con-<lb/>sensus on a category, but a species from that category was <lb/>not truly present in the video, either because the video <lb/>was blank or because it contained a species from a differ-<lb/>ent category. The false positive rate was consistently low <lb/>across categories, never exceeding 1.5% (Fig. 4a-e). <lb/>Using a Kendall&apos;s tau-b correlation, we correlated the <lb/>number of false positive videos per category with that <lb/>category&apos;s commonness for the original, &apos;duiker, hoofed <lb/>animal, hog&apos; and &apos;ungulate, hog&apos; umbrella categories, <lb/>resulting in correlation coefficients of 0.39, 0.52, and 0.54, <lb/>respectively. Thus, the more prevalent categories were also <lb/>those that had higher numbers of false positive classifica-<lb/>tions. In fact, of the ten categories not present in the <lb/>dataset, only three were incorrectly classified as a consen-<lb/>sus category by community scientists, and only small cat <lb/>continued to result in false positives under the umbrella <lb/>designations. <lb/></body>

			<note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<page>711 <lb/></page>

			<note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>False negatives occurred when a video did not reach a <lb/>consensus on the category of species that was truly pre-<lb/>sent, either because the video did not reach a consensus <lb/>at all, because it reached a consensus on a different cate-<lb/>gory, or because the video was classified as blank (Fig. 4f-<lb/>j). Combining the duikers resulted in a drastically lower <lb/>rate of false negatives, almost none of which were due to <lb/>the video not reaching consensus or reaching the wrong <lb/>consensus, suggesting the duikers were, as predicted, fre-<lb/>quently mistaken for one another. <lb/>Community scientists overwhelmingly agreed with the <lb/>PE on the number of individuals present, even if the wrong <lb/>classification reached consensus. Where there were differ-<lb/>ences, the majority were because the community scientists <lb/>classified fewer individuals than the PE (Figure S1). <lb/>Commonly misclassified species <lb/>We determined how often certain species were misclassi-<lb/>fied as certain other species (Fig. 5). The most common <lb/>misclassifications were classifying small grey duikers as <lb/>red duikers and vice versa, though there were multiple <lb/>instances of other duikers being mistaken for each other. <lb/>When duikers were considered together in the umbrella <lb/>duiker designation (Fig. 5b), the most common misclassi-<lb/>fications were hoofed animals being mistaken for duikers. <lb/>Duikers were also mistaken for many other species, and <lb/>many of those same species were mistaken for duikers. <lb/>This may be partially explained by the preponderance of <lb/>duikers in the dataset, leading community scientists to <lb/>guess duiker when they were unsure. We observed the <lb/>same trend for hoofed animals in the umbrella designa-<lb/>tion (Fig. 5c). When chimpanzees were misclassified, it <lb/>was always as human. However, upon further examina-<lb/>tion, these were not misclassifications, but were in fact <lb/>due to failing to omit these as multispecies videos by the <lb/>PE. That is, the presence of human researchers appearing <lb/>in the same or later clips within the same video as the <lb/>habituated Ta€ ı chimpanzees which the PE did not anno-<lb/>tate as such. In our data analysis pipeline, the community <lb/>Figure 3. Proportion agreement between consensus community scientist and professional ecologist (PE) responses. Each data point is the <lb/>proportion consensus agreement determined from all clips for a given video, lighter points represent fewer videos. All true positives fall above <lb/>50% agreement (dashed line). For values below 50% agreement, dots represent the proportion of votes that identified the correct category <lb/>irrespective of the final consensus. For example, if the final consensus was bird but 20% of classifications correctly annotated leopard, a dot will <lb/>be found at 0.2 proportion consensus agreement in the bird column. Green boxes represent 1st and 3rd quartiles intersected by the median. <lb/>Whiskers span the 2.5th to 97.5th percentiles. Boxes and whiskers were not presented for categories with fewer than ten videos. The proportion <lb/>correct (rounded) and the number of consensus classifications by community scientists (N ) for each category are listed above each respective box <lb/>plot. Proportion consensus agreement for (A) all non-umbrella categories ordered alphabetically, (B) hog categories separately and when <lb/>combined into the &apos;hog&apos; umbrella category, (C) duiker categories separately and when combined into the &apos;duiker&apos; umbrella category, (D) large <lb/>ungulate and small antelope categories separately and when combined into the &apos;hoofed&apos; umbrella category, (E) the &apos;ungulate&apos; umbrella category <lb/>(includes all duikers, large ungulates and small antelopes). Asterisk (*) denotes categories that were not actually present in the videos but were <lb/>annotated as being present by community scientists (small cat, warthog and small antelope). <lb/></body>

      <page>712 <lb/></page>

      <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>Figure 4. False positive and negative rates. (A-E) Rate of false positive videos per category, segmented by true content of video: black, videos <lb/>reached consensus on different species; white, video was classified as blank. For all categories, the rate is below 1.5%. False positives for (A) all <lb/>non-umbrella categories ordered alphabetically, (B) hog categories separately and when combined into the &apos;hog&apos; umbrella category, (C) duiker <lb/>categories separately and when combined into the &apos;duiker&apos; umbrella category, (D) large ungulate and small antelope categories separately and <lb/>when combined into the &apos;hoofed&apos; umbrella category, (E) the &apos;ungulate&apos; umbrella category (includes all duikers, large ungulates and small <lb/>antelopes). Numbers above the bars indicate the total number of false positive videos for each species. (F-J) Proportion of videos containing <lb/>incorrect classifications for each animal category, segmented by reason for incorrect classification: black, videos reached consensus on a different <lb/>species; grey, videos did not reach consensus; white, video was classified as blank. False negative rate for (F) all non-umbrella categories ordered <lb/>alphabetically, (G) hog categories separately and when combined into the &apos;hog&apos; umbrella category, (H) duiker categories separately and when <lb/>combined into the &apos;duiker&apos; umbrella category, (I) large ungulate and small antelope categories separately and when combined into the &apos;hoofed&apos; <lb/>umbrella category, (J) the &apos;ungulate&apos; umbrella category (includes all duikers, large ungulates and small antelopes). Numbers above the bars <lb/>indicate the total number of videos containing each species. <lb/></body>

			<note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<page>713 <lb/></page>

			<note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>scientists agreed that a human is present, while also agree-<lb/>ing to a lesser extent that a chimpanzee is present. <lb/>Consensus threshold <lb/>For most categories, there is very little change in precision <lb/>with increasing consensus threshold, with the notable <lb/>exception of the hoofed animal category, which shows an <lb/>increase in precision, albeit with a decrease in the number <lb/>of consensus videos in each category (Table 2). <lb/>Hashtags <lb/>After cleaning, a total of 6185 out of 11 894 (52.0%) <lb/>videos had at least one category-related hashtag (Fig. 6a), <lb/>representing 6120 of the 9277 PE-identified non-blank <lb/>videos and 65 of the 2617 PE-identified blank videos. <lb/>Table 3 summarizes the number of videos added and <lb/>removed from the analysis and the overall improvements <lb/>in precision by adding hashtag information according to <lb/>our criteria. <lb/>In addition to improving the classification data, we also <lb/>obtained species-and genus-level annotations with the <lb/>hashtagging method. In general, precision was very high <lb/>for the species-level hashtags (95.5%, N = 5792/6067 <lb/>hashtagged videos) (Fig. 6b). Few videos had genus-level <lb/>hashtags only (n = 214), but most of those videos had <lb/>only correct hashtags with no incorrect tags (n = 182, <lb/>85%), while 7 videos had mixed genus-level tags, and 25 <lb/>(12%) had only incorrect genus-level tags. Examples of <lb/>Figure 5. Number of instances where species truly present in videos (y-axis) were misclassified as other species by community scientists (x-axis). <lb/>Numbers in parentheses along the y-axis indicate the total number of videos that were misclassified for that category (A) for original species <lb/>categories, (B) for umbrella &apos;duiker, hoofed and hog&apos; categories, (C) for umbrella &apos;ungulate and hog&apos; categories. <lb/></body>

      <page>714 <lb/></page>

      <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>clips with matching, non-matching and no consensus <lb/>between PE and CS datasets and with correct hashtags <lb/>can be found in Table S2. <lb/>Retirement criteria <lb/>Although there is variation in the optimal number of <lb/>classifications until retirement for any given species cate-<lb/>gory (Fig. 7), overall, decreasing the number of classifica-<lb/>tions from 15 to 8 did not affect the precision of our <lb/>results, with the exception of the &apos;hoofed animal&apos; <lb/>umbrella category, which had relatively few videos. The <lb/>precision of the consensus categories is relatively stable <lb/>across retirement cut-offs, especially when the number of <lb/>videos is high, suggesting a lower cut-off provides a <lb/>comparable level of precision. Further, the number of <lb/>videos that reach consensus also remains rather stable, in <lb/>some cases even increasing as the cut-off decreases. <lb/>To assess whether lowering the perfect agreement rule <lb/>from 7/7 to 4/4 would alter our results we started with <lb/>the 13,410 clips that had up to 15 classifications and <lb/>reached a consensus. Of these, 971 clips had 4/4 matching <lb/>classifications. Only one of the 971 clips was assigned a <lb/>category that did not match the original consensus cate-<lb/>gory after 15 classifications. Thus 99.9% of clips that <lb/>could have been retired after only four classifications <lb/>would have reached the same consensus category as the <lb/>original retirement rule (obtaining up to 15 classifications <lb/>for the clip). Increasing the cut-off to five or six matching <lb/>classifications resulted in 462 and 182 clips being retired, <lb/>respectively, all of which matched the original consensus <lb/>category. <lb/>Discussion <lb/>In this study, we have demonstrated the high quality of <lb/>community scientists&apos; annotations of camera trap videos <lb/>from a challenging tropical rainforest environment, with <lb/>high occlusion and low visibility. Overall, classification <lb/>precision was 95.4% when using species and species <lb/>group assignments. This new dataset complements previ-<lb/>ously published CS annotated still image datasets (e.g. <lb/>Shepley et al., 2021; Swanson et al., 2015; Willi <lb/>et al., 2019). It provides evidence for accurate annotation <lb/>of the much larger dataset on Chimp&amp;See that is still <lb/>being processed and is accessible as it is growing. The <lb/>dataset offers great potential for numerous scientific <lb/>domains, including ecologists interested in the study of <lb/>African tropical wildlife communities or computer scien-<lb/>tists requiring data for training and testing classification <lb/>algorithms (Bain et al., 2021; Labuguen et al., 2019; Sana-<lb/>koyeu et al., 2020; Schofield et al., 2019). To date, this <lb/>dataset and the growing PanAf/Chimp&amp;See dataset have <lb/>been used for development of the AI species detection <lb/>platform ZambaCloud (www.ZambaCloud.org) and sub-<lb/>sets of the data have been both open-sourced and used <lb/>for species-specific training of various AI algorithms <lb/>(Brookes et al., 2023; Sakib &amp; Burghardt, 2020; Sanakoyeu <lb/>et al., 2020; Suessle et al., 2023). Additional datasets <lb/>derived from the PanAf videos annotated on Chimp&amp;See, <lb/>and including the behavioural annotations (Table S1), <lb/>have already been used in studies and reports on species <lb/>distribution, animal behaviour (Kalan et al., 2019; Tagg <lb/>et al., 2018), social network construction (McCarthy <lb/>et al., 2021), CS engagement (Amarasinghe et al., 2021) <lb/>and ongoing computer vision studies. We aim to make <lb/>all videos and their annotations available once annotation, <lb/>analysis and publication are completed. <lb/>Table 2. Proportion of videos annotated by community scientists as a <lb/>given category that were correct when compared to the PE annota-<lb/>tion at various consensus thresholds. <lb/>Proportion correct at <lb/>each consensus <lb/>threshold <lb/>Number of videos <lb/>included at consensus <lb/>threshold <lb/>Category <lb/>50% 65% 75% 50% <lb/>65% <lb/>75% <lb/>Bird <lb/>0.8 <lb/>0.81 <lb/>0.81 <lb/>103 <lb/>100 <lb/>97 <lb/>Chimpanzee <lb/>1 <lb/>1 <lb/>1 <lb/>239 <lb/>237 <lb/>236 <lb/>Elephant <lb/>1 <lb/>1 <lb/>1 <lb/>3 <lb/>3 <lb/>3 <lb/>Hippopotamus <lb/>0.98 <lb/>1 <lb/>1 <lb/>41 <lb/>36 <lb/>34 <lb/>Human <lb/>0.97 <lb/>0.98 <lb/>0.98 <lb/>515 <lb/>510 <lb/>509 <lb/>Leopard <lb/>0.99 <lb/>0.99 <lb/>0.98 <lb/>68 <lb/>67 <lb/>64 <lb/>Other (non-primate) 0.72 <lb/>0.8 <lb/>0.75 <lb/>105 <lb/>61 <lb/>44 <lb/>Other (primate) <lb/>0.98 <lb/>0.99 <lb/>0.99 <lb/>718 <lb/>708 <lb/>700 <lb/>Pangolin <lb/>1 <lb/>1 <lb/>1 <lb/>9 <lb/>7 <lb/>5 <lb/>Porcupine <lb/>1 <lb/>1 <lb/>1 <lb/>22 <lb/>7 <lb/>6 <lb/>Rodent <lb/>0.9 <lb/>0.98 <lb/>0.99 <lb/>115 <lb/>86 <lb/>71 <lb/>Small cat <lb/>0 <lb/>0 <lb/>0 <lb/>7 <lb/>4 <lb/>2 <lb/>Giant forest hog <lb/>1 <lb/>1 <lb/>1 <lb/>6 <lb/>5 <lb/>4 <lb/>Red river hog <lb/>0.99 <lb/>1 <lb/>1 <lb/>192 <lb/>145 <lb/>112 <lb/>Warthog <lb/>0 <lb/>NA <lb/>NA <lb/>3 <lb/>0 <lb/>0 <lb/>Hog <lb/>1 <lb/>1 <lb/>1 <lb/>214 <lb/>203 <lb/>197 <lb/>Dark duiker <lb/>0.64 <lb/>0.88 <lb/>0.89 <lb/>33 <lb/>16 <lb/>9 <lb/>Jentink&apos;s duiker <lb/>1 <lb/>1 <lb/>1 <lb/>78 <lb/>47 <lb/>31 <lb/>Red duiker <lb/>0.93 <lb/>0.97 <lb/>0.98 <lb/>1570 1144 <lb/>910 <lb/>Small grey duiker <lb/>0.97 <lb/>0.98 <lb/>0.98 <lb/>3654 2825 2242 <lb/>Zebra duiker <lb/>1 <lb/>1 <lb/>1 <lb/>100 <lb/>92 <lb/>84 <lb/>Duiker <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>5957 5769 5469 <lb/>Large ungulate <lb/>1 <lb/>1 <lb/>1 <lb/>4 <lb/>2 <lb/>2 <lb/>Small antelope <lb/>0 <lb/>0 <lb/>0 <lb/>10 <lb/>2 <lb/>1 <lb/>Hoofed animal <lb/>0.67 <lb/>0.75 <lb/>0.75 <lb/>6 <lb/>4 <lb/>4 <lb/>Ungulate <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>5983 5929 5883 <lb/>Data shown for original categories and &apos;duiker&apos;, &apos;hog&apos;, &apos;hoofed ani-<lb/>mal&apos; and &apos;ungulate&apos; umbrella categories. NA indicates no videos were <lb/>annotated by community scientists for a given category at the given <lb/>threshold. Warthogs, small antelopes, and small cats are not present <lb/>at the site and were all incorrectly annotated as present by commu-<lb/>nity scientists at the respective thresholds. <lb/></body>

			<note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<page>715 <lb/></page>

			<note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>Mismatches between Chimp&amp;See <lb/>community scientists and PEs <lb/>As found in other studies, professional annotations can <lb/>also contain errors (Swanson et al., 2015; Swanson <lb/>et al., 2016; Zett et al., 2022) and some mismatches here <lb/>may be accounted for by PE error as well (e.g. Table S2a-<lb/>c). Some additional patterns of mismatches we observed <lb/>were that our PE annotator only identified one video as <lb/>containing a bat, although bats are often seen and some-<lb/>times with other species. As they fly quickly through the <lb/>video such that species identification is impossible, and <lb/>often look like moths, they are easily overlooked by PE <lb/>annotators. Although bats fall into the &apos;other (non-<lb/>primate)&apos; category, we suspect that community scientists <lb/>were annotating bats which may have led to some of the <lb/>observed mismatches between the PE and community sci-<lb/>entists, between blank and non-blank videos, as well as in <lb/>some multispecies videos and clips. Similarly, PEs may <lb/>not watch entire videos and stop after they find some-<lb/>thing interesting, thereby missing some of the multispe-<lb/>cies videos and again causing mismatches between the <lb/>1-min and 15-s datasets. This appeared to occur with <lb/>videos containing birds as well as other terrestrial species. <lb/>Hashtags <lb/>Chimp&amp;See is a rather unique project in the Zooniverse <lb/>with its extensive use of the hashtag system for data col-<lb/>lection. We show that using hashtags in combination with <lb/>classification data can improve precision and accuracy by <lb/>both decreasing the number of incorrect and increasing <lb/>the number of correct, classifications, as well as allow for <lb/>species-specific annotations beyond the standard animal <lb/>categories (examples in Table S2). Further, they are easy <lb/>to create and implement so when new species or behav-<lb/>iours are identified they can be tagged. It is of paramount <lb/>importance to keep a curated list of official tags and to <lb/>do periodic checks of the data output to determine if any <lb/>tags are being misspelled such that downstream data ana-<lb/>lyses do not miss important sources of information. This <lb/>task is currently performed several times per year by a <lb/>dedicated community scientist moderator. <lb/>Multispecies videos <lb/>Determining when a video contains more than one spe-<lb/>cies with classification data alone is beyond the scope of <lb/>this study. Because multiple species may be present in the <lb/>same video but appear in different clips, it is impossible <lb/>to distinguish between cases where community scientists <lb/>disagree on which single species is present across multiple <lb/>clips within a video and cases where they are seeing mul-<lb/>tiple species in different clips within a video. However, <lb/>upon repeating the data cleaning steps, but without <lb/>removing the videos deemed multispecies by either the <lb/>PE or community scientists, there were 247 videos that <lb/>were annotated as multispecies by the PE annotator. <lb/>From the classification data, there were 1838 instances <lb/>across 821 clips (representing 644 videos) of community <lb/>Table 3. Consensus and hashtag results. <lb/>C&amp;S <lb/>classification <lb/>Comparison to PE <lb/>annotation <lb/>Original categories <lb/>&apos;Duiker, hoofed, hog&apos; umbrella <lb/>categories <lb/>&apos;Ungulate, hog&apos; umbrella <lb/>categories <lb/>Classification <lb/>only <lb/>Hashtag &amp; <lb/>classification <lb/>data <lb/>Classification <lb/>only <lb/>Hashtag &amp; <lb/>classification <lb/>data <lb/>Classification <lb/>only <lb/>Hashtag &amp; <lb/>classification <lb/>data <lb/>Non-consensus <lb/>videos <lb/>634 <lb/>424 <lb/>122 <lb/>101 <lb/>108 <lb/>88 <lb/>Blank consensus <lb/>videos <lb/>Correct <lb/>2559 <lb/>2553 <lb/>2559 <lb/>2552 <lb/>2559 <lb/>2552 <lb/>Incorrect <lb/>1106 <lb/>1099 <lb/>1106 <lb/>1099 <lb/>1106 <lb/>1099 <lb/>Non-blank <lb/>consensus <lb/>videos <lb/>Correct <lb/>7243 <lb/>7471 <lb/>7949 <lb/>7961 <lb/>7976 <lb/>7980 <lb/>Incorrect category <lb/>301 <lb/>225 <lb/>106 <lb/>58 <lb/>91 <lb/>50 <lb/>Incorrect, blank <lb/>51 <lb/>52 <lb/>52 <lb/>53 <lb/>54 <lb/>55 <lb/>Total non-blank <lb/>videos (precision) <lb/>7595 (95.4%) 7748 (96.4%) 8107 (98.1%) 8072 (98.6%) 8121 (98.2%) <lb/>8085 (98.7%) <lb/>Comparison of results using classification data alone and combining it with hashtag data for determining species annotations. C&amp;S classifications: <lb/>Non-consensus videos are videos that did not attain consensus on a category as determined by the process outlined in Figure 3. Blank consensus <lb/>videos are the videos that the community scientists classified as blank (4/4 clips classified as blank). Non-blank consensus videos are those videos <lb/>that were determined to reach a consensus species by the community scientists either correctly, incorrectly with consensus on another species <lb/>(incorrect category) or incorrectly because the video was actually blank (incorrect, blank). Precision is calculated as total correct non-blank consen-<lb/>sus videos divided by total non-blank consensus videos. <lb/></body>

      <page>716 <lb/></page>

      <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/>ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<page>717 <lb/></page>

			<note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>scientists classifying multiple species in a single clip, and <lb/>151 of these 644 videos (23.4%) were also classified as <lb/>multispecies by the PE. Further, 113 of the 151 videos <lb/>(74.8%) had at least one &apos;multispecies&apos; hashtag. <lb/>Solely looking at the hashtag data, of the 247 videos <lb/>PE-annotated as multispecies, 224 (90.7%) of these videos <lb/>contained hashtags, of which 118 contained one or more <lb/>#multispecies, #multi-species, or #multi_species hashtags <lb/>in at least one of the four clips. All hashtagged videos had <lb/>at least one hashtag that matched any of the true species <lb/>present. <lb/>Improvements to new Chimp&amp;See interface <lb/>Several changes have been implemented to the new inter-<lb/>face since 2019 based on the results of these and other <lb/>analyses, and feedback from the Chimp&amp;See community. <lb/>We removed the preview set stills as anecdotally, most of <lb/>our avid community scientists said they did not use the <lb/>feature as watching the moving videos was easier to detect <lb/>an animal if it was present. Thanks to the contributions <lb/>of volunteers via the Zooniverse translation interface, the <lb/>Chimp&amp;See platform is available in five additional lan-<lb/>guages: German, Spanish, French, Italian, Czech. <lb/>We now limit the species choices to only what is <lb/>known to be present at the site or that range nearby. This <lb/>will further reduce errors as, for example in the dataset <lb/>presented in this study, there were incorrect consensus <lb/>classifications for the small cat (N = 7), warthog (N = 3), <lb/>and small antelope (N = 10) categories, when animals in <lb/>these categories were not actually present at the site <lb/>(Fig. 2b). <lb/>Based on feedback we received from community scien-<lb/>tists, classification and hashtag results, we aimed to give <lb/>unique-looking species their own category and lumped <lb/>together species that were more difficult to differentiate <lb/>(changes listed in Table S3). For example, we created new <lb/>categories of &apos;civet/genet&apos;, &apos;mongoose&apos; and &apos;domesticated <lb/>animal&apos; rather than have them annotated in the catch-all <lb/>&apos;other (non-primate)&apos; category. On the other hand, we <lb/>combined all ungulates, antelopes and duikers into a sin-<lb/>gle category &apos;Antelope/duiker&apos; in the Species ID workflow. <lb/>We then created two additional downstream workflows <lb/>for every site. A Trotters ID workflow for species-level <lb/>classification of antelopes, hogs, and duikers, and the <lb/>MonkeySee workflow for species-level classification of <lb/>monkeys and prosimians. When a clip is classified by two <lb/>or more community scientists as one of these species of <lb/>interest, that clip becomes part of a downstream workflow <lb/>and is further classified to the species level. <lb/>The retirement criteria on the new platform have been <lb/>optimized thereby reducing time and effort for annota-<lb/>tions. For non-blank videos with perfect agreement, we <lb/>found four perfectly matching classifications rather than <lb/>seven are needed. For videos that were previously retired <lb/>with 15 classifications, we observe similar results with <lb/>eight or nine classifications depending on the species cate-<lb/>gory and therefore, to be conservative retire all non-blank <lb/>categories after 9 classifications Based on a pilot analysis <lb/>(data not shown) from the new platform for MonkeySee <lb/>and Trotters ID workflows, a subject is retired having <lb/>reached a consensus if the first two classifications are <lb/>identical, or after six total classifications. <lb/>In addition to these improvements, the ongoing pro-<lb/>gression in camera trap video quality and resolution is <lb/>poised to significantly contribute to the improvement of <lb/>species annotations in the future. It should be noted how-<lb/>ever that the Zooniverse platform currently only allows <lb/>videos of 1 MB to be uploaded meaning video clips need <lb/>to be shorter and/or compressed prior to upload and <lb/>annotation. Despite the associated increase in cost for <lb/>storage and the limitations to resolution on the Zooni-<lb/>verse platform, we do not advocate for videos to be col-<lb/>lected at a lower resolution. The Zooniverse &apos;Talk&apos; <lb/>interface can be used to discuss videos that are flagged by <lb/>community scientists that require more precision and <lb/>scrutiny and file-sharing services can be used to dissemi-<lb/>nate the original higher-quality videos. <lb/>Conclusion <lb/>Although time consuming, we show that community sci-<lb/>ence provides a reliable method for annotating species <lb/>and species numbers from camera trap videos in challeng-<lb/>ing environments at a range of taxonomic levels. Different <lb/>data processing, beyond what was presented here, may <lb/>also improve metrics, such as considering a community <lb/>scientist&apos;s experience level by filtering by minimum <lb/>Figure 6. Hashtags (A) Proportion of videos with correct, incorrect, mixed or no hashtags per original designation PE category. Videos that have <lb/>&apos;mixed hashtags&apos; have both correct and incorrect category-level hashtags present. Videos that contain a &apos;multispecies&apos; or &apos;vocal-only&apos; hashtag are <lb/>not included here. (B) Proportion of videos with correct, incorrect, mixed or no species-(or genus-) level hashtags per PE category. Videos that <lb/>have &apos;mixed hashtags&apos; have both correct and incorrect hashtags present at the species and/or genus level. Videos labelled as correct/incorrect <lb/>genus hashtags do not have any species-level hashtags present. The number above each bar represents the number of PE-identified videos for <lb/>each category. Videos that contain a &apos;multispecies&apos; or &apos;vocal-only&apos; hashtag are not included here. Species marked with an asterisk (*) are already <lb/>classified to the species level in the classification interface. <lb/></body>

      <page>718 <lb/></page>

      <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/>ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<page>719 <lb/></page>

			<note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<body>number of classifications for example. Further, additional <lb/>filtering of data, rather than omitting it from the dataset <lb/>as done here due to space constraints, may further <lb/>improve outputs. We anticipate more time-efficient anno-<lb/>tations in the future by integrating machine learning out-<lb/>puts to further decrease the amount of independent views <lb/>necessary per video (Willi et al., 2019) and/or taking user <lb/>experience into account when evaluating collective com-<lb/>munity science annotations (Ratnieks et al., 2016; Swan-<lb/>son et al., 2016). In addition to species annotations, <lb/>community scientists at Chimp&amp;See also annotate behav-<lb/>iours (Kalan et al., 2019), identify age/sex classes and <lb/>unique individuals of some species (Debetencourt <lb/>et al., 2023; McCarthy et al., 2021), detect health status <lb/>anomalies and document other topics of interest (Tagg <lb/>et al., 2018); annotations that often go beyond the scope <lb/>of most PE-annotated datasets. Further, like other studies <lb/>(Zett et al., 2022), we find that community scientists may <lb/>be more thoroughly annotating some videos, further <lb/>emphasizing the benefit of incorporating CS into biodi-<lb/>versity research where possible. <lb/></body>

			<div type="acknowledgement">Acknowledgements <lb/>We thank the Heinz L. Krekeler Foundation, the Max <lb/>Planck Society Innovation Fund, the Max Planck Society, <lb/>the Robert Bosch Foundation, the German Centre for Inte-<lb/>grative Biodiversity Research (iDiv) Halle-Jena-Leipzig, the <lb/>iDiv Data &amp; Code Unit for assistance with curating <lb/>and archiving the camera trap videos, the Deutsche For-<lb/>schungsgemeinschaft (DFG; FZT-118), the Centre Suisse de <lb/>Recherches Scientifiques, and the Centre for Forest <lb/>Research-Fonds de Recherche Qu ebec Nature et Technolo-<lb/>gies International internship program, for financial support. <lb/>We also thank the Minist ere de l&apos;Enseignement Sup erieur <lb/>et de la Recherche Scientifique of Côte d&apos;Ivoire, Minist ere <lb/>de l&apos;Environnement et des Eaux et Forêts of Côte d&apos;Ivoire, <lb/>the Office Ivoirien de Parcs et R eserves, and the Ta€ ı Chim-<lb/>panzee Project for permission to conduct research. We <lb/>thank the community scientists who have participated in <lb/>Chimp&amp;See data collection (https://www.zooniverse.org/ <lb/>projects/sassydumbledore/chimp-and-see/about/team). <lb/>In <lb/>particular, we are grateful to the following community sci-<lb/>entists for extensive contributions, organization, and sup-<lb/>port: the late Dawna Wallis (@snorticus, @aude69 (Aude), <lb/>@Batfan (Jane), @BK13 (Bettina), @bobodog (Andy), <lb/>@Bostrol (Gwen), @burdock (Libby), @Corcaroli (Dick), <lb/>@ckardamilas (Christiana), @Eweforia (Carol), @Fade2pink <lb/>(Maria), @FloridaFlamingoGirl (Lydia), @hoothoot (Percy), <lb/>@kellyhoover (Kelly), @Kikilee3 (Karen),), @LMcCM <lb/>(Linda), @LJE (Liz), @Loudoh (Lou), @Lynnab (Lynn), <lb/>@Maryutah (Mary), @NamiriTandon (Fatima), @Nickle-<lb/>nackletree (Geraldine), @puddock (Fiona), @rbkeen (Rona), <lb/>@Rudyn (Dani), @Samburu (Jenny), @Sotigan (Giulia), <lb/>@Stargravenmad (Jessica), @tgcummings (Tonnie), @The-<lb/>Weez15 (Zaneb), @zoogirl1 (Caryn). We also thank the fol-<lb/>lowing for volunteering their time and scientific support on <lb/>the platform: Alexander Mielke, Liran Samuni, Joana Per-<lb/>eira, Silke Atmaca, Tobias Deschner, Jack Lester, Alessandra <lb/>Mascaro, Simone Pika, Ingo Schmidinger, Philipp Henschel. <lb/>We also thank Sergio Marrocoli for providing PE-annotated <lb/>pilot data for the study. Further, we are deeply grateful to <lb/>Darren McRoy, Laura Whyte, Laura Trouille, Sarah Allen, <lb/>Grant Miller, Cliff Johnson, Cody Dirks and Adam McMas-<lb/>ter from The Zooniverse for providing support with the <lb/>Chimp&amp;See platform and code for generating the auto-<lb/>populated Google sheets. Finally, we thank the thoughtful <lb/>and helpful comments of two anonymous reviewers who <lb/>greatly improved the clarity and content of this manuscript. <lb/>Chimp&amp;See owes immense gratitude to the esteemed late <lb/>Prof. Christophe Boesch. Prof. Boesch reviewed and <lb/>endorsed all iterations of this manuscript, save for the ulti-<lb/>mate revised draft, prior to his untimely passing. In his <lb/>honour, we dedicate this work to his enduring memory. <lb/>Open Access funding enabled and organized by Projekt <lb/>DEAL.<lb/></div>

			<div type="availability">Data Accessibility Statement <lb/>The 13 531 1-min camera trap videos are available for <lb/>download through the German Centre for Integrative Bio-<lb/>diversity Research (iDiv) Data Portal at https://idata.idiv.de/ <lb/>(Dataset id: 3507 and accession number: 10.25829/ <lb/>idiv.3507-yo9f8q). This dataset contains 178 zip files, each <lb/>containing up to 3.6 GB of camera trap videos, totalling <lb/>13 531 1-min videos. The name of each zip file denotes the <lb/>animal category present in the videos as annotated by the <lb/>PE, and a number used to enumerate files containing the <lb/>same species (e.g. chimpanzee1.zip is the first batch of <lb/>chimpanzee videos, chimpanzee2.zip is the second batch, <lb/>etc). Each video file is named in the following way: <lb/>TaiEast_[number]_loc[number]_[date]_[file <lb/>name].mp4. <lb/>&apos;TaiEast_[number]&apos; refers to the number of the camera with <lb/>which the video was taken. &apos;loc[number]&apos; refers to the <lb/>Figure 7. Proportion agreement when incrementally altering the retirement cut-off from eight to 15 for each classification category and the <lb/>&apos;duiker&apos;, &apos;hog&apos;, &apos;hoofed animal&apos; and &apos;ungulate&apos; umbrella categories. Boxes represent 1st and 3rd quartiles intersected by the median. Whiskers <lb/>span the 2.5th to 97.5th percentiles. <lb/></div>

      <page>720 <lb/></page>

      <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<div type="availability">location of the camera. Each unique combination of GPS <lb/>coordinates constitutes its own location (e.g. loc1, loc2, <lb/>etc.), but the coordinates are not provided here due to <lb/>safety concerns. &apos;[date]&apos; refers to the date that the SD card <lb/>on which the video was originally recorded was put into <lb/>the camera. &apos;[file name]&apos; refers to the original file name of <lb/>the video, indicating the order in which the videos were <lb/>recorded on a particular SD card (e.g. PICT0001, <lb/>PICT0002, etc.). Videos have an H264 video codec and <lb/>FLAC audio codec. All videos are 1280 9 720 p. <lb/>Video clips: (video_data_CandS_Species_ID_MS.txt; <lb/>47 584 rows) contains URLs to access videos as well as <lb/>information relating clip IDs to video IDs. <lb/>• Clip.ID: the 10-character ID given to each video clip. <lb/>• Video.ID: the ID given to each video. Each video com-<lb/>prises four clips. <lb/>• Start.Time: the time into the video at which the clip <lb/>begins, in seconds. Values are 0, 15, 30, or 45, indicat-<lb/>ing the 15-s clip begins at 0, 15, 30, or 45 s through <lb/>the 1-min video to which it belongs. <lb/>• URL: the link at which the 15-s clip can be accessed via <lb/>web browser or downloaded (e.g. via the command line <lb/>using the &apos;curl&apos; tool). <lb/>• Year, Month, Day, Hour, Minute: the date and time at <lb/>which the clip was recorded. All clips within a single <lb/>video will have the same date and time. <lb/>• Camera.Bearing: the direction the camera is pointing, <lb/>as measured in the number of degrees past north going <lb/>counter-clockwise. <lb/>• Camera.Height: the height of the camera from the <lb/>ground, measured in meters. <lb/>• Camera.Number: a number designating which camera <lb/>was used to film each video. <lb/>• Video.Dimension: the width by the height of the clip, <lb/>measured in pixels. <lb/>Raw <lb/>classifications: <lb/>(all_raw_classification_data_-<lb/>CandS_Species_ID_MS.txt; 369,671 rows) raw classifica-<lb/>tions made by community scientists. One row per <lb/>community scientist, classification event, and animal <lb/>category. <lb/>• Unique.Row.Number: unique identifier for each <lb/>annotation. <lb/>• Date.Time: the date and time at which a classification <lb/>was made, in YYYY-MM-DD HH:MM:SS format. <lb/>• Classification.ID: a unique code given to each classifica-<lb/>tion event. Rows share the same classification ID when <lb/>a single community scientist identifies multiple catego-<lb/>ries as being present in the same clip. <lb/>• Clip.ID: the 10-character ID given to each video clip. <lb/>• Video.ID: the ID given to each video. Each video com-<lb/>prises four clips. <lb/>• Start.Time: the time into the video at which the clip <lb/>begins, in seconds. Values are 0, 15, 30, or 45, <lb/>indicating the 15-s clip begins at 0, 15, 30, or 45 s <lb/>through the 1-min video to which it belongs. <lb/>• User.Name: for logged-in individuals, this is their <lb/>unique login ID. For individuals not logged in, this is <lb/>information to anonymously identify the computer <lb/>being used. <lb/>• Species: the animal category, including blanks that rep-<lb/>resent &apos;no animal present&apos;. <lb/>• Umbrella.Duiker.Species: the animal category under the <lb/>umbrella duiker designation. <lb/>• Umbrella.Species: the animal category under the <lb/>umbrella category designation. <lb/>• Number.Individuals: the number of individuals for a <lb/>particular species that are present, chosen from 1, 2, 3, <lb/>4, and 5+. The number of individuals was not collected <lb/>for the human category and was always set to 1. This <lb/>value is blank where no species was seen. <lb/>• Behaviour: one or more behaviours being exhibited by <lb/>a particular species. Behaviours were not collected for <lb/>the human category. <lb/>Consensus clip classifications: (consensus_per_clip_ori-<lb/>ginal_CandS_Species_ID_MS.txt, consensus_per_clip_um-<lb/>brella_duiker_CandS_Species_ID_MS.txt, <lb/>and <lb/>consensus_per_clip_umbrella_CandS_Species_ID_MS.txt; <lb/>47,584 rows each) aggregated clip-level classifications <lb/>based on the raw classification data, using the method <lb/>described in the &apos;Clip consensus&apos; section. <lb/>• Clip.Id: the 10-character ID given to each video clip. <lb/>• Video.ID: the ID given to each video. Each video com-<lb/>prises four clips. <lb/>• PE.Species: the animal category determined by the pro-<lb/>fessional ecologist (PE). <lb/>• Consensus.Species: the consensus category determined <lb/>using raw classifications. <lb/>• Consensus.Prop: the proportion of classifications <lb/>matching the consensus category. This value is NA <lb/>where no consensus was reached or where there were <lb/>too few classifications to attempt the consensus process. <lb/>• Prop.Agreement: the proportion of classifications that <lb/>match the correct category. <lb/>• Number.Blanks: the number of blank classifications. <lb/>• Number.Nonblanks: the number of non-blank <lb/>classifications. <lb/>Consensus video classifications: (consensus_per_vi-<lb/>deo_original_CandS_Species_ID_MS.txt, consensus_per_vi-<lb/>deo_umbrella_duiker_CandS_Species_ID_MS.txt, <lb/>and <lb/>consensus_per_video_umbrella_CandS_Species_ID_MS.txt; <lb/>11,894 rows each) aggregated video-level classifications <lb/>using the clip consensus classifications and the method <lb/>described in the &apos;Aggregating clips into consensus videos&apos; <lb/>section. <lb/>• Video.ID: the ID given to each video. Each video com-<lb/>prises four clips. <lb/></div>

        <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<page>721 <lb/></page>

			<note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<div type="availability">• PE.Species: the animal category determined by the pro-<lb/>fessional ecologist (PE). <lb/>• Consensus.Species: the consensus category determined <lb/>using raw classifications. <lb/>• Consensus.Prop: the proportion of classifications <lb/>matching the consensus category. This value is NA <lb/>where no consensus was reached or where there were <lb/>too few classifications to attempt the consensus process. <lb/>• Prop.Agreement: the proportion of classifications that <lb/>match the correct category. <lb/>• Consensus.Type: describes the video consensus (consen-<lb/>sus reached, consensus blank, conflicting consensus spe-<lb/>cies, or all nonblank clips are non-consensus). <lb/>PE <lb/>classifications: <lb/>(all_pe_data_CandS_Specie-<lb/>s_ID_MS.txt; 13,814 rows) classifications done by a <lb/>trained ecologist. <lb/>• Video.ID: the ID given to each video. Each video com-<lb/>prises four clips. <lb/>• Vernacular.Name: the species-level classification, where <lb/>possible. <lb/>• Species: the animal category present in the video. <lb/>• Umbrella.Duiker.Species: the animal category under the <lb/>umbrella duiker designation. <lb/>• Umbrella.Species: the animal category under the <lb/>umbrella category designation. <lb/>• Number.Individuals: the number of individuals seen in <lb/>the video, ranges from one to 37. <lb/>Raw <lb/>hashtags: <lb/>(all_raw_hashtags_CandS_Specie-<lb/>s_ID_MS.txt; 54 124 rows) additional, optional identify-<lb/>ing and behavioural information noted by community <lb/>scientists in the Talk section of Chimp&amp;See. <lb/>• Clip.ID: the 10-character ID given to each video clip. <lb/>• Tags: the hashtags provided by community scientists. <lb/>Hashtags are shown as &apos;hashtag:number of times pre-<lb/>sent&apos;.&apos; A value of bat:2 means the bat hashtag was present <lb/>twice for that clip. Where there are different hashtags <lb/>present, the values are separated by semicolons. <lb/></div>

      <listBibl>References <lb/>Ahumada, J.A., Silva, C.E.F., Gajapersad, K., Hallam, C., Hurtado, <lb/>J., Martin, E. et al. (2011) Community structure and diversity of <lb/>tropical forest mammals: data from a global camera trap <lb/>network. Philosophical Transactions of the Royal Society of <lb/>London. Series B, Biological Sciences, 366(1578), 2703-2711. <lb/>Available from: https://doi.org/10.1098/rstb.2011.0115 <lb/>Amarasinghe I, Manske S, Hoppe HU, Santos P, Hern andez-<lb/>Leo D. Using network analysis to characterize participation <lb/>and interaction in a citizen science online community. In: <lb/>International conference on collaboration technologies and <lb/>social computing. Springer; 2021:67-82. <lb/>Bain, M., Nagrani, A., Schofield, D., Berdugo, S., Bessa, J., <lb/>Owen, J. et al. (2021) Automated audiovisual behavior <lb/>recognition in wild primates. Science Advances, 7(46), <lb/>eabi4883. <lb/>Balazs, C.L. &amp; Morello-Frosch, R. (2013) The three Rs: how <lb/>community-based participatory research strengthens the <lb/>rigor, relevance, and reach of science. Environmental Justice, <lb/>6(1), 9-16. Available from: https://doi.org/10.1089/env.2012. <lb/>0017 <lb/>Bessone, M., K€ uhl, H.S., Hohmann, G., Herbinger, I., <lb/>N&apos;Goran, K.P., Asanzi, P. et al. (2020) Drawn out of the <lb/>shadows: surveying secretive forest species with camera trap <lb/>distance sampling. Journal of Applied Ecology, 57(5), 963-<lb/>974. Available from: https://doi.org/10.1111/1365-2664.13602 <lb/>Brookes, O., Mirmehdi, M., K€ uhl, H. &amp; Burghardt, T. (2023) <lb/>Triple-stream deep metric learning of great ape Behavioural <lb/>actions. https://doi.org/10.48550/arXiv.2301.02642 <lb/>Burton, A.C., Neilson, E., Moreira, D., Ladle, A., Steenweg, R., <lb/>Fisher, J.T. et al. (2015) Wildlife camera trapping: a review <lb/>and recommendations for linking surveys to ecological <lb/>processes. Journal of Applied Ecology, 52(3), 675-685. <lb/>Caravaggi, A., Burton, A.C., Clark, D.A., Fisher, J.T., Grass, A., <lb/>Green, S. et al. (2020) A review of factors to consider when <lb/>using camera traps to study animal behavior to inform <lb/>wildlife ecology and conservation. Conservation Science and <lb/>Practice, 2(8), e239. <lb/>Cooper, C.B., Hawn, C.L., Larson, L.R., Parrish, J.K., Bowser, <lb/>G., Cavalier, D. et al. (2021) Inclusion in citizen science: the <lb/>conundrum of rebranding. Science, 372(6549), 1386-1388. <lb/>Available from: https://doi.org/10.1126/science.abi6487 <lb/>Debetencourt, B., Barry, M.M., Arandjelovic, M., Stephens, C., <lb/>Maldonado, N. &amp; Boesch, C. (2023) Camera traps unveil <lb/>demography, social structure, and home range of six <lb/>unhabituated Western chimpanzee groups in the Moyen <lb/>Bafing National Park, Guinea. American Journal of <lb/>Primatology, 86, e23578. Available from: https://doi.org/10. <lb/>1002/ajp.23578 <lb/>Despr es-Einspenner, M.L., Howe, E.J., Drapeau, P. &amp; K€ uhl, <lb/>H.S. (2017) An empirical evaluation of camera trapping and <lb/>spatially explicit capture-recapture models for estimating <lb/>chimpanzee density. American Journal of Primatology, 79(7), <lb/>e22647. Available from: https://doi.org/10.1002/ajp.22647 <lb/>Droissart, V., Azandi, L., Onguene, E.R., Savignac, M., Smith, <lb/>T.B. &amp; Deblauwe, V. (2021) PICT: a low-cost, modular, open-<lb/>source camera trap system to study plant-insect interactions. <lb/>Methods in Ecology and Evolution, 12(8), 1389-1396. Available <lb/>from: https://doi.org/10.1111/2041-210X.13618 <lb/>Dubost, G. (1980) L&apos; ecologie et la vie sociale du C ephalophe <lb/>bleu (Cephalophus monticola Thunberg), petit ruminant <lb/>forestier africain. Zeitschrift f€ ur Tierpsychologie, 54(3), 205-<lb/>266. Available from: https://doi.org/10.1111/j.1439-0310. <lb/>1980.tb01243.x <lb/>Egna, N., O&apos;Connor, D., Stacy-Dawes, J., Tobler, M.W., <lb/>Pilfold, N., Neilson, K. et al. (2020) Camera settings and <lb/>biome influence the accuracy of citizen science approaches <lb/></listBibl>

      <page>722 <lb/></page>

      <note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

      <note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

      <note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

      <listBibl>to camera trap image classification. Ecology and Evolution, <lb/>10(21), 11954-11965. <lb/>Eitzel, M.V., Cappadonna, J.L., Santos-Lang, C., Duerr, R., <lb/>West, S.E., Virapongse, A. et al. (2017) Citizen science <lb/>terminology matters: exploring key terms. Citizen Science: <lb/>Theory and Practice, 2(1), 1-20. Available from: https://doi. <lb/>org/10.5334/cstp.96 <lb/>Fraisl, D., Hager, G., Bedessem, B., Gold, M., Hsing, P.Y., <lb/>Danielsen, F. et al. (2022) Citizen science in environmental <lb/>and ecological sciences. Nature Reviews Methods Primers, 2 <lb/>(1), 64. Available from: https://doi.org/10.1038/s43586-022-<lb/>00144-4 <lb/>Frigerio, D., Pipek, P., Kimmig, S., Winter, S., Melzheimer, J., <lb/>Dibl ıkov a, L. et al. (2018) Citizen science and wildlife <lb/>biology: synergies and challenges. Ethology, 124(6), 365-377. <lb/>Available from: https://doi.org/10.1111/eth.12746 <lb/>Galvis, N., Link, A. &amp; Di Fiore, A. (2014) A novel use of <lb/>camera traps to study demography and life history in wild <lb/>animals: a case study of spider monkeys (Ateles belzebuth). <lb/>International Journal of Primatology, 35(5), 908-918. <lb/>Available from: https://doi.org/10.1007/s10764-014-9791-3 <lb/>Gogarten, J.F., Hoffmann, C., Arandjelovic, M., Sachse, A., <lb/>Merkel, K., Dieguez, P. et al. (2020) Fly-derived DNA and <lb/>camera traps are complementary tools for assessing <lb/>mammalian biodiversity. Environmental DNA, 2(1), <lb/>63-76. <lb/>Head, J., Boesch, C., Robbins, M.M., Rabanal, L.I., Makaga, L. <lb/>&amp; Kuehl, H. (2013) Effective socio-demographic population <lb/>assessment of elusive species in ecology and conservation <lb/>management. Ecology and Evolution, 3, 2903-2916. <lb/>Hockings, K.J., Mubemba, B., Avanzi, C., Pleh, K., D€ ux, A., <lb/>Bersacola, E. et al. (2021) Leprosy in wild chimpanzees. <lb/>Nature, 598, 652-656. <lb/>Howe, E.J., Buckland, S.T., Despr es-Einspenner, M.L. &amp; K€ uhl, <lb/>H.S. (2017) Distance sampling with camera traps. Methods <lb/>in Ecology and Evolution, 8(11), 1558-1565. <lb/>Kalan, A.K., Hohmann, G., Arandjelovic, M., Boesch, C., <lb/>McCarthy, M.S., Agbor, A. et al. (2019) Novelty response of <lb/>wild African apes to camera traps. Current Biology, 29(7), <lb/>1211-1217. <lb/>Kendall, M.G. (1945) The treatment of ties in ranking <lb/>problems. Biometrika, 33(3), 239-251. <lb/>Kosmala, M., Wiggins, A., Swanson, A. &amp; Simmons, B. (2016) <lb/>Assessing data quality in citizen science. Frontiers in Ecology <lb/>and the Environment, 14(10), 551-560. <lb/>K€ uhl, H.S., Kalan, A.K., Arandjelovic, M., Aubert, F., <lb/>D&apos;Auvergne, L., Goedmakers, A. et al. (2016) Chimpanzee <lb/>accumulative stone throwing. Scientific Reports, 6(1), 22219. <lb/>Available from: https://doi.org/10.1038/srep22219 <lb/>Labuguen, R., Bardeloza, D.K., Negrete, S.B., Matsumoto, J., <lb/>Inoue, K. &amp; Shibata, T. (2019) Primate markerless pose <lb/>estimation and movement analysis using DeepLabCut. In: <lb/>2019 joint 8th international conference on informatics, <lb/>Electronics &amp; Vision (ICIEV) and 2019 3rd international <lb/>conference on imaging, Vision &amp; Pattern Recognition <lb/>(icIVPR). Spokane, WA: IEEE, pp. 297-300. <lb/>Magle, S.B., Fidino, M., Lehrer, E.W., Gallo, T., Mulligan, <lb/>M.P., R ıos, M.J. et al. (2019) Advancing urban wildlife <lb/>research through a multi-city collaboration. Frontiers in <lb/>Ecology and the Environment, 17(4), 232-239. Available <lb/>from: https://doi.org/10.1002/fee.2030 <lb/>Marshall, B.M., Strine, C. &amp; Hughes, A.C. (2020) Thousands <lb/>of reptile species threatened by under-regulated global trade. <lb/>Nature Communications, 11(1), 4738. <lb/>Maselli, V., Rippa, D., Russo, G., Ligrone, R., Soppelsa, O., <lb/>D&apos;Aniello, B. et al. (2014) Wild boars&apos; social structure in the <lb/>Mediterranean habitat. The Italian Journal of Zoology, 81(4), <lb/>610-617. Available from: https://doi.org/10.1080/11250003. <lb/>2014.953220 <lb/>Mattioli, L., Canu, A., Passilongo, D., Scandura, M. &amp; <lb/>Apollonio, M. (2018) Estimation of pack density in grey <lb/>wolf (Canis lupus) by applying spatially explicit capture-<lb/>recapture models to camera trap data supported by genetic <lb/>monitoring. Frontiers in Zoology, 15(1), 38. Available from: <lb/>https://doi.org/10.1186/s12983-018-0281-x <lb/>McCarthy, M.S., Stephens, C., Dieguez, P., Samuni, L., <lb/>Despr es-Einspenner, M.L., Harder, B. et al. (2021) <lb/>Chimpanzee identification and social network construction <lb/>through an online citizen science platform. Ecology and <lb/>Evolution, 11(4), 1598-1608. <lb/>Muneza, A.B., Ortiz-Calo, W., Packer, C., Cusack, J.J., <lb/>Jones, T., Palmer, M.S. et al. (2019) Quantifying the severity <lb/>of giraffe skin disease via photogrammetry analysis of <lb/>camera trap data. Journal of Wildlife Diseases, 55(4), 770-<lb/>781. <lb/>Ottinger, G. (2017) Reconstructing or reproducing?: scientific <lb/>authority and models of change in two traditions of citizen <lb/>science. In: The Routledge handbook of the political economy <lb/>of science. Oxfordshire, England: Routledge. <lb/>Palmer, M.S. &amp; Packer, C. (2021) Reactive anti-predator <lb/>behavioral strategy shaped by predator characteristics. PLoS <lb/>One, 16(8), e0256147. <lb/>Pardo, L.E., Bombaci, S., Huebner, S.E., Somers, M.J., Fritz, <lb/>H., Downs, C. et al. (2021) Snapshot Safari: a large-scale <lb/>collaborative to monitor Africa&apos;s remarkable biodiversity. <lb/>South African Journal of Science, 117(1-2), 1-4. Available <lb/>from: https://doi.org/10.17159/sajs.2021/8134 <lb/>Ratnieks, F.L.W., Schrell, F., Sheppard, R.C., Brown, E., <lb/>Bristow, O.E. &amp; Garbuzov, M. (2016) Data reliability in <lb/>citizen science: learning curve and the effects of training <lb/>method, volunteer background and experience on <lb/>identification accuracy of insects visiting ivy flowers. <lb/>Methods in Ecology and Evolution, 7(10), 1226-1235. <lb/>Available from: https://doi.org/10.1111/2041-210X.12581 <lb/>Rowcliffe, J.M., Jansen, P.A., Kays, R., Kranstauber, B. &amp; <lb/>Carbone, C. (2016) Wildlife speed cameras: measuring <lb/>animal travel speed and day range using camera traps. <lb/>Remote Sensing in Ecology and Conservation, 2(2), 84-94. <lb/></listBibl>

			<note place="footnote">ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></note>

			<page>723 <lb/></page>

			<note place="headnote">M. Arandjelovic et al. <lb/>Precise Community Science Video CamTrap Annotation <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License <lb/></note>

			<listBibl>Sakib, F. &amp; Burghardt, T. (2020) Visual recognition of great <lb/>ape behaviours in the wild. https://doi.org/10.48550/arXiv. <lb/>2011.10759 <lb/>Sanakoyeu A, Khalidov V, McCarthy MS, Vedaldi A, Neverova <lb/>N. Transferring dense pose to proximal animal classes. In: <lb/>Proceedings of the IEEE/CVF Conference on Computer <lb/>Vision and Pattern Recognition. 2020:5233-5242. <lb/>Schofield, D., Nagrani, A., Zisserman, A., Hayashi, M., <lb/>Matsuzawa, T., Biro, D. et al. (2019) Chimpanzee face <lb/>recognition from videos in the wild using deep learning. <lb/>Science Advances, 5(9), eaaw0736. <lb/>Shepley, A., Falzon, G., Meek, P. &amp; Kwan, P. (2021) <lb/>Automated location invariant animal detection in camera <lb/>trap images using publicly available data sources. Ecology <lb/>and Evolution, 11(9), 4494-4506. Available from: https://doi. <lb/>org/10.1002/ece3.7344 <lb/>Suessle, V., Arandjelovic, M., Kalan, A.K., Agbor, A., Boesch, <lb/>C., Brazzola, G. et al. (2023) Automatic individual <lb/>identification of patterned solitary species based on <lb/>unlabeled video data. Journal of WSCG, 31(1-2), 1-10. <lb/>Available from: https://doi.org/10.24132/JWSCG.2023.1 <lb/>Swanson, A., Kosmala, M., Lintott, C. &amp; Packer, C. (2016) A <lb/>generalized approach for producing, quantifying, and <lb/>validating citizen science data from wildlife images. <lb/>Conservation Biology, 30(3), 520-531. <lb/>Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A. <lb/>&amp; Packer, C. (2015) Snapshot Serengeti, high-frequency <lb/>annotated camera trap images of 40 mammalian species in <lb/>an African savanna. Scientific Data, 2(1), 150026. <lb/>Swinnen, K.R.R., Reijniers, J., Breno, M. &amp; Leirs, H. (2014) A <lb/>novel method to reduce time investment when processing <lb/>videos from camera trap studies. PLoS One, 9(6), e98881. <lb/>Available from: https://doi.org/10.1371/journal.pone.0098881 <lb/>Tagg, N., McCarthy, M., Dieguez, P., Bocksberger, G., Willie, <lb/>J., Mundry, R. et al. (2018) Nocturnal activity in wild <lb/>chimpanzees (Pan troglodytes): evidence for flexible sleeping <lb/>patterns and insights into human evolution. American <lb/>Journal of Physical Anthropology, 166(3), 510-529. <lb/>Thel, L., Chamaill e-Jammes, S., Keurinck, L., Catala, M., <lb/>Packer, C., Huebner, S.E. et al. (2021) Can citizen science <lb/>analysis of camera trap data be used to study reproduction? <lb/>Lessons from Snapshot Serengeti program. Wildlife Biology, <lb/>2021(2), 1-9. <lb/>Tichon, J., Gilchrist, J.S., Rotem, G., Ward, P. &amp; Spiegel, O. <lb/>(2020) Social interactions in striped hyena inferred from <lb/>camera trap data: is it more social than previously thought? <lb/>Current Zoology, 66(4), 345-353. Available from: https://doi. <lb/>org/10.1093/cz/zoaa003 <lb/>UNESCO Digital Library. Draft recommendation on open <lb/>science. 2021. Accessed November 17, 2021. https://unesdoc. <lb/>unesco.org/ark:/48223/pf0000378841 <lb/>Unger, S., Hull, Z. &amp; Rollins, M. (2019) Diversity of vertebrate <lb/>and invertebrate scavenging communities of reptile carcasses <lb/>in the piedmont of North Carolina, USA. Biodiversity <lb/>Journal, 10(1), 47-56. Available from: https://doi.org/10. <lb/>31396/Biodiv.Jour.2019.10.1.47.56 <lb/>Vaidyanathan, G. (2011) Apes in Africa: the cultured <lb/>chimpanzees. Nat News, 476(7360), 266-269. <lb/>Walls, G.L. (1942) The vertebrate eye and its adaptive radiation. <lb/>Bloomfield Hills, Michigan: Cranbrook Institute of Science. <lb/>Wehn, U. (2020) Global Citizen Science perspectives on Open <lb/>Science: written input by the CSGP Citizen Science &amp; Open <lb/>Science Community of Practice to the UNESCO <lb/>Recommendation on Open Science. <lb/>Willi, M., Pitman, R.T., Cardoso, A.W., Locke, C., Swanson, <lb/>A., Boyer, A. et al. (2019) Identifying animal species in <lb/>camera trap images using deep learning and citizen science. <lb/>Methods in Ecology and Evolution, 10(1), 80-91. Available <lb/>from: https://doi.org/10.1111/2041-210X.13099 <lb/>Zett, T., Stratford, K.J. &amp; Weise, F.J. (2022) Inter-observer <lb/>variance and agreement of wildlife information extracted <lb/>from camera trap images. Biodiversity and Conservation, 31 <lb/>(12), 3019-3037. Available from: https://doi.org/10.1007/ <lb/>s10531-022-02472-z <lb/></listBibl>

			<div type="annex">Supporting Information <lb/>Additional supporting information may be found online <lb/>in the Supporting Information section at the end of the <lb/>article. <lb/>Table S1. Chimp&amp;See behavioural categories in the classi-<lb/>fication interface. <lb/>Table S2. Example videos clips and associated <lb/>annotation data. <lb/>Table S3. Original and new Chimp&amp;See classification <lb/>categories. <lb/>Data S1. Supporting methods. <lb/>Figure S1. Difference in number of animals counted by <lb/>the community scientists and the professional <lb/>ecologist (PE). <lb/>724 <lb/>ª 2024 The Author(s). Remote Sensing in Ecology and Conservation published by John Wiley &amp; Sons Ltd on behalf of Zoological Society of London. <lb/></div>

			<note place="headnote">Precise Community Science Video CamTrap Annotation <lb/>M. Arandjelovic et al. <lb/></note>

			<note place="footnote">20563485, 2024, 6, Downloaded from https://zslpublications.onlinelibrary.wiley.com/doi/10.1002/rse2.402 by Cochrane Portugal, Wiley Online Library on [10/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </note>


	</text>

</TEI>