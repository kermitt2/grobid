<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Nonparametric estimation in a semimartingale <lb/>regression model. <lb/>Part 2. Robust asymptotic efficiency. * <lb/>Konev, V. † <lb/>S. Pergamenshchikov ‡ <lb/>September 16, 2009 <lb/>Abstract <lb/>In this paper we prove the asymptotic efficiency of the model se-<lb/>lection procedure proposed by the authors in [10]. To this end we <lb/>introduce the robust risk as the least upper bound of the quadratical <lb/>risk over a broad class of observation distributions. Asymptotic upper <lb/>and lower bounds for the robust risk have been derived. The asymp-<lb/>totic efficiency of the procedure is proved. The Pinsker constant is <lb/>found. <lb/>Keywords: Non-parametric regression; Model selection; Sharp oracle inequal-<lb/>ity; Robust risk; Asymptotic efficiency; Pinsker constant; Semimartingale <lb/>noise. <lb/>AMS 2000 Subject Classifications: Primary: 62G08; Secondary: 62G05 <lb/> * The paper is supported by the RFFI-Grant 09-01-00172-a. <lb/> † Department of Applied Mathematics and Cybernetics, Tomsk State University, Lenin <lb/>str. 36, 634050 Tomsk, Russia, e-mail: vvkonev@mail.tsu.ru <lb/> ‡ Laboratoire de Mathématiques Raphael Salem, Avenue de l&apos;Université, BP. 12, Uni-<lb/>versité de Rouen, F76801, Saint Etienne du Rouvray, Cedex France, Department of Math-<lb/>ematics and Mechanics,Tomsk State University, Lenin str. 36, 634041 Tomsk, Russia, <lb/>e-mail: Serge.Pergamenchtchikov@univ-rouen.fr <lb/></front>

			<page>1 <lb/></page>

			<front>hal-00417600, version 1 -16 Sep 2009 <lb/></front>

			<body>1 Introduction <lb/>In this paper we will investigate the asymptotic efficiency of the model <lb/>selection procedure proposed in [10] for estimating a 1-periodic function <lb/>S : R → R, S ∈ L 2 [0, 1], in a continuous time regression model <lb/>dy t = S(t)dt + dξ t , 0 ≤ t ≤ n , <lb/>(1.1) <lb/>with a semimartingale noise ξ = (ξ t ) 0≤t≤n . The quality of an estimate S (any <lb/>real-valued function measurable with respect to σ{y t , 0 ≤ t ≤ n}) for S is <lb/>given by the mean integrated squared error, i.e. <lb/>R Q ( S, S) = E Q,S || S − S|| 2 , <lb/>(1.2) <lb/>where E Q,S is the expectation with respect to the noise distribution Q given <lb/>a function S; <lb/>||S|| 2 = <lb/>1 <lb/>0 <lb/>S 2 (x)dx . <lb/>The semimartingale noise (ξ t ) 0≤t≤n is assumed to take values in the Skorohod <lb/>space D[0, n] and has the distribution Q on D[0, n] such that for any function <lb/>f from L 2 [0, n] the stochastic integral <lb/>I n (f ) = <lb/>n <lb/>0 <lb/>f s dξ s <lb/>(1.3) <lb/>is well defined with <lb/>E Q I n (f ) = 0 and E Q I 2 <lb/>n (f ) ≤ σ * <lb/>n <lb/>0 <lb/>f 2 <lb/>s ds <lb/>(1.4) <lb/>where σ * is some positive constant which may, in general, depend on n, i.e. <lb/>σ * = σ * <lb/>n , such that <lb/>0 &lt; lim inf <lb/>n→∞ <lb/>σ * <lb/>n ≤ lim sup <lb/>n→∞ <lb/>σ * <lb/>n &lt; ∞ . <lb/>(1.5) <lb/>Now we define a robust risk function which is required to measure the <lb/>quality of an estimate S provided that a true distribution of the noise (ξ t ) 0≤t≤n <lb/>is known to belong to some family of distributions Q * <lb/>n which will be specified <lb/>below. Just as in [6] we define the robust risk as <lb/> R * <lb/>n ( S n , S) = sup <lb/>Q∈Q * <lb/>n <lb/>R Q ( S n , S) . <lb/>(1.6) <lb/></body>

			<page>2 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>The goal of this paper is to prove that the model selection procedure for <lb/>estimating S in the model (1.1) constructed in [10] is asymptotically efficient <lb/>with respect to this risk. When studying the asymptotic efficiency of this <lb/>procedure, described in detail in Section 2, we suppose that the unknown <lb/>function S in the model (1.1) belongs to the Sobolev ball <lb/>W k <lb/>r = {f ∈ C k <lb/>per [0, 1] , <lb/>k <lb/>j=0 <lb/>||f (j) || 2 ≤ r} , <lb/>(1.7) <lb/>where r &gt; 0 , k ≥ 1 are some parameters, C k <lb/>per [0, 1] is a set of k times <lb/>continuously differentiable functions f : [0, 1] → R such that f (i) (0) = f (i) (1) <lb/>for all 0 ≤ i ≤ k. The functional class W k <lb/>r can be written as the ellipsoid in <lb/>l 2 , i.e. <lb/>W k <lb/>r = {f ∈ C k <lb/>per [0, 1] : <lb/>∞ <lb/>j=1 <lb/>a j θ 2 <lb/>j ≤ r} <lb/>(1.8) <lb/>where <lb/>a j = <lb/>k <lb/>i=0 <lb/>(2π[j/2]) 2i . <lb/>In [10] we established a sharp non-asymptotic oracle inequality for mean <lb/>integrated squared error (1.2). The proof of the asymptotic efficiency of <lb/>the model selection procedure below largely bases on the counterpart of this <lb/>inequality for the robust risk (1.6) given in Theorem 2.1. <lb/>It will be observed that the notion &quot;nonparametric robust risk&quot; was ini-<lb/>tially introduced in [3] for estimating a regression curve at a fixed point. The <lb/>greatest lower bound for such risks have been derived and a point estimate <lb/>is found for which this bound is attained. The latter means that the point <lb/>estimate turns out to be robust efficient. In [1] this approach was applied for <lb/>pointwise estimation in a heteroscedastic regression model. <lb/>The optimal convergence rate of the robust quadratic risks has been ob-<lb/>tained in [9] for the non-parametric estimation problem in a continuous time <lb/>regression model with a coloured noise having unknown correlation properties <lb/>under full and partial observations. The asymptotic efficiency with respect <lb/>to the robust quadratic risks, has been studied in [6], [7] for the problem <lb/>of non-parametric estimation in heteroscedastic regression models. In this <lb/>paper we apply this approach for the model (1.1). <lb/></body>

			<page>3 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>The rest of the paper is organized as follows. In Section 2 we construct the <lb/>model selection procedure and formulate (Theorem 2.1) the oracle inequality <lb/>for the robust risk. Section 3 gives the main results. In Section 4 we consider <lb/>an example of the model (1.1) with the Levy type martingale noise. In <lb/>Section 5 and 6 we obtain the upper and lower bounds for the robust risk. <lb/>In Section 7 some technical results are established. <lb/>2 Oracle inequality for the robust risk <lb/>The model selection procedure is constructed on the basis of a weighted least <lb/>squares estimate having the form <lb/>S γ = <lb/>∞ <lb/>j=1 <lb/>γ(j) θ j,n φ j with θ j,n = <lb/>1 <lb/>n <lb/>n <lb/>0 <lb/>φ j (t) dy t , <lb/>(2.1) <lb/>where (φ j ) j≥1 is the standard trigonometric basis in L 2 [0, 1] defined as <lb/>φ 1 = 1 , φ j (x) = <lb/>√ <lb/>2 T r j (2π[j/2]x) , j ≥ 2 , <lb/>(2.2) <lb/>where the function T r j (x) = cos(x) for even j and T r j (x) = sin(x) for odd <lb/>j; [x] denotes the integer part of x. The sample functionals θ j,n are estimates <lb/>of the corresponding Fourier coefficients <lb/>θ j = (S, φ j ) = <lb/>1 <lb/>0 <lb/>S(t) φ j (t) dt . <lb/>(2.3) <lb/>Further we introduce the cost function as <lb/>J n (γ) = <lb/>∞ <lb/>j=1 <lb/>γ 2 (j) θ 2 <lb/>j,n − 2 <lb/>∞ <lb/>j=1 <lb/>γ(j) θ j,n + ρ P n (γ) . <lb/>Here <lb/>θ j,n = θ 2 <lb/>j,n − <lb/>σ n <lb/>n <lb/>with σ n = <lb/>n <lb/>j=l <lb/>θ 2 <lb/>j,n , l = [ <lb/>√ <lb/>n] + 1 ; <lb/>P n (γ) is the penalty term defined as <lb/>P n (γ) = <lb/>σ n |γ| 2 <lb/>n <lb/>. <lb/></body>

			<page>4 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>As to the parameter ρ, we assume that this parameter is a function of n, i.e. <lb/>ρ = ρ n such that 0 &lt; ρ &lt; 1/3 and <lb/>lim <lb/>n→∞ <lb/>n δ ρ n = 0 for all δ &gt; 0 . <lb/>We define the model selection procedure as <lb/>S * = S b <lb/>γ <lb/>(2.4) <lb/>where γ is the minimizer of the cost function J n (γ) in some given class Γ of <lb/>weight sequences γ = (γ(j)) j≥1 ∈ [0, 1] ∞ , i.e. <lb/>γ = argmin γ∈Γ J n (γ) . <lb/>(2.5) <lb/>Now we specify the family of distributions Q * <lb/>n in the robust risk (1.6). Let <lb/>P n denote the class of all distributions Q of the semimartingale (ξ t ) satisfying <lb/>the condition (1.4). It is obvious that the distribution Q 0 of the process <lb/>ξ t = <lb/>√ <lb/>σ * w t , where (w t ) is a standard Brownian motion, enters the class P n , <lb/>i.e. Q ∈ P n . In addition, we need to impose some technical conditions on <lb/>the distribution Q of the process (ξ t ) 0≤t≤n . Let denote <lb/>σ(Q) = lim n→∞ max <lb/>1≤j≤n <lb/>E Q ξ 2 <lb/>j,n , <lb/>(2.6) <lb/>where <lb/>ξ j,n = <lb/>1 <lb/>√ n <lb/>I n (φ j ) , <lb/>(I n (φ j ) is given in (1.3)) and introduce two P n → R + functionals <lb/>L 1,n (Q) = <lb/>sup <lb/>x∈H , #(x)≤n <lb/>∞ <lb/>j=1 <lb/>x j E Q ξ 2 <lb/>j,n − σ(Q) <lb/>and <lb/>L 2,n (Q) = <lb/>sup <lb/>|x|≤1 , #(x)≤n <lb/>E Q <lb/> <lb/> <lb/>∞ <lb/>j=1 <lb/>x j ξ j,n <lb/> <lb/> <lb/>2 <lb/>where H = [−1, 1] ∞ , |x| 2 = ∞ <lb/>j=1 x 2 <lb/>j , #(x) = ∞ <lb/>j=1 1 {|x j |&gt;0} and <lb/>ξ j,n = ξ 2 <lb/>j,n − E Q ξ 2 <lb/>j,n . <lb/></body>

			<page>5 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>Now we consider the family of all distributions Q from P n with the growth <lb/>restriction on L 1,n (Q) + L 2,n (Q), i.e. <lb/>P * <lb/>n = Q ∈ P n : L 1,n (Q) + L 2,n (Q) ≤ l n , <lb/>where l n is a slowly increasing positive function, i.e. l n → +∞ as n → +∞ <lb/>and for any δ &gt; 0 <lb/>lim <lb/>n→∞ <lb/>l n <lb/>n δ = 0 . <lb/>It will be observed that any distribution Q from P * <lb/>n satisfies conditions C 1 ) <lb/>and C 2 ) on the noise distribution from [10] with c * <lb/>1,n ≤ l n and c * <lb/>2,n ≤ l n . We <lb/>remind that these conditions are <lb/>C 1 ) <lb/>c * <lb/>1,n = L 1,n (Q) &lt; ∞ ; <lb/>C 2 ) <lb/>c * <lb/>2,n = L 2,n (Q) &lt; ∞ . <lb/>In the sequel we assume that the distribution of the noise (ξ t ) in (1.1) is <lb/>known up to its belonging to some distribution family satisfying the following <lb/>condition. <lb/>C * ) Let Q * <lb/>n be a family of the distributions Q from P * <lb/>n such that Q 0 ∈ Q * <lb/>n . <lb/>An important example for such family is given in Section 4. <lb/>Now we specify the set Γ in the model selection procedure (2.4) and state <lb/>the oracle inequality for the robust risk (1.6) which is a counterpart of that <lb/>obtained in [10] for the mean integrated squared error (1.2). Consider the <lb/>numerical grid <lb/>A n = {1, . . . , k * } × {t 1 , . . . , t m } , <lb/>(2.7) <lb/>where t i = iε and m = [1/ε 2 ]; parameters k * ≥ 1 and 0 &lt; ε ≤ 1 are functions <lb/>of n, i.e. k * = k * (n) and ε = ε(n), such that for any δ &gt; 0 <lb/> <lb/>  <lb/>  <lb/>lim n→∞ k * (n) = +∞ , lim n→∞ <lb/>k * (n) <lb/>ln n <lb/>= 0 , <lb/>lim n→∞ ε(n) = 0 and lim n→∞ n δ ε(n) = +∞ . <lb/>(2.8) <lb/></body>

			<page>6 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>For example, one can take <lb/>ε(n) = <lb/>1 <lb/>ln(n + 1) <lb/>and k * (n) = ln(n + 1) <lb/>for n ≥ 1. <lb/>Define the set Γ as <lb/>Γ = {γ α , α ∈ A n } , <lb/>(2.9) <lb/>where γ α is the weight sequence corresponding to an element α = (β, t) ∈ A n , <lb/>given by the formula <lb/>γ α (j) = 1 {1≤j≤j 0 } + 1 − (j/ω α ) β 1 {j 0 &lt;j≤ωα} <lb/>(2.10) <lb/>where j 0 = j 0 (α) = [ω α /(1 + ln n)], ω α = (τ β t n) 1/(2β+1) and <lb/>τ β = <lb/>(β + 1)(2β + 1) <lb/>π 2β β <lb/>. <lb/>Along the lines of the proof of Theorem 2.1 in [10] one can establish the <lb/>following result. <lb/>Theorem 2.1. Assume that the unknown function S is continuously differ-<lb/>entiable and the distribution family Q * <lb/>n in the robust risk (1.6) satisfies the <lb/>condition C * ). Then the estimator (2.4), for any n ≥ 1, satisfies the oracle <lb/>inequality <lb/>R * <lb/>n ( S * , S) ≤ <lb/>1 + 3ρ − 2ρ 2 <lb/>1 − 3ρ <lb/>min <lb/>γ∈Γ <lb/>R * <lb/>n ( S γ , S) + <lb/>1 <lb/>n <lb/>D n (ρ) , <lb/>(2.11) <lb/>where the term D n (ρ) is defined in [10] such that <lb/>lim <lb/>n→∞ <lb/>D n (ρ) <lb/>n δ = 0 <lb/>(2.12) <lb/>for each δ &gt; 0. <lb/>Remark 2.1. The inequality (2.11) will be used to derive the upper bound <lb/>for the robust risk (1.6). It will be noted that the second summand in (2.11) <lb/>when multiplied by the optimal rate n 2k/(2k+1) tends to zero as n → ∞ for <lb/>each k ≥ 1. Therefore, taking into account that ρ → 0 as n → ∞, the <lb/>principal term in the upper bound is given by the minimal risk over the family <lb/>of estimates ( S γ ) γ∈Γ . As is shown in [5], the efficient estimate enters this <lb/>family. However one can not use this estimate because it depends on the <lb/>unknown parameters k ≥ 1 and r &gt; 0 of the Sobolev ball. It is this fact <lb/>that shows an adaptive role of the oracle inequality (2.11) which gives the <lb/>asymptotic upper bound in the case when this information is not available. <lb/></body>

			<page>7 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>3 Main results <lb/>In this Section we will show, proceeding from (2.11), that the Pinsker con-<lb/>stant for the robust risk (1.6) is given by the equation <lb/>R * <lb/>k,n = ((2k + 1)r) 1/(2k+1) <lb/>σ * <lb/>n k <lb/>(k + 1)π <lb/>2k/(2k+1) <lb/>. <lb/>(3.1) <lb/>It is well known that the optimal (minimax) rate for the Sobolev ball W k <lb/>r is <lb/>n 2k/(2k+1) (see, for example, [13], [12]). We will see that asymptotically the <lb/>robust risk of the model selection (2.4) normalized by this rate is bounded <lb/>from above by R * <lb/>k,n . Moreover, this bound can not be diminished if one <lb/>considers the class of all admissible estimates for S. <lb/>Theorem 3.1. Assume that, in model (1.1), the distribution of (ξ t ) satisfies <lb/>the condition C * ). Then the robust risk (1.6) of the model selection estimator <lb/>S * defined in (2.4), (2.9), has the following asymptotic upper bound <lb/>lim sup <lb/>n→∞ <lb/>n 2k/(2k+1) 1 <lb/>R * <lb/>k,n <lb/>sup <lb/>S∈W k <lb/>r <lb/>R * <lb/>n ( S * , S) ≤ 1 . <lb/>(3.2) <lb/>Now we obtain a lower bound for the robust risk (1.6). Let Π n be the set <lb/>of all estimators S n measurable with respect to the sigma-algebra <lb/>σ{y t , 0 ≤ t ≤ n} generated by the process (1.1). <lb/>Theorem 3.2. Under the conditions of Theorem 3.1 <lb/>lim inf <lb/>n→∞ <lb/>n 2k/(2k+1) 1 <lb/>R * <lb/>k,n <lb/>inf <lb/>e <lb/>S n ∈Π n <lb/>sup <lb/>S∈W k <lb/>r <lb/>R * <lb/>n ( S n , S) ≥ 1 . <lb/>(3.3) <lb/>Theorem 3.1 and Theorem 3.2 imply the following result <lb/>Corollary 3.3. Under the conditions of Theorem 3.1 <lb/>lim <lb/>n→∞ <lb/>n 2k/(2k+1) 1 <lb/>R * <lb/>k,n <lb/>inf <lb/>e <lb/>S n ∈Π n <lb/>sup <lb/>S∈W k <lb/>r <lb/>R * <lb/>n ( S n , S) = 1 . <lb/>(3.4) <lb/>Remark 3.1. The equation (3.4) means that the sequence R * <lb/>k,n defined by <lb/>(3.1) is the Pinsker constant (see, for example, [13], [12]) for the model (1.1). <lb/></body>

			<page>8 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>4 Example <lb/>Let the process (ξ t ) be defined as <lb/>ξ t = ̺ 1 w t + ̺ 2 z t , <lb/>(4.1) <lb/>where (w t ) t≥0 is a standard Brownian motion, (z t ) t≥0 is a compound Poisson <lb/>process defined as <lb/>z t = <lb/>N t <lb/>j=1 <lb/>Y j , <lb/>where (N t ) t≥0 is a standard homogeneous Poisson process with unknown <lb/>intensity λ &gt; 0 and (Y j ) j≥1 is an i.i.d. sequence of random variables with <lb/>E Y j = 0 , E Y 2 <lb/>j = 1 and E Y 4 <lb/>j &lt; ∞ . <lb/>Substituting (4.1) in (1.3) yields <lb/>E I n (f ) = (̺ 2 <lb/>1 + ̺ 2 <lb/>2 λ)||f || 2 . <lb/>In order to meet the condition (1.4) the coefficients ̺ 1 , ̺ 2 and the intensity <lb/>λ &gt; 0 must satisfy the inequality <lb/>̺ 2 <lb/>1 + ̺ 2 <lb/>2 λ ≤ σ * . <lb/>(4.2) <lb/>Note that the coefficients ̺ 1 , ̺ 2 and the intensity λ in (1.4) as well as σ * may <lb/>depend on n, i.e. ̺ i = ̺ i (n) and λ = λ(n). <lb/>As is stated in ([10], Theorem 2.2), the conditions C 1 ) and C 2 ) hold for <lb/>the process (4.1) with σ = σ(Q) = ̺ 2 <lb/>1 + ̺ 2 <lb/>2 λ defined in (2.6), c * <lb/>1 (n) = 0 and <lb/>c * <lb/>2 (n) ≤ 4σ(σ + ̺ 2 <lb/>2 E Y 4 <lb/>1 ) . <lb/>Let now Q * <lb/>n be the family of distributions of the processes (4.1) with the <lb/>coefficients satisfying the conditions (4.2) and <lb/>̺ 2 <lb/>2 ≤ l n , <lb/>(4.3) <lb/>where the sequence l n is taken from the definition of the set P * <lb/>n . Note that <lb/>the distribution Q 0 belongs to Q * <lb/>n . One can obtain this distribution putting <lb/>in (4.1) ̺ 1 = <lb/>√ <lb/>σ * and ̺ 2 = 0. It will be noted that Q * <lb/>n ⊂ P * <lb/>n if <lb/>4σ * (σ * + <lb/>√ <lb/>l n E Y 4 <lb/>1 ) ≤ l n . <lb/></body>

			<page>9 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>5 Upper bound <lb/>5.1 Known smoothness <lb/>First we suppose that the parameters k ≥ 1, r &gt; 0 and σ * in (1.4) are known. <lb/>Let the family of admissible weighted least squares estimates ( S γ ) γ∈Γ for the <lb/>unknown function S ∈ W k <lb/>r be given (2.9), (2.10). Consider the pair <lb/>α 0 = (k, t 0 ) <lb/>where t 0 = [r n /ε]ε, r n = r/σ * <lb/>n and ε satisfies the conditions in (2.8). Denote <lb/>the corresponding weight sequence in Γ as <lb/>γ 0 = γ α 0 . <lb/>(5.1) <lb/>Note that for sufficiently large n the parameter α 0 belongs to the set (2.9). <lb/>In this section we obtain the upper bound for the empiric squared error of <lb/>the estimator (1.6). <lb/>Theorem 5.1. The estimator S γ 0 satisfies the following asymptotic upper <lb/>bound <lb/>lim sup <lb/>n→∞ <lb/>n 2k/(2k+1) 1 <lb/>R * <lb/>k,n <lb/>sup <lb/>S∈W k <lb/>r <lb/>R * <lb/>n ( S γ 0 , S) ≤ 1 . <lb/>(5.2) <lb/>Proof. First by substituting the model (1.1) in the definition of θ j,n in (2.1) <lb/>we obtain <lb/>θ j,n = θ j + <lb/>1 <lb/>√ n <lb/>ξ j,n , <lb/>where the random variables ξ j,n are defined in (2.6). Therefore, by the defi-<lb/>nition of the estimators S γ in (2.1) we get <lb/>|| S γ 0 − S|| 2 = <lb/>n <lb/>j=1 <lb/>(1 − γ 0 (j)) 2 θ 2 <lb/>j − 2M n + <lb/>n <lb/>j=1 <lb/>γ 2 <lb/>0 (j) ξ 2 <lb/>j,n <lb/>with <lb/>M n = <lb/>1 <lb/>√ n <lb/>n <lb/>j=1 <lb/>(1 − γ 0 (j)) γ 0 (j) θ j ξ j,n . <lb/>It should be observed that <lb/>E Q,S M n = 0 <lb/></body>

			<page>10 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>for any Q ∈ Q * <lb/>n . Further the condition (1.4) implies also the inequality <lb/>E Q ξ 2 <lb/>j,n ≤ σ * <lb/>n for each distribution Q ∈ Q * <lb/>n . Thus, <lb/>R * <lb/>n ( S γ 0 , S) ≤ <lb/>n <lb/>j=ι 0 <lb/>(1 − γ 0 (j)) 2 θ 2 <lb/>j + <lb/>σ * <lb/>n <lb/>n <lb/>n <lb/>j=1 <lb/>γ 2 <lb/>0 (j) <lb/>(5.3) <lb/>where ι 0 = j 0 (α 0 ). Denote <lb/>υ n = n 2k/(2k+1) sup <lb/>j≥ι 0 <lb/>(1 − γ 0 (j)) 2 /a j , <lb/>where a j is the sequence as defined in (1.8). Using this sequence we estimate <lb/>the first summand in the right hand of (5.3) as <lb/>n 2k/(2k+1) <lb/>n <lb/>j=ι 0 <lb/>(1 − γ 0 (j)) 2 θ 2 <lb/>j ≤ υ n <lb/>j≥1 <lb/>a j θ 2 <lb/>j . <lb/>From here and (1.8) we obtain that for each S ∈ W k <lb/>r <lb/>Υ 1,n (S) = n 2k/(2k+1) <lb/>n <lb/>j=ι 0 <lb/>(1 − γ 0 (j)) 2 θ 2 <lb/>j ≤ υ n r . <lb/>Further we note that <lb/>lim sup <lb/>n→∞ <lb/>(r n ) 2k/(2k+1) υ n ≤ <lb/>1 <lb/>π 2k (τ k ) 2k/(2k+1) , <lb/>where the coefficient τ k is given (2.10). Therefore, for any η &gt; 0 and suffi-<lb/>ciently large n ≥ 1 <lb/>sup <lb/>S∈W k <lb/>r <lb/>Υ 1,n (S) ≤ (1 + η) (σ * <lb/>n ) 2k/(2k+1) Υ * <lb/>1 <lb/>(5.4) <lb/>where <lb/>Υ * <lb/>1 = <lb/>r 1/(2k+1) <lb/>π 2k (τ k ) 2k/(2k+1) . <lb/>To examine the second summand in the right hand of (5.2) we set <lb/>Υ 2,n = <lb/>1 <lb/>n 1/(2k+1) <lb/>n <lb/>j=1 <lb/>γ 2 <lb/>0 (j) . <lb/></body>

			<page>11 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>Since by the condition (1.5) <lb/>lim <lb/>n→∞ <lb/>t 0 <lb/>r n <lb/>= 1 , <lb/>one gets <lb/>lim <lb/>n→∞ <lb/>1 <lb/>(r n ) 1/(2k+1) Υ 2,n = Υ * <lb/>2 <lb/>with Υ * <lb/>2 = <lb/>2(τ k ) 1/(2k+1) k 2 <lb/>(k + 1)(2k + 1) <lb/>. <lb/>Note that by the definition (3.2) <lb/>(σ * <lb/>n ) 2k/(2k+1) Υ * <lb/>1,n + σ * <lb/>n (r n ) 1/(2k+1) Υ * <lb/>2 = R * <lb/>k,n . <lb/>Therefore, for any η &gt; 0 and sufficiently large n ≥ 1 <lb/>n 2k/(2k+1) sup <lb/>S∈W k <lb/>r <lb/>R * <lb/>n ( S γ 0 , S) ≤ (1 + η)R * <lb/>k,n . <lb/>Hence Theorem 5.1. <lb/>5.2 Unknown smoothness <lb/>Combining Theorem 5.1 and Theorem 2.1 yields Theorem 3.1. <lb/>6 Lower bound <lb/>First we obtain the lower bound for the risk (1.2) in the case of &quot;white noise&quot; <lb/>model (1.1), when ξ t = <lb/>√ <lb/>σ * w t . As before let Q 0 denote the distribution of <lb/>(ξ t ) 0≤t≤n in D[0, n]. <lb/>Theorem 6.1. The risk (1.2) corresponding to the the distribution Q 0 in the <lb/>model (1.1) has the following lower bound <lb/>lim inf <lb/>n→∞ <lb/>n 2k/(2k+1) inf <lb/>e <lb/>S n ∈Π n <lb/>1 <lb/>R * <lb/>k,n <lb/>sup <lb/>S∈W k <lb/>r <lb/>R 0 ( S n , S) ≥ 1 , <lb/>(6.1) <lb/>where R 0 (•, •) = R Q 0 (•, •). <lb/></body>

			<page>12 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>Proof. The proof of this result proceeds along the lines of Theorem 4.2 from <lb/>[6]. Let V be a function from C ∞ (R) such that V (x) ≥ 0, <lb/>1 <lb/>−1 V (x)dx = 1 <lb/>and V (x) = 0 for |x| ≥ 1. For each 0 &lt; η &lt; 1 we introduce a smoother <lb/>indicator of the interval [−1 + η, 1 − η] by the formula <lb/>I η (x) = η −1 <lb/>R <lb/>1 (|u|≤1−η) G <lb/>u − x <lb/>η <lb/>du . <lb/>It will be noted that I η ∈ C ∞ (R), 0 ≤ I η ≤ 1 and for any m ≥ 1 and positive <lb/>constant c &gt; 0 <lb/>lim <lb/>η→0 <lb/>sup <lb/>{f : |f | * ≤c} <lb/>R <lb/>f (x)I m <lb/>η (x) dx − <lb/>1 <lb/>−1 <lb/>f (x) dx = 0 <lb/>(6.2) <lb/>where |f | * = sup −1≤x≤1 |f (x)|. Further, we need the trigonometric basis in <lb/>L 2 [−1, 1], that is <lb/>e 1 (x) = 1/ <lb/>√ <lb/>2 , e j (x) = T r j (π[j/2]x) , j ≥ 2 . <lb/>(6.3) <lb/>Now we will construct of a family of approximation functions for a given <lb/>regression function S following [6]. For fixed 0 &lt; ε &lt; 1 one chooses the <lb/>bandwidth function as <lb/>h = h n = (υ * <lb/>ε ) <lb/>1 <lb/>2k+1 N n n − 1 <lb/>2k+1 <lb/>(6.4) <lb/>with <lb/>υ * <lb/>ε = <lb/>σ * <lb/>n kπ 2k <lb/>(1 − ε)r2 2k+1 (k + 1)(2k + 1) <lb/>and N n = ln 4 n <lb/>and considers the partition of the interval [0, 1] with the points x m = 2hm, <lb/>1 ≤ m ≤ M, where <lb/>M = [1/(2h)] − 1 . <lb/>For each interval [ x m − h, x m + h] we specify the smoothed indicator as <lb/>I η (v m (x)), where v m (x) = (x − x m )/h. The approximation function for S(t) <lb/>is given by <lb/>S z,n (x) = <lb/>M <lb/>m=1 <lb/>N <lb/>j=1 <lb/>z m,j D m,j (x) , <lb/>(6.5) <lb/>where z = (z m,j ) 1≤m≤M ,1≤j≤N is an array of real numbers; <lb/>D m,j (x) = e j (v m (x))I η (v m (x)) <lb/></body>

			<page>13 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>are orthogonal functions on [0, 1]. <lb/>Note that the set W k <lb/>r is a subset of the ball <lb/>B r = {f ∈ L 2 [0, 1] : ||f || 2 ≤ r} . <lb/>Now for a given estimate S n we construct its projection in L 2 [0, 1] into B r <lb/>F n := P r B r ( S n ) . <lb/>In view of the convexity of the set B r one has <lb/>|| S n − S|| 2 ≥ || F n − S|| 2 <lb/>for each S ∈ W k <lb/>r ⊂ B r . <lb/>From here one gets the following inequalities for the the risk (1.2) <lb/>sup <lb/>S∈W k <lb/>r <lb/>R 0 ( S n , S) ≥ sup <lb/>S∈W k <lb/>r <lb/>R 0 ( F n , S) ≥ <lb/>sup <lb/>{z∈R d : S z,n ∈W k <lb/>r } <lb/>R 0 ( F n , S) , <lb/>where d = MN. <lb/>In order to continue this chain of estimates we need to introduce a special <lb/>prior distribution on R d . Let κ = (κ m,j ) 1≤m≤M ,1≤j≤N be a random array with <lb/>the elements <lb/>κ m,j = t m,j κ * <lb/>m,j , <lb/>(6.6) <lb/>where κ * <lb/>m,j are i.i.d. gaussian N (0, 1) random variables and the coefficients <lb/>t m,j = <lb/>σ * <lb/>n y * <lb/>j <lb/>√ <lb/>nh <lb/>. <lb/>We choose the sequence (y * <lb/>j ) 1≤j≤N in the same way as in [6]( see (8.11)) , i.e. <lb/>y * <lb/>j = N k <lb/>n j −k − 1 . <lb/>We denote the distribution of κ by µ κ . We will consider it as a prior distri-<lb/>bution of the random parametric regression S κ,n which is obtained from (6.5) <lb/>by replacing z with κ. <lb/>Besides we introduce <lb/>Ξ n = z ∈ R d : max <lb/>1≤m≤M <lb/>max <lb/>1≤j≤N <lb/>|z m,j | <lb/>t m,j <lb/>≤ ln n . <lb/>(6.7) <lb/></body>

			<page>14 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>By making use of the distribution µ κ , one obtains <lb/>sup <lb/>S∈W k <lb/>r <lb/>R 0 ( S n , S) ≥ <lb/>{z∈R d : S z,n ∈W k <lb/>r }∩Ξ n <lb/>E Q 0 ,S z,n || F n − S z,n || 2 µ κ (dz) . <lb/>Further we introduce the Bayes risk as <lb/>R( F n ) = <lb/>R d <lb/>R 0 ( F n , S z,n )µ κ (dz) <lb/>and noting that || F n || 2 ≤ r we come to the inequality <lb/>sup <lb/>S∈W k <lb/>r <lb/>R 0 ( S n , S) ≥ R( F n ) − ̟ n <lb/>(6.8) <lb/>where <lb/>̟ n = E(1 {S κ,n / <lb/>∈W k <lb/>r } + 1 Ξ c <lb/>n <lb/>)(r + ||S κ,n || 2 ) . <lb/>By Proposition A.1 from Appendix A.1 one has, for any p &gt; 0, <lb/>lim <lb/>n→∞ <lb/>n p ̟ n = 0 . <lb/>Now we consider the first term in the right-hand side of (6.8). To obtain <lb/>a lower bound for this term we use the L 2 [0, 1]-orthonormal function family <lb/>(G m,j ) 1≤m≤M,1≤j≤N which is defined as <lb/>G m,j (x) = <lb/>1 <lb/>√ <lb/>h <lb/>e j (v m (x)) 1 (|vm(x)|≤1) . <lb/>We denote by g m,j and g m,j (z) the Fourier coefficients for functions F n and <lb/>S z , respectively, i.e. <lb/>g m,j = <lb/>1 <lb/>0 <lb/>F n (x)G m,j (x)dx and g m,j (z) = <lb/>1 <lb/>0 <lb/>S z,n (x) G m,j (x)dx . <lb/>Now it is easy to see that <lb/>|| F n − S z,n || 2 ≥ <lb/>M <lb/>m=1 <lb/>N <lb/>j=1 <lb/>( g m,j − g m,j (z)) 2 . <lb/></body>

			<page>15 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>Let us introduce the functionals K j (•) : L 1 [−1, 1] → R as <lb/>K j (f ) = <lb/>1 <lb/>−1 <lb/>e 2 <lb/>j (v) f (v) dv . <lb/>In view of (6.5) we obtain that <lb/>∂ <lb/>∂z m,j <lb/>g m,j (z) = <lb/>1 <lb/>0 <lb/>D m,j (x) G m,j (x) dx = <lb/>√ <lb/>h K j (I η ) . <lb/>Now Proposition A.2 implies <lb/>R( F n ) ≥ <lb/>M <lb/>m=1 <lb/>N <lb/>j=1 R d <lb/>E S z,n ( g m,j − g m,j (z)) 2 µ κ (dz) <lb/>≥ h <lb/>M <lb/>m=1 <lb/>N <lb/>j=1 <lb/>σ * K 2 <lb/>j (I η ) <lb/>K j (I 2 <lb/>η ) nh + t −2 <lb/>m,j σ * . <lb/>Therefore, taking into account the definition of the coefficients (t m,j ) in (6.6) <lb/>we get <lb/>R( F n ) ≥ <lb/>σ * <lb/>2nh <lb/>N <lb/>j=1 <lb/>τ j (η, y * <lb/>j ) <lb/>with <lb/>τ j (η, y) = <lb/>K 2 <lb/>j (I η )y <lb/>K j (I 2 <lb/>η )y + 1 <lb/>. <lb/>Moreover, the limit equality (6.2) implies directly <lb/>lim <lb/>η→0 <lb/>sup <lb/>j≥1 <lb/>sup <lb/>y≥0 <lb/>(y + 1)τ j (η, y) <lb/>y <lb/>− 1 = 0 . <lb/>Therefore, we can write that for any ν &gt; 0 <lb/>R( F n ) ≥ <lb/>σ * <lb/>2nh(1 + ν) <lb/>N <lb/>j=1 <lb/>y * <lb/>j <lb/>y * <lb/>j + 1 <lb/>. <lb/>It is easy to check directly that <lb/>lim <lb/>n→∞ <lb/>σ * <lb/>n <lb/>2nhR * <lb/>k,n <lb/>N <lb/>j=1 <lb/>y * <lb/>j <lb/>y * <lb/>j + 1 <lb/>= (1 − ε) <lb/>1 <lb/>2k+1 , <lb/></body>

			<page>16 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>where the coefficient R * <lb/>k,n is defined in (3.1). Therefore, (6.8) implies for any <lb/>0 &lt; ε &lt; 1 <lb/>lim inf <lb/>T →∞ <lb/>inf <lb/>b <lb/>S n <lb/>n <lb/>2k <lb/>2k+1 <lb/>1 <lb/>R * <lb/>k,n <lb/>sup <lb/>S∈W k <lb/>r <lb/>R 0 ( S n , S) ≥ (1 − ε) <lb/>1 <lb/>2k+1 . <lb/>Taking here limit as ε → 0 implies Theorem 6.1. <lb/>7 Appendix <lb/>A.1 Properties of the parametric family (6.5) <lb/>In this subsection we consider the sequence of the random functions S κ,n <lb/>defined in (6.5) corresponding to the random array κ = (κ m,j ) 1≤m≤M,1≤j≤N <lb/>given in (6.6). <lb/>Proposition A.1. For any p &gt; 0 <lb/>lim <lb/>n→∞ <lb/>n p lim <lb/>n→∞ <lb/>E ||S κ,n || 2 1 {S κ,n / <lb/>∈W k <lb/>r } + 1 Ξ c <lb/>n <lb/>= 0 . <lb/>This proposition follows directly from Proposition 6.4 in [7]. <lb/>A.2 Lower bound for parametric &quot;white noise&quot; mod-<lb/>els. <lb/>In this subsection we prove some version of the van Trees inequality from [8] <lb/>for the following model <lb/>dy t = S(t, z)dt + <lb/>√ <lb/>σ * dw t , 0 ≤ t ≤ n , <lb/>(A.1) <lb/>where z = (z 1 , . . . , z d ) ′ is vector of unknown parameters, w = (w t ) 0≤t≤T is a <lb/>Winier process. We assume that the function S(t, z) is a linear function with <lb/>respect to the parameter z, i.e. <lb/>S(t, z) = <lb/>d <lb/>j=1 <lb/>z j S j (t) . <lb/>(A.2) <lb/>Moreover, we assume that the functions (S j ) 1≤j≤d are continuous. <lb/></body>

			<page>17 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>Let Φ be a prior density in R d having the following form: <lb/>Φ(z) = Φ(z 1 , . . . , z d ) = <lb/>d <lb/>j=1 <lb/>ϕ j (z j ) , <lb/>where ϕ j is some continuously differentiable density in R. Moreover, let g(z) <lb/>be a continuously differentiable R d → R function such that for each 1 ≤ j ≤ d <lb/>lim <lb/>|z j |→∞ <lb/>g(z) ϕ j (z j ) = 0 and <lb/>R d <lb/>|g ′ <lb/>j (z)| Φ(z) dz &lt; ∞ , <lb/>(A.3) <lb/>where <lb/>g ′ <lb/>j (z) = <lb/>∂g(z) <lb/>∂z j <lb/>. <lb/>Let now X n = C[0, T ] and B(X n ) be σ -field generated by cylindric sets in <lb/>X n . <lb/>For any B(X n ) B(R d )-measurable integrable function ξ = ξ(x, θ) we <lb/>denote <lb/>Eξ = <lb/>R d X <lb/>ξ(y, z) µ z (dy) Φ(z)dz , <lb/>where µ z is distribution of the process (A.1) in X n . Let now ν = µ 0 be the <lb/>distribution of the process (σ * w t ) 0≤t≤n in X . It is clear (see, for example <lb/>[11]) that µ z &lt;&lt; ν for any z ∈ R d . Therefore, we can use the measure ν <lb/>as a dominated measure, i.e. for the observations (A.1) in X n we use the <lb/>following likelihood function <lb/>f (y, z) = <lb/>dµ z <lb/>dν <lb/>= exp <lb/>n <lb/>0 <lb/>S(t, z) <lb/>√ <lb/>σ * dy t − <lb/>n <lb/>0 <lb/>S 2 (t, z) <lb/>2σ * dt . <lb/>(A.4) <lb/>Proposition A.2. For any square integrable function g n measurable with <lb/>respect to σ{y t , 0 ≤ t ≤ n} and for any 1 ≤ j ≤ d the following inequality <lb/>holdsẼ <lb/>( g n − g(z)) 2 ≥ <lb/>σ * B 2 <lb/>j <lb/>n <lb/>0 S 2 <lb/>j (t) dt + σ * I j <lb/>, <lb/>(A.5) <lb/>where <lb/>B j = <lb/>R d <lb/>g ′ <lb/>j (z) Φ(z) dz and I j = <lb/>Rφ <lb/>2 <lb/>j (z) <lb/>ϕ j (z) <lb/>dz . <lb/></body>

			<page>18 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<body>Proof. First of all note that the density (A.3) is bounded with respect <lb/>to θ j ∈ R for any 1 ≤ j ≤ d, i.e. for any y = (y t ) 0≤t≤n ∈ X <lb/>lim sup <lb/>|z j |→∞ <lb/>f (y, z) &lt; ∞ . <lb/>Therefore, putting <lb/>Ψ j = Ψ j (y, z) = <lb/>∂ <lb/>∂θ j <lb/>ln(f (y, z)Φ(z)) <lb/>and taking into account condition (A.3) by integration by parts one gets <lb/>E (( g T − g(z))Ψ j ) = <lb/>R N ×R d <lb/>( g T (y) − g(z)) <lb/>∂ <lb/>∂z j <lb/>(f (y, z)Φ(z)) dz dν(y) <lb/>= <lb/>R N ×R d <lb/>g ′ <lb/>j (z) f (y, z)Φ(z) dz dν(y) = B j . <lb/>Now by the Bounyakovskii-Cauchy-Schwarz inequality we obtain the follow-<lb/>ing lower bound for the quiadratic risk <lb/>E( g T − g(z)) 2 ≥ <lb/>B 2 <lb/>j <lb/>EΨ 2 <lb/>j <lb/>. <lb/>Note that from (A.4) it is easy to deduce that under the distribution µ z <lb/>∂ <lb/>∂z j <lb/>ln f (y, z) = <lb/>n <lb/>0 <lb/>S j (t) <lb/>√ <lb/>σ * dy t − <lb/>n <lb/>0 <lb/>S(t, z)S j (t) <lb/>σ * <lb/>dt <lb/>= <lb/>n <lb/>0 <lb/>S j (t) <lb/>√ <lb/>σ * dw t . <lb/>This implies directly <lb/>E z <lb/>∂ <lb/>∂z j <lb/>ln f (y, z) = 0 <lb/>and <lb/>E z <lb/>∂ <lb/>∂z j <lb/>ln f (y, z) <lb/>2 <lb/>= <lb/>1 <lb/>σ * <lb/>n <lb/>0 <lb/>S 2 <lb/>j (t) dt . <lb/>Therefore, <lb/>EΨ 2 <lb/>j = <lb/>1 <lb/>σ * <lb/>n <lb/>0 <lb/>S 2 <lb/>j (t) dt + I j . <lb/>Hence Proposition A.2. <lb/></body>

			<page>19 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<listBibl>References <lb/>[1] Brua, J. (2009) Asymptotically efficient estimators for nonparametric <lb/>heteroscedastic regression models. Stat. Methodol. 6(1), p. 47-60. <lb/>[2] Fourdrinier, D. and Pergamenshchikov, S. (2007) Improved selection <lb/>model method for the regression with dependent noise. Annals of the <lb/>Institute of Statistical Mathematics, 59 (3), p. 435-464. <lb/>[3] Galtchouk, L. and Pergamenshchikov, S. (2006) Asymptotically ef-<lb/>ficient estimates for non parametric regression models.Statistics and <lb/>Probability Letters, v. 76 , 8, p. 852-860 <lb/>[4] Galtchouk, L. and Pergamenshchikov, S. (2004) Nonparametric sequen-<lb/>tial estimation of the drift in diffusion processes. Mathematical Meth-<lb/>ods of Statistics, 13, 1, 25-49. <lb/>[5] Galtchouk, L. and Pergamenshchikov, S. (2009) Sharp non-asymptotic <lb/>oracle inequalities for nonparametric heteroscedastic regression models. <lb/>Journal of Nonparametric Statistics, 2009, 21, 1, p. 1-16 <lb/>[6] Galtchouk, L. and Pergamenshchikov, S. (2009) Adaptive asymptoti-<lb/>cally efficient estimation in heteroscedastic nonparametric regression. <lb/>Journal of Korean Statistical Society, <lb/>http://ees.elsivier.com/jkss <lb/>[7] Galtchouk, L. and Pergamenshchikov, S. (2009) Adaptive asymptot-<lb/>ically efficient estimation in heteroscedastic nonparametric regression <lb/>via model selection. <lb/>http://hal.archives-ouvertes.fr/hal-00326910/fr/ <lb/>[8] Gill, R.D. and Levit, B.Y. Application of the van Trees inequality: a <lb/>Bayesian Cramér-Rao bound. Bernoulli, 1 59-79 (1995) <lb/>[9] Konev, V.V. and Pergamenshchikov, S.M. (2008) General model selec-<lb/>tion estimation of a periodic regression with a Gaussian noise. -Annals <lb/>of the Institute of Statistical Mathematics, 2008, Available online at <lb/>http://dx.doi.org/10.1007/s10463-008-0193-1 <lb/></listBibl>

			<page>20 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 <lb/></note>

			<listBibl>[10] Konev, V.V. and Pergamenshchikov, S.M. (2009) Nonparametric esti-<lb/>mation in a semimartingale regression model. Part 1. Oracle Inequali-<lb/>ties. Vestnik Tomskogo Universiteta, Mathematics and Mechanics. <lb/>[11] Liptser, R. Sh. and Shiryaev, A.N. (1977) Statistics of Random Pro-<lb/>cesses. I. General theory. New York : Springer. <lb/>[12] Nussbaum, M. (1985) Spline smoothing in regression models and <lb/>asymptotic efficiency in L 2 . Ann. Statist., 13, p. 984-997. <lb/>[13] Pinsker, M.S. (1981) Optimal filtration of square integrable signals in <lb/>gaussian white noise. Problems of Transimission information, 17 120-<lb/>133 <lb/></listBibl>

			<page>21 <lb/></page>

			<note place="headnote">hal-00417600, version 1 -16 Sep 2009 </note>


	</text>
</tei>
