<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Received 17 January 2025, accepted 3 February 2025, date of publication 10 February 2025, date of current version 13 February 2025. <lb/>Digital Object Identifier 10.1109/ACCESS.2025.3540399 <lb/>Optimizing Vision Transformers: Unveiling <lb/>&apos;Focus and Forget&apos; for Enhanced <lb/>Computational Efficiency <lb/>BANAFSHEH SABER LATIBARI , HOUMAN HOMAYOUN , <lb/>AND AVESTA SASAN , (Senior Member, IEEE) <lb/>Electrical and Computer Engineering, University of California at Davis, Davis, CA 95616, USA <lb/>Corresponding author: Banafsheh Saber Latibari (bsaberlatibari@ucadvis.edu) <lb/>This work was supported by the National Science Foundation under Award 2233893. <lb/>ABSTRACT Vision Transformers are renowned for their accuracy in computer vision tasks but are <lb/>computationally and memory expensive, making them challenging to deploy on resource-constrained edge <lb/>devices. In our research paper, we introduce a revolutionary approach to designing energy-aware dynamically <lb/>prunable Vision Transformers for use in edge applications. Our solution denoted as Incremental Resolution <lb/>Enhancing Transformer (IRET), works by the sequential sampling of the input image. However, in our <lb/>case, the embedding size of input tokens is considerably smaller than prior-art solutions. This embedding <lb/>is used in the first few layers of the IRET vision transformer until a reliable attention matrix is formed. <lb/>Then the attention matrix is used to sample additional information using a learnable 2D lifting scheme only <lb/>for important tokens and IRET drops the tokens receiving low attention scores. Hence, as the model pays <lb/>more attention to a subset of tokens for its task, its focus and resolution also increase. This incremental <lb/>attention-guided sampling of input and dropping of unattended tokens allow IRET to significantly prune its <lb/>computation tree on demand. By controlling the threshold for dropping unattended tokens and increasing the <lb/>focus of attended ones, we can train a model that dynamically trades off complexity for accuracy. Moreover, <lb/>using early exiting our model is capable of doing anytime prediction. This is especially useful for real-word <lb/>energy-sensitive edge devices, where accuracy and complexity could be dynamically traded based on factors <lb/>such as battery life, reliability, etc. <lb/>INDEX TERMS Computer vision, deep learning, pruning, vision transformer. <lb/></front>

			<body>I. INTRODUCTION <lb/>Recent advancements in deep learning and GPU capabil-<lb/>ities [12] have significantly improved computer vision&apos;s <lb/>detection and prediction. A major innovation is the use of <lb/>transformer models, first for Natural Language Processing <lb/>(NLP) in 2017 and later for visual tasks [7]. Visual trans-<lb/>formers, especially those developed by Google Brain in 2020, <lb/>have outperformed traditional CNNs in accuracy, especially <lb/>with large datasets. However, their high computational <lb/>and memory requirements pose challenges for edge device <lb/>deployment [32], [78], primarily due to their reliance on <lb/></body>

			<front>The associate editor coordinating the review of this manuscript and <lb/>approving it for publication was Kumaradevan Punithakumar . <lb/></front>

			<body>complex global attention mechanisms and MLPs. To mitigate <lb/>these demands, various strategies like multi-scale processing, <lb/>token dropping, early prediction, softmax elimination, and <lb/>efficient attention approaches have been researched. These <lb/>approaches are summarized in Section III. While these solu-<lb/>tions address certain aspects of the computational challenges, <lb/>they fall short of fully optimizing context-aware computation. <lb/>The main contributions of this paper are as follows: <lb/>1) This paper introduces a novel context-aware <lb/>approximation technique for dynamic pruning of <lb/>computational trees in transformer models, diverging <lb/>significantly from existing methods. We identify an <lb/>underutilized potential in transformers for context-<lb/>based approximation, which we argue can greatly <lb/></body>

			<front>27908 <lb/>2025 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. <lb/>For more information, see https://creativecommons.org/licenses/by/4.0/ <lb/>VOLUME 13, 2025 <lb/></front>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 1. (Left): Overall structure of original Visual Transformer (ViT) in [10]. (right): Encoder solution used in ViT, illustrating the implementation <lb/>details of Multi-Head Self Attention (MSA) from h scaled dot-product attention units. <lb/>enhance their efficiency with minimal accuracy impact, <lb/>broadening their application scope. <lb/>2) We present the Incremental Resolution Enhancing <lb/>Transformer (IRET), a transformative model architec-<lb/>ture that employs attention-based input sampling. <lb/>3) Utilizing learnable 2D lifting schemes, IRET processes <lb/>three input samples incrementally, thereby building <lb/>contextual awareness early. This architecture allows <lb/>IRET to use temporal attention scores for two key <lb/>functions: a) forget: discarding unattended tokens, and <lb/>b) focus: selectively enhancing the embedding size of <lb/>attended tokens by merging existing features with new <lb/>ones from a 2D lifting scheme output. <lb/>This approach mirrors human visual perception, starting <lb/>with a broad context understanding and then focusing on <lb/>more pertinent image aspects. IRET thus uses minimal infor-<lb/>mation initially for context comprehension, subsequently <lb/>concentrating on key image tokens through incremental <lb/>sampling while ignoring less relevant ones. The remainder <lb/>of the paper is structured as follows: Section II covers <lb/>background information. Section III reviews related work. <lb/>Section IV details the IRET architecture. Section V presents <lb/>experimental evaluations. Finally, Section VI concludes the <lb/>paper. <lb/>II. BACKGROUND <lb/>Fig. 1. (left) shows the Visual Transformer (ViT) [10] <lb/>architecture, and Fig. 1. (right) captures the structure of its <lb/>encoder layer. In ViT the input image is split into fixed-size <lb/>patches by reshaping the image x ∈ R H ×W ×C into a sequence <lb/>of flattened 2D patches x p ∈ R N ×(P 2 .C) . The (H , W ) is the <lb/>image resolution, C is the number of channels, (P, P) is the <lb/>image patch resolution, and N = HW /P 2 is the number <lb/>of patches. The attention mechanism used in the encoder is <lb/>scaled dot-product attention suggested in [56]. The inputs <lb/>are queries Q and keys K of dimension d k , and values V <lb/>of dimension d v . The encoder is designed to linearly project <lb/>the queries, keys, and values h times with different learned <lb/>linear projections to d k , d k , and d v dimensions, respectively. <lb/>As shown in Fig. 1(right), each encoder layer uses h scaled <lb/>dot-product attention heads. Scaled dot-product attention <lb/>heads compute the matrix in Eq. 1 yielding d v -dimensional <lb/>output values that are later concatenated and projected. The <lb/>Multi-Head Self Attention (MSA), the function of which <lb/>is captured in Eq. 2, allows the model to jointly attend <lb/>to information from different representation subspaces at <lb/>different positions. Similar to BERT&apos;s class token [9], ViT <lb/>prepends a learnable embedding to embedded patches (z 0 <lb/>0 = <lb/>x class) , whose state at the output of the encoder (z 0 <lb/>L ) serves as <lb/>the image representation y. Layernorm (LN ) is applied before <lb/>and residual connections after every block. <lb/>Attention(Q, K , V ) = Softmax(QK T / d k )V <lb/>(1) <lb/>MSA(Q, K , V ) = Concat(head i , . . . , head h )W O , (2) <lb/>head i = Attention(QW <lb/>Q <lb/>i , KW K <lb/>i , VW V <lb/>i ) (3) <lb/>The Visual transformer function is captured using equa-<lb/>tions 4 through 7: <lb/>z 0 = [x class ; x 1 <lb/>p E; x 2 <lb/>p E; . . . .; x N <lb/>p E] + E pos , <lb/>E ∈ R (P 2 .C) <lb/>× D, E pos <lb/>(4) <lb/>z ′ <lb/>l = MSA(LN (z l-1 )) + z l-1 , l = 1 . . . L <lb/>(5) <lb/>z l = MLP(LN (z ′ <lb/>l )) + z ′ <lb/>l , <lb/>l = 1 . . . L <lb/>(6) <lb/>y = LN (z 0 <lb/>l ) <lb/>(7) <lb/>The classification head is attached to z 0 <lb/>L and implemented <lb/>by an MLP with one hidden layer at pre-training and <lb/>one linear layer at fine-tuning. 1-Dimensional Position <lb/>embedding is added to the patch embeddings to retain <lb/>positional information. In a similar vein, DETR [4] exploits <lb/>a pure transformer to create an end-to-end object detection <lb/>framework. Taking a different approach, DeiT [55] enhances <lb/>ViT by introducing the distillation token, and leverages a <lb/>teacher model to decrease the necessary training data. <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27909 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 2. Timeline illustrating the proposed Multi-Scale Vision Transformer architectures [13], [17], [23], [34], [40], [60], [61], [75], [81], [82]. <lb/>III. RELATED WORKS <lb/>Several studies have focused on reducing the high computa-<lb/>tional complexity of vision transformers, resulting in models <lb/>with similar accuracy but lower complexity. This section <lb/>offers a brief overview of these approaches. <lb/>A. MULTISCALE VIOSN TRANSFORMERS <lb/>A widely adopted strategy for addressing the computational <lb/>complexity of vision transformers is pyramid-style process-<lb/>ing. This technique processes input images at multiple scales, <lb/>effectively capturing both coarse and fine contextual informa-<lb/>tion [13], [42], [44], [68]. Fig. 2. provides a summary of the <lb/>key approaches within this category. Numerous models have <lb/>successfully implemented this strategy, including: Pyramid <lb/>Vision Transformer (PVT) [60], Swin Transformer [40], <lb/>Multi-scale Vision Transformer (MViT) [13], PVT v2 [61], <lb/>and Wave-ViT [75]. <lb/>PVT takes in detailed image patches to effectively capture <lb/>fine-grained information for high-resolution representation. <lb/>PVT employs a pyramid structure that gradually reduces in <lb/>size, helping manage computational complexity in deeper <lb/>layers. The authors introduce a spatial-reduction attention <lb/>layer, which plays a role in conserving resources during <lb/>computation. <lb/>The Swin Transformer introduces a hierarchical <lb/>transformer architecture that utilizes shifted windows for <lb/>representation computation. This approach ensures linear <lb/>computational complexity with respect to image size. The <lb/>model&apos;s early stages involve the processing of small patches, <lb/>with the gradual merging of neighboring patches in deeper <lb/>layers. The Swin Transformer adopts a shared key set among <lb/>patches within the same window, effectively mitigating <lb/>latency concerns associated with earlier sliding window-<lb/>based self-attention methods. <lb/>MViT incorporates several channel-resolution stages, each <lb/>serving a distinct purpose. In the initial layers, the model <lb/>operates at an elevated spatial resolution coupled with a con-<lb/>strained channel dimension. As the network delves deeper, <lb/>spatial resolution diminishes while channel dimensions <lb/>expand significantly. This ingenious design culminates in the <lb/>formation of a feature pyramid, effectively encompassing a <lb/>comprehensive spectrum of features across different scales. <lb/>PVT v2 offers reduced computational complexity while <lb/>preserving local image continuity. Additionally, the model <lb/>features a flexible position encoding scheme. In the Wave-<lb/>ViT, they utilize a down-sampling technique based on wavelet <lb/>transforms and integrate it with self-attention learning. In this <lb/>architecture, the wavelet block plays a central role. It employs <lb/>Discrete Wavelet Transform (DWT) to process the key and <lb/>value inputs separately, dividing them into four distinct <lb/>subbands. These subbands are then stacked together, followed <lb/>by a convolutional operation that maintains locality within <lb/>each subband. The output of this convolutional layer feeds <lb/>into both the multi-head attention and inverse DWT layers. <lb/>While multi-scale designs effectively capture contextual <lb/>information at various resolutions, they typically rely on fixed <lb/>embedding dimensions and lack mechanisms for incremental <lb/>refinement or adaptive token management. Our proposed <lb/>solution addresses these gaps by employing smaller initial <lb/>embedding dimensions that incrementally increase based on <lb/>attention-driven sampling, enabling dynamic refinement of <lb/>resolution as the model processes the input. Additionally, <lb/>our approach incorporates token pruning guided by attention <lb/>scores, ensuring that computational resources are focused on <lb/>the most relevant features. This integration of context-aware <lb/>approximation with incremental resolution enhancement <lb/>complements existing multi-scale methods. <lb/>B. PATCH AND TOKEN PRUNING <lb/>Numerous studies have highlighted the sparse nature of <lb/>attention matrices within transformer models and identified <lb/>instances of token redundancy that don&apos;t significantly con-<lb/>tribute to final predictions. Building upon these observations, <lb/></body>

			<page>27910 <lb/></page>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 3. Timeline illustrating the proposed token and patch pruning approaches for transformer architectures [14], [19], [26], [31], [37], [38], [41], [48], <lb/>[63], [73], [74], [76]. <lb/>various strategies have been introduced to trim these redun-<lb/>dant tokens and enhance efficiency [5], [25], [28], [39], <lb/>[59], [67]. Unlike convolutional models, transformers can <lb/>leverage unstructured pruned inputs to their advantage. The <lb/>proposed techniques for token pruning can be classified <lb/>into two primary categories: static pruning and dynamic <lb/>pruning techniques. Certain methods apply a consistent <lb/>approach across different types of inputs. Other techniques <lb/>adapt their strategies based on the specific characteristics <lb/>of different inputs. Rao et al. [48], introduced DynamicViT. <lb/>This approach centers around enhancing the transformer <lb/>architecture by integrating a predictive module into specific <lb/>layers. The purpose of this module is to forecast the <lb/>importance score assigned to each token. Consequently, <lb/>tokens with lower scores undergo a hierarchical pruning <lb/>process. This strategy entails disconnecting tokens that have <lb/>been pruned from the remaining tokens within the attention <lb/>matrix. This task is achieved using the Gumbel-Softmax <lb/>masking strategy. <lb/>Wang et al. [63] demonstrated a correlation between the <lb/>complexity of input images and the required number of <lb/>tokens for accurate predictions. This suggests that simpler <lb/>images can be accurately predicted using fewer tokens. They <lb/>introduced DVT, a sequential transformer model designed to <lb/>process images with varying token counts. In their approach, <lb/>the concept of early exiting is utilized. This involves <lb/>halting computation if accurate predictions are achieved, <lb/>thereby bypassing the use of models with higher token <lb/>counts. To optimize the utilization of upper-level transformer <lb/>models and prevent computational inefficiencies, they reused <lb/>features and relationships. <lb/>Kim et al. [26], introduced Learned Token Pruning (LTP). <lb/>This approach involves dynamically removing tokens that <lb/>are deemed less significant. This determination is made <lb/>by utilizing a threshold value, which the model learns <lb/>independently for each layer. Xu et al. [74] proposed an <lb/>approach called Evo-ViT in which they introduced the <lb/>concept of slow-fast updating. This involves employing <lb/>separate computational pathways to update tokens based <lb/>on their informational relevance. This way, informative and <lb/>uninformative tokens are processed differently, helping to <lb/>maintain the spatial structure of the data during updates. <lb/>Liang et al. [31] proposed EViT, which utilizes a token <lb/>reorganization strategy for detecting attentive tokens while <lb/>merging inattentive ones into a singular token. The unique <lb/>feature of this model is its independence from the necessity <lb/>of a fully trained ViT. Nevertheless, adjusting the target <lb/>ratio would still mandate retraining the model. Fayyaz et al. <lb/>[14] presented ATS, a distinctive module referred to as a <lb/>differentiable and parameter-free Adaptive Token Sampler. <lb/>This module facilitates the dynamic selection of tokens <lb/>from input images based on attention scores, allowing for <lb/>variability in the number of chosen tokens for each image. <lb/>ATS can be seamlessly incorporated into a pre-trained model <lb/>to enhance its performance. Liu et al. [37] established <lb/>that employing a technique known as PatchDropout, which <lb/>involves the random omission of input image patches, enables <lb/>efficient training of standard ViT models at high resolutions. <lb/>This method achieves a significant reduction of at least 50% <lb/>in both FLOPs and memory consumption on typical datasets <lb/>with natural images. <lb/>Meng et al. [41] introduced AdaViT, a framework that <lb/>autonomously determines the utilization of patches, self-<lb/>attention heads, and layers within ViT. The core innovation <lb/>involves integrating a lightweight multi-head subnetwork <lb/>(referred to as the decision network) into each transformer <lb/>block of the backbone network. This auxiliary network learns <lb/>to predict binary choices concerning patch embedding incor-<lb/>poration, self-attention head engagement, and block omission <lb/>across the network. Yin et al. [76] introduced A-ViT, which <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27911 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 4. Timeline of proposed early exiting approaches for transformers [3], [11], [15], [21], [33], [52], [53], [63], [70], [71], [72], [83], [84]. <lb/>involves an adaptive inference mechanism. This mechanism <lb/>intelligently stops the computation process for various tokens <lb/>at different depths, focusing computational resources only <lb/>on tokens that contribute significantly to discrimination. This <lb/>approach dynamically adjusts the allocation of computation <lb/>resources. A-ViT integrated with high-performance hard-<lb/>ware. This is facilitated by the removal of paused tokens from <lb/>ongoing computations, resulting in enhanced computational <lb/>efficiency. The entire halting process can be acquired through <lb/>the model&apos;s existing parameters. <lb/>These methods effectively reduce computational com-<lb/>plexity but are not without challenges. Many approaches <lb/>rely heavily on retraining or manual parameter adjustments, <lb/>which limits their scalability and adaptability. Static methods <lb/>often lack contextual awareness, whereas dynamic methods <lb/>can introduce significant overhead or fail to maintain spatial <lb/>and semantic coherence. Our proposed architecture bridges <lb/>these gaps by incorporating a context-aware approxima-<lb/>tion mechanism with incremental resolution enhancement. <lb/>Unlike static approaches, it dynamically adjusts embedding <lb/>dimensions and utilizes token pruning based on attention <lb/>scores, ensuring computational resources are dedicated to <lb/>the most informative tokens. Additionally, the token pruning <lb/>mechanisms discussed in this subsection seamlessly integrate <lb/>with our model, boosting its efficiency and adaptability. <lb/>C. EARLY TERMINATION <lb/>Earlier, scholars introduced the concept of anytime prediction <lb/>in the realm of computer vision. Multiexit architectures <lb/>can be created from deep neural networks by introducing <lb/>branches that exit early after certain intermediate layers. <lb/>This transformation enables the inference process to adapt <lb/>dynamically, which proves beneficial for IoT applications <lb/>with strict latency demands. These applications often face <lb/>fluctuating communication and computation resources [3]. <lb/>Building upon this idea, certain studies have extended this <lb/>approach to transformers, effectively striking a favorable <lb/>balance between prediction speed and accuracy. Elbayad et al. <lb/>[11] introduced a depth-adaptive transformer in which <lb/>predictions occur at different network stages, with net-<lb/>work length and computation adjusting according to input <lb/>sequences. They trained the decoder using aligned and mixed <lb/>methods, examining sequence-specific versus token-specific <lb/>depth prediction approaches. Xin et al. [70], revealed that <lb/>different layers of BERT exhibit varying behaviors, with <lb/>some layers being redundant. As a solution, they introduced <lb/>DeeBERT, which enables samples to exit earlier through <lb/>off-ramps to improve efficiency. Zhou et al. [83], introduced <lb/>a solution known as Patience-based Early Exit (PABEE), <lb/>which involves incorporating an internal classifier within <lb/>each layer of a pretrained language model (PLM). This <lb/>approach is designed to halt the model&apos;s inference process <lb/>when the internal classifier&apos;s accuracy remains consistent for <lb/>a predetermined period, effectively mitigating unnecessary <lb/>processing. Sun et al. [52], constructed an ensemble model <lb/>that utilized internal classifiers. This approach capitalizes <lb/>on the internal classifiers being trained for predictions <lb/>on the same task. They introduced a voting strategy that <lb/>leverages predictions from all preceding internal classifiers. <lb/>This strategy aids in determining both the optimal timing for <lb/>exiting the process and the corresponding label assignment. <lb/>Li et al [30] introduced an approach aimed at expediting <lb/>the inference process of a pre-trained sequence labeling <lb/>model. The proposed methodology includes two distinct <lb/>components: SENTEE, which operates at the sentence level <lb/>and enables early exiting in sequence labeling, and TOKEE, <lb/>an early-exit mechanism functioning at the token level. <lb/>LeeBERT [84] introduces a training approach in which every <lb/>exit learns not only from the final layer but also from each <lb/>other. In the BERxiT paper [71], the focus was on rectifying <lb/>drawbacks in earlier early exit methods for BERT. These <lb/>methods were restricted to classification tasks and couldn&apos;t <lb/>fully leverage BERT&apos;s potential due to limited fine-tuning <lb/>strategies. The solution introduced was a &apos;&apos;learning-to-exit&apos;&apos; <lb/>module, extending early exits to diverse tasks and enhancing <lb/>BERT&apos;s utilization. <lb/></body>

			<page>27912 <lb/></page>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 5. Softmax complexity reduction approaches in transformers [6], [27], [35], [46], [51], [65], [80]. <lb/>He et al. [21], presented Magic Pyramid (MP) which <lb/>utilizes token pruning for width-wise computation reduction <lb/>and incorporates early exit strategies to address depth-wise <lb/>computation reduction, ultimately leading to improved <lb/>efficiency in model inference. Liao et al. [33] presents a <lb/>solution for global early exits, utilizing information from <lb/>both preceding and subsequent layers to facilitate the exit <lb/>process. The Dynamic transformer [63] was introduced <lb/>as an approach aimed at automatically determining the <lb/>optimal number of tokens necessary for processing each <lb/>input image. This was accomplished by utilizing a series of <lb/>interconnected transformers, with each one accommodating <lb/>an increasing number of tokens. Throughout the testing <lb/>phase, these transformers were sequentially activated in an <lb/>adaptable fashion. In essence, the inference process would <lb/>conclude once a prediction of sufficient confidence was <lb/>generated. In this study [3], seven distinct designs for early <lb/>exit branches are introduced, which can be integrated into <lb/>ViT backbones. By conducting comprehensive experiments <lb/>involving tasks such as image classification and crowd <lb/>counting -the latter involving regression, it is demonstrated <lb/>that these architectures offer valuable options for striking a <lb/>balance between classification accuracy and inference speed, <lb/>depending on the specific task. <lb/>To conclude this subsection, we emphasize that the early <lb/>termination mechanisms discussed here offer a variety of <lb/>strategies for balancing computational efficiency and pre-<lb/>dictive accuracy. In our proposed architecture, classification <lb/>heads have been integrated into the IRET model to facilitate <lb/>real-time predictions, providing an inherent mechanism <lb/>for early exits. Furthermore, the diverse early prediction <lb/>techniques reviewed can be seamlessly incorporated into our <lb/>framework. <lb/>D. SOFTMAX COMPLEXITY REDUCTION <lb/>The softmax operation in transformers is a major computa-<lb/>tional bottleneck, especially with longer sequences. It relies <lb/>on costly exponential functions, and achieving numerical <lb/>stability often involves extra steps. Efforts to speed up, <lb/>approximate, or eliminate softmax have been made [6], [27], <lb/>[51], [57]. Choromanski et al. [6] proposed Performers, <lb/>for estimating regular full-rank-attention transformers with <lb/>linear space and time complexity. Unlike traditional methods <lb/>that rely on priors like sparsity or low-rankness, performers <lb/>employ fast attention to approximate softmax attention <lb/>kernels. This enables scalable and efficient modeling of atten-<lb/>tion mechanisms beyond softmax, addressing the quadratic <lb/>complexity challenge of conventional transformers. <lb/>Koohpayegani et al. [27] presentd SimA, an attention block <lb/>that replaces the softmax layer with L1-norm normalization, <lb/>simplifying attention to matrix multiplications. SimA main-<lb/>tains comparable accuracy to state-of-the-art transformer <lb/>variants like DeiT, XCiT, and CvT while removing the <lb/>computational overhead of Softmax. <lb/>Stevens et al. [51] introduced Softermax, an optimized <lb/>softmax algorithm designed for hardware efficiency. Soft-<lb/>ermax integrates base replacement, low-precision softmax <lb/>computations, and an online normalization calculation. <lb/>By leveraging the fine-tuning principles of transformer-<lb/>based networks, they apply Softermax-aware fine-tuning <lb/>to minimize accuracy loss without imposing additional <lb/>training burdens. Furthermore, they provided insights into <lb/>the microarchitecture required for implementing Softermax <lb/>in an inference accelerator. Qin et al. proposed cosFORMER, <lb/>which leverages non-negativity and a non-linear re-weighting <lb/>scheme in the softmax attention matrix to create a linear <lb/>transformer [46]. <lb/>The softmax overhead reduction methods discussed pro-<lb/>vide efficient alternatives to traditional softmax, improving <lb/>scalability and accuracy. By integrating these techniques into <lb/>our proposed model, we can further decrease computational <lb/>overhead and enhance performance. <lb/>E. NOVEL ATTENTIONS <lb/>The computational challenge of quadratic complexity in <lb/>self-attention has long been a prominent obstacle when <lb/>applying the models to tasks in computer vision. A pri-<lb/>mary research focus in enhancing the efficiency of vision <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27913 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 6. Attention optimization approaches for transformer models [16], [18], [22], [24], [36], [43], [45], [54], [58], [62], [66], [69], [77]. <lb/>transformers involves the reduction of computational costs <lb/>associated with self-attention modules [2], [29], [47], <lb/>[50], [79]. <lb/>In Flatten Transformer, the focused linear attention <lb/>module addresses the computational complexity issue of <lb/>self-attention by introducing a module that combines high <lb/>efficiency with expressiveness [16]. CrossFormer and its <lb/>enhanced version, CrossFormer++, explicitly leverage fea-<lb/>tures of different scales, while addressing issues such as <lb/>self-attention map enlargement and amplitude explosion [62]. <lb/>EfficientViT introduces a new building block and cascaded <lb/>group attention module to improve memory efficiency and <lb/>computational redundancy in transformer models, achieving <lb/>a commendable trade-off between speed and accuracy [36]. <lb/>FasterViT [18], a hybrid CNN-ViT neural network amalga-<lb/>mates the swift local representation learning of CNNs with <lb/>ViT&apos;s global modeling capabilities. Within FasterViT, the <lb/>Hierarchical Attention (HAT) technique efficiently breaks <lb/>down the quadratic complexity of global self-attention <lb/>into multi-level attention mechanisms, effectively curtail-<lb/>ing computational costs. By harnessing efficient window-<lb/>based self-attention, individual windows are endowed with <lb/>dedicated carrier tokens, fostering both local and global <lb/>representation learning. HiLo [45] encodes high frequen-<lb/>cies using local window self-attention and captures low <lb/>frequencies through global attention within the input feature <lb/>map. <lb/>Castling-ViT [77] tackles the challenge of enabling <lb/>ViTs to efficiently learn both global and local con-<lb/>text during inference. It uses linear-angular attention and <lb/>masked softmax-based quadratic attention during training <lb/>and switches to linear-angular attention alone during infer-<lb/>ence. The framework employs angular kernels for query-key <lb/>similarity, simplified by decomposing them into linear terms <lb/>and high-order residuals, and incorporates modules like <lb/>depthwise convolution and masked softmax attention to <lb/>efficiently learn global and local information. <lb/>The attention computation reduction methods discussed <lb/>here provide innovative solutions to address the quadratic <lb/>complexity of self-attention while preserving accuracy. <lb/>Integrating these techniques into our proposed model can <lb/>further enhance its efficiency, enabling it to handle complex <lb/>tasks with reduced computational overhead and improved <lb/>scalability. <lb/>IV. IRET: PROPOSED METHOD <lb/>To adapt vision transformers for resource-constrained <lb/>devices, reduce their computational and memory require-<lb/>ments, and address the shortcomings of previous solutions <lb/>discussed in the previous section, we introduce IRET. <lb/>A. ARCHITECTURE OF IRET <lb/>The high-level architecture of IRET is shown in Fig. 7. The <lb/>innovation in IRET is the ability to focus on attended tokens <lb/>in addition to forgetting unattended tokens. <lb/>As illustrated in Fig. 7, IRET replaces several transformer <lb/>encoder layers with IRET encoders. The architecture of an <lb/>IRET encoder is shown in Fig. 8. IRET encoder pre-processes <lb/>the tokens for token dropping and token focusing before <lb/>performing the encoding. More specifically, similar to prior <lb/>work in [14] and [48], IRET performs the token dropping <lb/>based on CLS token attention scores, dropping tokens with <lb/>low attention scores to prune the computational tree. <lb/>However, as illustrated in Fig. 8 IRET also has an <lb/>attention-based mechanism for an incremental sampling of <lb/>the input image using an &apos;&apos;attention-based focusing&apos;&apos; module. <lb/>The focusing module received a new sample of the input <lb/>image using a learnable 2D-lifting scheme in [49] that is <lb/>shared across IRET layers. Details of the 2D-lifting scheme <lb/>will be explained later. We refer to this input image sample <lb/>as a sub-band sample. Each generated sub-band is then <lb/>divided into patches with a 1-to-1 mapping relationship to <lb/>input image patches. Based on the attention-score of input <lb/>(existing) tokens, the token focusing module then decides for <lb/></body>

			<page>27914 <lb/></page>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 7. The IRET architecture processes input through four sampling steps: initially with a scaled low-pass filter and then three times using <lb/>learnable 2-D lifting schemes. With each IRET layer, the embedding size of each token increases as it assimilates additional information. <lb/>Concurrently, before each IRET layer, less-attended tokens are dropped. Therefore, each IRET layer has dual roles: discarding unattended tokens and <lb/>focusing on attended ones through extra sampling. The transformer encoder&apos;s increasing size visualizes the growth in embedding size at each IRET <lb/>encoder. <lb/>FIGURE 8. The IRET layer architecture utilizes the CLS token to identify unattended tokens, employing a token dropping method to remove them. <lb/>Additionally, it determines which tokens require more focus based on the CLS token. This process involves filtering patches from the input sample <lb/>created by the 2D lifting scheme, projecting these patches into new embeddings, and then concatenating new information to enhance the existing <lb/>token embeddings. By enlarging the embedding size, IRET increases focus on attended tokens. <lb/>each patch in the newly sampled sub-band to be ignored or <lb/>forwarded to the layer. If the corresponding token coming <lb/>from the previous encoder has an attention score above the <lb/>desired threshold, the token is deemed useful and is subjected <lb/>to embedding. The embedded information for each sub-band <lb/>that corresponds to an attended token is concatenated to the <lb/>embedding of that token, increasing the embedding size, <lb/>which is analogous to improving focus on that part. The <lb/>size of each encoder layer in Fig. 7 corresponds to the <lb/>embedding size of its token. Using this illustration, as shown <lb/>in Fig. 7, each IRET encoder layer (shown in blue) increases <lb/>the embedding size (shown in dark blue), while each regular <lb/>transformer encoder layer maintains the embedding size. <lb/>B. INPUT SAMPLING PROCEDURE <lb/>To obtain input image samples for incorporation into IRET <lb/>layers, we investigated three methodologies: 1. Utilizing <lb/>the original input, 2. Employing DWT subbands, and 3. <lb/>Adopting a learnable sampling approach known as the <lb/>2D-lifting Scheme. The outcomes obtained through these <lb/>various sampling approaches are thoroughly examined and <lb/>detailed in Section V, shedding light on the efficacy and <lb/>impact of the chosen sampling methods on the overall <lb/>performance of the model. <lb/>Original Input-In this experiment which is the baseline, <lb/>we feed the downsampled original input three more times to <lb/>the model using the IRET layers. The goal of this experiment <lb/>is to check the ability of the model to learn new features from <lb/>the new embedded samples of the original input. <lb/>DWT Subbands-An alternative sampling approach <lb/>involves the utilization of Discrete Wavelet Transform <lb/>(DWT). Numerous investigations have harnessed the capabil-<lb/>ities of DWT within the realm of computer vision to augment <lb/>diverse facets of image analysis and processing. DWT, <lb/>a mathematical technique adept at decomposing signals <lb/>or images into their fundamental frequency components, <lb/>provides a unique pathway for feature extraction, represen-<lb/>tation, and manipulation. We employ DWT to produce four <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27915 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 9. The Architecture of Learnable 2D Lifting Scheme. It receives the original image and learns four <lb/>output samples. S 1 , S 2 and, S 3 are used in the architecture of IRET. <lb/>subbands of images, each sized 112*112, denoted as LL, <lb/>LH, HL, and HH. Due to the LL subband containing a <lb/>greater entropy of information, it is assigned to the initial <lb/>layer. Simultaneously, the LH, HL, and HH subbands are <lb/>sequentially inserted into the subsequent IRET layers for <lb/>further processing. This distribution ensures an effective <lb/>utilization of the information content across the layers of the <lb/>model. <lb/>2D-lifting Scheme-The architecture of the 2D-lifting <lb/>scheme [49] used in the IRET layer is shown in Fig. 9. The <lb/>lifting scheme is designed to take a signal, denoted as x, as its <lb/>input and produce two key outputs: the approximation sub-<lb/>band (c) and the details sub-band (d) of the wavelet transform. <lb/>The process of designing this lifting scheme involves three <lb/>distinct stages: Splitting the signal, Updater, and Predictor. <lb/>Eq. 8 through 10 describes the functionality of these stages. <lb/>The predictor and updater components are implemented <lb/>using CNN layers. Each CNN consists of two layers: the first <lb/>layer uses a kernel size of 1 × 3 (for horizontal processing) or <lb/>3×1 (for vertical processing) and employs a ReLU activation <lb/>to enhance non-linear representations. The second layer uses <lb/>a 1×1 convolution with a tanh activation to stabilize the range <lb/>of outputs. The signal x is partitioned into two components in <lb/>splitting stage: an even component and an odd component. <lb/>The even component consists of all the values located at even <lb/>positions in the sequence and in the update operation is often <lb/>used as the basis for creating the approximation sub-band <lb/>c, which captures the low-frequency details of the signal. <lb/>The odd component consists of all the values located at odd <lb/>positions in the sequence and in the prediction operation <lb/>is used to derive the detail sub-band d, which captures the <lb/>high-frequency details of the signal. <lb/>x e [n] = x[2n], x o [n] = x[2n + 1], x : input signal <lb/>(8) <lb/>c[n] = x e [n] + U (x o <lb/>L U [n]), U (.) = update operator (9) <lb/>d[n] = x o [n] -P(c L P [n]), P(.) = prediction operator <lb/>(10) <lb/>The loss function of learnable updater and predictor is defined <lb/>as. <lb/>Loss(P) = n (P(c L P [n]) -x o [n]) 2 <lb/>(11) <lb/>Loss(U ) = n (U (x L U <lb/>o [n]) -(x o [n] -x e [n])) 2 <lb/>(12) <lb/>For the predictor, the loss function minimizes the mean <lb/>squared error between the predicted and actual odd samples <lb/>and the updater&apos;s loss function minimizes the error in <lb/>approximating the difference between the odd and even <lb/>components. <lb/>The initial convolutional layers extract discriminative <lb/>features from the data before downsampling. This is done <lb/>using two sequences of convolution, batch normalization, and <lb/>ReLU with a kernel size of 3 × 3. <lb/>It&apos;s important to note that to minimize overhead, a portion <lb/>of the 2D-lifting scheme is shared across IRET encoder <lb/>layers. Nonetheless, each IRET encoder layer is fed by a <lb/>unique segment of the 2D-lifting scheme, ensuring it receives <lb/>a distinct sample. Additionally, this 2D-lifting scheme is <lb/>designed to be learnable, enabling its integration and training <lb/>alongside the rest of the model in an end-to-end manner. <lb/>During the training phase, the lifting scheme is trained as an <lb/>integral component of the IRET architecture. This approach <lb/>allows each IRET layer to adaptively incorporate new and <lb/>unique features, differentiating them from previous sampled <lb/>information for each token. <lb/>To maintain the positional information of patches in <lb/>newly sampled images we employ a position embedding <lb/>layer to add this data to their embedding. Prior to adopting <lb/>learnable layers, we explored different sampling techniques <lb/>for the input image, like DWT, using each sub-band as a <lb/>separate input to the feature encoding layer. However, our <lb/></body>

			<page>27916 <lb/></page>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 10. The IRET layer features two main functions: attention-based token dropping and focusing. <lb/>It eliminates unattended tokens using attention scores to simplify computation and enlarges the embedding <lb/>size for attended tokens with extra features from a 2-D lifting scheme. This process, akin to human brain <lb/>focusing, allows IRET to selectively prioritize certain tokens, thereby boosting accuracy and lowering <lb/>computational complexity. <lb/>findings indicated that a learnable lifting scheme, which <lb/>learns features based on model loss and trained alongside the <lb/>main model, yields the highest accuracy. <lb/>Also, note that the input to IRET is a scaled version of <lb/>the input image. For example, input to the embedding layer <lb/>of DeiT is a 224 × 224 pixel image. For IRET, we take <lb/>a scaled 112 × 112 pixel image as input and also reduce <lb/>the embedding size of the first layer from 384 to 192. <lb/>subsequently in each IRET layer (that in the variant shown <lb/>in this proposal is positioned in layers 4, 5, and 6, the <lb/>embedding size of features is increased from 192 to 294, <lb/>348, and 384 respectively bringing in additional 102, 54, <lb/>and 36 embedding dimensions with each added IRET layer. <lb/>Starting with a smaller embedding size and working with a <lb/>smaller embedding size in the first 6 layers of the IRET layers <lb/>allows a significant reduction of the computation. By working <lb/>with a smaller embedding size, IRET first decides where <lb/>to look for information in the input image. As the attention <lb/>scores highlight the importance of various input tokens, then <lb/>IRET layers stop processing unattended tokens, and more <lb/>importantly, bring in additional details for the features in <lb/>attended tokens. <lb/>Fig. 10 visualizes the pre-processing function for token <lb/>dropping and token focusing in an IRET encoder layer. The <lb/>attention threshold, which is predefined, plays a crucial role <lb/>in determining the tokens to be dropped or focused. In the <lb/>left part of the figure, the attention matrix is depicted, with <lb/>the first row highlighted in red corresponding to the CLS <lb/>token. This token is essential for classification in the last <lb/>layer of transformers, as its attention to other tokens reflects <lb/>their importance. The attention scores in the CLS row are <lb/>what we use in IRET to decide if a token is to be forgotten <lb/>(drop) or focused by bringing additional information through <lb/>the use of a 2D-lifting scheme. In the right part of the <lb/>figure, the token dropping process is illustrated. At the first <lb/>layer, tokens with attention scores below the predefined <lb/>threshold are dropped. In the subsequent layer, the remaining <lb/>tokens are concatenated with their corresponding tokens <lb/>from the 2D-lifting scheme. This process increases the <lb/>embedding dimension, enabling the model to focus more on <lb/>these tokens, which is visualized by a clearer token at the <lb/>end. <lb/>Fig 11 is another visualization of the token dropping <lb/>and focusing concept in an IRET encoder. As illustrated, <lb/>each IRET layer increases the details of each token with a <lb/>high attention score (this is visualized by increasing image <lb/>resolution, but in reality, this is achieved by increasing <lb/>embedding size), while dropping the unattended tokens. <lb/>C. IMPROVED EARLY PREDICTION THROUGH MULTI-EXIT <lb/>ARCHITECTURE IN IRET <lb/>As explained in section III, early termination is one of <lb/>the proposed methods for expediting model prediction. <lb/>We have further investigated and incorporated this feature <lb/>to enhance the speed of IRET. As depicted in Fig. 12, <lb/>various input images present distinct content and pose <lb/>challenges, showcasing classification tasks with varying <lb/>levels of complexity. Therefore, our goal is to formulate <lb/>the IRET with multiple exit options, enabling it to perform <lb/>early classification after each encoder layer. The decision <lb/>to advance to the next encoder level is based on how sure <lb/>the model is about its prediction and our predetermined <lb/>confidence threshold. This formulation additionally enables <lb/>the application of IRET in low-power and real-time systems. <lb/>Fig. 13 depicts the real-time behavior of the IRET system, <lb/>wherein the model&apos;s execution is halted based on a predefined <lb/>deadline. Consequently, the model adjusts its complexity <lb/>by reducing the number of layers when faced with tighter <lb/>deadlines, and conversely, it utilizes more layers when more <lb/>time is available for computation. In scenarios with low-<lb/>power models, computations can be halted to adhere to energy <lb/>constraints. <lb/>It&apos;s crucial to consider that the energy constraints play a <lb/>pivotal role in determining at which layers IRET concludes its <lb/>operations. This variability stems from the diverse number of <lb/>skip computations influenced by context-aware computation <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27917 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 11. In IRET, the &apos;forget and focus&apos; concept hinges on CLS token attention values. Tokens with attention below a threshold are dropped <lb/>(&apos;forget&apos;), while those above the threshold see increased embedding size (&apos;focus&apos;) via a 2D-lifting scheme. The concept of focus is shown by <lb/>increased resolution. <lb/>FIGURE 12. IRET exit behavior in low-power and energy aware system. <lb/>FIGURE 13. IRET exit behavior in real-time system. <lb/>and patch elimination, especially when dealing with different <lb/>images. Figure 14 shows the overview of muti-exit IRET. <lb/>Extra heads are added to the model after each encoder layer <lb/>from layers 4 to 12. Classification before the first 3 layers is <lb/>not useful because the model has more understanding after <lb/>the 3 first layers. <lb/>V. EXPERIMENTS <lb/>Our model was developed based on the Facebook DeiT [55] <lb/>small model with hard distillation, utilizing the Timm <lb/>library [64]. We conducted our experiments on the Ima-<lb/>geNet dataset [8] using Nvidia A100 as the training <lb/>platform. <lb/></body>

			<page>27918 <lb/></page>
			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 14. The IRET multi-exit architecture. Employing MLP heads at layers 4 to 12 enables the model to make early predictions, effectively bypassing <lb/>the need for computations in subsequent layers. This strategic approach enhances efficiency and accelerates decision-making in IRET. <lb/>To explore the most effective input sampling strategies, <lb/>three distinct approaches were evaluated. These experiments <lb/>aimed to address the challenge of balancing computational <lb/>efficiency with model performance by identifying a sub-<lb/>sampling mechanism that retains essential information while <lb/>reducing redundancy. The performance of each sampling <lb/>method was assessed using top-1 accuracy metric. Accuracy <lb/>was defined as the proportion of correctly classified images <lb/>over the total test set. <lb/>The experimental results demonstrated that the <lb/>2D-lifting scheme significantly outperformed the other meth-<lb/>ods, emerging as the most effective and impactful sampling <lb/>approach. This scheme was designed to dynamically adapt <lb/>the subsampling process to capture and preserve essential <lb/>image features, enabling improved learning by the IRET <lb/>layers. <lb/>For the first sampling mechanism, we simply fed the <lb/>downsampled image to IRET layers as well Fig. 15a shows <lb/>the results. In implementing the second sampling approach, <lb/>the Pytorch Wavelet library was utilized [1]. However, this <lb/>method proved to be ineffective as the model struggled <lb/>to learn from subsequent samples introduced to the IRET <lb/>layers. Consequently, no notable improvement in accuracy <lb/>was observed following the insertion of LH, HL, and HH <lb/>subbands. The result of this method is illustrated in Fig. 15b. <lb/>These two sampling experiments served as motivation to <lb/>adopt a trainable subsampling approach. By doing so, the <lb/>model can gain valuable information from the inputs to the <lb/>IRET layers, potentially enhancing its learning capabilities. <lb/>A learnable approach for subsampling images, such as <lb/>the 2D-lifting scheme discussed, introduces adaptability and <lb/>flexibility into the subsampling process. Unlike fixed or <lb/>predetermined subsampling methods, a learnable approach <lb/>allows the model to dynamically adjust and optimize the <lb/>subsampling strategy during training. Learnable subsampling <lb/>enables the model to adapt its sampling strategy based on <lb/>the specific characteristics and requirements of the given <lb/>task. Different tasks or datasets may benefit from different <lb/>subsampling patterns, and a learnable approach allows the <lb/>model to learn the most effective strategy for the task at hand. <lb/>Moreover, Images often contain complex relationships and <lb/>structures that may vary across different regions. A learnable <lb/>approach allows the model to capture and exploit these intri-<lb/>cate relationships during subsampling, potentially leading to <lb/>the extraction of more relevant and discriminative features. <lb/>Also, traditional subsampling methods may discard certain <lb/>information during the downsampling process. A learnable <lb/>approach has the potential to minimize information loss by <lb/>intelligently selecting which details to retain or discard based <lb/>on the model&apos;s learning experience. In essence, a learnable <lb/>subsampling approach empowers the model to actively par-<lb/>ticipate in the decision-making process, learning and refining <lb/>its subsampling strategy during training. This adaptability <lb/>can lead to more effective feature extraction, better task <lb/>performance, and improved generalization capabilities. Each <lb/>sample focuses on acquiring insights into a distinct aspect of <lb/>the image, thereby introducing new features to enhance the <lb/>model&apos;s understanding. <lb/>The final model inputs are 112 × 112 pixels, with IRET <lb/>layers receiving 112 × 112 sub-bands generated by the 2D-<lb/>lifting scheme [49]. To enhance trainability, we integrated <lb/>three additional classification heads, each corresponding to <lb/>a CLS token of an IRET layer. These heads contribute to the <lb/>total classification error during backpropagation, accelerating <lb/>the training of the IRET layer and 2D-lifting scheme. These <lb/>heads are removed post-training for inference. Training lasts <lb/>for 300 epochs or until accuracy plateaus. Data augmentation <lb/>included randomly omitting information from the 2D-lifting <lb/>scheme to assess IRET&apos;s incremental learning capability. <lb/>We evaluated IRET&apos;s performance in four scenarios: 1) Using <lb/>only the input image, 2) Adding the first sub-band sample to <lb/>the first IRET layer, 3) Incorporating two sub-band samples <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27919 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 15. Top-1 accuracy of the IRET for different input sampling mechanisms and different combinations of inputs. In each experiment, <lb/>we have tested four combinations: Only the downsampled input I, downsampled input plus one sample I + S1, downsampled input plus <lb/>two samples I + S1 + S2, and downsampled input plus three samples I + S1 + S2 + S3. (a) Original downsampled input is given to the IRET <lb/>layers. (b) DWT is used to generate the samples. (c,d) 2D Lifting mechanism is used for sample generation for DeiT small and tiny <lb/>respectively. (e) Comparing the top1 accuracy of all the experiments using all samples as input. <lb/>TABLE 1. IRET&apos;s accuracy, FLOP count, and parameter count based on various attention thresholds in the IRET layer, which affect token dropping and <lb/>focusing. <lb/>in the first and second IRET layers, and 4) Including all three <lb/>sub-band samples. <lb/>Fig. 15c presents the top-1 training accuracy of IRET <lb/>across these scenarios. The figure shows IRET&apos;s proficiency <lb/>in incremental learning, with diminishing accuracy gains <lb/>upon adding more sub-band samples. The first sub-band&apos;s <lb/>addition notably boosts accuracy, but subsequent samples <lb/>yield lesser improvements. This observation made us limit <lb/>the number of IRET layers. The embedding size distribution <lb/>across sub-bands also affects incremental learning rate <lb/></body>

			<page>27920 <lb/></page>
			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 16. Change in accuracy and flop count as a function of attention threshold for token dropping and <lb/>token focusing. <lb/>FIGURE 17. Token dropping of layers with pruning policy based on different threshold values. For smaller <lb/>threshold values, the model drops fewer tokens. <lb/>and final accuracy. As shown, the model&apos;s top-1 accuracy <lb/>improves from 67.5% to 75.93%, 77.52%, and 78.12% with <lb/>the addition of new information extracted from sampled sub-<lb/>bands. We have also developed the IRET-tiny using DeiT <lb/>tiny model. Fig. 15d shows the IRET-tiny accuracy using <lb/>lifting-scheme with the top-1 accuracy close to 70%. Finally, <lb/>in Fig. 15e, the top-1 accuracy of the experiments is compared <lb/>when they receive all samples. This serves as evidence of the <lb/>superiority of the DeiT small model and the utilization of the <lb/>2D lifting scheme. <lb/>As mentioned above, the IRET layer facilitates a dynamic <lb/>balance between computational complexity and model accu-<lb/>racy. In the realm of approximate computing, the ideal <lb/>scenario is achieving a substantial reduction in computational <lb/>complexity with only a minor impact on performance. <lb/>IRET exemplifies this by enabling dynamic observation of <lb/>such trade-offs. The token dropping and focusing attention <lb/>threshold in each IRET layer is the control knob for this trade-<lb/>off. The threshold could be different for each IRET encoder. <lb/>However, for simplicity in this study, we apply a uniform <lb/>attention threshold across all IRET layers, leaving detailed <lb/>exploration of threshold variations for future research. <lb/>Table 1 presents the top-1 and top-5 accuracy, FLOP <lb/>count, and parameters of IRET under various attention <lb/>thresholds for token dropping and focusing. The IRET&apos;s <lb/>parameter count remains constant at 17.24M, but attention <lb/>thresholding reduces the number of parameters actively used <lb/>by discarding those related to dropped tokens. It&apos;s important <lb/>to differentiate between used parameters and those loaded <lb/>from memory, as data movement depends on the hardware <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27921 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 18. Visualizing forget and focus with attention threshold of 0.0004 for samples from the ImageNet dataset. <lb/>accelerator&apos;s architecture, including buffer sizes and mapping <lb/>solutions. Reduction in used parameters leads to decreased <lb/>data movement in the hardware accelerator, which we plan <lb/>to explore further in future work. Fig. 16 visualizes how <lb/>increasing the threshold size effectively prunes the model <lb/>with minimal impact on accuracy. This balance is achieved <lb/>by the token-dropping module reducing complexity and the <lb/>focusing module maintaining accuracy. <lb/>Fig. 17 presents the average pruning results across different <lb/>threshold values, illustrating how the number of dropped <lb/>tokens varies within each layer, as averaged over the <lb/>ImageNet test set. This figure demonstrates the sensitivity <lb/>of our pruning strategy to the attention threshold. In the <lb/>IRET model presented in this paper, there are 3 IRET encoder <lb/>layers. As illustrated, by increasing the pruning threshold, the <lb/>number of dropped tokens in each layer and total number <lb/>of dropped tokens increases. In the extreme case, with the <lb/>attention pruning threshold of 3E-3, as illustrated in this <lb/>figure, 109 tokens are dropped in layer 4 (IRET layer 1), 61 in <lb/>layer 5, and 15 in layer 6. In this case, from table 1, the top-1 <lb/>accuracy of 71.11 and top-5 accuracy of 88.97 is achieved by <lb/>focusing on only 13 tokens. <lb/>Fig. 18 showcases the practical impact of our pruning <lb/>strategy on individual samples. This figure visually compares <lb/>the original token distribution with the pruned results at <lb/>different layers, emphasizing how the model selectively <lb/>focuses on the most critical tokens. The visualizations <lb/>provide a clear depiction of how irrelevant or less important <lb/>tokens are gradually removed as the layers progress, allowing <lb/>the model to allocate more resources to the most informative <lb/>regions. <lb/>Fig. 19 illustrates the trade-off between computational <lb/>complexity and accuracy for IRET, comparing it to prior <lb/>art solutions. Increasing the attention threshold in IRET <lb/>leads to a gradual decline in accuracy but with a significant <lb/>reduction in computational complexity. It&apos;s crucial to note <lb/>that the data points for ATS [14], DeiT [55], ResNet [20], <lb/>and AdaVIT [41] represent different models. For ResNet, <lb/>the accuracies correspond to models with varying depths <lb/>from 18 to 152 layers. DeiT and ATS models differ in <lb/>embedding sizes (384, 318, 258, 192), meaning each point <lb/>reflects a distinct model architecture optimized for specific <lb/>accuracy. In contrast, all IRET data points are derived from <lb/>the same architecture, starting with an embedding size of <lb/>192 and incrementally increasing it through the IRET encoder <lb/>layers to 294, 348 and 384 respectively. The variations <lb/>in IRET&apos;s FLOP count and accuracy are due to different <lb/>attention thresholds for token dropping, assumed uniform <lb/>across all layers in this study. Adjusting these thresholds <lb/>layer-wise in IRET, with incremental increases, could further <lb/>enhance accuracy. <lb/>It is also worth noting that in IRET, token focusing <lb/>and dropping occur in layers 4, 5, and 6 (IRET layers), <lb/>whereas in ATS, token dropping is applied in all layers <lb/>past the third encoder. Combining IRET and ATS could <lb/>potentially yield higher accuracy. This approach, alongside <lb/>the exploration of various thresholds, learnable thresholds, <lb/>and the integration of ATS with other pruning techniques, <lb/>will be a focus of our future work. As shown, IRET <lb/>initially has slightly lower accuracy than ATS and DeiT <lb/>without token dropping. However, with the implementation <lb/>of the Focus concept and increased token dropping, IRET <lb/>achieves better accuracy than ATS and DeiT at similar FLOP <lb/>counts for higher attention thresholds. IRET&apos;s consistent <lb/>architecture and the FLOP reduction achieved solely through <lb/>threshold control, coupled with its superior accuracy in lower <lb/>FLOP count regions, positions it as an efficient solution <lb/>for edge applications balancing accuracy with computational <lb/>complexity, enabling its use in energy and latency-sensitive <lb/>applications. <lb/></body>

			<page>27922 <lb/></page>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 19. Comparing the tradeoff between accuracy and flop count in IRET with that of prior art <lb/>solutions. Adopting the concept of Focus allows the IRET to enjoy a gentler drop in accuracy while <lb/>increasing the attention threshold used for token dropping and focusing. <lb/>TABLE 2. IRET with multi-head architecture Top-1 accuracy of heads. <lb/>TABLE 3. IRET with multi-head architecture GFLOPS count based on different confidence threshold. <lb/>A. MUTIHEADED IRET ENERGY-AWARE BEHAVIOR <lb/>Table 2 shows the Top-1 accuracy of MLP heads for different <lb/>variants of IRET. As you can see from head 7 we have an <lb/>acceptable prediction accuracy for all variants of IRET, so the <lb/>model gives us the possibility to do the prediction earlier, <lb/>especially for simple images, and avoid the computation of <lb/>final layers of the model and save power and energy. For the <lb/>real-time scenario, with a preset deadline, the model can do <lb/>the prediction with high confidence and meet the deadline. <lb/>The proposed multi-headed architecture provides an energy-<lb/>effective solution. This means that for some applications <lb/>using this architecture, we can do earlier predictions when <lb/>we reach the desired confidence level. We have tested the <lb/>effect of different confidence values on IRET. Fig. 20 shows <lb/>the behavior of the IRET variant with attention threshold = <lb/>0.0004 based on different confidence values. As the figure <lb/>shows for smaller confidence values like 60 the model can <lb/>do the prediction for all the images using heads 4-6 so the <lb/></body>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27923 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>FIGURE 20. The effect of different confidence thresholds on the <lb/>multi-exit IRET. The attention threshold is 0.0004. <lb/>computation of the reset of layers is skipped. The dark orange <lb/>color shows the percentage of classified images using the <lb/>current head and the light orange shows the percentage of <lb/>FIGURE 21. The effect of different attention thresholds on the multi-exit <lb/>IRET. The confidence threshold is 70. <lb/>classified images in the previous layers. But these predictions <lb/>are based on the confidence threshold so not all of the images <lb/>are classified correctly. The dark gray shows the percentage <lb/></body>

			<page>27924 <lb/></page>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<body>of the correctly classified images in each head and the light <lb/>gray shows the percentage of the correctly classified images <lb/>in previous heads. By increasing the confidence threshold <lb/>the ability of the model to do the prediction in earlier layers <lb/>decreases and the models switch to later layers for prediction <lb/>so the computation requirement of the model and as a result <lb/>of that the energy requirement of it increases by increasing <lb/>the confidence. Moreover, the gap between the images <lb/>classified in each head and the number of correctly classified <lb/>images in each head decreases. Fig. 21 shows the effect of <lb/>different Attention threshold values for a constant confidence <lb/>threshold = 70. By increasing the Attention threshold more <lb/>tokens drop so the model needs more layers to extract the <lb/>features and do the classification. In both experiments, some <lb/>of the images remain unclassified. The gap between 100 and <lb/>the last bar shows the number of remaining images that are <lb/>not classified. Table 3 shows the flop count of the different <lb/>variants of multi-head IRET based on different confidence <lb/>values. For smaller confidence values the GFLOPS of the <lb/>model reduces significantly. With a smaller confidence value, <lb/>the model can perform the prediction at an earlier head <lb/>and avoid the rest of the computation. However, increasing <lb/>confidence needs more understanding of the image features <lb/>by the model and more number layers and as a result more <lb/>computation. <lb/>VI. CONCLUSION <lb/>In this study, we introduced the IRET encoder, a novel <lb/>encoder layer that not only drops unattended tokens but <lb/>also enhances the model&apos;s focus on attended ones using <lb/>incremental input sampling and increased embedding size. <lb/>IRET transformer, constructed using a mix of IRET and <lb/>basic transformer encoders. Based on the choice of attention <lb/>threshold for token dropping and token focusing, IRET allows <lb/>us to trade accuracy for computational complexity. The <lb/>IRET&apos;s ability to focus attended tokens using incremental <lb/>input sampling allows a more graceful degradation in <lb/>accuracy in the result of dropping tokens compared to <lb/>prior art solutions. Notably, its computational complexity is <lb/>modulated through attention threshold adjustments, rather <lb/>than changes in embedding size or model architecture. <lb/>The combination of this unique feature alongside early <lb/>exiting renders IRET ideal for applications requiring a <lb/>balance between accuracy, energy efficiency, and latency <lb/>considerations. <lb/>While IRET offers promising advancements, there are <lb/>areas for further improvement. Future work will explore <lb/>incorporating token pruning mechanisms, such as mixing <lb/>IRET with ATS, to enhance accuracy. Additionally, we aim <lb/>to investigate new attention computation methods to further <lb/>optimize performance. To address practical implementation <lb/>challenges, we plan to pursue a hardware-software co-design <lb/>approach, exploring both the hardware and algorithmic <lb/>aspects to make IRET more efficient and scalable across <lb/>diverse deployment scenarios. Moreover, we plan to leverage <lb/>the IRET architecture for tasks that demand high-resolution <lb/>inputs or involve datasets with limited structural regularities. <lb/>We will assess its performance and fine-tune it to ensure <lb/>optimal functioning for these specific applications. <lb/></body>

			<listBibl>REFERENCES <lb/>[1] Pytorch Wavelets. Accessed: Mar. 1, 2024. [Online]. Available: <lb/>https://pytorch-wavelets.readthedocs.io/en/latest/readme.html <lb/>[2] J. Alman and Z. Song, &apos;&apos;Fast attention requires bounded entries,&apos;&apos; in <lb/>Advances in Neural Information Processing Systems, vol. 36, A. Oh, <lb/>T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., <lb/>Red Hook, NY, USA: Curran Associates, 2023, pp. 63117-63135. <lb/>[3] A. Bakhtiarnia, Q. Zhang, and A. Iosifidis, &apos;&apos;Multi-exit vision transformer <lb/>for dynamic inference,&apos;&apos; 2021, arXiv:2106.15183. <lb/>[4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and <lb/>S. Zagoruyko, &apos;&apos;End-to-end object detection with transformers,&apos;&apos; in <lb/>Proc. 16th Eur. conf. Comput. Vis. (ECCV), Glasgow, U.K. Cham, <lb/>Switzerland: Springer, Jan. 2020, pp. 213-229. <lb/>[5] J. Choi, S. Lee, J. Chu, M. Choi, and H. J. Kim, &apos;&apos;Vid-TLDR: <lb/>Training free token merging for light-weight video transformer,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2024, <lb/>pp. 18771-18781. <lb/>[6] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, <lb/>T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger, <lb/>L. Colwell, and A. Weller, &apos;&apos;Rethinking attention with performers,&apos;&apos; 2020, <lb/>arXiv:2009.14794. <lb/>[7] M. Dehghani, B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, <lb/>A. Steiner, J. Puigcerver, R. Geirhos, I. M. Alabdulmohsin, A. Oliver, <lb/>P. Padlewski, A. Gritsenko, M. Lucic, and N. Houlsby, &apos;&apos;Patch n&apos;pack: <lb/>Navit, a vision transformer for any aspect ratio and resolution,&apos;&apos; in <lb/>Proc. Adv. Neural Inf. Process. Syst., vol. 36, 2024, pp. 1-23. <lb/>[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, <lb/>&apos;&apos;ImageNet: A large-scale hierarchical image database,&apos;&apos; in Proc. IEEE <lb/>Conf. Comput. Vis. Pattern Recognit., Jun. 2009, pp. 248-255. <lb/>[9] J. Devlin, M. Chang, K. Lee, and K. Toutanova, &apos;&apos;BERT: Pre-training <lb/>of deep bidirectional transformers for language understanding,&apos;&apos; in <lb/>Proc. NAACL-HLT, Jan. 2018. <lb/>[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, <lb/>T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, <lb/>J. Uszkoreit, and N. Houlsby, &apos;&apos;An image is worth 16×16 words: <lb/>Transformers for image recognition at scale,&apos;&apos; 2020, arXiv:2010.11929. <lb/>[11] M. Elbayad, J. Gu, E. Grave, and M. Auli, &apos;&apos;Depth-adaptive transformer,&apos;&apos; <lb/>2019, arXiv:1910.10073. <lb/>[12] H. Falahati, M. Sadrosadati, Q. Xu, J. Gómez-Luna, B. S. Latibari, <lb/>H. Jeon, S. Hesaabi, H. Sarbazi-Azad, O. Mutlu, M. Annavaram, and <lb/>M. Pedram, &apos;&apos;Cross-core data sharing for energy-efficient GPUs,&apos;&apos; ACM <lb/>Trans. Archit. Code Optim., vol. 21, no. 3, pp. 1-32, Sep. 2024. <lb/>[13] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and <lb/>C. Feichtenhofer, &apos;&apos;Multiscale vision transformers,&apos;&apos; in Proc. IEEE/CVF <lb/>Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 6804-6815. <lb/>[14] M. Fayyaz, S. A. Koohpayegani, F. R. Jafari, S. Sengupta, H. R. V. Joze, <lb/>E. Sommerlade, H. Pirsiavash, and J. Gall, &apos;&apos;Adaptive token sampling for <lb/>efficient vision transformers,&apos;&apos; in Proc. Eur. Conf. Comput. Vis. Cham, <lb/>Switzerland: Springer, Jan. 2022, pp. 396-414. <lb/>[15] Z. Fei, X. Yan, S. Wang, and Q. Tian, &apos;&apos;DeeCap: Dynamic early exiting for <lb/>efficient image captioning,&apos;&apos; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern <lb/>Recognit. (CVPR), Jun. 2022, pp. 12206-12216. <lb/>[16] D. Han, X. Pan, Y. Han, S. Song, and G. Huang, &apos;&apos;FLatten transformer: <lb/>Vision transformer using focused linear attention,&apos;&apos; in Proc. IEEE/CVF <lb/>Int. Conf. Comput. Vis. (ICCV), Oct. 2023, pp. 5938-5948. <lb/>[17] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, &apos;&apos;Transformer <lb/>in transformer,&apos;&apos; in Proc. Adv. Neural Inf. Process. Syst., Jan. 2021, <lb/>pp. 15908-15919. <lb/>[18] A. Hatamizadeh, G. Heinrich, H. Yin, A. Tao, J. M. Alvarez, J. Kautz, <lb/>and P. Molchanov, &apos;&apos;FasterViT: Fast vision transformers with hierarchical <lb/>attention,&apos;&apos; 2023, arXiv:2306.06189. <lb/>[19] J. D. Havtorn, A. Royer, T. Blankevoort, and B. E. Bejnordi, <lb/>&apos;&apos;MSViT: Dynamic mixed-scale tokenization for vision transformers,&apos;&apos; in <lb/>Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshops (ICCVW), Oct. 2023, <lb/>pp. 838-848. <lb/>[20] K. He, X. Zhang, S. Ren, and J. Sun, &apos;&apos;Deep residual learning for image <lb/>recognition,&apos;&apos; in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), <lb/>Jun. 2016, pp. 770-778. <lb/></listBibl>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27925 <lb/></page>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<listBibl>[21] X. He, I. Keivanloo, Y. Xu, X. He, B. Zeng, S. Rajagopalan, and <lb/>T. Chilimbi, &apos;&apos;Magic pyramid: Accelerating inference with early exiting <lb/>and token pruning,&apos;&apos; 2021, arXiv:2111.00230. <lb/>[22] W. Hua, Z. Dai, H. Liu, and Q. V. Le, &apos;&apos;Transformer quality in linear time,&apos;&apos; <lb/>in Proc. Int. Conf. Mach. Learn., Jan. 2022, pp. 9099-9117. <lb/>[23] B. Kang, X. Chen, D. Wang, H. Peng, and H. Lu, &apos;&apos;Exploring <lb/>lightweight hierarchical vision transformers for efficient visual track-<lb/>ing,&apos;&apos; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2023, <lb/>pp. 9612-9621. <lb/>[24] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, &apos;&apos;Transformers <lb/>are RNNs: Fast autoregressive transformers with linear attention,&apos;&apos; in <lb/>Proc. Int. Conf. Mach. Learn., Jan. 2020, pp. 5156-5165. <lb/>[25] M. Kim, S. Gao, Y.-C. Hsu, Y. Shen, and H. Jin, &apos;&apos;Token fusion: Bridging <lb/>the gap between token pruning and token merging,&apos;&apos; in Proc. IEEE/CVF <lb/>Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2024, pp. 1372-1381. <lb/>[26] S. Kim, S. Shen, D. Thorsley, A. Gholami, W. Kwon, J. Hassoun, <lb/>and K. Keutzer, &apos;&apos;Learned token pruning for transformers,&apos;&apos; 2021, <lb/>arXiv:2107.00910. <lb/>[27] S. Abbasi Koohpayegani and H. Pirsiavash, &apos;&apos;SimA: Simple softmax-free <lb/>attention for vision transformers,&apos;&apos; 2022, arXiv:2206.08898. <lb/>[28] S. Lee, J. Choi, and H. J. Kim, &apos;&apos;Multi-criteria token fusion with <lb/>one-step-ahead attention for efficient vision transformers,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2024, <lb/>pp. 15741-15750. <lb/>[29] G. Li, Z. Cui, M. Li, Y. Han, and T. Li, &apos;&apos;Multi-attention fusion transformer <lb/>for single-image super-resolution,&apos;&apos; Sci. Rep., vol. 14, no. 1, p. 10222, <lb/>May 2024. <lb/>[30] X. Li, Y. Shao, T. Sun, H. Yan, X. Qiu, and X. Huang, &apos;&apos;Accelerating BERT <lb/>inference for sequence labeling via early-exit,&apos;&apos; 2021, arXiv:2105.13878. <lb/>[31] Y. Liang, C. Ge, Z. Tong, Y. Song, J. Wang, and P. Xie, &apos;&apos;Not all patches are <lb/>what you need: Expediting vision transformers via token reorganizations,&apos;&apos; <lb/>2022, arXiv:2202.07800. <lb/>[32] Y. Liang, Z. Wang, X. Xu, Y. Tang, J. Zhou, and J. Lu, &apos;&apos;MCUFormer: <lb/>Deploying vision tranformers on microcontrollers with limited memory,&apos;&apos; <lb/>in Advances in Neural Information Processing Systems, vol. 36, A. Oh, <lb/>T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., <lb/>Red Hook, NY, USA: Curran Associates, 2023, pp. 8501-8512. <lb/>[33] K. Liao, Y. Zhang, X. Ren, Q. Su, X. Sun, and B. He, &apos;&apos;A global past-<lb/>future early exit method for accelerating inference of pre-trained language <lb/>models,&apos;&apos; in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: <lb/>Human Lang. Technol., 2021, pp. 2013-2023. <lb/>[34] W. Lin, Z. Wu, J. Chen, J. Huang, and L. Jin, &apos;&apos;Scale-aware modulation <lb/>meet transformer,&apos;&apos; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), <lb/>Oct. 2023, pp. 6015-6026. <lb/>[35] S. Liu, G. Tao, Y. Zou, D. Chow, Z. Fan, K. Lei, B. Pan, D. Sylvester, <lb/>G. Kielian, and M. Saligane, &apos;&apos;ConSmax: Hardware-friendly alternative <lb/>softmax with learnable parameters,&apos;&apos; 2024, arXiv:2402.10930. <lb/>[36] X. Liu, H. Peng, N. Zheng, Y. Yang, H. Hu, and Y. Yuan, &apos;&apos;EfficientViT: <lb/>Memory efficient vision transformer with cascaded group attention,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023, <lb/>pp. 14420-14430. <lb/>[37] Y. Liu, C. Matsoukas, F. Strand, H. Azizpour, and K. Smith, &apos;&apos;Patch-<lb/>Dropout: Economizing vision transformers using patch dropout,&apos;&apos; 2022, <lb/>arXiv:2208.07220. <lb/>[38] Y. Liu, Q. Zhou, J. Wang, Z. Wang, F. Wang, J. Wang, and W. Zhang, <lb/>&apos;&apos;Dynamic token-pass transformers for semantic segmentation,&apos;&apos; in <lb/>Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2024, <lb/>pp. 1816-1825. <lb/>[39] Y. Liu, M. Gehrig, N. Messikommer, M. Cannici, and D. Scaramuzza, <lb/>&apos;&apos;Revisiting token pruning for object detection and instance segmentation,&apos;&apos; <lb/>in Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2024, <lb/>pp. 2646-2656. <lb/>[40] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and <lb/>B. Guo, &apos;&apos;Swin transformer: Hierarchical vision transformer using shifted <lb/>windows,&apos;&apos; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, <lb/>pp. 9992-10002. <lb/>[41] L. Meng, H. Li, B.-C. Chen, S. Lan, Z. Wu, Y.-G. Jiang, and S.-N. Lim, <lb/>&apos;&apos;AdaViT: Adaptive vision transformers for efficient image recognition,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, <lb/>pp. 12299-12308. <lb/>[42] Y. Mo, P. Zuo, Q. Zhou, Z. Mo, Y. Fan, S. Zhang, and B. Kang, &apos;&apos;PWLT: <lb/>Pyramid window-based lightweight transformer for image classification,&apos;&apos; <lb/>Comput. Electr. Eng., vol. 116, May 2024, Art. no. 109209. <lb/>[43] T. M. Nguyen, V. Suliafu, S. Osher, L. Chen, and W. Bao, &apos;&apos;FMMformer: <lb/>Efficient and flexible transformer via decomposed near-field and far-<lb/>field attention,&apos;&apos; in Proc. Adv. Neural Inf. Process. Syst., Jan. 2021, <lb/>pp. 29449-29463. <lb/>[44] I. Ntinoutl, E. Sanchez, and G. Tzimiropoulos, &apos;&apos;Multiscale vision <lb/>transformers meet bipartite matching for efficient single-stage action <lb/>localization,&apos;&apos; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. <lb/>(CVPR), Jun. 2024, pp. 18827-18836. <lb/>[45] Z. Pan, J. Cai, and B. Zhuang, &apos;&apos;Fast vision transformers with HiLo <lb/>attention,&apos;&apos; in Proc. Adv. Neural Inf. Process. Syst., Jan. 2022. <lb/>[46] Z. Qin, W. Sun, H. Deng, D. Li, Y. Wei, B. Lv, J. Yan, L. Kong, <lb/>and Y. Zhong, &apos;&apos;CosFormer: Rethinking softmax in attention,&apos;&apos; 2022, <lb/>arXiv:2202.08791. <lb/>[47] M. M. Rahman and R. Mărculescu, &apos;&apos;Multi-scale hierarchical vision <lb/>transformer with cascaded attention decoding for medical image seg-<lb/>mentation,&apos;&apos; in Medical Imaging With Deep Learning (Proceedings of <lb/>Machine Learning Research), vol. 227, I. Oguz, J. Noble, X. Li, M. Styner, <lb/>C. Baumgartner, M. Rusu, T. Heinmann, D. Kontos, B. Landman, and <lb/>B. Dawant, Eds., Jul. 2024, pp. 1526-1544. <lb/>[48] Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C. Hsieh, &apos;&apos;DynamicViT: <lb/>Efficient vision transformers with dynamic token sparsification,&apos;&apos; in <lb/>Proc. Adv. Neural Inf. Process. Syst., Jan. 2021, pp. 13937-13949. <lb/>[49] M. X. Bastidas Rodriguez, A. Gruson, L. F. Polanía, S. Fujieda, F. P. Ortiz, <lb/>K. Takayama, and T. Hachisuka, &apos;&apos;Deep adaptive wavelet network,&apos;&apos; <lb/>in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Mar. 2020, <lb/>pp. 3100-3108. <lb/>[50] Y. Song, Q. Zhou, X. Li, D.-P. Fan, X. Lu, and L. Ma, &apos;&apos;BA-SAM: <lb/>Scalable bias-mode attention mask for segment anything model,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2024, <lb/>pp. 3162-3173. <lb/>[51] J. R. Stevens, R. Venkatesan, S. Dai, B. Khailany, and A. Raghunathan, <lb/>&apos;&apos;Softermax: Hardware/software co-design of an efficient softmax for <lb/>transformers,&apos;&apos; in Proc. 58th ACM/IEEE Design Autom. Conf. (DAC), <lb/>Dec. 2021, pp. 469-474. <lb/>[52] T. Sun, Y. Zhou, X. Liu, X. Zhang, H. Jiang, Z. Cao, X. Huang, <lb/>and X. Qiu, &apos;&apos;Early exiting with ensemble internal classifiers,&apos;&apos; 2021, <lb/>arXiv:2105.13792. <lb/>[53] S. Tang, Y. Wang, Z. Kong, T. Zhang, Y. Li, C. Ding, Y. Wang, <lb/>Y. Liang, and D. Xu, &apos;&apos;You need multiple exiting: Dynamic <lb/>early exiting for accelerating unified vision language model,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023, <lb/>pp. 10781-10791. <lb/>[54] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng, <lb/>&apos;&apos;Synthesizer: Rethinking self-attention for transformer models,&apos;&apos; in <lb/>Proc. Int. Conf. Mach. Learn., May 2021, pp. 10183-10192. <lb/>[55] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and <lb/>H. Jegou, &apos;&apos;Training data-efficient image transformers &amp; distillation <lb/>through attention,&apos;&apos; in Proc. Int. conf. Mach. Learn., Jul. 2021, <lb/>pp. 10347-10357. <lb/>[56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, <lb/>L. Kaiser, and I. Polosukhin, &apos;&apos;Attention is all you need,&apos;&apos; in Proc. Adv. Neu-<lb/>ral Inf. Process. Syst., vol. 30, Jun. 2017, pp. 5998-6008. <lb/>[57] I. Vasyltsov and W. Chang, &apos;&apos;Efficient softmax approximation for deep <lb/>neural networks with attention mechanism,&apos;&apos; 2021, arXiv:2111.10770. <lb/>[58] A. Vyas, A. Katharopoulos, and F. Fleuret, &apos;&apos;Fast transformers with <lb/>clustered attention,&apos;&apos; in Proc. Adv. Neural Inf. Process. Syst., Jan. 2020, <lb/>pp. 21665-21674. <lb/>[59] H. Wang, B. Dedhia, and N. K. Jha, &apos;&apos;Zero-TPrune: Zero-shot token <lb/>pruning through leveraging of the attention graph in pre-trained transform-<lb/>ers,&apos;&apos; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), <lb/>Jun. 2024, pp. 16070-16079. <lb/>[60] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, <lb/>P. Luo, and L. Shao, &apos;&apos;Pyramid vision transformer: A versatile backbone <lb/>for dense prediction without convolutions,&apos;&apos; in Proc. IEEE/CVF Int. <lb/>Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 548-558. <lb/>[61] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and <lb/>L. Shao, &apos;&apos;PVT v2: Improved baselines with pyramid vision transformer,&apos;&apos; <lb/>Comput. Vis. Media, vol. 8, no. 3, pp. 415-424, Sep. 2022. <lb/>[62] W. Wang, W. Chen, Q. Qiu, L. Chen, B. Wu, B. Lin, X. He, and <lb/>W. Liu, &apos;&apos;CrossFormer++: A versatile vision transformer hinging on cross-<lb/>scale attention,&apos;&apos; IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 5, <lb/>pp. 3123-3136, May 2024. <lb/></listBibl>

			<page>27926 <lb/></page>
			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<note place="headnote">B. S. Latibari et al.: Optimizing Vision Transformers: Unveiling &apos;Focus and Forget&apos; <lb/></note>

			<listBibl>[63] Y. Wang, R. Huang, S. Song, Z. Huang, and G. Huang, &apos;&apos;Not all <lb/>images are worth 16×16 words: Dynamic transformers for efficient <lb/>image recognition,&apos;&apos; in Proc. Adv. Neural Inf. Process. Syst., Jan. 2021, <lb/>pp. 11960-11973. <lb/>[64] R. Wightman. (2019). Pytorch Image Models. [Online]. Available: <lb/>https://github.com/rwightman/pytorch-image-models <lb/>[65] M. Wortsman, J. Lee, J. Gilmer, and S. Kornblith, &apos;&apos;Replacing softmax with <lb/>ReLU in vision transformers,&apos;&apos; 2023, arXiv:2309.08586. <lb/>[66] C. Wu, F. Wu, T. Qi, Y. Huang, and X. Xie, &apos;&apos;Fastformer: Additive attention <lb/>can be all you need,&apos;&apos; 2021, arXiv:2108.09084. <lb/>[67] J. Wu, B. Duan, W. Kang, H. Tang, and Y. Yan, &apos;&apos;Token transformation <lb/>matters: Towards faithful post-hoc explanation for vision transformer,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2024, <lb/>pp. 10926-10935. <lb/>[68] C. Xia, X. Wang, F. Lv, X. Hao, and Y. Shi, &apos;&apos;ViT-CoMer: Vision <lb/>transformer with convolutional multi-scale feature interaction for dense <lb/>predictions,&apos;&apos; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. <lb/>(CVPR), Jun. 2024, pp. 5493-5502. <lb/>[69] T. Xiao, Y. Li, J. Zhu, Z. Yu, and T. Liu, &apos;&apos;Sharing attention weights for <lb/>fast transformer,&apos;&apos; 2019, arXiv:1906.11024. <lb/>[70] J. Xin, R. Tang, J. Lee, Y. Yu, and J. Lin, &apos;&apos;DeeBERT: Dynamic early <lb/>exiting for accelerating BERT inference,&apos;&apos; 2020, arXiv:2004.12993. <lb/>[71] J. Xin, R. Tang, Y. Yu, and J. Lin, &apos;&apos;BERxiT: Early exiting for BERT <lb/>with better fine-tuning and extension to regression,&apos;&apos; in Proc. 16th <lb/>Conf. Eur. Chapter Assoc. Comput. Linguistics, 2021, pp. 91-104. <lb/>[72] G. Xu, J. Hao, L. Shen, H. Hu, Y. Luo, H. Lin, and J. Shen, &apos;&apos;LGViT: <lb/>Dynamic early exiting for accelerating vision transformer,&apos;&apos; in Proc. 31st <lb/>ACM Int. Conf. Multimedia, Oct. 2023, pp. 9103-9114. <lb/>[73] X. Xu, C. Li, Y. Chen, X. Chang, J. Liu, and S. Wang, &apos;&apos;No token <lb/>left behind: Efficient vision transformer via dynamic token idling,&apos;&apos; in <lb/>Proc. Australas. Joint Conf. Artif. Intell. Cham, Switzerland: Springer, <lb/>Jan. 2023, pp. 28-41. <lb/>[74] Y. Xu, Z. Zhang, M. Zhang, K. Sheng, K. Li, W. Dong, L. Zhang, <lb/>C. Xu, and X. Sun, &apos;&apos;Evo-ViT: Slow-fast token evolution for dynamic <lb/>vision transformer,&apos;&apos; in Proc. AAAI Conf. Artif. Intell., vol. 36, Jun. 2022, <lb/>pp. 2964-2972. <lb/>[75] T. Yao, Y. Pan, Y. Li, C.-W. Ngo, and T. Mei, &apos;&apos;Wave-ViT: Unifying <lb/>wavelet and transformers for visual representation learning,&apos;&apos; 2022, <lb/>arXiv:2207.04978. <lb/>[76] H. Yin, A. Vahdat, J. M. Alvarez, A. Mallya, J. Kautz, and P. Molchanov, <lb/>&apos;&apos;A-ViT: Adaptive tokens for efficient vision transformer,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, <lb/>pp. 10799-10808. <lb/>[77] H. You, Y. Xiong, X. Dai, B. Wu, P. Zhang, H. Fan, P. Vajda, and <lb/>Y. C. Lin, &apos;&apos;Castling-ViT: Compressing self-attention via switching <lb/>towards linear-angular attention at vision transformer inference,&apos;&apos; in <lb/>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023, <lb/>pp. 14431-14442. <lb/>[78] S. Yun and Y. Ro, &apos;&apos;SHViT: Single-head vision transformer with memory <lb/>efficient macro design,&apos;&apos; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern <lb/>Recognit. (CVPR), Jun. 2024, pp. 5756-5767. <lb/>[79] Q. Zhang, J. Zhang, Y. Xu, and D. Tao, &apos;&apos;Vision transformer with <lb/>quadrangle attention,&apos;&apos; IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, <lb/>no. 5, pp. 3608-3624, May 2024. <lb/>[80] Y. Zhang, D. Chen, S. Kundu, C. Li, and P. A. Beerel, &apos;&apos;SAL-ViT: <lb/>Towards latency efficient private inference on ViT using selective attention <lb/>search with a learnable softmax approximation,&apos;&apos; in Proc. IEEE/CVF <lb/>Int. Conf. Comput. Vis. (ICCV), Oct. 2023, pp. 5116-5125. <lb/>[81] Y. Zhang, Y. Liu, D. Miao, Q. Zhang, Y. Shi, and L. Hu, &apos;&apos;MG-ViT: <lb/>A multi-granularity method for compact and efficient vision transformers,&apos;&apos; <lb/>in Proc. Adv. Neural Inf. Process. Syst., vol. 36, 2024, pp. 1-20. <lb/>[82] Z. Zhang, H. Zhang, L. Zhao, T. Chen, S. Ö. Arık, and T. Pfister, &apos;&apos;Nested <lb/>hierarchical transformer: Towards accurate, data-efficient and interpretable <lb/>visual understanding,&apos;&apos; in Proc. AAAI Conf. Artif. Intell., vol. 36, Jun. 2022, <lb/>pp. 3417-3425. <lb/>[83] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, &apos;&apos;BERT loses <lb/>patience: Fast and robust inference with early exit,&apos;&apos; in Proc. Adv. Neural <lb/>Inf. Process. Syst., Jan. 2020, pp. 18330-18341. <lb/>[84] W. Zhu, &apos;&apos;LeeBERT: Learned early exit for BERT with cross-level <lb/>optimization,&apos;&apos; in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics <lb/>11th Int. Joint Conf. Natural Lang. Process., 2021, pp. 2968-2980. <lb/></listBibl>

			<div type="annex">BANAFSHEH SABER LATIBARI received the <lb/>B.Sc. degree in computer engineering from the <lb/>K. N. Toosi University of Technology, in 2014, and <lb/>the M.Sc. degree in computer architecture from <lb/>the Sharif University of Technology, in 2017. She <lb/>is currently pursuing the Ph.D. degree with the <lb/>Department of Electrical and Computer Engineer-<lb/>ing, University of California at Davis (UC Davis). <lb/>Her research interests include computer vision, <lb/>machine learning, security, and hardware design. <lb/>HOUMAN HOMAYOUN received the B.S. <lb/>degree in electrical engineering from the Sharif <lb/>University of Technology, in 2003, the M.S. degree <lb/>in computer engineering from the University <lb/>of Victoria, in 2005, and the Ph.D. degree in <lb/>computer science (CS) from the University of <lb/>California at Irvine (UCI), in 2010. He is currently <lb/>working as a Professor with the Department <lb/>of Electrical and Computer Engineering (ECE), <lb/>University of California at Davis (UC Davis). Prior <lb/>to that, he was an Associate Professor at the Department of ECE, George <lb/>Mason University (GMU). From 2010 to 2012, he spent two years at the <lb/>University of California at San Diego, as an NSF Computing Innovation <lb/>Fellow (CIFellow) awarded by CRA-CCC. He is the Director of UC <lb/>Davis Accelerated, Secure, and Energy-Efficient Computing Laboratory <lb/>(ASEEC). He conducts research in hardware security and trust, data-<lb/>intensive computing, and heterogeneous computing. <lb/>AVESTA SASAN (Senior Member, IEEE) received <lb/>the B.Sc. degree in computer engineering and <lb/>the M.Sc. and Ph.D. degrees in electrical and <lb/>computer engineering (ECE) from the University <lb/>of California at Irvine (UCI), in 2005, 2006, <lb/>and 2010, respectively. In 2010, he joined the <lb/>Office of CTO, Broadcom Company, working <lb/>on the physical design and implementation of <lb/>ARM processors, working as a Physical Designer, <lb/>a Timing Signoff Specialist, and the Lead of signal <lb/>and power integrity signoff in this team. In 2014, he was recruited by <lb/>Qualcomm Office of VLSI Technology, where he developed different <lb/>methodologies and in-house EDAs for accurate signoff and analysis of <lb/>hardened ASIC solutions. He joined the Department of ECE, George Mason <lb/>University, in 2016, while simultaneously working as an Associate Chair <lb/>for Research with the Department of ECE. In 2021, he joined as a Faculty <lb/>Member with the Department of ECE, University of California at Davis. <lb/>His research interests include hardware security, machine learning hardware, <lb/>efficient learning on edge, low-power design, approximate computing, and <lb/>the IoT. <lb/></div>

			<note place="footnote">VOLUME 13, 2025 <lb/></note>

			<page>27927 </page>


	</text>
</tei>
