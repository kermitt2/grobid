<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Journal of <lb/>Information Systems Engineering <lb/>and Business Intelligence <lb/>Vol.10, No.2, June 2024 <lb/>Available online at: http://e-journal.unair.ac.id/index.php/JISEBI <lb/>ISSN 2443-2555 (online) 2598-6333 (print) © 2024 The Authors. Published by Universitas Airlangga. <lb/>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/) <lb/>doi: http://dx.doi.org/10.20473/jisebi.10.2.302-313 <lb/>Hepatitis Identification using Backward Elimination and <lb/>Extreme Gradient Boosting Methods <lb/>Jasman Pardede 1)* , Desita Nurrohmah 2) <lb/>1)2) Department of Informatics, Faculty of Industrial Technology, National Institute of Technology, Bandung, Indonesia <lb/>1) jasman@itenas.ac.id, 2) desitanurrohmah89@gmail.com <lb/>Abstract <lb/>Background: Hepatitis is a contagious inflammatory disease of the liver and is a public health problem because it is easily <lb/>transmitted. The main factors causing hepatitis are viral infections, disease complications, alcohol, autoimmune diseases, and <lb/>drug effects. Some hepatitis variants such as B, C, and D can also cause liver cancer if left untreated. <lb/>Objective: This research aims to determine the effect of Backward Elimination feature selection on the performance of hepatitis <lb/>disease identification compared to cases where Backward Elimination is not applied. <lb/>Methods: XGBoost classification, capable of handling machine learning problems, was utilized. Additionally, Backward <lb/>Elimination was used as a featured selection to increase accuracy by reducing the number of less important features in the data <lb/>classification process. <lb/>Results: The results for training XGBoost model with Backward Elimination, and applying Random Search for hyperparameter <lb/>optimization, achieved an accuracy of 98.958% at 0.64 seconds. This performance was better than using Bayesian search, which <lb/>produced the same accuracy of 98.958% but required a longer training time of 0.70 seconds. <lb/>Conclusion: The use of features obtained from Backward Elimination process as well as the use of feature average values for <lb/>missing value treatment, produced an accuracy of 98.958%. Meanwhile, the precision in training XGBoost model with <lb/>hyperparameter Bayesian search achieved accuracy, recall, and F1 score of 98.934%, 98.934%, and 98.934%, respectively. <lb/>Consequently, the use of Backward Elimination in XGBoost model led to faster training, improved accuracy, and decreased <lb/>overfitting. <lb/>Keywords: Hepatitis, Backward Elimination, XGBoost, Bayesian Search, Random Search <lb/>Article history: Received 2 August 2023, first decision 16 January 2024, accepted 13 March 2024, available online 28 June 2024 <lb/></front>

			<body>I. INTRODUCTION <lb/>Liver disease is often considered a silent killer because it tends not to show symptoms. Hepatitis is a contagious <lb/>inflammatory liver disease that causes public health problem [1]. According to WHO, more than 3,000 people die <lb/>every day from liver disease caused by hepatitis virus. Hepatitis variants such as B, C, and D can lead to liver cancer <lb/>when left untreated [2]. In 2019, WHO reported that 78,000 deaths occurred worldwide due to complications of acute <lb/>hepatitis A to E infection [3]. The main causes of hepatitis are viral infections, disease complications, alcohol, <lb/>autoimmune diseases, and the effects of drugs [4]. <lb/>The WHO&apos;s Regional Office in Southeast Asia reported that the prevalence of hepatitis B in Indonesia reached <lb/>7.1% or around 18 million cases, while hepatitis C reached 2.34% or around 6 million cases. The prevalence rate is <lb/>the highest among other Southeast Asian countries after Myanmar and Thailand. Additionally, the 2018 Basic Health <lb/>Research Report (riskesdas) stated that the highest prevalence of hepatitis cases in Indonesia occurred in children aged <lb/>5 to 14 years [5]. <lb/>To avoid further transmission, there are various types of hepatitis which have different causes, symptoms, and <lb/>treatments. This implies that the results of hepatitis diagnosis need to be known in order to administer proper treatment. <lb/>Typically, adequate inspection is necessary to be carried out in a short time, while laboratory tests are usually required <lb/>when diagnosing the disease. However, disease diagnosis based on laboratory results produces errors in the initial <lb/>analysis or in determining the disease suffered by the patient. Based on the research conducted by [6], pre-analytical <lb/>errors contribute to 46-68.2% of the total errors in laboratory tests, with incorrect patient identification accounting for <lb/>26.8% of these errors. Therefore, laboratory test results are necessary to be validated, specifically in hepatitis research. <lb/></body>

			<front>* Corresponding author <lb/></front>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>303 <lb/></page>

			<body>It is crucial to be aware that the validation based on factors that influence the disease can be predicted using machine <lb/>learning. <lb/>Machine learning has an impact on the rapid development of technology in various fields, including the medical <lb/>field. In general, the technology uses computers to learn from data and make predictions. According to [7], predictions <lb/>with high accuracy make it easier for explorers to evaluate an experiment quickly and accurately. <lb/>Following machine learning technology, Extreme Gradient Boosting (XGBoost) is an evolution of Gradient Tree <lb/>Boosting algorithm based on ensemble algorithms, which can handle machine learning problems efficiently. XGBoost <lb/>excels in solving a variety of classification, regression, and ranking problems. In addition, the algorithm has also <lb/>succeeded in becoming one of the most popular methods in machine learning [8], particularly in identifying hepatitis <lb/>disease, where precise predictions are essential for effective and accurate treatment. <lb/>In research conducted by [9], seven different machine learning algorithms were utilized, including XGBoost, used <lb/>to predict liver disease. The exploration performed by [10] predicted hepatitis B Surface Antigen Seroclearance using <lb/>several machine learning algorithms such as logistic regression, decision trees, random forests, and XGBoost. <lb/>Furthermore, [11] predicted heart disease using machine learning such as multi-layer perceptron Random Forest, <lb/>Decision Tree, and XGBoost. Similarly, George Obaido et al. [12] performed Diagnosis using various machine-<lb/>learning methods such as Decision Trees, Logistic Regression, SVM, Random Forest, XGBoost, and AdaBoost. <lb/>To improve data processing efficiency and machine learning model performance, feature selection method can be <lb/>adopted. This is because the method can reduce data dimensionality by selecting the most important and relevant <lb/>features [13]. An example of feature selection that can increase accuracy and reduce the number of insignificant <lb/>features in the data classification process is Backward Elimination. In machine learning-based data classification, <lb/>accuracy is low due to the large number of attributes. When identifying diseases, the large number of attributes in <lb/>medical data can present a complex challenge. In addition, feature extraction can make an important contribution by <lb/>helping machine learning models identify the different characteristic features of each variant of hepatitis. This implies <lb/>that the features improve the ability of the model to provide accurate predictions. <lb/>Backward Elimination was used as a feature selection method for the identification of volatile organic compounds <lb/>(VOC) when applying SVM algorithm [14]. Consequently, the model achieved an accuracy of 75.6%, but when <lb/>Backward Elimination was not used, the value was 73.2%. This implies that Backward Elimination method reduces <lb/>the number of features and increases model accuracy. In order to improve the performance of data mining algorithms, <lb/>such as KNN, Naïve Bayes, and C4.5, [15] classified diabetes using Backward Elimination. Based on the accuracy <lb/>and AUC values, it is concluded that Backward Elimination can improve the total performance of the data mining <lb/>algorithm. <lb/>According to the results of [14] and [15], the use of feature selection, specifically Backward Elimination has the <lb/>potential to improve model performance and reduce the number of features. In addition, the popular adoption and <lb/>effectiveness of XGBoost, an ensemble algorithm that combines multiple learning models have been investigated. <lb/>Several research results from [9], [10], [11], and [12], showed that XGBoost has better performance in the <lb/>classification process. Therefore, this research aims to validate hepatitis test results by using Backward Elimination <lb/>feature selection, and to improve the classification of XGBoost in order to avoid errors in diagnosing hepatitis. The <lb/>impact of Backward Elimination on hepatitis disease is determined by comparing the performance results of disease <lb/>identification using Backward Elimination with those without using Backward Elimination. <lb/>II. METHODS <lb/>This section explains the datasets, methods, and system architecture used in this research. <lb/>A. Dataset <lb/>The dataset used in this research consisted of liver function test results which were a combination of primary and <lb/>secondary data. Primary data were obtained from 375 laboratory results of patient medical records at Dustira Hospital <lb/>Tk.II, Cimahi City. According to the agreement with Dustira Hospital, the research documents and files can only be <lb/>accessed by officers involved in the research. Therefore, the primary dataset will be stored in a secure location and <lb/>will remain unpublished. Meanwhile, the secondary data were sourced from the Kaggle website with the data used <lb/>being the Indian Liver Patient Dataset https://www.kaggle.com/datasets/jeevannagaraj/indian-liver-patient-dataset <lb/>containing 583 laboratory results collected in North East of Andhra Pradesh, India. Therefore, this research used 958 <lb/>instances and 9 features (variables), which were categorized into 5 classes, namely hepatitis A, hepatitis B, hepatitis <lb/>C, Unspecified Hepatitis, and Non-Hepatitis. Consequently, the variables used in this research were listed in Table 1. <lb/>Based on Table 1, some variables in the dataset have normal value limits, such as Total Bilirubin 0.1-1.2 mg/dL, <lb/>Direct Bilirubin 0-0.3 mg/dL, SGOT 5-40 U/L, and SGPT 7-56 U/L. These variables help in establishing the disease <lb/></body>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>304 <lb/></page>

			<body>of hepatitis by observing the extent of liver function damage. Furthermore, the HBsAg, HAV, and HCV variables <lb/>check the type of hepatitis suffered to ensure the normal value is negative. <lb/>TABLE 1 <lb/>DATASET VARIABLES USED FOR IMPLEMENTATION AND TESTING <lb/>No Variable <lb/>Description <lb/>Type of Data <lb/>1 <lb/>Age <lb/>Age of Patient <lb/>Numeric <lb/>2 <lb/>Gender <lb/>Gender of the Patient <lb/>Categorical <lb/>3 <lb/>Bilirubin Total <lb/>The total amount of bilirubin present in the blood <lb/>Numeric <lb/>4 <lb/>Bilirubin Direct The total bilirubin that was directly excreted into the bile Numeric <lb/>5 <lb/>SGOT <lb/>serum glutamic oxaloacetic transaminase <lb/>Numeric <lb/>6 <lb/>SGPT <lb/>serum glutamic pyruvic transaminase <lb/>Numeric <lb/>7 <lb/>HbSAg <lb/>The surface antigen of hepatitis B virus (HBV) <lb/>Categorical <lb/>8 <lb/>HAV <lb/>The virus that caused hepatitis A <lb/>Categorical <lb/>9 <lb/>HCV <lb/>The virus that caused hepatitis C <lb/>Categorical <lb/>B. Backward Elimination <lb/>Backward Elimination is a method that could be used to remove insignificant attributes from the model [16]. The <lb/>method is a wrapper-type feature selection technique performed by entering all predictor variables into a linear <lb/>regression model, as shown in Equation (1). In addition, the method gradually eliminates the variables that do not <lb/>meet the eligibility requirements, until a model was formed with only significant predictor variables [17]. The <lb/>representation of Backward Elimination process could be seen in Fig. 1. <lb/>= + * + * + … + * <lb/> <lb/>where: <lb/>: Dependent variable <lb/>: Independent variable <lb/>: Regression coefficients <lb/>Fig. 1 Image of Backward Elimination <lb/>C. Extreme Gradient Boosting <lb/>XGBoost is a machine learning method, and it is a regression and classification algorithm with ensemble methods. <lb/>The method is also a variant of Tree Gradient Boosting algorithm, developed with optimization that is 10 times faster <lb/>than Gradient Boosting [18]. Furthermore, XGBoost could be formed from several decision trees, with each <lb/>subsequent tree construction relying on the previous tree to form a stronger classification tree based on the sum of all <lb/>tree weights. Fig. 2 showed an image of Extreme Gradient Boosting. <lb/>D. Bayesian Search Optimization <lb/>Bayesian Search Optimization is used as a hyperparameter search method to obtain the optimal XGBoost model. <lb/>To achieve better performance, hyperparameter search utilized information from previous experiments to select <lb/>hyperparameter combinations. Additionally, prior probability was used to determine the best point until the last <lb/>iteration [20]. <lb/></body>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<body>305 <lb/>E. Random Search <lb/>Random search is a hyperparameter search method that efficiently and randomly selects a combination of <lb/>hyperparameters from each iteration. Generally, the selection of optimal hyperparameters was performed by <lb/>considering the highest cross-validation accuracy value from all the candidates generated. According to Li &amp; <lb/>Talwalker, random search is a simple method that has a strong basis compared to more complex algorithms [21]. <lb/>Fig. 2 Image of Extreme Gradient Boosting (based on the Flow chart of XGBoost [19]) <lb/>F. System Architecture <lb/>The workflow of this research system was presented in the form of a business process model shown in Fig. 3. <lb/>Fig. 3 System Architecture <lb/>According to Fig. 3, the system started by inputting the dataset and later divided the dataset into two parts, namely <lb/>training and test data, with the training data comprising 80% and the test data being 20% of the total dataset. <lb/>Furthermore, the necessary preprocessing process was carried out by performing Missing Value Treatment and Label <lb/>Encoding method. To reduce features or data that were not very significant, Backward Elimination process was <lb/>initially performed before developing XGBoost model using a combination of Bayesian and Random Search <lb/></body>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>306 <lb/></page>

			<body>hyperparameters. Subsequently, predictions were conducted to obtain model performance evaluation results in the <lb/>form of a confusion matrix and k-fold cross-validation with 5-fold cross-validation <lb/>G. Evaluation Metrics <lb/>Classification model evaluation was performed to measure the performance of the classification model used [22]. <lb/>Measurement of evaluation metrics in this result used the confusion matrix method and k-fold cross-validation with <lb/>5-fold cross-validation. <lb/>1) Confusion Matrix <lb/>Confusion matrix is a method commonly used to calculate accuracy in the classification model evaluation stage <lb/>[23]. The method produced several values that were used as an evaluation of model performance, namely f1 score, <lb/>accuracy, precision, and recall [24]. <lb/>Accuracy: Accuracy refers to the percentage result of the number of correctly classified test data. The calculation <lb/>of the accuracy value could be seen in Equation (2). <lb/>= <lb/>+ <lb/> <lb/>Precision: Measures the certainty of the actual percentage of tuples labeled as positive were true in reality [25]. The <lb/>calculation of the precision value is shown in Equation (3). <lb/>= <lb/>+ <lb/> <lb/>Recall: Recall measures the completeness of the exact percentage of positive tuples that are positively labeled, and <lb/>the calculation of recall value was shown in Equation (4). <lb/>= <lb/>+ <lb/> <lb/>F1 Score: The sum of the harmonic mean between precision and recall, and the calculation of the f1 score value <lb/>was shown in Equation (5). <lb/>1 <lb/>= 2 * <lb/> * <lb/>+ <lb/> <lb/>where: <lb/>TP (True Positive): The number of positives that were correctly predicted as positive. <lb/>FP (False Positive): The number of negatives that were incorrectly predicted as positive. <lb/>TN (True Negative): The number of negatives that were correctly predicted as negative. <lb/>FN (False Negative): The number of positives that were incorrectly predicted as negative. <lb/>2) K-Fold Cross Validation <lb/>K-fold cross-validation was used to estimate prediction errors when evaluating model performance. The data was <lb/>divided into k almost equal parts and the classification model was trained and tested k times. In addition, model <lb/>classification accuracy was determined by averaging the accuracy at each repetition. In the duplication, a set of parts <lb/>was used as training and testing data [26]. <lb/>III. RESULTS <lb/>A. Pre-Processing <lb/>The initial preprocessing process was carried out by handling missing values in each row of data that had empty <lb/>values. Various methods were used to handle missing values, which were described in Table 2. The next step <lb/>comprises the process of converting categorical data into numerical values, which was performed by using the label <lb/>encoding method. Subsequently, the data was split with a ratio of 80% for training data and 20% for test data. <lb/></body>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>307 <lb/></page>

			<body>TABLE 2 <lb/>MISSING VALUE TREATMENT METHOD IN THE PRE-PROCESSING PROCESS <lb/>Missing Value Treatment Description <lb/>Dropna <lb/>Deleted data rows that have empty values <lb/>Median <lb/>Filled empty values using the median value of the variable <lb/>Mean <lb/>Filled empty values using the mean value of the variable <lb/>Forward Fill <lb/>Filled the empty value with the closest value in front of the row <lb/>B. Training Model <lb/>Model training was performed by conducting a feature selection process using Backward Elimination. The process <lb/>was performed by selecting features based on the p-value of each feature obtained through regressor_OLS.summary. <lb/>The results of the p-value calculation could be seen in Table 3. <lb/>Based on Table 3, it could be seen that the variables x1 (age), x2 (gender), and x5 (SGOT) were not significant in <lb/>the model, as evidenced by p-values exceeding the significance level (α = 0.05). Meanwhile, the remaining variables <lb/>(total bilirubin, direct bilirubin, SGPT, HBsAg, HAV, and HCV) were influential (significant) features in the model, <lb/>indicated by their p-value, which was lower than the significant level. This difference was due to the use of Backward <lb/>Elimination during the model development. Regarding the hyperparameter tuning process, Bayesian and random <lb/>search was performed using a set of parameters listed in Table 4. <lb/>TABLE 3 <lb/>THE SIGNIFICANCE OF THE RELATIONSHIPS IN THE MODEL <lb/>Variable Coefficient Standard Error t <lb/>P-value <lb/>Const <lb/>3.2724 <lb/>0.033 <lb/>99.689 0.000 <lb/>x1 <lb/>-0.0012 <lb/>0.003 <lb/>-1.919 0.055 <lb/>x2 <lb/>0.0292 <lb/>0.001 <lb/>1.301 0.194 <lb/>x3 <lb/>-0.0109 <lb/>0.022 <lb/>-3.408 0.001 <lb/>x4 <lb/>0.0472 <lb/>0.003 <lb/>7.666 0.000 <lb/>x5 <lb/>-5.215e-05 0.006 <lb/>-1.266 0.206 <lb/>x6 <lb/>0.0003 <lb/>4.12e-05 <lb/>6.419 0.000 <lb/>x7 <lb/>-2.3048 <lb/>0.027 <lb/>-85.551 0.000 <lb/>x8 <lb/>-3.3040 <lb/>0.026 <lb/>-124.684 0.000 <lb/>x9 <lb/>-1.3299 <lb/>0.033 <lb/>-39.806 0.000 <lb/>*Significant α = 0,05 <lb/>TABLE 4 <lb/>BAYESIAN SEARCH AND RANDOM SEARCH HYPERPARAMETER RANGE <lb/>Parameter <lb/>Description <lb/>Range <lb/>n_estimators <lb/>Number of trees to be created <lb/>100 <lb/>-<lb/>300 <lb/>max_depth <lb/>Maximum depth of the tree <lb/>4 <lb/>-<lb/>8 <lb/>min_child_weight <lb/>Minimum number of weights of child nodes in the tree <lb/>0 <lb/>-<lb/>7 <lb/>learning_rate <lb/>Rate of learning patterns in the data <lb/>0.025 -<lb/>0.3 <lb/>gamma <lb/>Minimum loss reduction value <lb/>0 <lb/>-<lb/>2 <lb/>colsample_bylevel Column subsample ratio at each level <lb/>0.25 <lb/>-<lb/>1 <lb/>subsample <lb/>Number of samples used during the training process <lb/>0.5 <lb/>-<lb/>1 <lb/>Random Search hyperparameter tuning process used XGBoost RandomSearchCV model initialization, where the <lb/>CV was cross-validated with K-fold cross-validation. Kfold used = 5. Based on the hyperparameter range in Table 3, <lb/>the hyperparameter tuning process based on Bayesian Search and Random Search was performed for 100 iterations to <lb/>get the best hyperparameter values, which could be seen in Table 5. <lb/>Model training experiments were performed with backward feature selection and without Backward Elimination, <lb/>different missing value treatments, and each model with different hyperparameters, namely Bayesian search and <lb/>random search. Furthermore, for each model training, the aim settings were &quot;multi:softmax&quot;, num_class = 5, and <lb/>eval_metric = [&apos;mlogloss&apos;, &apos;merror&apos;]. The following model training scenarios without Backward Elimination were <lb/>shown in Table 6. When training the model with Backward Elimination, the following datasets were used, namely the <lb/>total bilirubin, direct bilirubin, SGPT, HBsAg, HAV, and HCV features. The following model training scenario with <lb/>Backward Elimination was shown in Table 7. <lb/></body>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>308 <lb/></page>

			<div type="annex">TABLE 5 <lb/>BAYESIAN SEARCH AND RANDOM SEARCH HYPERPARAMETER TUNING RESULTS <lb/>Parameter <lb/>Bayesian Search Random Search <lb/>n_estimators <lb/>240 <lb/>200 <lb/>max_depth <lb/>5 <lb/>4 <lb/>min_child_weight <lb/>1 <lb/>0 <lb/>learning_rate <lb/>0.2 <lb/>0.025 <lb/>gamma <lb/>0.3 <lb/>0 <lb/>colsample_bylevel <lb/>0.610 <lb/>0.75 <lb/>subsample <lb/>0.445 <lb/>0.15 <lb/>TABLE 6 <lb/>SCENARIO OF XGBOOST MODEL TRAINING WITHOUT BACKWARD ELIMINATION <lb/>Number Of Models Dataset Missing Value Treatment Hyperparameter Optimization <lb/>Normal Dropna <lb/>Bayesian Search <lb/>2 <lb/>Normal Dropna <lb/>Random Search <lb/>3 <lb/>Normal Median <lb/>Bayesian Search <lb/>4 <lb/>Normal Median <lb/>Random Search <lb/>5 <lb/>Normal Mean <lb/>Bayesian Search <lb/>6 <lb/>Normal Mean <lb/>Random Search <lb/>7 <lb/>Normal Forward Fill <lb/>Bayesian Search <lb/>8 <lb/>Normal Forward Fill <lb/>Random Search <lb/>TABLE 7 <lb/>SCENARIO OF XGBOOST MODEL TRAINING WITH BACKWARD ELIMINATION <lb/>Number Of Models Dataset <lb/>Missing Value Treatment Hyperparameter Optimization <lb/>1 <lb/>Backward Elimination Dropna <lb/>Bayesian Search <lb/>2 <lb/>Backward Elimination Dropna <lb/>Random Search <lb/>3 <lb/>Backward Elimination Median <lb/>Bayesian Search <lb/>4 <lb/>Backward Elimination Median <lb/>Random Search <lb/>5 <lb/>Backward Elimination Mean <lb/>Bayesian Search <lb/>6 <lb/>Backward Elimination Mean <lb/>Random Search <lb/>7 <lb/>Backward Elimination Forward Fill <lb/>Bayesian Search <lb/>8 <lb/>Backward Elimination Forward Fill <lb/>Random Search <lb/>C. Results of XGBoost Model Testing Without Backward Elimination <lb/>The results of testing 8 models using 20% of the test data were measured based on the degree of accuracy, precision, <lb/>recall, F1 score, training time, and average precision of k-fold cross-validation results with a value of Kfold = 5. The <lb/>following were the results of testing the model without Backward Elimination, which could be seen in Table 8. <lb/>According to Table 8, the results of training XGBoost model without Backward Elimination, using mean feature <lb/>value to handle missing data, and applying &quot;Random Search&quot; for hyperparameter optimization with specific <lb/>parameters (n_estimators = 200, max_depth = 4, min_child_weight = 0, learning_rate = 0.025, gamma = 0, <lb/>colsample_bylevel = 0.75, and subsample = 0.15) produced an accuracy of 98.437% at 0.74 seconds. This performance <lb/>was better than using Bayesian Search hyperparameter optimization with parameters n_estimators = 240, max_depth <lb/>= 5, min_child_weight = 1, learning_rate = 0.2, gamma = 0.3, colsample_bylevel = 0.610, and subsample = 0.445, <lb/>which achieved the same accuracy of 98.437%, but with a training time of 0.85s. <lb/>Even though model training without Backward Elimination achieved high accuracy based on the results of the <lb/>confusion matrix or 5-fold cross-validation average, it could be seen from the graph in Fig. 4 that there were several <lb/>models where overfitting occurred. In these cases, the training data had a high level of accuracy, but simultaneously, <lb/>the accuracy of the validation data was low. <lb/></div>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>309 <lb/></page>

			<body>TABLE 8 <lb/>RESULTS OF XGBOOST MODEL TESTING WITHOUT BACKWARD ELIMINATION <lb/>Number Of Models <lb/>Evaluation of Model Performance <lb/>TrnT <lb/>5-Fold CrossVal <lb/>Acc. <lb/>Prec. <lb/>Rec. <lb/>F1 Scr <lb/>1 <lb/>95.890 % 95.252 % 95.894 % 95.521 % 0.68 s 0.983 <lb/>2 <lb/>96.575 % 96.061 % 96.421 % 96.227 % 0.59 s 0.982 <lb/>3 <lb/>95.833 % 95.495 % 96.466 % 95.878 % 0.92 s 0.980 <lb/>4 <lb/>97.916 % 97.636 % 98.233 % 97.906 % 0.72 s 0.973 <lb/>5 <lb/>98.437 % 98.263 % 98.583 % 98.416 % 0.85 s 0.982 <lb/>6 <lb/>98.437 % 98.263 % 98.583 % 98.416 % 0.74 s 0.983 <lb/>7 <lb/>96.354 % 96.033 % 96.817 % 96.365 % 0.77 s 0.971 <lb/>8 <lb/>97.916 % 97.500 % 98.596 % 97.939 % 0.65 s 0.973 <lb/>*Acc: Accuracy, Prec: Precision, Rec: Recall, F1 Scr.: F1-Score, TrnT.: Training Time, <lb/>K-Fold Cross Val: Mean of 5-Fold Cross Validation <lb/>D. Test results of XGBoost model with Backward Elimination <lb/>The results of testing 8 models using 20% of the test data were measured based on the level of accuracy, precision, <lb/>recall, F1 score, training time, and average precision of k-fold cross-validation results with a value of Kfold = 5. The <lb/>following are the results of testing the model without Backward Elimination, which could be seen in Table 9. <lb/>Table 9 showed that XGBoost model trained with Backward Elimination, mean feature value treatment for missing <lb/>value, and using &quot;Random Search&quot; with specific parameters (n_estimators = 200, max_depth = 4, min_child_weight <lb/>= 0, learning_rate = 0.025, gamma = 0, colsample_bylevel = 0.75, and subsample = 0.15) achieved an accuracy of <lb/>98.958% at 0.64 seconds. This performance was better than using Bayesian Search (n_estimators = 240, max_depth <lb/>= 5, min_child_weight = 1, learning_rate = 0.2, gamma = 0.3, colsample_bylevel = 0.610, and subsample = 0.445), <lb/>which achieved the same accuracy but required a longer training time of 0.70 seconds. <lb/>TABLE 9 <lb/>RESULTS OF XGBOOST MODEL TESTING WITH BACKWARD ELIMINATION <lb/>Number Of Models <lb/>Evaluation of Model Performance <lb/>TrnT <lb/>5-Fold CrossVal <lb/>Acc. <lb/>Prec. <lb/>Rec. <lb/>F1 Scr <lb/>1 <lb/>97.945 % 97.921 % 97.473 % 97.682 % 0.64 s 0.983 <lb/>2 <lb/>97.945 % 97.921 % 97.473 % 97.682 % 0.57 s 0.982 <lb/>3 <lb/>96.354 % 95.979 % 97.180 % 96.420 % 0.87 s 0.980 <lb/>4 <lb/>97.395 % 97.048 % 97.882 % 97.404 % 0.69 s 0.973 <lb/>5 <lb/>98.958 % 98.934 % 98.934 % 98.934 % 0.70 s 0.982 <lb/>6 <lb/>98.958 % 98.934 % 98.934 % 98.934 % 0.64 s 0.983 <lb/>7 <lb/>98.437 % 98.064 % 98.947 % 98.442 % 0.65 s 0.971 <lb/>8 <lb/>98.958 % 98.666 % 99.298 % 98.953 % 0.64 s 0.973 <lb/>*Acc: Accuracy, Prec: Precision, Rec: Recall, F1 Scr.: F1-Score, TrnT.: Training Time, <lb/>K-Fold Cross Val: Mean of 5-Fold Cross Validation <lb/>The model training using Backward Elimination feature selection according to the findings in Table 9 achieved <lb/>high accuracy based on the results of the confusion matrix or five-fold cross-validation average. The graph in Fig. 5 <lb/>showed that model training using Backward Elimination achieved high accuracy. Furthermore, feature selection <lb/>increased the accuracy of the training data to the same level as the validation data, showing a reduction in the <lb/>overfitting effect. The reduction could be understood by observing how the two accuracy curves, namely the training <lb/>data and validation data, come closer together. The observation showed that the model could perform well on new, <lb/>unseen data. However, some models still experienced overfitting, obviously due to a growing disparity between the <lb/>training and validation data curves longer time. <lb/></body>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

            <page>310 <lb/></page>

			<body>(a) <lb/>(b) <lb/>(c) <lb/>(d) <lb/>(e) <lb/>(f) <lb/>(g) <lb/>(h) <lb/>Fig. 4 Graph Of Accuracy For Xgboost Model Testing Without Backward Elimination (a) Drop+Bayesian, (b) <lb/>Drop+Random, (c) Mean+Bayesian, (d) Mean+Random, (e) Median+Bayesian, (f) Median+Random, (g) Ffill+Bayesian, (h) <lb/>Ffill+Random <lb/></body>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

            <page>311 <lb/></page>

			<body>(a) <lb/>(b) <lb/>(c) <lb/>(d) <lb/>(e) <lb/>(f) <lb/>(g) <lb/>(h) <lb/>Fig. 5 Graph Of Accuracy For Xgboost Model Testing With Backward Elimination (BE) (a) Drop+BE+Bayesian, (b) Drop+ <lb/>BE+Random, (c) Mean+ BE+Bayesian, (d) Mean+ BE+Random, (e) Median+ BE+Bayesian, (f) Median+ BE+Random, (g) <lb/>Ffill+ BE+Bayesian, (h) Ffill+ BE+Random <lb/>IV. DISCUSSION <lb/>Several research including [9], had examined the implementation results of seven machine learning algorithms <lb/>including SVM, Decision Tree, Random Forest, Naive Bayes, Logistic Regression, Adaptive Boosting, and Extreme <lb/>Gradient Boosting for predicting liver disease. Among the algorithms, Extreme Gradient was used, which showed the <lb/>highest accuracy reaching 81% [10]. Additionally, XGBoost showed superior accuracy of 95% in predicting hepatitis <lb/>B surface antigen zero-clearance [11]. The use of this model for liver disease prediction obtained the highest accuracy <lb/></body>

			<note place="footnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>312 <lb/></page>

			<div type="annex">of 87.02%. According to George Obaido et al. [12], hepatitis B diagnosis using XGBoost achieved 90% accuracy. In <lb/>this current research, model training using XGBoost achieved the highest accuracy of 98.437% at 0.74 seconds. <lb/>The test results using Backward Elimination when training XGBoost model showed an accuracy of 98.958% at <lb/>0.64 seconds. The improvement of 0.521% in accuracy was attributed to Backward Elimination which aided in <lb/>reducing the number of features used. <lb/>The inclusion of Backward Elimination in the training process improved accuracy and reduced overfitting by <lb/>minimizing the use of insignificant features. Furthermore, the choice of hyperparameters alongside Backward <lb/>Elimination also affected the training time with &quot;Random Search&quot; showing faster training time compared to &quot;Bayesian <lb/>Search&quot;. <lb/>Other aspects that could be investigated in further research were feature selection methods. Although Backward <lb/>Elimination was used in this exploration, different feature selection methods could be used to provide a more <lb/>comprehensive comparison. Further exploration can also be carried out to understand the way in which different <lb/>feature selection methods impact results and contribute to significant improvements. While considering the balance <lb/>of each class, the number of datasets needs to be taken into account because an increase in the number could also <lb/>affect performance. <lb/>V. CONCLUSIONS <lb/>In conclusion, the results of the research showed that feature selection using Backward Elimination method had a <lb/>positive impact on the performance of XGBoost model. By using only six relevant features, the model achieved high <lb/>accuracy, precision, recall, and F1 score. Apart from increasing the accuracy of hepatitis identification, the use of <lb/>Backward Elimination also reduced overfitting. Additionally, the results of hyperparameter optimization of XGBoost <lb/>model using Bayesian and Random Search methods showed that this method was effective. <lb/></div>

			<div type="contribution">Author Contributions: Jasman Pardede: Writing -Review, Supervision, Validation. Desita Nurrohmah: <lb/>Conceptualisation, Methodology, Software, Writing -Original Draft. <lb/>All authors have read and agreed to the published version of the manuscript. <lb/></div>

            <div type="funding">Funding: This research received no specific grant from any funding agency. <lb/></div>

            <div type="acknowledgment">Acknowledgments: This research is supported by Department of Informatics, Faculty of Industrial Technology, <lb/>National Institute of Technology. <lb/></div>

            <div type="conflict">Conflicts of Interest: The authors declare no conflict of interest. <lb/></div>

            <div type="availability">Data Availability: The primary data that has been used is confidential. Meanwhile, the secondary data that support <lb/>the findings of this study are openly available in Kaggle at https://www.kaggle.com/datasets/jeevannagaraj/indian-<lb/>liver-patient-dataset. <lb/></div>

            <div type="annex">Institutional Review Board Statement: This study received ethical approval from the LPPM-Institut Teknologi <lb/>Nasional (Itenas) Institutional Review Board on June 23, 2023. <lb/>Informed Consent: Informed Consent was obtained, and a detailed explanation was presented in the Methods section. <lb/>Animal Subjects: There were no animal subjects. <lb/></div>

            <front>ORCID: <lb/>Jasman Pardede: https://orcid.org/0000-0001-7773-0296 <lb/>Desita Nurrohmah: https://orcid.org/0009-0002-5391-8256 <lb/></front>

            <listBibl>REFERENCES <lb/>[1] K. Y. Raharja, H. Oktavianto, and R. Umilasari, &quot;Perbandingan Kinerja Algoritma Gaussian Naive Bayes Dan K-Nearest Neighbor <lb/>(KNN) Untuk Mengklasifikasi Penyakit Hepatitis C Virus (HCV),&quot; Undergraduate thesis, Department of Informatics Engineering, <lb/>Universitas Muhammadiyah Jember, 2021. [Online]. Available: http://repository.unmuhjember.ac.id/id/eprint/8590 <lb/></listBibl>

			<note place="headnote">Pardede &amp; Nurrohmah <lb/>Journal of Information Systems Engineering and Business Intelligence, 2024, 10 (2), 302-313 <lb/></note>

			<page>313 <lb/></page>

			<listBibl>[2] Pan American Health Organization and World Health Organization, &quot;5 Things to Know About Viral Hepatitis,&quot; PAHO.org. Accessed: <lb/>August. 2, 2023. [Online.] Available: https://www.paho.org/en/topics/hepatitis/5-things-you-should-know-about-viral-hepatitis <lb/>[3] P. <lb/>Khetrapal <lb/>Singh, <lb/>&quot;Bringing <lb/>hepatitis <lb/>care <lb/>closer <lb/>to <lb/>you,&quot; <lb/>WHO.int, <lb/>2022. <lb/>[Online]. <lb/>Available: <lb/>https://www.who.int/southeastasia/news/opinion-editorials/detail/bringing-hepatitis-care-closer-to-you. <lb/>[4] M. <lb/>K. <lb/>dr. <lb/>Wening <lb/>Sari, <lb/>Care <lb/>Your <lb/>self: <lb/>Hepatitis. <lb/>Niaga <lb/>Swadaya. <lb/>[Online]. <lb/>Available: <lb/>https://books.google.co.id/books?id=jQdJz1maiXwC <lb/>[5] Kementerian Kesehatan, &quot;Laporan Riskesdas 2018 Nasional,&quot; Kementrian Kesehatan RI, Indonesia, 2018. [Online]. Available: <lb/>https://repository.badankebijakan.kemkes.go.id/id/eprint/3514/1/Laporan%20Riskesdas%202018%20Nasional.pdf <lb/>[6] A. K. Saurav, Patra MD; Mukherjee, Brijesh; Das, &quot;Pre-analytical errors in the clinical laboratory and how to minimize them quality <lb/>control view project,&quot; Int. J. Bioassays, vol. 2, no. May 2014, pp. 551-553, 2013, [Online]. Available: <lb/>https://www.researchgate.net/publication/236020318 <lb/>[7] Y. Rombe, &quot;penggunaan metode XGboost untuk klasifikasi status obesitas di Indonesia,&quot; Thesis, Fakultas Matematika dan Ilmu <lb/>Pengetahuan Alam, Hasanuddin University, 2022. [Online]. Available: http://repository.unhas.ac.id:443/id/eprint/13027 <lb/>[8] A. N. Rachmi, &quot;Implementasi metode Random Forest dan Xgboost pada klasifikasi customer churn,&quot; Undergraduate thesis, Faculty of <lb/>Mathematics and Natural Sciences, Universitas Islam Indonesia, 2020. [Online]. Available: https://dspace.uii.ac.id/123456789/30082 <lb/>[9] Pranitha Gadde, G. Deepthi, C. Shivani, K. Nagavinith, and K. H. Kumar, &quot;Heart disease prediction using machine learning algorithms,&quot; <lb/>Int. J. Manag. Technol. Eng., vol. 11, no. 6, pp. 29-35, 2021, doi: 16.10089.IJMTE.2021.V10I6.21.50804. <lb/>[10] X. Tian et al., &quot;Using machine learning algorithms to predict hepatitis B surface antigen Seroclearance,&quot; Comput. Math. Methods Med., <lb/>vol. 2019, pp. 1-7, Jun. 2019, doi: 10.1155/2019/6915850. <lb/>[11] C. M. Bhatt, P. Patel, T. Ghetia, and P. L. Mazzeo, &quot;Effective heart disease prediction using machine learning techniques,&quot; Algorithms, <lb/>vol. 16, no. 2, p. 88, Feb. 2023, doi: 10.3390/a16020088. <lb/>[12] G. Obaido et al., &quot;An interpretable machine learning approach for hepatitis B diagnosis,&quot; Appl. Sci., vol. 12, no. 21, p. 11127, Nov. 2022, <lb/>doi: 10.3390/app122111127. <lb/>[13] Y. Saeys, I. Inza, and P. Larrañaga, &quot;A review of feature selection techniques in bioinformatics,&quot; Bioinformatics, vol. 23, no. 19, pp. <lb/>2507-2517, Oct. 2007, doi: 10.1093/bioinformatics/btm344. <lb/>[14] M. Tharmakulasingam, C. Topal, A. Fernando, and R. La Ragione, &quot;Backward feature elimination for accurate pathogen recognition <lb/>using portable electronic nose,&quot; in 2020 IEEE International Conference on Consumer Electronics (ICCE), IEEE, Jan. 2020, pp. 1-5. <lb/>doi: 10.1109/ICCE46568.2020.9043043. <lb/>[15] M. A. Wiratama and W. M. Pradnya, &quot;Optimization of data mining algorithm using backward elimination for diabetes classification,&quot; J. <lb/>Nas. Pendidik. Tek. Inform., vol. 11, no. 1, p. 1, Apr. 2022, doi: 10.23887/janapati.v11i1.45282. <lb/>[16] D. H. Vu, K. M. Muttaqi, and A. P. Agalgaonkar, &quot;A variance inflation factor and backward elimination based robust regression model <lb/>for forecasting monthly electricity demand using climatic variables,&quot; Appl. Energy, vol. 140, pp. 385-394, Feb. 2015, doi: <lb/>10.1016/j.apenergy.2014.12.011. <lb/>[17] Kurniawan and B. Yuniarto, Analisis Regresi: Dasar dan penerapannya dengan R. Indonesia: Kencana Prenada Media Group, 2016. <lb/>[18] T. Chen and C. Guestrin, &quot;XGBoost: A Scalable Tree Boosting System,&quot; in Proceedings of the 22nd ACM SIGKDD International <lb/>Conference on Knowledge Discovery and Data Mining, New York, NY, USA: ACM, Aug. 2016, pp. 785-794. doi: <lb/>10.1145/2939672.2939785. <lb/>[19] R. Guo, Z. Zhao, T. Wang, G. Liu, J. Zhao, and D. Gao, &quot;Degradation State Recognition of Piston Pump Based on ICEEMDAN and <lb/>XGBoost,&quot; Appl. Sci., vol. 10, no. 18, p. 6593, Sep. 2020, doi: 10.3390/app10186593. <lb/>[20] R. Ubaidillah, M. Muliadi, D. T. Nugrahadi, M. R. Faisal, and R. Herteno, &quot;Implementasi XGBoost Pada Keseimbangan Liver Patient <lb/>Dataset dengan SMOTE dan Hyperparameter Tuning Bayesian Search,&quot; J. MEDIA Inform. BUDIDARMA, vol. 6, no. 3, p. 1723, Jul. <lb/>2022, doi: 10.30865/mib.v6i3.4146. <lb/>[21] L. Hertel, P. Baldi, and D. L. Gillen, &quot;Quantity vs. Quality: On Hyperparameter Optimization for Deep Reinforcement Learning,&quot; Jul. <lb/>2020, [Online]. Available: http://arxiv.org/abs/2007.14604 <lb/>[22] P. Purwono, A. Wirasto, and K. Nisa, &quot;Comparison of Machine Learning Algorithms for Classification of Drug Groups,&quot; SISFOTENIKA, <lb/>vol. 11, no. 2, p. 196, Jul. 2021, doi: 10.30700/jst.v11i2.1134. <lb/>[23] M. F. Rahman, D. Alamsah, M. I. Darmawidjadja, and I. Nurma, &quot;Classification for Diabetes Diagnosis Using Bayesian Regularization <lb/>Neural Network (RBNN) Method,&quot; J. Inform., vol. 11, no. 1, p. 36, Jan. 2017, doi: 10.26555/jifo.v11i1.a5452. <lb/>[24] Suyanto, <lb/>Machine <lb/>Learning <lb/>Tingkat <lb/>Dasar <lb/>Dan <lb/>Lanjut. <lb/>Informatika, <lb/>2018. <lb/>[Online]. <lb/>Available: <lb/>https://books.google.co.id/books?id=QWbuzwEACAAJ <lb/>[25] I. Saputra and D. Rosiyadi, &quot;Perbandingan kinerja algoritma K-Nearest Neighbor, Naïve Bayes Classifier dan Support Vector Machine <lb/>dalam klasifikasi tingkah laku bully pada aplikasi Whatsapp,&quot; Fakt. Exacta, vol. 12, no. 2, p. 101, Jul. 2019, doi: <lb/>10.30998/faktorexacta.v12i2.4181. <lb/>[26] Nurhayati, I. Soekarno, I. K. Hadihardaja, and M. Cahyono, &quot;A study of Hold-Out and K-Fold Cross Validation for accuracy of <lb/>groundwater modeling in Tidal Lowland Reclamation using Extreme Learning Machine,&quot; in 2014 2nd International Conference on <lb/>Technology, Informatics, Management, Engineering &amp; Environment, IEEE, Aug. 2014, pp. 228-233. doi: 10.1109/TIME-<lb/>E.2014.7011623. <lb/></listBibl>

			<div type="annex">Publisher&apos;s Note: Publisher stays neutral with regard to jurisdictional claims in published maps and institutional <lb/>affiliations. </div>


	</text>
</tei>
