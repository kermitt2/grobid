<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_J09-3003"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Recognizing Contextual Polarity: <lb/>An Exploration of Features for Phrase-Level <lb/>Sentiment Analysis <lb/> Theresa Wilson  * <lb/> University of Edinburgh <lb/> Janyce Wiebe  * * <lb/> University of Pittsburgh <lb/> Paul Hoffmann  * * <lb/> University of Pittsburgh <lb/> Many approaches to automatic sentiment analysis begin with a large lexicon of words marked <lb/>with their prior polarity (also called semantic orientation). However, the contextual polarity of <lb/>the phrase in which a particular instance of a word appears may be quite different from the <lb/>word&apos;s prior polarity. Positive words are used in phrases expressing negative sentiments, or <lb/>vice versa. Also, quite often words that are positive or negative out of context are neutral in <lb/>context, meaning they are not even being used to express a sentiment. The goal of this work is to <lb/>automatically distinguish between prior and contextual polarity, with a focus on understanding <lb/>which features are important for this task. Because an important aspect of the problem is <lb/>identifying when polar terms are being used in neutral contexts, features for distinguishing <lb/>between neutral and polar instances are evaluated, as well as features for distinguishing between <lb/>positive and negative contextual polarity. The evaluation includes assessing the performance <lb/>of features across multiple machine learning algorithms. For all learning algorithms except <lb/>one, the combination of all features together gives the best performance. Another facet of the <lb/>evaluation considers how the presence of neutral instances affects the performance of features for <lb/>distinguishing between positive and negative polarity. These experiments show that the presence <lb/>of neutral instances greatly degrades the performance of these features, and that perhaps the <lb/>best way to improve performance across all polarity classes is to improve the system&apos;s ability to <lb/>identify when an instance is neutral. <lb/></front>

			<body> 1. Introduction <lb/> Sentiment analysis is a type of subjectivity analysis (Wiebe 1994) that focuses on iden-<lb/>tifying positive and negative opinions, emotions, and evaluations expressed in natural <lb/>language. It has been a central component in applications ranging from recognizing <lb/></body>

			<front>  *  School of Informatics, Edinburgh EH8 9LW, U.K. E-mail: twilson@inf.ed.ac.uk. <lb/>  * *  Department of Computer Science, Pittsburgh, PA 15260, USA. E-mail: {wiebe,hoffmanp}@cs.pitt.edu. <lb/> Submission received: 14 November 2006; revised submission received: 8 March 2008; accepted for publication: <lb/>16 April 2008. <lb/></front>
			
			<front>© 2009 Association for Computational Linguistics <lb/></front>

			<front> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></front>

			<body> inflammatory messages (Spertus 1997), to tracking sentiments over time in online <lb/>discussions (Tong 2001), to classifying positive and negative reviews (Pang, Lee, and <lb/>Vaithyanathan 2002; Turney 2002). Although a great deal of work in sentiment analy-<lb/>sis has targeted documents, applications such as opinion question answering (Yu and <lb/>Hatzivassiloglou 2003; Maybury 2004; Stoyanov, Cardie, and Wiebe 2005) and re-<lb/>view mining to extract opinions about companies and products (Morinaga et al. 2002; <lb/>Nasukawa and Yi 2003) require sentence-level or even phrase-level analysis. For exam-<lb/>ple, if a question answering system is to successfully answer questions about people&apos;s <lb/>opinions, it must be able not only to pinpoint expressions of positive and negative <lb/>sentiments, such as we find in sentence (1), but also to determine when an opinion is not <lb/> being expressed by a word or phrase that typically does evoke one, such as condemned <lb/> in sentence (2). <lb/>(1) African observers generally approved (positive) of his victory while <lb/>Western governments denounced (negative) it. <lb/>(2) Gavin Elementary School was condemned in April 2004. <lb/>A common approach to sentiment analysis is to use a lexicon with information <lb/>about which words and phrases are positive and which are negative. This lexicon may <lb/>be manually compiled, as is the case with the General Inquirer (Stone et al. 1966), a <lb/>resource often used in sentiment analysis. Alternatively, the information in the lexicon <lb/>may be acquired automatically. Acquiring the polarity of words and phrases is itself <lb/>an active line of research in the sentiment analysis community, pioneered by the <lb/>work of Hatzivassiloglou and McKeown (1997) on predicting the polarity or semantic <lb/>orientation of adjectives. Various techniques have been proposed for learning the <lb/>polarity of words. They include corpus-based techniques, such as using constraints <lb/>on the co-occurrence in conjunctions of words with similar or opposite polarity <lb/>(Hatzivassiloglou and McKeown 1997) and statistical measures of word association <lb/>(Turney and Littman 2003), as well as techniques that exploit information about lexical <lb/>relationships (Kamps and Marx 2002; Kim and Hovy 2004) and glosses (Esuli and <lb/>Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet. <lb/>Acquiring the polarity of words and phrases is undeniably important, and there <lb/>are still open research challenges, such as addressing the sentiments of different senses <lb/>of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006), and so on. However, <lb/>what the polarity of a given word or phrase is when it is used in a particular context is <lb/>another problem entirely. Consider, for example, the underlined positive and negative <lb/>words in the following sentence. <lb/>(3) Philip Clapp, president of the National Environment Trust, sums up well <lb/>the general thrust of the reaction of environmental movements: &quot; There is <lb/>no reason at all to believe that the polluters are suddenly going to become <lb/>reasonable. &quot; <lb/>The first underlined word is Trust. Although many senses of the word trust express a <lb/>positive sentiment, in this case, the word is not being used to express a sentiment at <lb/>all. It is simply part of an expression referring to an organization that has taken on <lb/>the charge of caring for the environment. The adjective well is considered positive, and <lb/>indeed it is positive in this context. However, the same is not true for the words reason <lb/>

			<page> 400 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			and reasonable. Out of context, we would consider both of these words to be positive. 1 <lb/> In context, the word reason is being negated, changing its polarity from positive to <lb/>negative. The phrase no reason at all to believe changes the polarity of the proposition that <lb/>follows; because reasonable falls within this proposition, its polarity becomes negative. <lb/>The word polluters has a negative connotation, but here in the context of the discussion <lb/>of the article and its position in the sentence, polluters is being used less to express a <lb/>sentiment and more to objectively refer to companies that pollute. To clarify how the <lb/>polarity of polluters is affected by its subject role, consider the purely negative sentiment <lb/>that emerges when it is used as an object: They are polluters. <lb/> We call the polarity that would be listed for a word in a lexicon the word&apos;s prior <lb/> polarity, and we call the polarity of the expression in which a word appears, con-<lb/>sidering the context of the sentence and document, the word&apos;s contextual polarity. <lb/> Although words often do have the same prior and contextual polarity, many times <lb/>a word&apos;s prior and contextual polarities differ. Words with a positive prior polarity <lb/>may have a negative contextual polarity, or vice versa. Quite often words that are <lb/>positive or negative out of context are neutral in context, meaning that they are not <lb/>even being used to express a sentiment. Similarly, words that are neutral out of context, <lb/>neither positive or negative, may combine to create a positive or negative expression in <lb/>context. <lb/>The focus of this work is on the recognition of contextual polarity—in particular, <lb/>disambiguating the contextual polarity of words with positive or negative prior polar-<lb/>ity. We begin by presenting an annotation scheme for marking sentiment expressions <lb/>and their contextual polarity in the Multi-perspective Question Answering (MPQA) <lb/>opinion corpus. We show that, given a set of subjective expressions (identified from <lb/>the existing annotations in the MPQA corpus), contextual polarity can be annotated <lb/>reliably. <lb/>Using the contextual polarity annotations, we conduct experiments in automatically <lb/>distinguishing between prior and contextual polarity. Beginning with a large lexicon of <lb/>clues tagged with prior polarity, we identify the contextual polarity of the instances <lb/>of those clues in the corpus. The process that we use has two steps, first classifying <lb/>each clue as being in a neutral or polar phrase, and then disambiguating the contextual <lb/>polarity of the clues marked as polar. For each step in the process, we experiment with a <lb/>variety of features and evaluate the performance of the features using several different <lb/>machine learning algorithms. <lb/>Our experiments reveal a number of interesting findings. First, being able to accu-<lb/>rately identify neutral contextual polarity—when a positive or negative clue is not being <lb/>used to express a sentiment—is an important aspect of the problem. The importance of <lb/>neutral examples has previously been noted for classifying the sentiment of documents <lb/>(Koppel and Schler 2006), but ours is the first work to explore how neutral instances <lb/>affect classifying the contextual polarity of words and phrases. In particular, we found <lb/>that the performance of features for distinguishing between positive and negative po-<lb/>larity greatly degrades when neutral instances are included in the experiments. <lb/>We also found that achieving the best performance for recognizing contextual po-<lb/>larity requires a wide variety of features. This is particularly true for distinguishing <lb/>

			<note place="footnote"> 1 It is open to question whether reason should be listed as positive in a sentiment lexicon, because the more <lb/>frequent senses of reason involve intention, not sentiment. However, any existing sentiment lexicon one <lb/>would start with will have some noise and errors. The task in this article is to disambiguate instances of <lb/>the entries in a given sentiment lexicon. <lb/></note>

			<page> 401 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			between neutral and polar instances. Although some features help to increase polar or <lb/>neutral recall or precision, it is only the combination of features together that achieves <lb/>significant improvements in accuracy over the baselines. Our experiments show that for <lb/>distinguishing between positive and negative instances, features capturing negation are <lb/>clearly the most important. However, there is more to the story than simple negation. <lb/>Features that capture relationships between instances of clues also perform well, indi-<lb/>cating that identifying features that represent more complex interdependencies between <lb/>sentiment clues may be an important avenue for future research. <lb/>The remainder of this article is organized as follows. Section 2 gives an overview <lb/>of some of the things that can influence contextual polarity. In Section 3, we describe <lb/>our corpus and present our annotation scheme and inter-annotator agreement study <lb/>for marking contextual polarity. Sections 4 and 5 describe the lexicon used in our <lb/>experiments and how the contextual polarity annotations are used to determine the <lb/>gold-standard tags for instances from the lexicon. In Section 6, we consider what kind of <lb/>performance can be expected from a simple, prior-polarity classifier. Section 7 describes <lb/>the features that we use for recognizing contextual polarity, and our experiments <lb/>and results are presented in Section 8. In Section 9 we discuss related work, and we <lb/>conclude in Section 10. <lb/> 2. Polarity Influencers <lb/> Phrase-level sentiment analysis is not a simple problem. Many things besides negation <lb/>can influence contextual polarity, and even negation is not always straightforward. <lb/>Negation may be local (e.g., not good), or involve longer-distance dependencies such as <lb/>the negation of the proposition (e.g., does not look very good) or the negation of the subject <lb/>(e.g., no one thinks that it&apos;s good). In addition, certain phrases that contain negation words <lb/>intensify rather than change polarity (e.g., not only good but amazing). Contextual polarity <lb/>may also be influenced by modality: whether the proposition is asserted to be real (realis) <lb/>or not real (irrealis) (no reason at all to believe is irrealis, for example); word sense (e.g., <lb/> Environmental Trust vs. He has won the people&apos;s trust); the syntactic role of a word in the <lb/>sentence: whether the word is the subject or object of a copular verb (consider polluters <lb/>are versus they are polluters); and diminishers such as little (e.g., little truth, little threat). <lb/> Polanyi and Zaenen (2004) give a detailed discussion of many of these types of polarity <lb/>influencers. Many of these contextual polarity influencers are represented as features in <lb/>our experiments. <lb/>Contextual polarity may also be influenced by the domain or topic. For example, <lb/>the word cool is positive if used to describe a car, but it is negative if it is used to <lb/>describe someone&apos;s demeanor. Similarly, a word such as fever is unlikely to be expressing <lb/>a sentiment when used in a medical context. We use one feature in our experiments to <lb/>represent the topic of the document. <lb/>Another important aspect of contextual polarity is the perspective of the person <lb/>who is expressing the sentiment. For example, consider the phrase failed to defeat <lb/> in the sentence Israel failed to defeat Hezbollah. From the perspective of Israel, failed <lb/>to defeat is negative. From the perspective of Hezbollah, failed to defeat is positive. <lb/>Therefore, the contextual polarity of this phrase ultimately depends on the perspec-<lb/>tive of who is expressing the sentiment. Although automatically detecting this kind <lb/>of pragmatic influence on polarity is beyond the scope of this work, this as well as <lb/>the other types of polarity influencers all are considered when annotating contextual <lb/>polarity. <lb/>

			<page>402 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			3. Data and Annotations <lb/> For the experiments in this work, we need a corpus that is annotated comprehensively <lb/>for sentiment expressions and their contextual polarity. Rather than building a corpus <lb/>from scratch, we chose to add contextual polarity annotations to the existing annota-<lb/>tions in the Multi-perspective Question Answering (MPQA) opinion corpus 2 (Wiebe, <lb/>Wilson, and Cardie 2005). <lb/>The MPQA corpus is a collection of English-language versions of news documents <lb/>from the world press. The documents contain detailed, expression-level annotations <lb/>of attributions and private states (Quirk et al. 1985). Private states are mental and <lb/>emotional states; they include beliefs, speculations, intentions, and sentiments, among <lb/>others. Although sentiments are not distinguished from other types of private states <lb/>in the existing annotations, they are a subset of what already is annotated. This makes <lb/>the annotations in the MPQA corpus a good starting point for annotating sentiment <lb/>expressions and their contextual polarity. <lb/> 3.1 Annotation Scheme <lb/> When developing our annotation scheme for sentiment expressions and contextual <lb/>polarity, there were three main questions to address. First, which of the existing annota-<lb/>tions in the MPQA corpus have the possibility of being sentiment expressions? Second, <lb/>which of the possible sentiment expressions actually are expressing sentiments? Third, <lb/>what coding scheme should be used for marking contextual polarity? <lb/>The MPQA annotation scheme has four types of annotations: objective speech event <lb/>frames, two types of private state frames, and agent frames that are used for marking <lb/>speakers of speech events and experiencers of private states. A full description of <lb/>the MPQA annotation scheme and an agreement study evaluating key aspects of the <lb/>scheme are found in Wiebe, Wilson, and Cardie (2005). <lb/>The two types of private state frames, direct subjective frames and expressive sub-<lb/>jective element frames, are where we will find sentiment expressions. Direct subjective <lb/>frames are used to mark direct references to private states as well as speech events in <lb/>which private states are being expressed. For example, in the following sentences, fears, <lb/> praised, and said are all marked as direct subjective annotations. <lb/>(4) The U.S. fears a spill-over of the anti-terrorist campaign. <lb/>(5) Italian senator Renzo Gubert praised the Chinese government&apos;s efforts. <lb/>(6) &quot; The report is full of absurdities, &quot; he said. <lb/> The word fears directly refers to a private state; praised refers to a speech event <lb/>in which a private state is being expressed; and said is marked as direct subjective <lb/>because a private state is being expressed within the speech event referred to by <lb/> said. Expressive subjective elements indirectly express private states through the way <lb/>something is described or through a particular wording. In example (6), the phrase <lb/> full of absurdities is an expressive subjective element. Subjectivity (Banfield 1982; Wiebe <lb/>

			<note place="footnote"> 2 Available at http://nrrc.mitre.org/NRRC/publications.htm. <lb/></note>

			<page> 403 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			1994) refers to the linguistic expression of private states, hence the names for the two <lb/>types of private state annotations. <lb/>All expressive subjective elements are included in the set of annotations that have <lb/>the possibility of being sentiment expressions, but the direct subjective frames to include <lb/>in this set can be pared down further. Direct subjective frames have an attribute, expres-<lb/> sion intensity, that captures the contribution of the annotated word or phrase to the <lb/>overall intensity of the private state being expressed. Expression intensity ranges from <lb/> neutral to high. In the given sentences, fears and praised have an expression intensity of <lb/>medium, and said has an expression intensity of neutral. A neutral expression intensity <lb/>indicates that the direct subjective phrase itself is not contributing to the expression <lb/>of the private state. If this is the case, then the direct subjective phrase cannot be <lb/>a sentiment expression. Thus, only direct subjective annotations with a non-neutral <lb/> expression intensity are included in the set of annotations that have the possibility of <lb/>being sentiment expressions. We call this set of annotations, the union of the expres-<lb/>sive subjective elements and the direct subjective frames with a non-neutral intensity, <lb/>the subjective expressions in the corpus; these are the annotations we will mark for <lb/>contextual polarity. <lb/>Table 1 gives a sample of subjective expressions marked in the MPQA corpus. <lb/>Although many of the words and phrases express what we typically think of as <lb/>sentiments, others do not, for example, believes, very definitely, and unconditionally and <lb/>without delay. <lb/> Now that we have identified which annotations have the possibility of being sen-<lb/>timent expressions, the next question is which of these annotated words and phrases <lb/>are actually expressing sentiments. We define a sentiment as a positive or negative <lb/>emotion, evaluation, or stance. On the left of Table 2 are examples of positive sentiments; <lb/>examples of negative sentiments are on the right. <lb/> Table 1 <lb/> Sample of subjective expressions from the MPQA corpus. <lb/>victory of justice and freedom <lb/>such a disadvantageous situation <lb/>grown tremendously <lb/>must <lb/>such animosity <lb/>not true at all <lb/>throttling the voice <lb/>imperative for harmonious society <lb/>disdain and wrath <lb/>glorious <lb/>so exciting <lb/>disastrous consequences <lb/>could not have wished for a better situation <lb/>believes <lb/>freak show <lb/>the embodiment of two-sided justice <lb/>if you&apos;re not with us, you&apos;re against us <lb/>appalling <lb/>vehemently denied <lb/>very definitely <lb/>everything good and nice <lb/>once and for all <lb/>under no circumstances <lb/>shameful mum <lb/>most fraudulent, terrorist and extremist <lb/>enthusiastically asked <lb/>number one democracy <lb/>hate <lb/>seems to think <lb/>gross misstatement <lb/>indulging in blood-shed and their lunaticism surprised, to put it mildly <lb/>take justice to pre-historic times <lb/>unconditionally and without delay <lb/>so conservative that it makes Pat Buchanan look vegetarian <lb/>those digging graves for others, get engraved themselves <lb/>lost the reputation of commitment to principles of human justice <lb/>ultimately the demon they have reared will eat up their own vitals <lb/>

			<page> 404 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			Table 2 <lb/> Examples of positive and negative sentiments. <lb/>Positive sentiments <lb/>Negative sentiments <lb/>Emotion <lb/>I&apos;m happy <lb/>I&apos;m sad <lb/>Evaluation Great idea! <lb/>Bad idea! <lb/>Stance <lb/>She supports the bill She&apos;s against the bill <lb/> The final issue to address is the actual annotation scheme for marking contextual <lb/>polarity. The scheme we developed has four tags: positive, negative, both, and neutral. <lb/> The positive tag is used to mark positive sentiments. The negative tag is used to mark <lb/>negative sentiments. The both tag is applied to expressions in which both a positive and <lb/>negative sentiment are being expressed. Subjective expressions with positive, negative, or <lb/> both tags are our sentiment expressions. The neutral tag is used for all other subjective <lb/>expressions, including emotions, evaluations, and stances that are neither positive or <lb/>negative. Instructions for the contextual-polarity annotation scheme are available at <lb/> http://www.cs.pitt.edu/mpqa/databaserelease/polarityCodingInstructions.txt. <lb/> Following are examples from the corpus of each of the different contextual-polarity <lb/>annotations. Each underlined word or phrase is a subjective expression that was marked <lb/>in the original MPQA annotations. 3 In bold following each subjective expression is the <lb/>contextual polarity with which it was annotated. <lb/>(7) Thousands of coup supporters celebrated (positive) overnight, waving <lb/>flags, blowing whistles . . . <lb/>(8) The criteria set by Rice are the following: the three countries in question are <lb/>repressive (negative) and grave human rights violators (negative) . . . <lb/>(9) Besides, politicians refer to good and evil (both) only for purposes of <lb/>intimidation and exaggeration. <lb/>(10) Jerome says the hospital feels (neutral) no different than a hospital in the <lb/>states. <lb/>As a final note on the annotation scheme, annotators are asked to judge the con-<lb/>textual polarity of the sentiment that is ultimately being conveyed by the subjective <lb/>expression, that is, once the sentence has been fully interpreted. Thus, the subjective <lb/>expression, they have not succeeded, and will never succeed, is marked as positive in the <lb/>following sentence: <lb/>(11) They have not succeeded, and will never succeed (positive), in breaking <lb/>the will of this valiant people. <lb/>The reasoning is that breaking the will of a valiant people is negative, so to not succeed <lb/>in breaking their will is positive. <lb/>

			<note place="footnote"> 3 Some sentences contain additional subjective expressions that are not underlined as examples. <lb/></note>

			<page> 405 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Table 3 <lb/> Contingency table for contextual polarity agreement. <lb/>Neutral Positive Negative Both Total <lb/>Neutral <lb/> 123 <lb/> 14 <lb/>24 <lb/>0 <lb/>161 <lb/>Positive <lb/>16 <lb/> 73 <lb/> 5 <lb/>2 <lb/>9 6 <lb/>Negative <lb/>14 <lb/>2 <lb/> 167 <lb/> 1 <lb/>184 <lb/>Both <lb/>0 <lb/>3 <lb/>0 <lb/> 3 <lb/> 6 <lb/>Total <lb/>153 <lb/>92 <lb/>196 <lb/>6 <lb/> 447 <lb/>Table 4 <lb/> Contingency table for contextual polarity agreement, borderline cases removed. <lb/>Neutral Positive Negative Both Total <lb/>Neutral <lb/> 113 <lb/> 7 <lb/>8 <lb/>0 <lb/>128 <lb/>Positive <lb/>9 <lb/> 59 <lb/> 3 <lb/>0 <lb/>7 1 <lb/>Negative <lb/>5 <lb/>2 <lb/> 156 <lb/> 1 <lb/>164 <lb/>Both <lb/>0 <lb/>2 <lb/>0 <lb/> 2 <lb/> 4 <lb/>Total <lb/>127 <lb/>70 <lb/>167 <lb/>3 <lb/> 367 <lb/> 3.2 Agreement Study <lb/> To measure the reliability of the polarity annotation scheme, we conducted an agree-<lb/>ment study with two annotators 4 using 10 documents from the MPQA corpus. The 10 <lb/>documents contain 447 subjective expressions. Table 3 shows the contingency table for <lb/>the two annotators&apos; judgments. Overall agreement is 82%, with a kappa value of 0.72. <lb/>As part of the annotation scheme, annotators are asked to judge how certain they <lb/>are in their polarity tags. For 18% of the subjective expressions, at least one annotator <lb/>used the uncertain tag when marking polarity. If we consider these cases to be borderline <lb/>and exclude them from the study, percent agreement increases to 90% and kappa rises to <lb/>0.84. Table 4 shows the revised contingency table with the uncertain cases removed. This <lb/>shows that annotator agreement is especially high when both annotators are certain, and <lb/>that annotators are certain for over 80% of their tags. <lb/>Note that all annotations are included in the experiments. <lb/> 3.3 Contextual Polarity Annotations <lb/> In total, all 19,962 subjective expressions in the 535 documents (11,112 sentences) of the <lb/>MPQA corpus were annotated with their contextual polarity as just described. 5 Three <lb/>annotators carried out the task: the two who participated in the annotation study and <lb/>a third who was trained later. 6 Table 5 gives the distribution of the contextual polarity <lb/>tags. Looking at this table, we see that a small majority of subjective expressions (54.6%) <lb/>

			<note place="footnote"> 4 Both annotators are authors of this article. <lb/></note>
			
			<note place="footnote"> 5 The revised version of the MPQA corpus with the contextual polarity annotations is available at <lb/> http://www.cs.pitt.edu/mpqa. <lb/></note> 
			
			<note place="footnote"> 6 The third annotator received training until her reliability of performance on the task was comparable to <lb/>that of the first two annotators who participated in the study. <lb/></note>

			<page> 406 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			Table 5 <lb/> Distribution of contextual polarity tags. <lb/>Neutral Positive Negative Both <lb/>Total <lb/>9,057 <lb/>3,311 <lb/>7,294 <lb/>299 <lb/>19,961 <lb/>45.4% <lb/>16.6% <lb/>36.5% <lb/>1.5% 100% <lb/> are expressing a positive, negative, or both (positive and negative) sentiment. We refer to <lb/>these expressions as polar in context. Many of the subjective expressions are neutral <lb/>and do not express a sentiment. This suggests that, although sentiment is a major type <lb/>of subjectivity, distinguishing other prominent types of subjectivity will be important <lb/>for future work in subjectivity analysis. <lb/>As many NLP applications operate at the sentence level, one important issue to <lb/>consider is the distribution of sentences with respect to the subjective expressions <lb/>they contain. In the 11,112 sentences in the MPQA corpus, 28% contain no subjective <lb/>expressions, 24% contain only one, and 48% contain two or more. Of the 5,304 sentences <lb/>containing two or more subjective expressions, 17% contain mixtures of positive and <lb/>negative expressions, and 61% contain mixtures of polar (positive/negative/both) and <lb/>neutral subjective expressions. <lb/> 4. Prior-Polarity Subjectivity Lexicon <lb/> For the experiments in this article, we use a lexicon of over 8,000 subjectivity clues. <lb/> Subjectivity clues are words and phrases that may be used to express private states. In <lb/>other words, subjectivity clues have subjective usages, though they may have objective <lb/>usages as well. For this work, only single-word clues are used. <lb/>To compile the lexicon, we began with the list of subjectivity clues from Riloff and <lb/>Wiebe (2003), which includes the positive and negative adjectives from Hatzivassiloglou <lb/>and McKeown (1997). The words in this list were grouped in previous work according <lb/>to their reliability as subjectivity clues. Words that are subjective in most contexts are <lb/>considered strong subjective clues, indicated by the strongsubj tag. Words that may <lb/>only have certain subjective usages are considered weak subjective clues, indicated by <lb/>the weaksubj tag. <lb/>We expanded the list using a dictionary and a thesaurus, and added words from the <lb/>General Inquirer positive and negative word lists (Stone et al. 1966) that we judged to be <lb/>potentially subjective. 7 We also gave the new words strongsubj and weaksubj reliability <lb/>tags. The final lexicon has a coverage of 67% of subjective expressions in the MPQA <lb/>corpus, where coverage is the percentage of subjective expressions containing one or <lb/>more instances of clues from the lexicon. The coverage of just sentiment expressions is <lb/>even higher: 75%. <lb/>The next step was to tag the clues in the lexicon with their prior polarity: positive, <lb/>negative, both, or neutral. A word in the lexicon is tagged as positive if out of context <lb/>it seems to evoke something positive, and negative if it seems to evoke something <lb/>negative. If a word has both positive and negative meanings, it is tagged with the <lb/>polarity that seems the most common. A word is tagged as both if it is at the same time <lb/>

			<note place="footnote"> 7 In the end, about 70% of the words from the General Inquirer positive word list and 80% of the words <lb/>from the negative word list were included in the subjectivity lexicon. <lb/></note>

			<page> 407 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			both positive and negative. For example, the word bittersweet evokes something both <lb/>positive and negative. Words like brag are also tagged as both, because the one who is <lb/>bragging is expressing something positive, yet at the same time describing someone as <lb/>bragging is expressing a negative evaluation of that person. A word is tagged as neutral <lb/> if it does not evoke anything positive or negative. <lb/>For words that came from positive and negative word lists (Stone et al. 1966; <lb/>Hatzivassiloglou and McKeown 1997), we largely retained their original polarity. <lb/>However, we did change the polarity of a word if we strongly disagreed with its <lb/>original class. 8 For example, the word apocalypse is listed as positive in the General <lb/>Inquirer; we changed its prior polarity to negative for our lexicon. <lb/>By far, the majority of clues in the lexicon (92.8%) are marked as having either <lb/>positive (33.1%) or negative (59.7%) prior polarity. Only a small number of clues (0.3%) <lb/>are marked as having both positive and negative polarity. We refer to the set of clues <lb/>marked as positive, negative, or both as sentiment clues. A total of 6.9% of the clues in <lb/>the lexicon are marked as neutral. Examples of neutral clues are verbs such as feel, look, <lb/> and think, and intensifiers such as deeply, entirely, and practically. Although the neutral <lb/>clues make up a small proportion of the total words in the lexicon, we retain them for <lb/>our later experiments in recognizing contextual polarity because many of them are good <lb/>clues that a sentiment is being expressed (e.g., feels slighted, feels satisfied, look kindly on, <lb/>look forward to). Including them increases the coverage of the system. <lb/>At the end of the previous section, we considered the distribution of sentences in the <lb/>MPQA corpus with respect to the subjective expressions they contain. It is interesting <lb/>to compare that distribution with the distribution of sentences with respect to the <lb/>instances they contain of clues from the lexicon. We find that there are more sentences <lb/>with two or more clue instances (62%) than sentences with two or more subjective <lb/>expressions (48%). More importantly, many more sentences have mixtures of positive <lb/>and negative clue instances than actually have mixtures of positive and negative sub-<lb/>jective expressions. Only 880 sentences have a mixture of both positive and negative <lb/>subjective expressions, whereas 3,234 sentences have a mixture of positive and negative <lb/>clue instances. Thus, a large number of positive and negative instances are either neutral <lb/>in context, or they are combining to form more complex polarity expressions. Either <lb/>way, this provides strong evidence of the need to be able to disambiguate the contextual <lb/>polarity of subjectivity and sentiment clues. <lb/> 5. Definition of the Gold Standard <lb/> In the experiments described in the following sections, the goal is to classify the con-<lb/>textual polarity of the expressions that contain instances of the subjectivity clues in our <lb/>lexicon. However, determining which clue instances are part of the same expression and <lb/>identifying expression boundaries are not the focus of this work. Thus, instead of trying <lb/>to identify and label each expression, in the following experiments, each clue instance <lb/>is labeled individually as to its contextual polarity. <lb/>We define the gold-standard contextual polarity of a clue instance in terms of the <lb/>manual annotations (Section 3) as follows. If a clue instance is not in a subjective <lb/>expression (and therefore not in a sentiment expression), its gold class is neutral. If <lb/>a clue instance appears in just one subjective expression or in multiple subjective <lb/>

			<note place="footnote"> 8 We decided on a different polarity for about 80 of the words in our lexicon that appeared on other <lb/>positive and negative word lists. <lb/></note>

			<page> 408 <lb/></page>

			<note place="headnote"> Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note>

			expressions with the same contextual polarity, its gold class is the contextual polarity <lb/>of the subjective expression(s). If a clue instance appears in a mixture of negative and <lb/>neutral subjective expressions, its gold class is negative; if it is in a mixture of positive and <lb/>neutral subjective expressions, its gold class is positive. Finally, if a clue instance appears <lb/>in at least one positive and one negative subjective expression (or in a subjective ex-<lb/>pression marked as both), then its gold class is both. A clue instance can appear in more <lb/>than one subjective expression because in the MPQA annotation scheme, it is possible <lb/>for direct subjective frames and expressive subjective elements frames to overlap. <lb/> 6. A Prior-Polarity Classifier <lb/> Before delving into the task of recognizing contextual polarity, an important question <lb/>to address is how useful prior polarity alone is for identifying contextual polarity. To <lb/>answer this question, we create a classifier that simply assumes the contextual polarity <lb/>of a clue instance is the same as the clue&apos;s prior polarity. We explore this classifier&apos;s <lb/>performance on a small amount of development data, which is not part of the data used <lb/>in the subsequent experiments. <lb/>This simple classifier has an accuracy of 48%. From the confusion matrix given in <lb/>Table 6, we see that 76% of the errors result from words with non-neutral prior polarity <lb/>appearing in phrases with neutral contextual polarity. Only 12% of the errors result from <lb/>words with neutral prior polarity appearing in expressions with non-neutral contextual <lb/>polarity, and only 11% of the errors come from words with a positive or negative prior <lb/>polarity appearing in expressions with the opposite contextual polarity. Table 6 also <lb/>shows that positive clues tend to be used in negative expressions far more often than <lb/>negative clues tend to be used in positive expressions. <lb/>Given that by far the largest number of errors come from clues with positive, <lb/>negative, or both prior polarity appearing in neutral contexts, we were motivated to try <lb/>a two-step approach to the problem of sentiment classification. The first step, Neutral– <lb/>Polar Classification, tries to determine if an instance is neutral or polar in context. The <lb/>second step, Polarity Classification, takes all instances that step one classified as polar, <lb/>and tries to disambiguate their contextual polarity. This two-step approach is illustrated <lb/>in Figure 1. <lb/> 7. Features <lb/> The features used in our experiments were motivated both by the literature and by <lb/>exploration of the contextual-polarity annotations in our development data. A number <lb/> Table 6 <lb/> Confusion matrix for the prior-polarity classifier on the development set. <lb/> Prior-Polarity Classifier <lb/> Neutral Positive Negative Both Total <lb/>Neutral <lb/> 798 <lb/> 784 <lb/>698 <lb/>4 <lb/>2284 <lb/> Gold Positive <lb/>81 <lb/> 371 <lb/> 40 <lb/>0 <lb/>492 <lb/> Class Negative <lb/>149 <lb/>181 <lb/> 622 <lb/> 0 <lb/>9 5 2 <lb/>Both <lb/>4 <lb/>11 <lb/>13 <lb/> 5 <lb/> 33 <lb/>Total <lb/>1032 <lb/>1347 <lb/>1373 <lb/>9 <lb/> 3761 <lb/>

			<page> 409 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Figure 1 <lb/> Two-step approach to recognizing contextual polarity. <lb/> of features were inspired by the paper on contextual-polarity influencers by Polanyi <lb/>and Zaenan (2004). Other features are those that have been found useful in the past <lb/>for recognizing subjective sentences (Wiebe, Bruce, and O&apos;Hara 1999; Wiebe and Riloff <lb/>2005). <lb/> 7.1 Features for Neutral–Polar Classification <lb/> For distinguishing between neutral and polar instances, we use the features listed in <lb/>Table 7. For ease of description, we group the features into six sets: word features, gen-<lb/>eral modification features, polarity modification features, structure features, sentence <lb/>features, and one document feature. <lb/> Word Features In addition to the word token (the token of the clue instance being <lb/>classified), the word features include the parts of speech of the previous word, the word <lb/>itself, and the next word. The prior polarity and reliability class features represent those <lb/>pieces of information about the clue which are taken from the lexicon. <lb/> General Modification Features These are binary features that capture different <lb/>types of relationships involving the clue instance. <lb/>The first four features involve relationships with the word immediately before or af-<lb/>ter the clue instance. The preceded by adjective feature is true if the clue instance is a noun <lb/>preceded by an adjective. The preceded by adverb feature is true if the preceding word <lb/>is an adverb other than not. The preceded by intensifier feature is true if the preceding <lb/>word is an intensifier, and the self intensifier feature is true if the clue instance itself is an <lb/>intensifier. A word is considered to be an intensifier if it appears in a list of intensifiers <lb/>and if it precedes a word of the appropriate part of speech (e.g., an intensifier adjective <lb/>must come before a noun). The list of intensifiers is a compilation of those listed in Quirk <lb/>et al. (1985), intensifiers identified from existing entries in the subjectivity lexicon, and <lb/>intensifiers identified during explorations of the development data. <lb/>The modifies/modifed by features involve the dependency parse tree of the sentence, <lb/>obtained by first parsing the sentence (Collins 1997) and then converting the tree into <lb/>its dependency representation (Xia and Palmer 2001). In a dependency representation, <lb/>every node in the tree structure is a surface word (i.e., there are no abstract nodes such <lb/>as NP or VP). The parent word is called the head, and its children are its modifiers. The <lb/>

			<page>410 <lb/></page>

			<note place="headnote"> Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			Table 7 <lb/> Features for neutral–polar classification. <lb/>Word Features <lb/>word token <lb/>word part of speech <lb/>previous word part of speech <lb/>next word part of speech <lb/>prior polarity: positive, negative, both, neutral <lb/> reliability class: strongsubj or weaksubj <lb/> General Modification Features <lb/>preceded by adjective: binary <lb/> preceded by adverb (other than not): binary <lb/> preceded by intensifier: binary <lb/> self intensifier: binary <lb/> modifies strongsubj: binary <lb/> modifies weaksubj: binary <lb/> modified by strongsubj: binary <lb/> modified by weaksubj: binary <lb/> Polarity Modification Features <lb/>modifies polarity: positive, negative, neutral, both, notmod <lb/> modified by polarity: positive, negative, neutral, both, notmod <lb/> conjunction polarity: positive, negative, neutral, both, notmod <lb/> Structure Features <lb/>in subject: binary <lb/> in copular: binary <lb/> in passive: binary <lb/> Sentence Features <lb/>strongsubj clues in current sentence: 0, 1, 2, 3 (or more) <lb/> strongsubj clues in previous sentence: 0, 1, 2, 3 (or more) <lb/> strongsubj clues in next sentence: 0, 1, 2, 3 (or more) <lb/> weaksubj clues in current sentence: 0, 1, 2, 3 (or more) <lb/> weaksubj clues in previous sentence: 0, 1, 2, 3 (or more) <lb/> weaksubj clues in next sentence: 0, 1, 2, 3 (or more) <lb/> adjectives in sentence: 0, 1, 2, 3 (or more) <lb/> adverbs in sentence (other than not): 0, 1, 2, 3 (or more) <lb/> cardinal number in sentence: binary <lb/> pronoun in sentence: binary <lb/> modal in sentence (other than will): binary <lb/> Document Feature <lb/>document topic/domain <lb/> edge between a parent and a child specifies the grammatical relationship between the <lb/>two words. Figure 2 shows an example of a dependency parse tree. Instances of clues in <lb/>the tree are marked with the clue&apos;s prior polarity and reliability class from the lexicon. <lb/>For each clue instance, the modifies/modifed by features capture whether there are <lb/> adj, mod, or vmod relationships between the clue instance and any other instances from <lb/>the lexicon. Specifically, the modifies strongsubj feature is true if the clue instance and <lb/>its parent share an adj, mod, or vmod relationship, and if its parent is an instance of <lb/>a strongsubj clue from the lexicon. The modifies weaksubj feature is the same, except <lb/>that it looks in the parent for an instance of a weaksubj clue. The modified by strongsubj <lb/>

			<page> 411 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Figure 2 <lb/> The dependency tree for the sentence The human rights report poses a substantial challenge to the <lb/>U.S. interpretation of good and evil. Prior polarity and reliability class are marked in parentheses <lb/>for words that match clues from the lexicon. <lb/> feature is true for a clue instance if one of its children is an instance of a strongsubj <lb/> clue, and if the clue instance and its child share an adj, mod, or vmod relationship. The <lb/> modified by weaksubj feature is the same, except that it looks for instances of weaksubj <lb/> clues in the children. Although the adj and vmod relationships are typically local, the <lb/> mod relationship involves longer-distance as well as local dependencies. Figure 2 helps <lb/>to illustrate these features. The modifies weaksubj feature is true for substantial, because <lb/> substantial modifies challenge, which is an instance of a weaksubj clue. For rights, the <lb/> modifies weaksubj feature is false, because rights modifies report, which is not an instance <lb/>of a weaksubj clue. The modified by weaksubj feature is false for substantial, because it has <lb/>no modifiers that are instances of weaksubj clues. For challenge, the modified by weaksubj <lb/> feature is true because it is being modified by substantial, which is an instance of a <lb/> weaksubj clue. <lb/> Polarity Modification Features The modifies polarity, modified by polarity, and conj <lb/>polarity features capture specific relationships between the clue instance and other senti-<lb/>ment clues it may be related to. If the clue instance and its parent in the dependency tree <lb/>share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior <lb/>polarity of the parent. If the parent is not in the prior-polarity lexicon, its prior polarity <lb/>is considered neutral. If the clue instance is at the root of the tree and has no parent, <lb/>the value of the feature is notmod. The modified by polarity feature is similar, looking <lb/>for adj, mod, and vmod relationships and other sentiment clues in the children of the clue <lb/>instance. The conj polarity feature determines if the clue instance is in a conjunction. If so, <lb/>the value of this feature is its sibling&apos;s prior polarity. As before, if the sibling is not in the <lb/>

			<page>412 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			lexicon, its prior polarity is neutral. If the clue instance is not in a conjunction, the value <lb/>for this feature is notmod. Figure 2 also helps to illustrate these modification features. The <lb/>word substantial with positive prior polarity modifies the word challenge with negative <lb/>prior polarity. Therefore the modifies polarity feature is negative for substantial, and the <lb/> modified by polarity feature is positive for challenge. The words good and evil are in a con-<lb/>junction together; thus the conj polarity feature is negative for good and positive for evil. <lb/> Structure Features These are binary features that are determined by starting with <lb/>the clue instance and climbing up the dependency parse tree toward the root, looking <lb/>for particular relationships, words, or patterns. The in subject feature is true if we find <lb/>a subj relationship on the path to the root. The in copular feature is true if in subject is <lb/>false and if a node along the path is both a main verb and a copular verb. The in passive <lb/> feature is true if a passive verb pattern is found on the climb. <lb/>The in subject and in copular features were motivated by the intuition that the syn-<lb/>tactic role of a word may influence whether a word is being used to express a sentiment. <lb/>For example, consider the word polluters in each of the following two sentences. <lb/>(12) Under the application shield, polluters are allowed to operate if they have <lb/>a permit. <lb/>(13) &quot; The big-city folks are pointing at the farmers and saying you are <lb/> polluters . . . &quot; <lb/>In the first sentence, polluters is simply being used as a referring expression. In the <lb/>second sentence, polluters is clearly being used to express a negative evaluation of the <lb/>farmers. The motivation for the in passive feature was previous work by Riloff and Wiebe <lb/>(2003), who found that different words are more or less likely to be subjective depending <lb/>on whether they are in the active or passive. <lb/> Sentence Features These are features that previously were found useful for <lb/>sentence-level subjectivity classification (Wiebe, Bruce, and O&apos;Hara 1999; Wiebe and <lb/>Riloff 2005). They include counts of strongsubj and weaksubj clue instances in the cur-<lb/>rent, previous and next sentences, counts of adjectives and adverbs other than not in <lb/>the current sentence, and binary features to indicate whether the sentence contains a <lb/>pronoun, a cardinal number, and a modal other than will. <lb/> Document Feature There is one document feature representing the topic or domain <lb/>of the document. The motivation for this feature is that whether or not a word is <lb/>expressing a sentiment or even a private state in general may depend on the subject <lb/>of the discourse. For example, the words fever and sufferer may express a negative <lb/>sentiment in certain contexts, but probably not in a health or medical context, as is the <lb/>case in the following sentence. <lb/>(14) The disease can be contracted if a person is bitten by a certain tick or if a <lb/>person comes into contact with the blood of a congo fever sufferer. <lb/> In the creation of the MPQA corpus, about two-thirds of the documents were <lb/>selected to be on one of the 10 topics listed in Table 8. The documents for each topic were <lb/>identified by human searches and by an information retrieval system. The remaining <lb/>documents were semi-randomly selected from a very large pool of documents from <lb/>the world press. In the corpus, these documents are listed with the topic miscellaneous. <lb/> Rather than leaving these documents unlabeled, we chose to label them using the <lb/>

			<page>413 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Table 8 <lb/> Topics in the MPQA corpus. <lb/>Topic <lb/>Description <lb/>argentina <lb/>Economic collapse in Argentina <lb/>axisofevil <lb/>U.S. President&apos;s State of the Union Address <lb/>guantanamo Detention of prisoners in Guantanamo Bay <lb/>humanrights U.S. State Department Human Rights Report <lb/>kyoto <lb/>Kyoto Protocol ratification <lb/>settlements <lb/>Israeli settlements in Gaza and the West Bank <lb/>space <lb/>Space missions of various countries <lb/>taiwan <lb/>Relationship between Taiwan and China <lb/>venezuela <lb/>Presidential coup in Venezuela <lb/>zimbabwe <lb/>Presidential election in Zimbabwe <lb/> following general domain categories: economics, general politics, health, report events, <lb/>and war and terrorism. <lb/> 7.2 Features for Polarity Classification <lb/> Table 9 lists the features that we use for step two, polarity classification. Word token, <lb/>word prior polarity, and the polarity-modification features are the same as described for <lb/>neutral–polar classification. <lb/>We use two features to capture two different types of negation. The negated feature <lb/>is a binary feature that is used to capture more local negations: Its value is true if a <lb/>negation word or phrase is found within the four words preceding the clue instance, <lb/>and if the negation word is not also in a phrase that acts as an intensifier rather than a <lb/>negator. Examples of phrases that intensify rather than negate are not only and nothing if <lb/>not. The negated subject feature captures a longer-distance type of negation. This feature <lb/> Table 9 <lb/> Features for polarity classification. <lb/>Word Features <lb/>word token <lb/>word prior polarity: positive, negative, both, neutral <lb/> Negation Features <lb/>negated: binary <lb/> negated subject: binary <lb/> Polarity Modification Features <lb/>modifies polarity: positive, negative, neutral, both, notmod <lb/> modified by polarity: positive, negative, neutral, both, notmod <lb/> conj polarity: positive, negative, neutral, both, notmod <lb/> Polarity Shifters <lb/>general polarity shifter: binary <lb/> negative polarity shifter: binary <lb/> positive polarity shifter: binary <lb/>

			<page> 414 <lb/></page>

			<note place="headnote"> Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			is true if the subject of the clause containing the clue instance is negated. For example, <lb/>the negated subject feature is true for support in the following sentence. <lb/>(15) No politically prudent Israeli could support either of them. <lb/>The last three polarity features look in a window of four words before the clue <lb/>instance, searching for the presence of particular types of polarity influencers. Gen-<lb/>eral polarity shifters reverse polarity (e.g., little truth, little threat). Negative polarity <lb/>shifters typically make the polarity of an expression negative (e.g., lack of understand-<lb/>ing). Positive polarity shifters typically make the polarity of an expression positive <lb/>(e.g., abate the damage). The polarity influencers that we used were identified through <lb/>explorations of the development data. <lb/> 8. Experiments in Recognizing Contextual Polarity <lb/> We have two primary goals with our experiments in recognizing contextual polarity. <lb/>The first is to evaluate the features described in Section 7 as to their usefulness for <lb/>this task. The second is to investigate the importance of recognizing neutral instances— <lb/>recognizing when a sentiment clue is not being used to express a sentiment—for classi-<lb/>fying contextual polarity. <lb/>To evaluate features, we investigate their performance, both together and sep-<lb/>arately, across several different learning algorithms. Varying the learning algorithm <lb/>allows us to verify that the features are robust and that their performance is not the <lb/>artifact of a particular algorithm. We experiment with four different types of machine <lb/>learning: boosting, memory-based learning, rule learning, and support vector learning. <lb/>For boosting, we use BoosTexter (Schapire and Singer 2000) AdaBoost.MH. For rule <lb/>learning, we use Ripper (Cohen 1996). For memory-based learning, we use TiMBL <lb/>(Daelemans et al. 2003b) IB1 (k-nearest neighbor). For support vector learning, we <lb/>use SVM-light and SVM-multiclass (Joachims 1999). SVM-light is used for the experi-<lb/>ments involving binary classification (neutral–polar classification), and SVM-multiclass <lb/>is used for experiments with more than two classes. These machine learning algorithms <lb/>were chosen because they have been used successfully for a number of natural language <lb/>processing tasks, and they represent several different types of learning. <lb/>For all of the classification algorithms except for SVM, the features for a clue in-<lb/>stance are represented as they are presented in Section 7. For SVM, the representations <lb/>for numeric and discrete-valued features are changed. Numeric features, such as the <lb/>count of strongsubj clue instances in a sentence, are scaled to range between 0 and 1. <lb/>Discrete-valued features, such as the reliability class feature, are converted into multiple <lb/>binary features. For example, the reliability class feature is represented by two binary <lb/>features: one for whether the clue instance is a strongsubj clue and one for whether the <lb/>clue instance is a weaksubj clue. <lb/>To investigate the importance of recognizing neutral instances, we perform two sets <lb/>of polarity classification (step two) experiments. First, we experiment with classifying <lb/>the polarity of all gold-standard polar instances—the clue instances identified as polar <lb/>in context by the manual polarity annotations. Second, we experiment with using the <lb/>polar instances identified automatically by the neutral–polar classifiers. Because the <lb/>second set of experiments includes the neutral instances misclassified in step one, we <lb/>can compare results for the two sets of experiments to see how the noise of neutral <lb/>instances affects the performance of the polarity features. <lb/>

			<page>415 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			All experiments are performed using 10-fold cross validation over a test set of <lb/>10,287 sentences from 494 MPQA corpus documents. We measure performance in terms <lb/>of accuracy, recall, precision, and F-measure. Accuracy is simply the total number of <lb/>instances correctly classified. Recall, precision, and F-measure for a given class C are <lb/>defined as follows. Recall is the percentage of all instances of class C correctly identified. <lb/> Rec(C)  = <lb/> |  instances of C correctly identified  | <lb/>|  all instances of C  | <lb/> Precision is the percentage of instances classified as class C that are class C in truth. <lb/> Prec(C)  = <lb/> |  instances of C correctly identified  | <lb/>|  all instances identified as C  | <lb/> F-measure is the harmonic mean of recall and precision. <lb/> F(C)  = <lb/> 2  ×Rec(C) ×  Prec(C) <lb/>Rec(C)  +  Prec(C) <lb/> All results reported are averages over the 10 folds. <lb/> 8.1 Neutral–Polar Classification <lb/> In our two-step process for recognizing contextual polarity, the first step is neutral–polar <lb/>classification, determining whether each instance of a clue from the lexicon is neutral or <lb/>polar in context. In our test set, there are 26,729 instances of clues from the lexicon. The <lb/>features we use for this step were listed above in Table 7 and described in Section 7.1. <lb/>In this section, we perform two sets of experiments. In the first, we compare <lb/>the results of neutral–polar classification using all the neutral–polar features against <lb/>two baselines. The first baseline uses just the word token feature. The second baseline <lb/>(word+priorpol) uses the word token and prior polarity features. In the second set of <lb/>experiments, we explore the performance of different sets of features for neutral–polar <lb/>classification. <lb/>Research has shown that the performance of learning algorithms for NLP tasks can <lb/>vary widely depending on their parameter settings, and that the optimal parameter <lb/>settings can also vary depending on the set of features being evaluated (Daelemans <lb/>et al. 2003a; Hoste 2005). Although the goal of this work is not to identify the optimal <lb/>configuration for each algorithm and each set of features, we still want to make a rea-<lb/>sonable attempt to find a good configuration for each algorithm. To do this, we perform <lb/>10-fold cross validation of the more challenging baseline classifier (word+priorpol) <lb/>on the development data, varying select parameter settings. The results from those <lb/>experiments are then used to select the parameter settings for each algorithm. For <lb/>BoosTexter, we vary the number of rounds of boosting. For TiMBL, we vary the value <lb/>for k (the number of neighbors) and the distance metric (overlap or modified value <lb/>difference metric [MVDM]). For Ripper, we vary whether negative tests are disallowed <lb/>for nominal (-!n) and set (-!s) valued attributes and how much to simplify the hypothesis <lb/>(-S). For SVM, we experiment with linear, polynomial, and radial basis function kernels. <lb/>Table 10 gives the settings selected for the neutral–polar classification experiments for <lb/>the different learning algorithms. <lb/>

			<page>416 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			Table 10 <lb/> Algorithm settings for neutral–polar classification. <lb/>Algorithm <lb/>Settings <lb/>BoosTexter 2,000 rounds of boosting <lb/>TiMBL <lb/> k=25, MVDM distance metric <lb/>Ripper <lb/>-!n, -S 0.5 <lb/>SVM <lb/>linear kernel <lb/> 8.1.1 Classification Results. The results for the first set of experiments are given in <lb/>Table 11. For each algorithm, we give the results for the two baseline classifiers, followed <lb/>by the results for the classifier trained using all the neutral–polar features. The results <lb/>shown in bold are significantly better than both baselines (two-sided t-test, p  ≤  0.05) for <lb/>the given algorithm. <lb/>Working together, how well do the neutral–polar features perform? For BoosTexter, <lb/>TiMBL, and Ripper, the classifiers trained using all the features improve significantly <lb/>over the two baselines in terms of accuracy, polar recall, polar F-measure, and neutral <lb/>precision. Neutral F-measure is also higher, but not significantly so. These consistent <lb/>results across three of the four algorithms show that the neutral–polar features are <lb/>helpful for determining when a sentiment clue is actually being used to express a <lb/>sentiment. <lb/>Interestingly, Ripper is the only algorithm for which the word-token baseline per-<lb/>formed better than the word+priorpol baseline. Nevertheless, the prior polarity feature <lb/>is an important component in the performance of the Ripper classifier using all the <lb/>features. Excluding prior polarity from this classifier results in a significant decrease in <lb/> Table 11 <lb/> Results for neutral–polar classification (step one). <lb/> Polar <lb/>Neutral <lb/> Acc Rec Prec <lb/>F <lb/>Rec Prec <lb/>F <lb/> BoosTexter <lb/> word token baseline <lb/>74.0 41.9 77.0 54.3 92.7 73.3 81.8 <lb/>word+priorpol baseline 75.0 55.6 70.2 62.1 86.2 76.9 81.3 <lb/>neutral–polar features <lb/> 76.5 58.3 72.4 64.6 87.1 78.2 82.4 <lb/> TiMBL <lb/> word token baseline <lb/>74.6 47.9 73.9 58.1 90.1 74.8 81.8 <lb/>word+priorpol baseline 74.6 48.2 73.7 58.3 90.0 74.9 81.7 <lb/>neutral–polar features <lb/> 76.5 59.5 71.7 65.0 86.3 78.5 82.3 <lb/> Ripper <lb/> word token baseline <lb/>66.3 11.2 80.6 19.6 98.4 65.6 78.7 <lb/>word+priorpol baseline 65.5 07.7 84.5 14.1 99.1 64.8 78.4 <lb/>neutral–polar features <lb/> 71.4 49.4 64.6 56.0 84.2 74.1 78.8 <lb/> SVM <lb/> word token baseline <lb/>74.6 47.9 73.9 58.1 90.1 74.8 81.8 <lb/>word+priorpol baseline 75.6 54.5 72.5 62.2 88.0 76.8 82.0 <lb/>neutral–polar features <lb/>75.3 52.6 72.7 61.0 88.5 76.2 81.9 <lb/>

			<page> 417 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			performance for every metric. Decreases range from 2.5% for neutral recall to 9.5% for <lb/>polar recall. <lb/>The best SVM classifier is the word+priorpol baseline. In terms of accuracy, this <lb/>classifier does not perform much worse than the BoosTexter and TiMBL classifiers that <lb/>use all the neutral–polar features: The SVM word+priorpol baseline classifier has an <lb/>accuracy of 75.6%, and both the BoosTexter and TiMBL classifiers have an accuracy of <lb/>76.5%. However, the BoosTexter and TiMBL classifiers using all the features perform <lb/>notably better in terms of polar recall and F-measure. The BoosTexter and TiMBL <lb/>classifiers have polar recalls that are 7% and 9.2% higher than the SVM baseline. Polar <lb/>F-measures for BoosTexter and TiMBL are 3.9% and 4.5% higher. These increases are <lb/>significant for p  ≤  0.01. <lb/> 8.1.2 Feature Set Evaluation. To evaluate the contribution of the various features for <lb/>neutral–polar classification, we perform a series of experiments in which different <lb/>sets of neutral–polar features are added to the word+priorpol baseline and new clas-<lb/>sifiers are trained. We then compare the performance of these new classifiers to the <lb/>word+priorpol baseline, with the exception of the Ripper classifiers, which we compare <lb/>to the higher word baseline. Table 12 lists the sets of features tested in these experiments. <lb/>The feature sets generally correspond to how the neutral–polar features are presented <lb/>in Table 7, although some of the groups are broken down into more fine-grained sets <lb/>that we believe capture meaningful distinctions. <lb/>Table 13 gives the results for these experiments. Increases and decreases for a <lb/>given metric as compared to the word+priorpol baseline (word baseline for Ripper) <lb/>are indicated by + or –, respectively. Where changes are significant at the p  ≤  0.1 level, <lb/>++ or – – are used, and where changes are significant at the p  ≤  0.05 level, +++ or – – – <lb/>are used. An &quot; nc &quot; indicates no change (a change of less than  ±  0.05) compared to the <lb/>baseline. <lb/>What does Table 13 reveal about the performance of various feature sets for neutral– <lb/>polar classification? Most noticeable is that no individual feature sets stand out as strong <lb/>performers. The only significant improvements in accuracy come from the PARTS-<lb/>OF-SPEECH and RELIABILITY-CLASS feature sets for Ripper. These improvements are <lb/>perhaps not surprising given that the Ripper baseline was much lower to begin with. <lb/>Very few feature sets show any improvement for SVM. Again, this is not unexpected <lb/>given that all the features together performed worse than the word+priorpol baseline <lb/> Table 12 <lb/> Neutral–polar feature sets for evaluation. <lb/>Experiment <lb/>Features <lb/> PARTS-OF-SPEECH <lb/> parts of speech for clue instance, previous word, and next word <lb/> RELIABILITY-CLASS <lb/> reliability class of clue instance <lb/> PRECEDED-POS <lb/> preceded by adjective, preceded by adverb <lb/> INTENSIFY <lb/> preceded by intensifier, self intensifier <lb/> RELCLASS-MOD <lb/> modifies strongsubj/weaksubj, modified by strongsubj/weaksubj <lb/> POLARITY-MOD <lb/> polarity-modification features <lb/> STRUCTURE <lb/> structure features <lb/> CURSENT-COUNTS <lb/> strongsubj/weaksubj clue instances in sentence <lb/> PNSENT-COUNTS <lb/> strongsubj/weaksubj clue instances in previous/next sentence <lb/> CURSENT-OTHER <lb/> adjectives/adverbs/cardinal number/pronoun/modal in sentence <lb/> TOPIC <lb/> document topic <lb/>

			<page> 418 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			Table 13 <lb/> Results for neutral–polar feature ablation experiments. <lb/>Polar Neut <lb/>Polar Neut <lb/> BoosTexter <lb/> Acc <lb/>F <lb/>F <lb/> Ripper <lb/> Acc <lb/>F <lb/>F <lb/> PARTS-OF-SPEECH <lb/> + <lb/>– <lb/>+ <lb/> PARTS-OF-SPEECH <lb/> +++ <lb/>+++ <lb/>– – – <lb/> RELIABILITY-CLASS <lb/> + <lb/>– <lb/>+ <lb/> RELIABILITY-CLASS <lb/> +++ <lb/>+++ <lb/>+ <lb/> PRECEDED-POS <lb/> nc <lb/>– <lb/>nc <lb/> PRECEDED-POS <lb/> – <lb/>– <lb/>– <lb/> INTENSIFY <lb/> -<lb/>nc <lb/>-<lb/> INTENSIFY <lb/> – <lb/>– – – <lb/>– <lb/> RELCLASS-MOD <lb/> + <lb/>++ <lb/>+ <lb/> RELCLASS-MOD <lb/> + <lb/>+++ <lb/>+ <lb/> POLARITY-MOD <lb/> nc <lb/>– <lb/>+ <lb/> POLARITY-MOD <lb/> – <lb/>+++ <lb/>– <lb/> STRUCTURE <lb/> – <lb/>– – – <lb/>+ <lb/> STRUCTURE <lb/> – <lb/>+ <lb/>– <lb/> CURSENT-COUNTS <lb/> + <lb/>– – – <lb/>+ <lb/> CURSENT-COUNTS <lb/> – – <lb/>+++ <lb/>– – – <lb/> PNSENT-COUNTS <lb/> + <lb/>– – – <lb/>+ <lb/> PNSENT-COUNTS <lb/> – – – +++ <lb/>– – – <lb/> CURSENT-OTHER <lb/> nc <lb/>– <lb/>+ <lb/> CURSENT-OTHER <lb/> – – – +++ <lb/>– – – <lb/> TOPIC <lb/> + <lb/>+ <lb/>+ <lb/> TOPIC <lb/> – <lb/>+++ <lb/>– – – <lb/>Polar Neut <lb/>Polar Neut <lb/> TiMBL <lb/> Acc <lb/>F <lb/>F <lb/> SVM <lb/> Acc <lb/>F <lb/>F <lb/> PARTS-OF-SPEECH <lb/> + <lb/>+++ <lb/>+ <lb/> PARTS-OF-SPEECH <lb/> – – <lb/>– – – <lb/>– <lb/> RELIABILITY-CLASS <lb/> + <lb/>+ <lb/>nc <lb/> RELIABILITY-CLASS <lb/> + <lb/>– <lb/>+ <lb/> PRECEDED-POS <lb/> nc <lb/>+ <lb/>nc <lb/> PRECEDED-POS <lb/> nc <lb/>nc <lb/>nc <lb/> INTENSIFY <lb/> nc <lb/>nc <lb/>nc <lb/> INTENSIFY <lb/> nc <lb/>nc <lb/>nc <lb/> RELCLASS-MOD <lb/> + <lb/>+ <lb/>+ <lb/> RELCLASS-MOD <lb/> nc <lb/>+ <lb/>nc <lb/> POLARITY-MOD <lb/> + <lb/>+ <lb/>+ <lb/> POLARITY-MOD <lb/> – – <lb/>– – – <lb/>– – <lb/> STRUCTURE <lb/> nc <lb/>+ <lb/>– <lb/> STRUCTURE <lb/> – <lb/>+ <lb/>– <lb/> CURSENT-COUNTS <lb/> – <lb/>+ <lb/>– <lb/> CURSENT-COUNTS <lb/> – <lb/>– <lb/>– <lb/> PNSENT-COUNTS <lb/> + <lb/>+++ <lb/>– <lb/> PNSENT-COUNTS <lb/> – <lb/>– <lb/>– <lb/> CURSENT-OTHER <lb/> + <lb/>+++ <lb/>– <lb/> CURSENT-OTHER <lb/> – <lb/>– <lb/>– <lb/> TOPIC <lb/> – <lb/>+ <lb/>– <lb/> TOPIC <lb/> – <lb/>– <lb/>– <lb/>Increases and decreases for a given metric as compared to the word+priorpol baseline <lb/>(word baseline for Ripper) are indicated by + or –, respectively; ++ or – – indicates the <lb/>change is significant at the p  &lt;  0.1 level; +++ or – – – indicates significance at the <lb/>p  &lt;  0.05 level; nc indicates no change. <lb/> for SVM. The performance of the feature sets for BoosTexter and TiMBL are perhaps <lb/>the most revealing. In the previous experiments using all the features together, these <lb/>algorithms produced classifiers with the same high performance. In these experiments, <lb/>six different feature sets for each algorithm show improvements in accuracy over the <lb/>baseline, yet none of those improvements are significant. This suggests that achieving <lb/>the highest performance for neutral–polar classification requires a wide variety of fea-<lb/>tures working together in combination. <lb/>We further test this result by evaluating the effect of removing the features that <lb/>produced either no change or a drop in accuracy from the respective all-feature classi-<lb/>fiers. For example, we train a TiMBL neutral–polar classifier using all the features except <lb/>for those in the PRECEDED-POS, INTENSIFY, STRUCTURE, CURSENT-COUNTS, and TOPIC <lb/> feature sets, and then compare the performance of this new classifier to the TiMBL, all-<lb/>feature classifier. Although removing the non-performing features has little effect for <lb/>BoosTexter, performance does drop for both TiMBL and Ripper. The primary source of <lb/>this performance drop is a decrease in polar recall: 2% for TiMBL and 3.2% for Ripper. <lb/>

			<page>419 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Although no feature sets stand out in Table 13 as far as giving an overall high <lb/>performance, there are some features that consistently improve performance across <lb/>the different algorithms. The reliability class of the clue instance (RELIABILITY-CLASS) <lb/>improves accuracy over the baseline for all four algorithms. It is the only feature that <lb/>does so. The RELCLASS-MOD features give improvements for all metrics for BoosTexter, <lb/>Ripper, and TiMBL, as well as improving polar F-measure for SVM. The PARTS-OF-<lb/>SPEECH features are also fairly consistent, improving performance for all the algorithms <lb/>except for SVM. There are also a couple of feature sets that consistently do not improve <lb/>performance for any of the algorithms: the INTENSIFY and PRECEDED-POS features. <lb/> 8.2 Polarity Classification <lb/> For the second step of recognizing contextual polarity, we classify the polarity of all clue <lb/>instances identified as polar in step one. The features for polarity classification were <lb/>listed in Table 9 and described in Section 7.2. <lb/>We investigate the performance of the polarity features under two conditions: <lb/>(1) perfect neutral–polar recognition and (2) automatic neutral–polar recognition. For <lb/>condition 1, we identify the polar instances according to the gold-standard, manual <lb/>contextual-polarity annotations. In the test data, 9,835 instances of the clues from the <lb/>lexicon are polar in context according to the manual annotations. Experiments under <lb/>condition 1 classify these instances as having positive, negative, or both (positive and <lb/>negative) polarity. For condition 2, we take the best performing neutral–polar classifier <lb/>for each algorithm and use the output from those algorithms to identify the polar <lb/>instances. Because polar instances now are being identified automatically, there will be <lb/>noise in the form of misclassified neutral instances. Therefore, for experiments under <lb/>condition 2 we include the neutral class and perform four-way classification instead of <lb/>three-way. Condition 1 allows us to investigate the performance of the different polarity <lb/>features without the noise of misclassified neutral instances. Also, because the set of <lb/>polar instances being classified is the same for all the algorithms, condition 1 allows <lb/>us to compare the performance of the polarity features across the different algorithms. <lb/>However, condition 2 is the more natural one. It allows us to see how the noise of neutral <lb/>instances affects the performance of the polarity features. <lb/>The following sections describe three sets of experiments. First, we investigate the <lb/>performance of the polarity features used together for polarity classification under <lb/>condition 1. As before, the word and word+priorpol classifiers provide our baselines. In <lb/>the second set of experiments, we explore the performance of different sets of features <lb/>for polarity classification, again assuming perfect recognition of the polar instances. <lb/>Finally, we experiment with polarity classification using all the polarity features under <lb/>condition 2, automatic recognition of the polar instances. <lb/>As before, we use the development data to select the parameter settings for each al-<lb/>gorithm. The settings for polarity classification are given in Table 14. They were selected <lb/>based on the performance of the word+priorpol baseline classifier under condition 2. <lb/> 8.2.1 Classification Results: Condition 1. The results for polarity classification using all the <lb/>polarity features, assuming perfect neutral–polar recognition for step one, are given in <lb/>Table 15. For each algorithm, we give the results for the two baseline classifiers, followed <lb/>by the results for the classifier trained using all the polarity features. For the metrics <lb/>where the polarity features perform statistically better than both baselines (two-sided <lb/>t-test, p  ≤  0.05), the results are given in bold. <lb/>

			<page>420 <lb/></page>

			<note place="headnote"> Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note>
			
			Table 14 <lb/> Algorithm settings for polarity classification. <lb/>Algorithm <lb/>Settings <lb/>BoosTexter 2,000 rounds of boosting <lb/>TiMBL <lb/> k=1, MVDM distance metric <lb/>Ripper <lb/>-!s, -S 0.5 <lb/>SVM <lb/>linear kernel <lb/> Table 15 <lb/> Results for polarity classification (step two) using gold-standard polar instances. <lb/> Positive <lb/>Negative <lb/>Both <lb/> Acc Rec Prec <lb/>F <lb/>Rec Prec <lb/>F <lb/>Rec Prec <lb/>F <lb/> BoosTexter <lb/> word token baseline <lb/>78.7 57.7 72.8 64.4 91.5 80.8 85.8 12.9 53.6 20.8 <lb/>word+priorpol baseline 79.7 70.5 68.8 69.6 87.2 85.1 86.1 13.7 53.7 21.8 <lb/>polarity features <lb/> 83.2 76.7 74.3 75.5 89.7 87.7 88.7 11.8 54.2 19.4 <lb/> TiMBL <lb/> word token baseline <lb/>78.5 63.3 69.2 66.1 88.6 82.5 85.4 14.1 51.0 22.1 <lb/>word+priorpol baseline 79.4 69.7 68.4 69.1 87.0 84.8 85.9 14.6 53.5 22.9 <lb/>polarity features <lb/> 82.2 75.4 73.3 74.3 88.5 87.6 88.0 18.3 34.6 23.9 <lb/> Ripper <lb/> word token baseline <lb/>70.0 14.5 74.5 24.3 98.3 69.7 81.6 09.1 74.4 16.2 <lb/>word+priorpol baseline 78.9 75.5 65.2 70.0 83.8 86.4 85.1 09.8 75.4 17.4 <lb/>polarity features <lb/> 83.2 77.8 73.5 75.6 89.2 87.8 88.5 09.8 74.9 17.4 <lb/> SVM <lb/> word token baseline <lb/>69.9 62.4 69.6 65.8 76.0 84.1 79.9 14.1 31.2 19.4 <lb/>word+priorpol baseline 78.2 76.7 63.7 69.6 82.2 86.7 84.4 09.8 75.4 17.4 <lb/>polarity features <lb/> 81.6 74.9 71.1 72.9 88.1 86.6 87.3 09.5 77.6 16.9 <lb/> How well do the polarity features perform working all together? For all algorithms, <lb/>the polarity classifier using all the features significantly outperforms both baselines <lb/>in terms of accuracy, positive F-measure, and negative F-measure. These consistent <lb/>improvements in performance across all four algorithms show that these features are <lb/>quite useful for polarity classification. <lb/>One interesting thing that Table 15 reveals is that negative polarity words are much <lb/>more straightforward to recognize than positive polarity words, at least in this corpus. <lb/>For the negative class, precisions and recalls for the word+priorpol baseline range from <lb/>82.2 to 87.2. For the positive class, precisions and recalls for the word+priorpol baseline <lb/>range from 63.7 to 76.7. However, it is with the positive class that polarity features seem <lb/>to help the most. With the addition of the polarity features, positive F-measure improves <lb/>by 5 points on average; improvements in negative F-measures average only 2.75 points. <lb/> 8.2.2 Feature Set Evaluation. To evaluate the performance of the various features for <lb/>polarity classification, we again perform a series of ablation experiments. As before, we <lb/>start with the word+priorpol baseline classifier, add different sets of polarity features, <lb/>train new classifiers, and compare the results of the new classifiers to the baseline. <lb/>

			<page>421 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Table 16 <lb/> Polarity feature sets for evaluation. <lb/>Experiment <lb/>Features <lb/> NEGATION <lb/> negated, negated subject <lb/> POLARITY-MOD <lb/> modifies polarity, modified by polarity, conjunction polarity <lb/> SHIFTERS <lb/> general, negative, positive polarity shifters <lb/> Table 17 <lb/> Results for polarity feature ablation experiments. <lb/> Positive <lb/>Negative <lb/> Acc Rec <lb/>Prec F <lb/>Rec <lb/>Prec F <lb/> BoosTexter <lb/> NEGATION <lb/> +++ ++ <lb/>+++ +++ +++ + <lb/>+++ <lb/> POLARITY-MOD <lb/> ++ <lb/>+++ + <lb/>+++ + <lb/>++ <lb/>+ <lb/> SHIFTERS <lb/> + <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/> TiMBL <lb/> NEGATION <lb/> +++ +++ +++ +++ +++ +++ +++ <lb/> POLARITY-MOD <lb/> + <lb/>+ <lb/>+ <lb/>+ <lb/>– <lb/>+ <lb/>+ <lb/> SHIFTERS <lb/> + <lb/>+ <lb/>+ <lb/>+ <lb/>– <lb/>+ <lb/>+ <lb/> Ripper <lb/> NEGATION <lb/> +++ – – <lb/>+++ +++ +++ – <lb/>+++ <lb/> POLARITY-MOD <lb/> + <lb/>+++ ++ <lb/>+++ + <lb/>+ <lb/>+ <lb/> SHIFTERS <lb/> + <lb/>– <lb/>+ <lb/>+ <lb/>+ <lb/>– <lb/>+ <lb/> SVM <lb/> NEGATION <lb/> +++ – <lb/>+++ +++ +++ + <lb/>+++ <lb/> POLARITY-MOD <lb/> + <lb/>– <lb/>+++ + <lb/>+ <lb/>– <lb/>+ <lb/> SHIFTERS <lb/> + <lb/>– <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>+ <lb/>Increases and decreases for a given metric as compared to the <lb/>word+priorpol baseline are indicated by + or –, respectively; <lb/>++ or – – indicates the change is significant at the p  &lt;  0.1 level; <lb/>+++ or – – – indicates significance at the p  &lt;  0.05 level. <lb/> Table 16 lists the sets of features tested in each experiment, and Table 17 shows the <lb/>results of the experiments. Results are reported as they were previously in Section 8.1.2, <lb/>with increases and decreases compared to the baseline for a given metric indicated by + <lb/>or –, respectively. <lb/>Looking at Table 17, we see that all three sets of polarity features help to increase <lb/>performance as measured by accuracy and positive and negative F-measures. This is <lb/>true for all the classification algorithms. As we might expect, including the negation <lb/>features has the most marked effect on the performance of polarity classification, with <lb/>statistically significant improvements for most metrics across all the algorithms. 9 The <lb/>

			<note place="footnote"> 9 Although the negation features give the best performance improvements of the three feature sets, these <lb/>classifiers still do not perform as well as the respective all-feature polarity classifiers for each algorithm. <lb/></note>

			<page> 422 <lb/></page>

			<note place="headnote"> Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			polarity-modification features also seem to be important for polarity classification, <lb/>in particular for disambiguating the positive instances. For all the algorithms except <lb/>TiMBL, including the polarity-modification features results in significant improvements <lb/>for at least one of the positive metrics. The polarity shifters also help classification, but <lb/>they seem to be the weakest of the features: Including them does not result in significant <lb/>improvements for any algorithm. <lb/>Another question that is interesting to consider is how much the word token feature <lb/>contributes to polarity classification, given all the other polarity features. Is it enough <lb/>to know the prior polarity of a word, whether it is being negated, and how it is related <lb/>to other polarity influencers? To answer this question, we train classifiers using all the <lb/>polarity features except for word token. Table 18 gives the results for these classifiers; <lb/>for comparison, the results for the all-feature polarity classifiers are also given. Inter-<lb/>estingly, excluding the word token feature produces only small changes in the overall <lb/>results. The results for BoosTexter and Ripper are slightly lower, and the results for <lb/>SVM are practically unchanged. TiMBL actually shows a slight improvement, with the <lb/>exception of the both class. This provides further evidence of the strength of the polarity <lb/>features. Also, a classifier not tied to actual word tokens may potentially be a more <lb/>domain-independent classifier. <lb/> 8.2.3 Classification Results: Condition 2. The experiments in Section 8.2.1 show that the <lb/>polarity features perform well under the ideal condition of perfect recognition of polar <lb/>instances. The next question to consider is how well the polarity features perform <lb/>under the more natural but less-than-perfect condition of automatic recognition of <lb/>polar instances. To investigate this, the polarity classifiers (including the baselines) for <lb/>each algorithm in these experiments start with the polar instances identified by the <lb/>best performing neutral–polar classifier for that algorithm (from Section 8.1.1). The <lb/>results for these experiments are given in Table 19. As before, statistically significant <lb/>improvements over both baselines are given in bold. <lb/>How well do the polarity features perform in the presence of noise from misclas-<lb/>sified neutral instances? Our first observation comes from comparing Table 15 with <lb/>Table 19: Polarity classification results are much lower for all classifiers with the noise <lb/>of neutral instances. Yet in spite of this, the polarity features still produce classifiers that <lb/> Table 18 <lb/> Results for polarity classification without and with the word token feature. <lb/>Acc Pos F Neg F Both F <lb/> BoosTexter <lb/> excluding word token 82.5 <lb/>74.9 <lb/>88.0 <lb/>17.4 <lb/>all polarity features <lb/>83.2 <lb/>75.5 <lb/>88.7 <lb/>19.4 <lb/> TiMBL <lb/> excluding word token 83.2 <lb/>75.9 <lb/>88.4 <lb/>17.3 <lb/>all polarity features <lb/>82.2 <lb/>74.3 <lb/>88.0 <lb/>23.9 <lb/> Ripper <lb/> excluding word token 82.9 <lb/>75.4 <lb/>88.3 <lb/>17.4 <lb/>all polarity features <lb/>83.2 <lb/>75.6 <lb/>88.5 <lb/>17.4 <lb/> SVM <lb/> excluding word token 81.5 <lb/>72.9 <lb/>87.3 <lb/>16.8 <lb/>all polarity features <lb/>81.6 <lb/>72.9 <lb/>87.3 <lb/>16.9 <lb/>

			<page> 423 <lb/></page>

			<note place="headnote"> Computational  Linguistics <lb/> Volume 35, Number 3 <lb/></note>

			Table 19 <lb/> Results for polarity classification (step two) using automatically identified polar instances. <lb/> Positive <lb/>Negative <lb/>Both <lb/>Neutral <lb/> Acc <lb/>R <lb/>P <lb/>F <lb/>R <lb/>P <lb/>F <lb/>R <lb/>P <lb/>F <lb/>R <lb/>P <lb/>F <lb/> BoosTexter <lb/> word token <lb/>61.5 62.3 62.7 62.5 86.4 64.6 74.0 11.4 49.3 18.5 20.8 44.5 28.3 <lb/>word+priorpol 63.3 70.0 57.9 63.4 81.3 71.5 76.1 12.5 47.3 19.8 30.9 47.5 37.4 <lb/>polarity feats <lb/> 65.9 73.6 62.2 67.4 84.9 72.3 78.1 13.4 40.7 20.2 31.0 50.6 38.4 <lb/>TiMBL <lb/> word token <lb/>60.1 68.3 58.9 63.2 81.8 65.0 72.5 11.2 39.6 17.4 21.6 43.1 28.8 <lb/>word+priorpol 61.0 73.2 53.4 61.8 80.6 69.8 74.8 12.7 41.7 19.5 23.0 44.2 30.3 <lb/>polarity feats <lb/> 64.4 75.3 58.6 65.9 81.1 73.0 76.9 16.9 32.7 22.3 32.1 50.0 39.1 <lb/>Ripper <lb/> word token <lb/>54.4 22.2 69.4 33.6 95.1 50.7 66.1 00.0 00.0 00.0 21.7 76.5 33.8 <lb/>word+priorpol 51.4 24.0 71.7 35.9 97.7 48.9 65.1 00.0 00.0 00.0 09.2 75.8 16.3 <lb/>polarity feats <lb/>54.8 38.0 67.2 48.5 95.5 52.7 67.9 00.0 00.0 00.0 14.5 66.8 23.8 <lb/> SVM <lb/> word token <lb/>64.5 70.0 60.9 65.1 70.9 74.9 72.9 16.6 41.5 23.7 53.3 51.0 52.1 <lb/>word+priorpol 62.8 89.0 51.2 65.0 88.4 69.2 77.6 11.1 48.5 18.0 02.4 58.3 04.5 <lb/>polarity feats <lb/>64.1 90.8 53.0 66.9 90.4 70.1 79.0 12.7 52.3 20.4 02.2 61.4 04.3 <lb/>

			<page> 424 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			outperform the baselines. For three of the four algorithms, the classifier using all the <lb/>polarity features has the highest accuracy. For BoosTexter and TiMBL, the improvements <lb/>in accuracy over both baselines are significant. Also for all algorithms, using the polarity <lb/>features gives the highest positive and negative F-measures. <lb/>Because the set of polarity instances being classified by each algorithm is different, <lb/>we cannot directly compare the results from one algorithm to the next. <lb/> 8.3 Two-step versus One-step Recognition of Contextual Polarity <lb/> Although the two-step approach to recognizing contextual polarity allows us to focus <lb/>our investigation on the performance of features for both neutral–polar classification <lb/>and polarity classification, the question remains: How does the two-step approach <lb/>compare to recognizing contextual polarity in a single classification step? The results <lb/>shown in Table 20 help to answer this question. The first row in Table 20 for each <lb/>algorithm shows the combined result for the two stages of classification. For BoosTexter, <lb/>TiMBL, and Ripper, this is the combination of results from using all the neutral–polar <lb/>features for step one, together with the results from using all of the polarity features for <lb/>step two. 10 For SVM, this is the combination of results from the word+priorpol baseline <lb/>from step one, together with results for using all the polarity features for step two. <lb/>Recall that the word+priorpol classifier was the best neutral–polar classifier for SVM <lb/>(see Table 11). The second rows for BoosTexter, TiMBL, and Ripper show the results of <lb/>a single classifier trained to recognize contextual polarity using all the neutral–polar <lb/>and polarity features together. For SVM, the second row shows the results of classifying <lb/>the contextual polarity using just the word token feature. This classifier outperformed <lb/>all others for SVM. In the table, the best result for each metric for each algorithm is <lb/>highlighted in bold. <lb/>When comparing the two-step and one-step approaches, contrary to our expecta-<lb/>tions, we see that the one-step approach performs about as well or better than the <lb/>two-step approach for recognizing contextual polarity. For SVM, the improvement in <lb/>accuracy achieved by the two-step approach is significant, but this is not true for <lb/>the other algorithms. One fairly consistent difference between the two approaches is <lb/>that the two-step approach scores slightly higher for neutral F-measure, and the one-<lb/>step approach achieves higher F-measures for the polarity classes. The difference in <lb/>negative F-measure is significant for BoosTexter, TiMBL, and Ripper. The exception to <lb/>this is SVM. For SVM, the two-step approach achieves significantly higher positive and <lb/>negative F-measures. <lb/>One last question we consider is how much the neutral–polar features contribute <lb/>to the performance of the one-step classifiers. The third line in Table 20 for BoosTexter, <lb/>TiMBL, and Ripper gives the results for a one-step classifier trained without the neutral– <lb/>polar features. Although the differences are not always large, excluding the neutral– <lb/>polar features consistently degrades performance in terms of accuracy and positive, <lb/>negative, and neutral F-measures. The drop in negative F-measure is significant for all <lb/>three algorithms, the drop in neutral F-measure is significant for BoosTexter and TiMBL, <lb/>and the drop in accuracy is significant for TiMBL and Ripper (and for BoosTexter at the <lb/>p  ≤  0.1 level). <lb/>

			<note place="footnote"> 10 To clarify, Section 8.2.3 only reported results for instances identified as polar in step one. Here, we report <lb/>results for all clue instances, including the instances classified as neutral in step one. <lb/></note>

			<page> 425 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Table 20 <lb/> Results for contextual polarity classification for both two-step and one-step approaches. <lb/>Acc Pos F Neg F Both F Neutral F <lb/> BoosTexter <lb/> two-step <lb/> 74.5 <lb/> 47.1 <lb/>57.5 <lb/>12.9 <lb/> 83.4 <lb/> one-step all feats <lb/>74.3 <lb/> 49.1 <lb/>59.8 <lb/> 14.1 <lb/>82.9 <lb/>one-step – neut-pol feats 73.3 <lb/>48.4 <lb/>58.7 <lb/> 16.3 <lb/> 81.9 <lb/> TiMBL <lb/> two-step <lb/> 74.1 <lb/> 47.6 <lb/>56.4 <lb/>13.8 <lb/> 83.2 <lb/> one-step all feats <lb/>73.9 <lb/> 49.6 <lb/>59.3 <lb/> 15.2 <lb/>82.6 <lb/>one-step – neut-pol feats 72.5 <lb/>49.5 <lb/>56.9 <lb/> 21.6 <lb/> 81.4 <lb/> Ripper <lb/> two-step <lb/>68.9 <lb/>26.6 <lb/>49.0 <lb/>00.0 <lb/> 80.1 <lb/> one-step all feats <lb/> 69.5 <lb/>30.2 <lb/>52.8 <lb/>14.0 <lb/> 79.4 <lb/>one-step – neut-pol feats 67.0 <lb/>28.9 <lb/>33.0 <lb/>11.4 <lb/>78.6 <lb/> SVM <lb/> two-step <lb/> 73.1 <lb/>46.6 <lb/>58.0 <lb/> 13.0 <lb/> 82.1 <lb/> one-step <lb/>71.6 <lb/>43.4 <lb/>51.7 <lb/> 17.0 <lb/> 81.6 <lb/> The modest drop in performance that we see when excluding the neutral–polar <lb/>features in the one-step approach seems to suggest that discriminating between neutral <lb/>and polar instances is helpful but not necessarily crucial. However, consider Figure 3. <lb/>In this figure, we show the F-measures for the positive, negative, and both classes for <lb/>the BoosTexter polarity classifier that uses the gold-standard neutral/polar instances <lb/>(from Table 15) and for the BoosTexter one-step polarity classifier that uses all features <lb/>(from Table 20). Plotting the same sets of results for the other three algorithms produces <lb/>very similar figures. The difference when the classifiers have to contend with the noise <lb/>from neutral instances is dramatic. Although Table 20 shows that there is room for <lb/>improvement across all the contextual polarity classes, Figure 3 shows us that perhaps <lb/>the best way to achieve these improvements is to improve the ability to discriminate the <lb/>neutral class from the others. <lb/> Figure 3 <lb/> Chart showing the positive, negative, and both class F-measures for the BoosTexter classifier that <lb/>uses the gold-standard neutral/polar classes and the BoosTexter one-step classifier that uses all <lb/>the features. <lb/>

			<page> 426 <lb/></page>

			<note place="headnote"> Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note>

			9. Related Work <lb/>9.1 Phrase-Level Sentiment Analysis <lb/> Other researchers who have worked on classifying the contextual polarity of sentiment <lb/>expressions are Yi et al. (2003), Popescu and Etzioni (2005), and Suzuki, Takamura, and <lb/>Okumura (2006). Yi et al. use a lexicon and manually developed patterns to classify <lb/>contextual polarity. Their patterns are high-quality, yielding quite high precision over <lb/>the set of expressions that they evaluate. Popescu and Etzioni use an unsupervised clas-<lb/>sification technique called relaxation labeling (Hummel and Zucker 1983) to recognize <lb/>the contextual polarity of words that are at the heads of select opinion phrases. They <lb/>take an iterative approach, using relaxation labeling first to determine the contextual <lb/>polarities of the words, then again to label the polarities of the words with respect to <lb/>their targets. A third stage of relaxation labeling then is used to assign final polarities to <lb/>the words, taking into consideration the presence of other polarity terms and negation. <lb/>As we do, Popescu and Etzioni use features that represent conjunctions and dependency <lb/>relations between polarity words. Suzuki et al. use a bootstrapping approach to classify <lb/>the polarity of tuples of adjectives and their target nouns in Japanese blogs. Included <lb/>in the features that they use are the words that modify the adjectives and the word that <lb/>the adjective modifies. They consider the effect of a single negation term, the Japanese <lb/>equivalent of not. <lb/> Our work in recognizing contextual polarity differs from this research on <lb/>expression-level sentiment analysis in several ways. First, the set of expressions they <lb/>evaluate is limited either to those that target specific items of interest, such as products <lb/>and product features, or to tuples of adjectives and nouns. In contrast, we seek to classify <lb/>the contextual polarity of all instances of words from a large lexicon of subjectivity clues <lb/>that appear in the corpus. Included in the lexicon are not only adjectives, but nouns, <lb/>verbs, adverbs, and even modals. <lb/>Our work also differs from other research in the variety of features that we use. As <lb/>other researchers do, we consider negation and the words that directly modify or are <lb/>modified by the expression being classified. However, with negation, we have features <lb/>for both local and longer-distance types of negation, and we take care to count negation <lb/>terms only when they are actually being used to negate, excluding, for example, nega-<lb/>tion terms when they are used in phrases that intensify (e.g., not only). We also include <lb/>contextual features to capture the presence of other clue instances in the surrounding <lb/>sentences, and features that represent the reliability of clues from the lexicon. <lb/>Finally, a unique aspect of the work presented in this article is the evaluation of <lb/>different features for recognizing contextual polarity. We first presented the features <lb/>explored in this research in Wilson, Wiebe, and Hoffman (2005), but this work signif-<lb/>icantly extends that initial evaluation. We explore the performance of features across <lb/>different learning algorithms, and we evaluate not only features for discriminating <lb/>between positive and negative polarity, but features for determining when a word is <lb/>or is not expressing a sentiment in the first place (neutral in context). This is also the <lb/>first work to evaluate the effect of neutral instances on the performance of features for <lb/>discriminating between positive and negative contextual polarity. <lb/> 9.2 Other Research in Sentiment Analysis <lb/> Recognizing contextual polarity is just one facet of the research in automatic senti-<lb/>ment analysis. Research ranges from work on learning the prior polarity (semantic <lb/>

			<page>427 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			orientation) of words and phrases (e.g., Hatzivassiloglou and McKeown 1997; Kamps <lb/>and Marx 2002; Turney and Littman 2003; Hu and Liu 2004; Kim and Hovy 2004; Esuli <lb/>and Sebastiani 2005; Takamura, Inui, and Okumura 2005; Popescu and Etzioni 2005; <lb/>Andreevskaia and Bergler 2006; Esuli and Sebastiani 2006a; Kanayama and Nasukawa <lb/>2006) to characterizing the sentiment of documents, such as recognizing inflammatory <lb/>messages (Spertus 1997), tracking sentiment over time in online discussions (Tong <lb/>2001), and classifying the sentiment of online messages (e.g., Das and Chen 2001; <lb/>Koppel and Schler 2006), customer feedback data (Gamon 2004), or product and movie <lb/>reviews (e.g., Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, and <lb/>Pennock 2003; Beineke, Hastie, and Vaithyanathan 2004; Mullen and Collier 2004; Bai, <lb/>Padman, and Airoldi 2005; Whitelaw, Garg, and Argamon 2005; Kennedy and Inkpen <lb/>2006; Koppel and Schler 2006). <lb/>

			Identifying prior polarity is a different task than recognizing contextual polarity, <lb/>although the two tasks are complementary. The goal of identifying prior polarity is <lb/>to automatically acquire the polarity of words or phrases for listing in a lexicon. Our <lb/>work on recognizing contextual polarity begins with a lexicon of words with established <lb/>prior polarities and then disambiguates in the corpus the polarity being expressed <lb/>by the phrases in which instances of those words appear. To make the relationship <lb/>between that task and ours clearer, some word lists that are used to evaluate methods for <lb/>recognizing prior polarity (positive and negative word lists from the General Inquirer <lb/>[Stone et al. 1966] and lists of positive and negative adjectives created for evaluation by <lb/>Hatzivassiloglou and McKeown [1997]) are included in the prior-polarity lexicon used <lb/>in our experiments. <lb/>For the most part, the features explored in this work differ from the ones used to <lb/>identify prior polarity with just a few exceptions. Using a feature to capture conjunc-<lb/>tions between clue instances was motivated in part by the work of Hatzivassiloglou and <lb/>McKeown (1997). They use constraints on the co-occurrence in conjunctions of words <lb/>with similar or opposite polarity to predict the prior polarity of adjectives. Esuli and <lb/>Sebastiani (2005) consider negation in some of their experiments involving WordNet <lb/>glosses. Takamura et al. (2005) use negation words and phrases, including phrases such <lb/>as lack of that are members in our lists of polarity shifters, and conjunctive expressions <lb/>that they collect from corpora. <lb/>Esuli and Sebastiani (2006a) is the only work in prior-polarity identification to <lb/>include a neutral (objective) category and to consider a three-way classification between <lb/>positive, negative, and neutral words. Although identifying prior polarity is a different <lb/>task, they report a finding similar to ours, namely, that accuracy is lower when neutral <lb/>words are included. <lb/>Some research in sentiment analysis classifies the sentiments of sentences. Morinaga <lb/>et al. (2002), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), <lb/>and Grefenstette et al. (2004) 11 all begin by first creating prior-polarity lexicons. Yu and <lb/>Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic <lb/>orientations of instances of lexicon words in the sentence. Thus, they do not identify the <lb/>contextual polarity of individual phrases containing clue instances, which is the focus <lb/>of this work. Morinaga et al. only consider the positive or negative clue instance in <lb/>each sentence that is closest to some target reference; Kim and Hovy, Hu and Liu, and <lb/>Grefenstette et al. multiply or count the prior polarities of clue instances in the sentence. <lb/>

			<note place="footnote"> 11 In Grefenstette et al. (2004), the units that are classified are fixed windows around named entities rather <lb/>than sentences. <lb/></note>

			<page> 428 <lb/></page>

			<note place="headnote">Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/></note> 
			
			These researchers also consider local negation to reverse polarity, with Morinaga et al. <lb/>also taking into account the negating effect of words like insufficient. However, they <lb/>do not use the other types of features that we consider in our experiments. Kaji and <lb/>Kitsuregawa (2006) take a different approach to recognizing positive and negative <lb/>sentences. They bootstrap from information easily obtained in &quot; Pro &quot; and &quot; Con &quot; <lb/>HTML tables and lists, and from one high-precision linguistic pattern, to automatically <lb/>construct a large corpus of positive and negative sentences. They then use this corpus to <lb/>train a naive Bayes sentence classifier. In contrast to our work, sentiment classification <lb/>in all of this research is restricted to identifying only positive and negative sentences <lb/>(excluding our both and neutral categories). In addition, only one sentiment is assigned <lb/>per sentence; our system assigns contextual polarity to individual expressions, which <lb/>would allow for a sentence to be assigned to multiple sentiment categories. As we saw <lb/>when exploring the contextual polarity annotations, it is not uncommon for sentences <lb/>to contain more than one sentiment expression. <lb/>Classifying the sentiment of documents is a very different task than recognizing <lb/>the contextual polarity of words and phrases. However, some researchers have re-<lb/>ported findings about document-level classification that are similar to our findings <lb/>about phrase-level classification. Bai et al. (2005) argue that dependencies among key <lb/>sentiment terms are important for classifying document sentiment. Similarly, we show <lb/>that features for capturing when clue instances modify each other are important for <lb/>phrase-level classification, in particular, for identifying positive expressions. Gamon <lb/>(2004) achieves his best results for document classification using a wide variety of <lb/>features, including rich linguistic features, such as features that capture constituent <lb/>structure, features that combine part-of-speech and semantic relations (e.g., sentence <lb/>subject or negated context), and features that capture tense information. We also achieve <lb/>our best results for phrase-level classification using a wide variety of features, many <lb/>of which are linguistically rich. Kennedy and Inkpen (2006) report consistently higher <lb/>results for document sentiment classification when select polarity influencers, including <lb/>negators and intensifiers, are included. 12 Koppel and Schler (2006) demonstrate the <lb/>importance of neutral examples for document-level classification. In this work, we show <lb/>that being able to correctly identify neutral instances is also very important for phrase-<lb/>level sentiment analysis. <lb/> 10. Conclusions and Future Work <lb/> Being able to determine automatically the contextual polarity of words and phrases is <lb/>an important problem in sentiment analysis. In the research presented in this article, we <lb/>tackle this problem and show that it is much more complex than simply determining <lb/>whether a word or phrase is positive or negative. In our analysis of a corpus with <lb/>annotations of subjective expressions and their contextual polarity, we find that positive <lb/>and negative words from a lexicon are used in neutral contexts much more often than <lb/>they are used in expressions of the opposite polarity. The importance of identifying <lb/>

			<note place="footnote"> 12 Das and Chen (2001), Pang, Lee, and Vaithyanathan (2002), and Dave, Lawrence, and Pennock (2003) also <lb/>represent negation. In their experiments, words which follow a negation term are tagged with a negation <lb/>marker and then treated as new words. Pang, Lee and Vaithyanathan report that representing negation in <lb/>this way slightly helps their results, whereas Dave, Lawrence, and Pennock report a slightly detrimental <lb/>effect. Whitelaw, Garg, and Argamon (2005) also represent negation terms and intensifiers. However, in <lb/>their experiments, the effect of negation is not separately evaluated, and intensifiers are not found to be <lb/>beneficial. <lb/></note>

			<page> 429 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			when contextual polarity is neutral is further revealed in our classification experiments: <lb/>When neutral instances are excluded, the performance of features for distinguishing <lb/>between positive and negative polarity greatly improves. <lb/>A focus of this research is on understanding which features are important for <lb/>recognizing contextual polarity. We experiment with a wide variety of linguistically <lb/>motivated features, and we evaluate the performance of these features using several <lb/>different machine learning algorithms. Features for distinguishing between neutral and <lb/>polar instances are evaluated, as well as features for distinguishing between positive <lb/>and negative contextual polarity. For classifying neutral and polar instances, we find <lb/>that, although some features produce significant improvements over the baseline in <lb/>terms of polar or neutral recall or precision, it is the combination of features together <lb/>that is needed to achieve significant improvements in accuracy. For classifying positive <lb/>and negative contextual polarity, features for capturing negation prove to be the most <lb/>important. However, we find that features that also perform well are those that cap-<lb/>ture when a word is (or is not) modifying or being modified by other polarity terms. <lb/>This suggests that identifying features that represent more complex interdependencies <lb/>between polarity clues will be an important avenue for future research. <lb/>Another direction for future work will be to expand our lexicon using existing <lb/>techniques for acquiring the prior polarity of words and phrases. It follows that a larger <lb/>lexicon will have a greater coverage of sentiment expressions. However, expanding the <lb/>lexicon with automatically acquired prior-polarity tags may result in an even greater <lb/>proportion of neutral instances to contend with. Given the degradation in performance <lb/>created by the neutral instances, whether expanding the lexicon automatically will <lb/>result in improved performance for recognizing contextual polarity is an empirical <lb/>question. <lb/>Finally, the overall goal of our research is to use phrase-level sentiment analysis in <lb/>higher-level NLP tasks, such as opinion question answering and summarization. <lb/>
		
		</body>

		<back>

			<div type="acknowledgement"> Acknowledgments <lb/> We would like to thank the anonymous <lb/>reviewers for their valuable comments and <lb/>suggestions. This work was supported in <lb/>part by an Andrew Mellow Predoctoral <lb/>Fellowship, by the NSF under grant <lb/>IIS-0208798, by the Advanced Research and <lb/>Development Activity (ARDA), and by the <lb/>European IST Programme through the <lb/>AMIDA Integrated Project FP6-0033812. <lb/></div>

			<listBibl> References <lb/> Andreevskaia, Alina and Sabine Bergler. <lb/>2006. Mining WordNet for fuzzy <lb/>sentiment: Sentiment tag extraction from <lb/>WordNet glosses. In Proceedings of the 11th <lb/>Meeting of the European Chapter of the <lb/>Association for Computational Linguistics <lb/>(EACL-2006), pages 209–216, Trento. <lb/>Bai, Xue, Rema Padman, and Edoardo <lb/> Airoldi. 2005. On learning parsimonious <lb/>models for extracting consumer opinions. <lb/>In Proceedings of the 38th Annual Hawaii <lb/>International Conference on System <lb/>Sciences (HICSS&apos;05) -Track 3, page 75.2, <lb/>Waikoloa, HI. <lb/>Banfield, Ann. 1982. Unspeakable Sentences. <lb/> Routledge and Kegan Paul, Boston. <lb/>Beineke, Philip, Trevor Hastie, and <lb/>Shivakumar Vaithyanathan. 2004. The <lb/>sentimental factor: Improving review <lb/>classification via human-provided <lb/>information. In Proceedings of the 42nd <lb/>Annual Meeting of the Association for <lb/>Computational Linguistics (ACL-04), <lb/> pages 263–270, Barcelona. <lb/>Cohen, William W. 1996. Learning trees <lb/>and rules with set-valued features. In <lb/> Proceedings of the 13th National Conference <lb/>on Artificial Intelligence, pages 709–717, <lb/>Portland, OR. <lb/>Collins, Michael. 1997. Three generative, <lb/>lexicalised models for statistical parsing. <lb/>In Proceedings of the 35th Annual Meeting of <lb/>the Association for Computational Linguistics <lb/>(ACL-97), pages 16–23, Madrid. <lb/>Daelemans, Walter, Véronique Hoste, <lb/>Fien De Meulder, and Bart Naudts. <lb/>2003a. Combined optimization of feature <lb/>selection and algorithm parameter <lb/>

			<page> 430 <lb/></page>

			<note place="headnote"> Wilson, Wiebe, and Hoffmann <lb/> Recognizing Contextual Polarity <lb/></note>

			interaction in machine learning of <lb/>language. In Proceedings of the 14th <lb/>European Conference on Machine Learning <lb/>(ECML-2003), pages 84–95, <lb/>Cavtat-Dubrovnik. <lb/>Daelemans, Walter, Jakub Zavrel, Ko van der <lb/>Sloot, and Antal van den Bosch. 2003b. <lb/>TiMBL: Tilburg Memory Based Learner, <lb/>version 5.0 Reference Guide. ILK Technical <lb/>Report 03-10, Induction of Linguistic <lb/>Knowledge Research Group, Tilburg <lb/>University. Available at http://ilk.uvt. <lb/> nl/downloads/pub/papers/ilk0310.pdf. <lb/> Das, Sanjiv Ranjan and Mike Y. Chen. 2001. <lb/>Yahoo! for Amazon: Sentiment parsing <lb/>from small talk on the Web. In Proceedings <lb/>of the August 2001 Meeting of the European <lb/>Finance Association (EFA), Barcelona, <lb/>Spain. Available at http://ssrn.com/ <lb/>abstract=276189. <lb/> Dave, Kushal, Steve Lawrence, and David M. <lb/>Pennock. 2003. Mining the peanut <lb/>gallery: Opinion extraction and <lb/>semantic classification of product <lb/>reviews. In Proceedings of the 12th <lb/>International World Wide Web Conference <lb/>(WWW2003), Budapest. Available at <lb/> http://www2003.org. <lb/> Esuli, Andrea and Fabrizio Sebastiani. 2005. <lb/>Determining the semantic orientation of <lb/>terms through gloss analysis. In <lb/> Proceedings of ACM SIGIR Conference on <lb/>Information and Knowledge Management <lb/>(CIKM-05), pages 617–624, Bremen. <lb/>Esuli, Andrea and Fabrizio Sebastiani. 2006a. <lb/>Determining term subjectivity and term <lb/>orientation for opinion mining. In <lb/> Proceedings the 11th Meeting of the European <lb/>Chapter of the Association for Computational <lb/>Linguistics (EACL-2006), pages 193–200, <lb/>Trento. <lb/>Esuli, Andrea and Fabrizio Sebastiani. 2006b. <lb/>SentiWordNet: A publicly available lexical <lb/>resource for opinion mining. In Proceedings <lb/>of LREC-06, the 5th Conference on Language <lb/>Resources and Evaluation, pages 417–422, <lb/>Genoa. <lb/>Gamon, Michael. 2004. Sentiment <lb/>classification on customer feedback data: <lb/>Noisy data, large feature vectors, and the <lb/>role of linguistic analysis. In Proceedings <lb/>of the 20th International Conference on <lb/>Computational Linguistics (COLING-2004), <lb/> pages 611–617, Geneva. <lb/>Grefenstette, Gregory, Yan Qu, James G. <lb/>Shanahan, and David A. Evans. 2004. <lb/>Coupling niche browsers and affect <lb/>analysis for an opinion mining application. <lb/>In Proceedings of the Conference Recherche <lb/> d&apos;Information Assistee par Ordinateur <lb/>(RIAO-2004), pages 186–194, Avignon. <lb/>Hatzivassiloglou, Vasileios and Kathy <lb/>McKeown. 1997. Predicting the semantic <lb/>orientation of adjectives. In Proceedings of <lb/>the 35th Annual Meeting of the Association <lb/>for Computational Linguistics (ACL-97), <lb/> pages 174–181, Madrid. <lb/>Hoste, Véronique. 2005. Optimization Issues in <lb/>Machine Learning of Coreference Resolution. <lb/> Ph.D. thesis, Language Technology Group, <lb/>University of Antwerp. <lb/>Hu, Minqing and Bing Liu. 2004. Mining <lb/>and summarizing customer reviews. In <lb/> Proceedings of ACM SIGKDD Conference <lb/>on Knowledge Discovery and Data Mining <lb/>2004 (KDD-2004), pages 168–177, <lb/>Seattle, WA. <lb/>Hummel, Robert A. and Steven W. Zucker. <lb/>1983. On the foundations of relaxation <lb/>labeling processes. IEEE Transactions on <lb/>Pattern Analysis and Machine Intelligence <lb/>(PAMI), 5(3):167–187. <lb/>Joachims, Thorsten. 1999. Making large-scale <lb/>SVM learning practical. In B. Scholkopf, <lb/>C. Burgess, and A. Smola, editors, <lb/> Advances in Kernel Methods – Support Vector <lb/>Learning, pages 169–184. MIT Press, <lb/>Cambridge, MA. <lb/>Kaji, Nobuhiro and Masaru Kitsuregawa. <lb/>2006. Automatic construction of <lb/>polarity-tagged corpus from HTML <lb/>documents. In Proceedings of the <lb/>COLING/ACL 2006 Main Conference <lb/>Poster Sessions, pages 452–459, Sydney. <lb/>Kamps, Jaap and Maarten Marx. 2002. <lb/>Words with attitude. In Proceedings of the <lb/>1st International Conference on Global <lb/>WordNet, pages 332–341, Mysore. <lb/>Kanayama, Hiroshi and Tetsuya Nasukawa. <lb/>2006. Fully automatic lexicon expansion <lb/>for domain-oriented sentiment analysis. <lb/>In Proceedings of the Conference on <lb/>Empirical Methods in Natural Language <lb/>Processing (EMNLP-2006), pages 355–363, <lb/>Sydney. <lb/>Kennedy, Alistair and Diana Inkpen. 2006. <lb/>Sentiment classification of movie reviews <lb/>using contextual valence shifters. <lb/> Computational Intelligence, 22(2):110–125. <lb/>Kim, Soo-Min and Eduard Hovy. 2004. <lb/>Determining the sentiment of opinions. <lb/>In Proceedings of the 20th International <lb/>Conference on Computational Linguistics <lb/>(COLING-2004), pages 1267–1373, Geneva. <lb/>Koppel, Moshe and Jonathan Schler. 2006. <lb/>The importance of neutral examples for <lb/>learning sentiment. Computational <lb/>Intelligence, 22(2):100–109. <lb/>

			<page> 431 <lb/></page>

			<note place="headnote"> Computational Linguistics <lb/>Volume 35, Number 3 <lb/></note>

			Maybury, Mark T., editor. 2004. New <lb/> Directions in Question Answering. American <lb/>Association for Artificial Intelligence, <lb/>Menlo Park, CA. <lb/>Morinaga, Satoshi, Kenji Yamanishi, Kenji <lb/>Tateishi, and Toshikazu Fukushima. 2002. <lb/>Mining product reputations on the Web. <lb/>In Proceedings of the 8th ACM SIGKDD <lb/>International Conference on Knowledge <lb/>Discovery and Data Mining (KDD-2002), <lb/> pages 341–349, Edmonton. <lb/>Mullen, Tony and Nigel Collier. 2004. <lb/>Sentiment analysis using support <lb/>vector machines with diverse <lb/>information sources. In Proceedings <lb/>of the Conference on Empirical Methods <lb/>in Natural Language Processing <lb/>(EMNLP-2004), pages 412–418, <lb/>Barcelona. <lb/>Nasukawa, Tetsuya and Jeonghee Yi. <lb/>2003. Sentiment analysis: Capturing <lb/>favorability using natural language <lb/>processing. In Proceedings of the 2nd <lb/>International Conference on Knowledge <lb/>Capture (K-CAP 2003), pages 70–77, <lb/>Sanibel Island, FL. <lb/>Pang, Bo, Lillian Lee, and Shivakumar <lb/>Vaithyanathan. 2002. Thumbs up? <lb/>Sentiment classification using machine <lb/>learning techniques. In Proceedings of the <lb/>Conference on Empirical Methods in Natural <lb/>Language Processing (EMNLP-2002), <lb/> pages 79–86, Philadelphia, PA. <lb/>Polanyi, Livia and Annie Zaenen. 2004. <lb/>Contextual valence shifters. In Working <lb/>Notes of the AAAI Spring Symposium on <lb/>Exploring Attitude and Affect in Text: <lb/>Theories and Applications, pages 106–111, <lb/>The AAAI Press, Menlo Park, CA. <lb/>Popescu, Ana-Maria and Oren Etzioni. <lb/>2005. Extracting product features and <lb/>opinions from reviews. In Proceedings <lb/>of the Human Language Technologies <lb/>Conference/Conference on Empirical <lb/>Methods in Natural Language Processing <lb/>(HLT/EMNLP-2005), pages 339–346, <lb/>Vancouver. <lb/>Quirk, Randolph, Sidney Greenbaum, <lb/>Geoffry Leech, and Jan Svartvik. 1985. <lb/> A Comprehensive Grammar of the English <lb/>Language. Longman, New York. <lb/>Riloff, Ellen and Janyce Wiebe. 2003. <lb/>Learning extraction patterns for subjective <lb/>expressions. In Proceedings of the Conference <lb/>on Empirical Methods in Natural Language <lb/>Processing (EMNLP-2003), pages 105–112, <lb/>Sapporo. <lb/>Schapire, Robert E. and Yoram Singer. 2000. <lb/>BoosTexter: A boosting-based system for <lb/>text categorization. Machine Learning, <lb/> 39(2/3):135–168. <lb/>Spertus, Ellen. 1997. Smokey: Automatic <lb/>recognition of hostile messages. In <lb/> Proceedings of the 8th Annual Conference <lb/>on Innovative Applications of Artificial <lb/>Intelligence (IAAI-97), pages 1058–1065, <lb/>Providence, RI. <lb/>Stone, Philip J., Dexter C. Dunphy, <lb/>Marshall S. Smith, and Daniel M. Ogilvie. <lb/>1966. The General Inquirer: A Computer <lb/>Approach to Content Analysis. MIT Press, <lb/>Cambridge, MA. <lb/>Stoyanov, Veselin, Claire Cardie, and <lb/>Janyce Wiebe. 2005. Multi-perspective <lb/>question answering using the OpQA <lb/>corpus. In Proceedings of the Human <lb/>Language Technologies Conference/ <lb/>Conference on Empirical Methods in Natural <lb/>Language Processing (HLT/EMNLP-2005), <lb/> pages 923–930, Vancouver. <lb/>Suzuki, Yasuhiro, Hiroya Takamura, and <lb/>Manabu Okumura. 2006. Application of <lb/>semi-supervised learning to evaluative <lb/>expression classification. In Proceedings of <lb/>the 7th International Conference on Intelligent <lb/>Text Processing and Computational <lb/>Linguistics (CICLing-2006), pages 502–513, <lb/>Mexico City. <lb/>Takamura, Hiroya, Takashi Inui, and <lb/>Manabu Okumura. 2005. Extracting <lb/>emotional polarity of words using spin <lb/>model. In Proceedings of the 43rd Annual <lb/>Meeting of the Association for Computational <lb/>Linguistics (ACL-05), pages 133–140, <lb/>Ann Arbor, MI. <lb/>Tong, Richard. 2001. An operational <lb/>system for detecting and tracking <lb/>opinions in online discussions. In <lb/> Working Notes of the SIGIR Workshop on <lb/>Operational Text Classification, pages 1–6, <lb/>New Orleans, LA. <lb/>Turney, Peter. 2002. Thumbs up or thumbs <lb/>down? Semantic orientation applied to <lb/>unsupervised classification of reviews. <lb/>In Proceedings of the 40th Annual Meeting <lb/>of the Association for Computational <lb/>Linguistics (ACL-02), pages 417–424, <lb/>Philadelphia, PA. <lb/>Turney, Peter and Michael L. Littman. 2003. <lb/>Measuring praise and criticism: Inference <lb/>of semantic orientation from association. <lb/> ACM Transactions on Information Systems <lb/>(TOIS), 21(4):315–346. <lb/>Whitelaw, Casey, Navendu Garg, and <lb/>Shlomo Argamon. 2005. Using appraisal <lb/>groups for sentiment analysis. In <lb/> Proceedings of the 14th ACM International <lb/>Conference on Information and Knowledge <lb/>

			<page> 432 <lb/></page>

			Wilson, Wiebe, and Hoffmann <lb/>Recognizing Contextual Polarity <lb/> Management (CIKM-2005), pages 625–631, <lb/>Bremen. <lb/>Wiebe, Janyce. 1994. Tracking point of view <lb/>in narrative. Computational Linguistics, <lb/> 20(2):233–287. <lb/>Wiebe, Janyce, Rebecca Bruce, and <lb/>Thomas O&apos;Hara. 1999. Development <lb/>and use of a gold standard data set <lb/>for subjectivity classifications. In <lb/> Proceedings of the 37th Annual Meeting <lb/>of the Association for Computational <lb/>Linguistics (ACL-99), pages 246–253, <lb/>College Park, MD. <lb/>Wiebe, Janyce and Rada Mihalcea. <lb/>2006. Word sense and subjectivity. <lb/>In Proceedings of the 21st International <lb/>Conference on Computational Linguistics <lb/>and 44th Annual Meeting of the <lb/>Association for Computational Linguistics, <lb/> pages 1065–1072, Sydney. <lb/>Wiebe, Janyce and Ellen Riloff. 2005. <lb/>Creating subjective and objective sentence <lb/>classifiers from unannotated texts. In <lb/> Proceedings of the 6th International <lb/>Conference on Intelligent Text Processing and <lb/>Computational Linguistics (CICLing-2005), <lb/> pages 486–497, Mexico City. <lb/>Wiebe, Janyce, Theresa Wilson, and Claire <lb/>Cardie. 2005. Annotating expressions <lb/>of opinions and emotions in language. <lb/> Language Resources and Evaluation <lb/>(formerly Computers and the Humanities), <lb/> 39(2/3):164–210. <lb/>Wilson, Theresa, Janyce Wiebe, and Paul <lb/>Hoffmann. 2005. Recognizing contextual <lb/>polarity in phrase-level sentiment <lb/>analysis. In Proceedings of the Human <lb/>Language Technologies Conference/ <lb/>Conference on Empirical Methods in Natural <lb/>Language Processing (HLT/EMNLP-2005), <lb/> pages 347–354, Vancouver. <lb/>Xia, Fei and Martha Palmer. 2001. <lb/>Converting dependency structures to <lb/>phrase structures. In Proceedings of the <lb/>Human Language Technology Conference <lb/>(HLT-2001), pages 1–5, San Diego, CA. <lb/>Yi, Jeonghee, Tetsuya Nasukawa, Razvan <lb/>Bunescu, and Wayne Niblack. 2003. <lb/>Sentiment analyzer: Extracting sentiments <lb/>about a given topic using natural language <lb/>processing techniques. In Proceedings of the <lb/>3rd IEEE International Conference on Data <lb/>Mining (ICDM&apos;03), pages 427–434, <lb/>Melbourne, FL. <lb/>Yu, Hong and Vasileios Hatzivassiloglou. <lb/>2003. Towards answering opinion <lb/>questions: Separating facts from opinions <lb/>and identifying the polarity of opinion <lb/>sentences. In Proceedings of the Conference <lb/>on Empirical Methods in Natural Language <lb/>Processing (EMNLP-2003), pages 129–136, <lb/>Sapporo. <lb/></listBibl>

			<page> 433 </page>

		</back>
	</text>
</tei>
