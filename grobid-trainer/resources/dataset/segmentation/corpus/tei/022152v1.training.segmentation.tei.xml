<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Learning from heterogeneous data sources: an <lb/>application in spatial proteomics <lb/>Lisa M. Breckels 1,2 , Sean Holden 3 , David Wonjar 4 , Claire M. Mulvey 2 , Andy <lb/>Christoforou 2 , Arnoud Groen 2 , Oliver Kohlbacher 4 , Kathryn S. Lilley 2 , and <lb/>Laurent Gatto *1,2 <lb/>1 Computational Proteomics Unit, Department of Biochemistry, University <lb/>of Cambridge, Tennis Court Road, Cambridge, CB2 1QR, UK <lb/>2 Cambridge Centre for Proteomics, Department of Biochemistry, University <lb/>of Cambridge, Tennis Court Road, Cambridge, CB2 1QR, UK <lb/>3 Computer Laboratory, University of Cambridge, 15 JJ Thomson Avenue, <lb/>Cambridge CB3 0FD, UK <lb/>4 Center for Bioinformatics, Universität Tübingen, Tübingen, Germany <lb/>July 7, 2015 <lb/>* email: lg390@cam.ac.uk <lb/> 1 <lb/> Transfer learning for spatial proteomics <lb/> Abbreviations LOPIT: Localisation of Organelle Proteins by Isotope Tagging, PCP: Pro-<lb/>tein Correlation Profiling, ML: Machine learning, TL: Transfer learning, SVM: Support <lb/>vector machine, PCA: Principal component analysis GO: Gene Ontology CC: Cellular com-<lb/>partment iTRAQ: Isobaric tags for relative and absolute quantitation TMT: Tandem mass <lb/>tags MS: Mass spectrometry <lb/>Running title A Transfer Learning Framework for Spatial Proteomics Data <lb/></front>

			<page>2 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<front>Abstract <lb/>Sub-cellular localisation of proteins is an essential post-translational regulatory <lb/>mechanism that can be assayed using high-throughput mass spectrometry (MS). These <lb/>MS-based spatial proteomics experiments enable to pinpoint the sub-cellular distribu-<lb/>tion of thousands of proteins in a specific system under controlled conditions. Recent <lb/>advances in high-throughput MS methods have yielded a plethora of experimental <lb/>spatial proteomics data for the cell biology community. Yet, there are many third-<lb/>party data sources, such as immunofluorescence microscopy or protein annotations <lb/>and sequences, which represent a rich and vast source of complementary information. <lb/>We present a unique transfer learning classification framework that utilises a nearest <lb/>neighbour or support vector machine system, to integrate heterogeneous data sources <lb/>to considerably improve on the quantity and quality of sub-cellular protein assignment. <lb/>We demonstrate the utility of our algorithms through evaluation of five experimental <lb/>datasets, from four different species in conjunction with three different auxiliary data <lb/>sources to classify proteins to tens of sub-cellular compartments with high generali-<lb/>sation accuracy. We further apply the method to a experiment on pluripotent mouse <lb/>embryonic stem cells to classify a set of previously unknown proteins, and validate our <lb/>findings against a recent high resolution map of the mouse stem cell proteome. The <lb/>methodology is distributed as part of the open-source Bioconductor pRoloc suite for <lb/>spatial proteomics data analysis. <lb/></front>

			<page>3 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>1 Introduction <lb/>Cell biology is currently undergoing a data-driven paradigm [1] shift. As highlighted by [2], <lb/>the experimental tools of molecular biology, imaging and biochemistry enable cell biologists <lb/>to track the complexity of many fundamental processes such as signal transduction, gene <lb/>regulation, protein interactions and sub-cellular localisation. They note that &quot;with the <lb/>culmination of &apos;omic technologies, the molecular and cellular parts lists of cells are known, <lb/>quantifiable, and increasingly readily available in electronic databases. This remarkable <lb/>success at the same time signifies that biology has irreversibly changed to a data rich science.&quot; <lb/>Over the last decade, there has been a dramatic growth in data, both in terms of size and <lb/>heterogeneity. Coupled with this influx of experimental data, databases such as Uniprot [3] <lb/>and the Gene Ontology [4] have become more information rich, providing valuable resources <lb/>for the community. The time is ripe to take advantage of complementary data sources in a <lb/>systematic way to support hypothesis-and data-driven research. Indeed, one of the biggest <lb/>challenges in computational biology is how to meaningfully integrate heterogenous data; <lb/>transfer learning, a paradigm in machine learning, is ideally suited to this task. <lb/>Transfer learning has yet to be fully exploited in computational biology and is still a <lb/>growing field within the machine learning community. To date, various data mining and <lb/>machine learning tools, in particular classification algorithms have been widely applied in <lb/>many areas of biology [5]. A classifier is trained to learn a mapping between a set of observed <lb/>instances and associated external attributes (class labels) which is subsequently used to <lb/>predict the attributes on data with unknown class labels (unlabelled data). In transfer <lb/>learning, one has a primary task which one wishes to solve, and associated primary data <lb/>which is typically expensive, of high quality and targeted to a address a specific question <lb/>about a specific biological system/condition of interest. While standard supervised learning <lb/>algorithms seek to learn a classifier on this data alone, the general idea in transfer learning <lb/>is to complement the primary data by drawing upon a auxiliary data source, from which one <lb/>can extract complementary information to help solve the primary task. The secondary data <lb/>typically contains information that is related to the primary learning objective, but was not <lb/>primarily collected to tackle the specific primary research question at hand. These data can <lb/>be heterogenous to the primary data and are often, but not necessarily, cheaper to obtain <lb/>and more plentiful but with lower signal-to-noise ratio. <lb/>There are several challenges associated with the integration of information from auxiliary <lb/>sources. If the primary and auxiliary sources are combined via straightforward concatenation <lb/>the signal in the primary can be lost through dilution with the auxiliary due to the plentiful <lb/>and often lower signal-to-noise ratio found in the auxiliary for the primary task. Feature <lb/>selection can be used to extract the attributes with the most distinct signals, however the <lb/>challenge still remains in how to combine this data in a meaningful way. Data heterogeneity <lb/>is also a challenge; combining data that exist in different data spaces is often not straightfor-<lb/>ward and different data types can be sensitive to the classifier employed, in terms of classifier <lb/>accuracy. <lb/>In one of the first applications of transfer learning Wu and Dietterich [6] used a k-nearest <lb/>neighbours (k-NN) and support vector machine (SVM) framework for plant image classifi-<lb/>cation. Their primary data consisted of high-resolution images of isolated plant leaves and <lb/>the primary task was to determine the tree species given an isolated leaf. An auxiliary data <lb/></body>

			<page>4 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>source was available in the form of dried leaf samples from a Herbarium. Using a kernel <lb/>derived from the shapes of the leaves and using the auxiliary transfer learning framework de-<lb/>scribed in [6], Wu and Dietterich showed that when primary training data is small, training <lb/>with auxiliary data improves classification accuracy considerably. There were several limita-<lb/>tions in their methods: firstly, the data sources in the k-NN transfer learning (TL) classifier <lb/>could only be weighted by data source and not on a class-by-class basis, and secondly in the <lb/>SVM framework the primary and auxiliary data were expected to have the same cardinality <lb/>and lie in the same data space. Here, we present an adaption and significant improvement <lb/>of this framework and extend the usability of the method by (i) incorporating a multi-class <lb/>weighting schema in the k-NN TL classifier, and (ii) by allowing the integration of primary <lb/>and auxiliary data with different cardinality in the SVM schema to allow the integration <lb/>of heterogenous data types. We apply this framework to the task of protein sub-cellular <lb/>localisation prediction from high resolution mass spectrometry (MS)-based data. While we <lb/>demonstrate algorithmic usage for the spatial proteomics community the framework can be <lb/>applied in many areas of computational biology. <lb/>Spatial proteomics, the systematic large-scale analysis of a cell&apos;s proteins and their assign-<lb/>ment to distinct sub-cellular compartments, is vital for deciphering a protein&apos;s function(s) <lb/>and possible interaction partners. Eukaryotic cells are divided into sub-cellular niches, which <lb/>include organelles and macro-molecular complexes of proteins which represent specialised <lb/>compartments with unique and dedicated functions [7]. Knowledge of where a protein spa-<lb/>tially resides within the cell is covetable to biologists as it not only provides the physiological <lb/>context for their function but also plays an important role in furthering our understanding <lb/>of a protein&apos;s complex molecular interactions e.g. signalling and transport mechanisms, by <lb/>matching certain molecular functions to specific organelles. It has been shown that there <lb/>is a significant correlation between aberrantly localised proteins and many human diseases <lb/>as diverse as Alzheimer&apos;s disease, kidney stones and cancer [8], further highlighting the im-<lb/>portance of protein localisation and the role that spatial proteomics may play in developing <lb/>new platforms for therapeutic intervention. <lb/>There exist a number of sources of information which can be utilised to assign a protein <lb/>to a sub-cellular niche. These range from high quality data produced from experimental <lb/>high-throughput quantitative MS-based methods and imaging data, to freely available data <lb/>from repositories and amino acid sequences. In the field of high-throughput quantitative <lb/>proteomics, many modern experimental designs and multivariate data analysis methods have <lb/>been developed which involve the creation of single or multiple fractions of a cell lysate to <lb/>quantify and identify the protein content of a population of potentially heterogeneous cells <lb/>to permit the assignment of proteins to tens of different sub-cellular niches at the whole <lb/>proteome level [9]. Other approaches consider more global distribution patterns of proteins <lb/>in sub-cellular niches using defined enrichment patterns, for example the Localisation of <lb/>Organelle Proteins by Isotope Tagging (LOPIT) pioneered by Dunkley et al [10] and Protein <lb/>Correlation Profiling (PCP) by Foster et al [11] in 2006. <lb/>These methods involve gentle cell lysis followed by several rounds of differential centrifu-<lb/>gation or gradient-based ultra-centrifugation to separate the cell content as a function of its <lb/>density. Several fractions across the gradient are then collected and their respective pro-<lb/>tein complements are identified and quantified by high resolution MS. Protein distributions <lb/>are then determined by measuring their relative abundance across the fractions employed. <lb/></body>

			<page>5 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>The resulting data from these methods is in the form of a matrix where the rows represent <lb/>proteins and the columns contain the relative abundance of each protein in each fraction <lb/>along the sub-cellular fractionation gradient. Proteins with similar organelle residency will <lb/>share similar distribution profiles characteristic of the sub-cellular compartment with which <lb/>they are associated [12]. These approaches have been heavily utilised to gain information <lb/>about the sub-cellular location of proteins in numerous species, for example Arabidopsis [10, <lb/>13, 14, 15, 16, 17], Drosophila [18], yeast [19], human cell lines [20, 21], mouse [11, 22] and <lb/>chicken [23]. Such analyses has resulted in large-scale data sets to enable the simultaneous <lb/>assignment of thousands of proteins to multiple sub-cellular locations. <lb/>Based on the distribution of a set of known genuine organelle residents, termed marker <lb/>proteins, pattern recognition and machine learning (ML) methods can be used to match and <lb/>associate the distributions of unknown residents to that of one of the markers. Traditional <lb/>spatial proteomics relies extensively on reliable organelle markers and multivariate statistical <lb/>and supervised ML methods for high-throughput reliable proteome-wide localisation predic-<lb/>tion [24]. To date, classification had been tackled using a number of popular supervised <lb/>ML algorithms, for example, support vector machines (SVMs) [25], the k-nearest neighbours <lb/>(k-NN) algorithm [16], random forest [26], naive Bayes [15], neural networks [27], and other <lb/>classic multivariate statistical methods such as partial-least squares discriminant analysis <lb/>[10], [18], [23], and the χ 2 metric [20, 11]. <lb/>Computational development applied to MS-based protein-organelle association are a re-<lb/>cent development, but the computational determination of protein localisation using in silico <lb/>data is an established bioinformatics challenge (reviewed in [28, 29, 30]). Many methods have <lb/>been developed to predict protein localisation from amino acid sequence features e.g. amino-<lb/>acid composition information (e.g. [31, 32, 33, 34, 35, 36, 37]), localisation signals and motifs <lb/>relevant to protein sorting (e.g. [38, 39, 40, 41, 42, 43, 44, 45]). Annotation-based prediction <lb/>methods have also been widely used that use information about functional domains (e.g. [46, <lb/>47]), protein-protein interaction (e.g. [48, 49, 50]) and Gene Ontology (GO) [4] terms (e.g. <lb/>[51, 52, 53, 54]). Although not all proteins in GO are reliably annotated, for example, ac-<lb/>cording to the 2015 03 release of UniProtKB [3] the human, mouse, Drosophila melanogaster <lb/>and Arabidopsis thaliana proteomes have less than 14%, 14%, 6% and 13% experimentally-<lb/>verified GO CC sub-cellular annotations, in each proteome respectively, these data cover the <lb/>entire proteome of the organism. <lb/>Despite improvements in generalisation accuracy of sequence-based classifiers, a fun-<lb/>damental problem concerns the biological relevance and ultimate utility to cell biology of <lb/>sequence-based prediction. Annotated sequence does not change according to cellular con-<lb/>dition or cell type, whereas protein localisation does. Furthermore, this type of data does <lb/>not adequately describe the range of mechanisms via which a particular protein may reside <lb/>in a particular organelle. Not all protein sequences contain motifs or exhibit compositional <lb/>properties indicative of organelle residency. This considerable body of ML research into the <lb/>prediction of protein-organelle association from annotated protein sequence has yet to be <lb/>exploited within organelle proteomics experiments. Despite the inherent limitations of us-<lb/>ing in silico data to predict dynamic cell-and condition-specific protein properties, transfer <lb/>learning [6, 55] may allow the transfer of complementary information available from these <lb/>data to classify proteins in experimental proteomics datasets. Transfer learning has been <lb/>used to predict sub-cellular localisation from in silico data sources such as GO terms and <lb/></body>

			<page>6 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>Chou&apos;s pseudo amino-acid composition [52, 53, 54], but no framework has yet been developed <lb/>to allow the integration of experimental data and third-party sources. It is well documented <lb/>that training ML models on multiple related data sources can lead to higher generalisation <lb/>accuracies than those obtained on each data set individually [25, 56, 57, 58]. Other data <lb/>sources include protein-protein interaction partners (which must share sub-cellular localisa-<lb/>tion in order to interact), wider annotation, and imaging data, for example data available <lb/>from such projects as the Human Protein Atlas [59]. <lb/>Here, we present a new transfer learning framework, inspired by Wu and Dietterich&apos;s <lb/>classic inductive transfer learning framework [6]. The primary task is protein localisation <lb/>prediction from MS-based quantitative proteomics datasets, and we exploit a secondary aux-<lb/>iliary data source to improve classification. We use, among others, Gene Ontology Cellular <lb/>Compartment (GO CC) terms as an auxiliary data source, to improve upon the classification <lb/>of experimental and condition-specific sub-cellular localisation predictions from MS-based <lb/>quantitative proteomics data in an organelle specific manner. Using the k-Nearest Neigh-<lb/>bour (k-NN) and support vector machine (SVM) algorithms in a transfer learning framework <lb/>we find that when given data from a high quality MS experiment, integrating data from a <lb/>second less information rich but more plentiful auxiliary data source directly in to classifier <lb/>training and classifier creation results in the assignment of proteins to organelles with high <lb/>generalisation accuracy. Five experimental MS LOPIT datasets, from four different species, <lb/>were employed in testing the classifiers. We further show the flexible of the pipeline through <lb/>testing two other auxiliary data sources; (1) tagging-based sub-cellular imaging data [59], and <lb/>(2) sequence and annotation features (see table 1) obtained from a correlation-based feature <lb/>selection [60] on the input features used for the classifier YLoc [61, 62]. The results obtained <lb/>demonstrate that this transfer learning method outperforms a single classifier trained on each <lb/>single data source alone and on an class-by-class basis, highlighting that the primary data <lb/>is not diluted by the auxiliary data. A new transfer learning framework for the integration <lb/>of heterogeneous data sources in proposed. This methodology forms part the open-source <lb/>open-development Bioconductor [63] pRoloc [64] suite of computational methods available <lb/>for organelle proteomics data analysis. <lb/>2 Materials and methods <lb/>2.1 Data sources <lb/>2.1.1 Primary data <lb/>Five datasets, from studies on Arabidopsis thaliana [10, 16], Drosophila embryos [18], human <lb/>embryonic kidney fibroblast (HEK293T) [21], and mouse pluripotent embryonic stem cells <lb/>(E14TG2a) (unpublished) were collected using the standard LOPIT approach as described <lb/>by Sadowski et al. [13]. In the LOPIT protocol, organelles and large protein complexes are <lb/>separated by iodixanol density gradient ultracentrifugation. Proteins from a set of enriched <lb/>sub-cellular fractions are then digested and labelled separately with iTRAQ or TMT reagents, <lb/>pooled, and the relative abundance of the peptides in the different fractions is measured by <lb/>tandem MS. The number of measurements obtained per gradient occupancy profile (which <lb/>comprises of a set of isotope abundance measurements) is thus dependent on the reagents <lb/></body>

			<page>7 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>and LOPIT methodology used. <lb/>The first Arabidopsis thaliana dataset [10] on callus cultures employed dual use of four <lb/>isotopes across eight fractions and thus yielding 8 values per protein profiles. The aim of this <lb/>experiment was to resolve Golgi membrane proteins from other organelles. Gradient-based <lb/>separation was used to facilitate this, including separating and discarding as much nuclear <lb/>material as possible during a pre-centrifugation step, and carbonate washing of membrane <lb/>fractions to remove peripherally associated proteins, thereby maximising the likelihood of <lb/>assaying less abundant integral membrane proteins from organelles involved in the secretory <lb/>pathway. <lb/>The second Arabidopsis thaliana dataset on whole roots is one of the replicates published <lb/>by Groen et al. [16], which was set up to identify new markers of the trans-Golgi network <lb/>(TGN). The TGN is an important protein trafficking hub where proteins from the Golgi <lb/>are transported to and from the plasma membrane and the vacuole. The dynamics of this <lb/>organelle are therefore complex which makes it a challenge to identify true residents of this <lb/>organelle. For each replicate, sucrose gradient fractions were subjected to a carbonate wash <lb/>to enrich for membrane proteins and four fractions were iTRAQ labelled. Following MS <lb/>the resultant iTRAQ reporter ion intensities for the four fractions were normalised to six <lb/>ratios and then each proteins abundance was further normalised across its six ratios by sum. <lb/>In Groen&apos;s original experiment the iTRAQ quantitation information for common proteins <lb/>between the three different gradient were concatenated to increase the resolution of the TGN <lb/>[25]. <lb/>The aim of the Drosophila experiment [18] was to apply LOPIT to an organism with het-<lb/>erogeneous cell types. Tan et al. were particularly interested in capturing the plasma mem-<lb/>brane proteome (personal communication). There was a pre-centrifugation step to deplete <lb/>nuclei, but no carbonate washing, thus peripheral and luminal proteins were not removed. <lb/>In this experiment four isotopes across four distinct fractions were implemented and thus <lb/>yield four measurements (features) per protein profile. <lb/>The human dataset [65, 21] was a proof-of-concept for the use of LOPIT with adherent <lb/>mammalian cell culture. Human embryonic kidney fibroblast cells were used and LOPIT was <lb/>employed with 8-plex iTRAQ reagents, thus returning eight values per protein profile within <lb/>a single labelling experiment. As in the LOPIT experiments in Arabidopsis and Drosophila, <lb/>the aim was to resolve the multiple sub-cellular niches of post-nuclear membranes, and also <lb/>the soluble cytosolic protein pool. Nuclei were discarded at an early stage in fractionation <lb/>scheme as previously described, and membranes were not carbonate washed in order to retain <lb/>peripheral membrane and lumenal proteins for analysis. <lb/>The E14TG2a embryonic mouse dataset (unpublished) also employed iTRAQ 8-plex la-<lb/>belling, with the aim of cataloguing protein localisation in pluripotent stem cells cultured <lb/>under conditions favouring self-renewal. In order to achieve maximal coverage of sub-cellular <lb/>compartments, fractions enriched in nuclei and cytosol were included in the iTRAQ labelling <lb/>scheme, along with other organelles and large protein complexes as for the previously de-<lb/>scribed datasets. No carbonate wash was performed. <lb/>All datasets are freely distributed as part of the Bioconductor [63] pRolocdata data <lb/>package [64]. <lb/></body>

			<page>8 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>2.1.2 Auxiliary data <lb/>The Gene Ontology (GO) project provides controlled structured vocabulary for the descrip-<lb/>tion of biological processes, cellular compartments and molecular functions of gene and gene <lb/>products across species [4]. For each protein seen in every LOPIT experiment the protein&apos;s <lb/>associated Gene Ontology (GO) cellular component (CC) namespace terms were retrieved <lb/>using the pRoloc package [64]. Given all possible GO CC terms associated to the proteins <lb/>in the experiment we constructed a binary matrix representing the presence/absence of a <lb/>given term for each protein, for each experiment. <lb/>YLoc [61, 62] is an interpretable web sever developed by Briesemeister and co-workers for <lb/>the prediction of protein sub-cellular localisation. The YLoc classifier uses features derived <lb/>from numerous data sources from both sequence and annotation. A summary of the features <lb/>included in the YLoc classifier are shown in Table 1. These features provide a source of <lb/>complementary auxiliary data for the high quality MS based datasets described in 2.1.1. <lb/>To use these features as an auxiliary source of information, a large-scale correlation-based <lb/>feature selection (CFS) approach [60], as described in [61, 62], was used with the markers <lb/>from the E14TG2a mouse dataset to find the set of the most important features. <lb/>Sequence derived <lb/>Annotation based <lb/>Amino acid sequence <lb/>PROSITE patterns [66] <lb/>e.g. amino-acid composition (AAC), <lb/>Gene Ontology Terms <lb/>pseudo-and normalised-AAC [32] <lb/>e.g. cellular compartment namespace <lb/>Physiochemical properties <lb/>terms from close homologues <lb/>e.g. hydrophobic, positively/negatively <lb/>charged, aromatic, small etc. <lb/>Autocorrelation features <lb/>e.g. autocorrelation of properties such <lb/>as charge, volume etc. <lb/>Sorting signals <lb/>e.g. mono nuclear localisation signal, <lb/>nuclear export signal, secretory <lb/>pathways etc. <lb/>Table 1: A summary of the types of features considered in training and building Briesemeister et al&apos;s YLoc <lb/>classifier. <lb/>The Human Protein Atlas [67] (version 13, released on 11/06/2014) was used as an auxil-<lb/>iary source of information to complement the LOPIT human HEK293 data. The sub-cellular <lb/>atlas provides protein expression patterns on a sub-cellular level using immunofluorescently <lb/>staining for human U-2 OS cells. We used the hpar Bioconductor package [68] to query the <lb/>atlas. The data was encoded as a binary matrix describing the localisation of 670 proteins <lb/>in 18 sub-cellular localisations that have been supportively identified. <lb/>The definition of primary and auxiliary is not set algorithmically, by the quality or the <lb/>size of the data but rather by the data and question at had. For example, here LOPIT was <lb/>considered the primary data because it represented the experiment of interest that was to <lb/>be complemented by the imaging data. In fact, from an algorithm point of view, primary <lb/>and auxiliary are reciprocal. <lb/></body>

			<page>9 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>2.1.3 Markers <lb/>Spatial proteomics relies extensively on reliable sub-cellular protein markers to infer proteome <lb/>wide localisation. Markers are proteins that are defined as reliable residents and can be used <lb/>as reference points to identify new members of that sub-cellular niche. Here, marker proteins <lb/>are selected by domain experts through careful mining of the literature. Markers for each <lb/>LOPIT experiment were specific to the system under study and conditions of interest and <lb/>are distributed as part of the Bioconductor [63] pRoloc package [64]. <lb/>2.2 Incorporating auxiliary data <lb/>2.2.1 Notation <lb/>The primary MS-based experimental datasets P consist of multivariate protein profiles. The <lb/>auxiliary data A is a presence/absence binary matrix of Gene Ontology Cellular Compart-<lb/>ment (GO CC) terms. Data are annotated to either (i) a single known organelle (labelled <lb/>data), or (ii) have unknown localisation (unlabelled data). Thus we split P and A into <lb/>labelled (L) and unlabelled (U ) sections such that P = (L P , U P ) and A = (L A , U A ). <lb/>The labelled examples for P and A are represented by L P = {(x l , y l )|l = 1, ..., |L P |} <lb/>where x l ∈ R S , and L A = {(v l , y l )|l = 1, ..., |L A |} where v l ∈ R T . Thus each l th protein <lb/>is described by vectors of S and T features (generally, S &lt;&lt; T ), for P and A respectively. <lb/>Each dataset shares a common set of proteins that is annotated to one of the same y l ∈ C = <lb/>{1, ..., |C|} sub-cellular classes, where |C| ∈ N is the total number of sub-cellular classes. <lb/>Unlabelled data, U P and U A are represented by U P = {x u |u = 1, ..., |U P |} where x u ∈ R S <lb/>and U A = {v u |u = 1, ..., |U A |} where v u ∈ R T , respectively. <lb/>The labelled data for the i th organelle class, with N i indicating the number of proteins <lb/>for the i th organelle class, is given for P by g P <lb/>i = {(x, y) ∈ L P |y = i} and for A by <lb/>g A <lb/>i = {(v, y) ∈ L A |y = i}. The labelled dataset of all available proteins over the |C| different <lb/>sub-cellular classes is given for P by L P = |C| <lb/>i=1 g P <lb/>i and for A by L A = |C| <lb/>i=1 g A <lb/>i . <lb/>2.2.2 Transfer learning using a k-nearest neighbours framework <lb/>We adapt Wu and Dietterich&apos;s [6] classic application of inductive transfer using experimental <lb/>quantitative proteomics data as the primary source (P ) and GO CC terms as the auxiliary <lb/>source (A). We aim to exploit auxiliary data to improve upon the sub-cellular classification <lb/>of proteins found in MS-based LOPIT experiments in an organelle specific way, using the <lb/>baseline k-nearest neighbours (k-NN) algorithm in a transfer learning framework. <lb/>In k-NN classification, an unknown example is classified by a majority vote of its labelled <lb/>neighbours, with the example being assigned to the class most common among its k nearest <lb/>neighbours. Independent of the transfer learning classifier we compute the best k for each <lb/>data source for values k ∈ {3, 5, 7, 9, 11, 13, 15} through an initial 100 rounds of 5-fold cross-<lb/>validation using each set of labelled training data for P and then independently for A (as <lb/>implemented in pRoloc). We denote by k P the best k for P , and by k A the best k for A. <lb/>Having obtained the best k for each data source, the transfer learning algorithm works <lb/>as follows. For the u th protein (x u , v u ) we wish to classify in U , we start by finding the k P <lb/>and k A labelled nearest neighbours for x u and v u in L P and L A , respectively. Denote these <lb/></body>

			<page>10 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>sets N P <lb/>u and N A <lb/>u . We then define the vectors p T <lb/>u = (p u <lb/>1 , . . . , p u <lb/>|C| ) and q T <lb/>u = (q u <lb/>1 , . . . , q u <lb/>|C| ) to <lb/>contain counts for each class in the sets of nearest neighbours; that is, <lb/>p u <lb/>i = |{(x, y) ∈ N P <lb/>u |y = i}| <lb/>q u <lb/>i = |{(v, y) ∈ N A <lb/>u |y = i}|. <lb/>For each protein, let pu = p u /k P and qu = q u /k A be normalized vectors with elements sum-<lb/>ming to 1 and representing the distribution of classes among the sets of nearest neighbours <lb/>for each protein. Finally, let NN P = {p u |u = 1, ..., |U P |} and NN A = {q u |u = 1, ..., |U P |}. <lb/>To include both the primary and auxiliary data in the set of potential neighbours we <lb/>took a weighted combination of the votes in NN P and NN A for each sub-cellular class. Class <lb/>weights are defined by the parameter vector θ T = (θ 1 , . . . , θ |C| ) with values θ i ∈ {0, 1 <lb/>3 , 2 <lb/>3 , 1} <lb/>chosen by optimisation through a prior 100 independent rounds of 5-fold cross-validation on <lb/>a separate training partition of the labelled data. For the u th unknown protein (x u , v u ) in <lb/>U , the voting scores for each class i ∈ C are calculated as <lb/>V (i) = θ i <lb/>pu <lb/>i + (1 − θ i )q u <lb/>i <lb/>(1) <lb/>and the protein is assigned to the class c ∈ C maximizing V (i) <lb/>c = arg max <lb/>i <lb/>V (i). <lb/>The class weights θ i in equation 1 control the relative importance of the two types of neigh-<lb/>bours for each class i ∈ C. This differs from Wu and Dietterich&apos;s [6] original approach as <lb/>they only weight the data sources and not the classes and the data sources. In this paper <lb/>we select each class weight θ i from the set {0, 1 <lb/>3 , 2 <lb/>3 , 1}; however, the algorithm allows us to <lb/>use any real-valued θ i ∈ [0, 1]. If θ i = 1, then all weight is given to the primary data in class <lb/>i and only primary nearest neighbours in class i are considered. Similarly, if θ i = 0, then <lb/>all weight is given to the auxiliary data in class i and only auxiliary nearest neighbours in <lb/>class i are considered. If 0 &lt; θ i &lt; 1 then a combination of neighbours in the primary and <lb/>auxiliary data sources is considered. <lb/>2.2.3 Transfer learning using a SVM framework <lb/>Linear programming SVMs <lb/>The method is based on the use of the linear programming formulation of the SVM (lpSVM). <lb/>This formulation promotes classifiers that are sparse, in the sense that where possible only <lb/>a few parameters obtained through training are non-zero; for a detailed introduction see <lb/>Mangasarian [69]. <lb/>We begin by describing the standard lpSVM used for classical two-class classification <lb/>problems with a single labelled training set. We use the multiple-class version of this ap-<lb/>proach with the individual primary and auxiliary sets P and A as a comparison later in <lb/>the paper; we present the method here assuming that the primary set P is being used <lb/>and can be set up as a binary classification problem; for example, we might wish to pre-<lb/>dict whether or not a protein should be assigned to a single specified sub-cellular class. <lb/></body>

			<page>11 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>For binary classification problems with class labels y ∈ {+1, −1}, and given labelled data <lb/>L P = {(x l , y l )|l = 1, . . . , m} where m = |L P | the classifier takes the form <lb/>h(x) = <lb/>+1 if f (x; α P , b) ≥ 0 <lb/>−1 otherwise <lb/>(2) <lb/>where f is the latent function <lb/>f (x; α P , b) = <lb/>m <lb/>l=1 <lb/>y l α P <lb/>l K P (x l , x) + b. <lb/>Here, K P is a kernel (Shawe-Taylor and Cristianini [70]) associated with the primary data <lb/>and α T <lb/>P = (α P <lb/>1 , . . . , α P <lb/>m ) and b are parameters determined by training. <lb/>For any vector x T = (x 1 , . . . , x n ) let |.| 1 denote the 1-norm <lb/>|x| 1 = <lb/>n <lb/>i=1 <lb/>|x i |. <lb/>The training algorithm requires that we solve the linear programme <lb/>min <lb/>α P ,ξ,b <lb/>|α P | 1 + C|ξ| 1 <lb/>(3) <lb/>such that for each i = 1, . . . , m <lb/>y i f (x i ; α P , b) + ξ i ≥ 1 <lb/>and α P , ξ ≥ 0. 1 The parameters ξ and C act in the same way as the corresponding pa-<lb/>rameters in the standard SVM: ξ contains the slack variables allowing some examples to be <lb/>misclassified, and C controls the extent to which such misclassifications are penalized during <lb/>training. <lb/>Transfer learning for binary classification <lb/>Once again we adapt the method of Wu and Dietterich [6] to our problem. The original <lb/>method requires adaptation as it is designed for data having two important differences com-<lb/>pared with ours. First, it does not require examples in the labelled data sets L P and L A to be <lb/>in correspondence and for corresponding training examples to share the same label. Second it <lb/>assumes that P and A share the same number of features. While the first of these differences <lb/>is easily dealt with as our data is a special case that is already covered, the second is more <lb/>problematic. If we now introduce the labelled auxilliary data L A = {(v l , y l )|l = 1, . . . , m} a <lb/>direct application of the approach in [6] requires us to evaluate kernels of the form K(x, v). <lb/>As P and A contain data with different numbers of features this presents a problem for any <lb/>SVM-type method, as kernels are usually required to satisfy the Mercer conditions (Mer-<lb/>cer [71]), one of which is that they are symmetric, such that K(x, x ) = K(x , x). While <lb/></body>

			<note place="footnote">1 Note that it is possible for the linear programme to have no solution, although we found this to be <lb/>extremely rare. When this was the case the classifier reverted to predicting the most common class in the <lb/>labelled data. <lb/></note>

			<page>12 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>research on the use of asymmetric kernels has appeared-see for example [72]-even if we <lb/>relax this requirement a kernel is essentially a measure of the similarity of its arguments, <lb/>and the question arises of how one might sensibly measure the similarity of a protein profile <lb/>with a presence/absence vector of GO CC terms. This problem does not arise with Wu and <lb/>Dietterich&apos;s data as the two sets they use have the same dimension and are derived in a way <lb/>that makes measuring similarity straightforward. <lb/>We therefore simplify the original method as follows. We maintain the machinary em-<lb/>ployed above for the primary data, and introduce a separate kernel K A and parameter vector <lb/>α A for the auxilliary data. A vector to be classified now contains both a protein profile x <lb/>and a GO vector v. The latent function becomes <lb/>f (x, v; α P , α A , b) = <lb/>m <lb/>l=1 <lb/>y l α P <lb/>l K P (x l , x) + α A <lb/>l K A (v l , v) + b <lb/>and training requires us to solve the linear program <lb/>min <lb/>α P ,α A ,ξ,b <lb/>|α P | 1 + |α A | 1 + C|ξ| 1 <lb/>(4) <lb/>such that for each i = 1, . . . , m <lb/>y i f (x i , v i ; α P , α A , b) + ξ i ≥ 1 <lb/>and α P , α A , ξ ≥ 0. <lb/>Note that this differs from the method of Multiple Kernel Learning (MKL) (Lanckriet <lb/>et al. [73], Gönen and Alpaydin [74]) in that in MKL the single kernel K is replaced in the <lb/>usual SVM formulation by a weighted sum of kernels <lb/>K(x 1 , x 2 ) = <lb/>D <lb/>i=1 <lb/>d i K i (x 1 , x 2 ) <lb/>where d i ≥ 0 and <lb/>D <lb/>i=1 d i = 1. The d i are then included with α and b in a more in-<lb/>volved constrained optimisation problem. Our approach has the advantages that it remains <lb/>a straightforward linear program and in fact introduces fewer constraints on the form of the <lb/>latent function f . <lb/>Throughout our experiments we used for K P and K A the Gaussian kernel <lb/>K(x 1 , x 2 ) = exp(−γ||x 1 − x 2 || 2 ) <lb/>where ||.|| denotes the 2-norm ||x|| = ( i x 2 <lb/>i ) <lb/>1/2 . We optimized over the value of C, and <lb/>also separate values γ P and γ A for the two kernels as described below, with C in the range <lb/>{0.125, 0.25, 0.5, 1, 2, 4, 8, 16} and γ P , γ A in the range {0.01, 0.1, 1, 10, 100, 1000}. <lb/>Multiple classes, class imbalance and probabilistic outputs <lb/>As a baseline comparison in our experiments we used a standard SVM as implemented in <lb/>the package LIBSVM (Chang and Lin [75]). In extending our transfer learning technique <lb/></body>

			<page>13 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>to deal with multiple classes and probabilistic outputs we therefore maintained as close a <lb/>similarity as possible to the methods used by that library. <lb/>SVMs and lpSVMs are in their basic form inherently binary classifiers. In order to <lb/>address multiple-class problems using non-probabilistic outputs such as the one presented <lb/>here we use the method of Knerr et al. [76]. We train a binary classifier to separate each <lb/>pair of classes. In order to classify a new example we then take a vote among these binary <lb/>classifiers, assigning the example to the class with the most votes. <lb/>As we typically have several sub-cellular classes the binary classification problems used <lb/>in constructing the multiple-class classifier are inherently unbalanced. We adjust for this <lb/>using the method of Morik et al. [77]. In each binary problem let n + denote the number <lb/>of positive examples and n − the number of negative examples. In the linear programme <lb/>objective functions (equations 3 and 4) we replace the single value for C with the adjusted <lb/>values <lb/>C + = C n − /n + <lb/>C − = C n + /n − <lb/>for the positive and negative examples respectively. Let S + denote the set of indices of the <lb/>positive examples and S − the set of indices for the negative examples. The term C|ξ| 1 in <lb/>equations 3 and 4 becomes <lb/>C + <lb/>i∈S + <lb/>|ξ i | + C − <lb/>i∈S − <lb/>|ξ i |. <lb/>Finally, we prefer to employ probabilistic outputs rather than simply thresholding as in <lb/>equation 2. Once again we employ the same techniques as LIBSVM. The method for binary <lb/>classifiers is presented by Platt [78] and Lin et al. [79], and for multiple-class classifiers by <lb/>Wu et al. [6]. <lb/>2.2.4 Assessing classifier generalisation accuracy <lb/>In order to evaluate the generalisation accuracy of each transfer learning classifier we em-<lb/>ployed the following schema in all experiments. A set of LOPIT profiles labelled with known <lb/>markers, and their counterpart auxiliary GO CC profiles, were separated at random into <lb/>training (80%) and test (20%) partitions. The split was stratified, such that the relative <lb/>proportions of each class in each of the two sets matched that of the complete set of data. <lb/>The test profiles were withheld from classifier training and employed to test the generali-<lb/>sation accuracy of the trained classifiers. On each 80% training partition 5-fold stratified <lb/>cross-validation was conducted to test all free parameters via a grid search and select the <lb/>best set of parameters for each classifier. In each experiment, for each dataset, this process <lb/>of 80/20% stratified splitting, training with 5-fold stratified cross-validation on the 80% and <lb/>testing on the 20% was repeated 100 times in order to produce 100 sets of macro F1 scores <lb/>and class-specific F1 scores. The F1 score (He [80]) is a well-known common measure used <lb/>to assess classifier performance. It is the harmonic mean of precision and recall, where <lb/>precision = <lb/>tp <lb/>tp + fp <lb/>, recall = <lb/>tp <lb/>tp + fn <lb/></body>

			<page>14 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>and tp denotes the number of true positives, fp the number of false positives, and fn the <lb/>number of false negatives. Thus <lb/>F1 = 2 × <lb/>precision × recall <lb/>precision + recall <lb/>. <lb/>A high macro F1 score indicates that the marker proteins in the test data set are consis-<lb/>tently correctly assigned by the algorithm. <lb/>To assess whether incorporating an auxiliary data source into classifier training and <lb/>classifier creation was better than using primary or auxiliary data alone, we conducted three <lb/>independent experiments for each data source and for each transfer learning method. We <lb/>used the above schema to assess the generalisation accuracy of using (1) the transfer learning <lb/>k-Nearest Neighbours (k-NN) classifier, (2) the primary LOPIT data alone, using a baseline <lb/>k-NN, (3) the auxiliary GO CC data alone, using a baseline k-NN. We repeated this for the <lb/>lpSVM transfer learning classifier and used a standard SVM with an RBF kernel for single <lb/>data source experiments. Using these experiments we were able to compare using a simple <lb/>k-NN versus the transfer learning k-NN, and also the use of a standard SVM versus the <lb/>combined transfer learning lpSVM approach. <lb/>A two-sample two-tailed t-test, assuming unequal variance, was used to assess whether <lb/>over the 100 test partitions, the estimated generalisation performance using the optimised <lb/>class-specific fusion approach was better than using either primary data alone, or auxiliary <lb/>data alone. A threshold of 0.01 was used in all t-tests to determine significance. <lb/>3 Results and Discussion <lb/>Here, we have adapted Wu and Dietterich&apos;s [6] classic application of inductive transfer learn-<lb/>ing using experimental quantitative proteomics data as the primary source and Gene Ontol-<lb/>ogy Cellular Compartment (GO CC) terms as the auxiliary source. In this framework, we <lb/>exploit auxiliary data to improve upon the protein localisation prediction from quantitative <lb/>MS-based spatial proteomics experiments using (1) a class-weighted k-nearest neighbours <lb/>(k-NN) classifier, and (2) a Support Vector Machine in a transfer learning framework. We <lb/>also show the flexibility of the framework by using data from the Human Protein Atlas [67] <lb/>and input sequence and annotation features from the YLoc [61, 62] web server as auxiliary <lb/>data sources. <lb/>3.1 Transfer learning with k-NN and SVMs <lb/>To assess classifier performance we employed the classic machine learning schema of parti-<lb/>tioning our labelled data in to training and testing sets, and used the testing sets to assess <lb/>the strength of our classifiers. In this setup the training partition is used to optimise the free <lb/>parameters of the classifier. Here, for the k-NN transfer learning algorithm these parame-<lb/>ters are the weights assigned to each class for each data source, and for the Support Vector <lb/>Machine (SVM) transfer learning algorithm these are C, γ P and γ A for the two kernels as <lb/>described in section 2.2.3. The testing set is then used to assess the generalisation accuracy <lb/></body>

			<page>15 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>of the classifier. By applying the best parameters found in the training phase on test data, <lb/>observed and expected classification results can be compared, and then used to assess how <lb/>well a given model works by getting an estimate of the classifiers ability to achieve a good <lb/>generalisation i.e. that is given an unknown example predict its class label with high ac-<lb/>curacy. We applied this schema to the five LOPIT datasets and calculated the the macro-<lb/>and class-F1 scores on the test partitions of each dataset. For simplicity, throughout this <lb/>manuscript we refer to the mouse pluripotent embryonic stem cell (E14TG2a) dataset as the <lb/>&apos;mouse dataset&apos;, the human embryonic kidney fibroblast (HEK293T) dataset as the &apos;human <lb/>dataset&apos;, the Drosophila embryos dataset as the &apos;fly dataset&apos;, the Arabidopsis thaliana callus <lb/>dataset as the &apos;callus dataset&apos; and finally the second Arabidopsis thaliana roots dataset, as <lb/>the &apos;roots dataset&apos;. <lb/>3.1.1 The k-NN transfer learning classifier <lb/>The median macro-F1 scores for the mouse, human, callus, roots and fly datasets were 0.879, <lb/>0.853, 0.863, 0.979, 0.965, respectively, for the combined k-NN transfer learning approach. <lb/>A two sample t-test showed that over 100 test partitions, the mean estimated generalisation <lb/>performance for the k-NN transfer learning approach was significantly higher than on profiles <lb/>trained solely from only primary or auxiliary alone for the mouse (p = 2.283e −21 for primary <lb/>alone and p = 6.926e −78 for auxiliary alone), human (p = 1.119e −7 for primary alone and <lb/>p = 8.104e −32 for auxiliary alone), callus roots (p = 3.761e −17 and p = 3.807e −22 ), and fly <lb/>(p = 2.618e −5 for primary alone, p = 1.379e −112 for auxiliary alone) data (Figure 1). <lb/>We found that the callus datatset on the full Arabidopsis thaliana proteome did not <lb/>significantly benefit (neither fall detriment) to the incorporation of auxiliary data. This was <lb/>unsurprising as this dataset is extremely well-resolved in LOPIT (Supporting Figure 1, top <lb/>right) and the median macro F1-score over 100 rounds of training and testing with a baseline <lb/>k-NN classifier resulted in a median macro F1-score of 0.985 (the combined approach yielded <lb/>a macro F1-score of 0.973). <lb/>q <lb/>q <lb/>q q <lb/>q <lb/>q <lb/>q <lb/>q q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>q q <lb/>q <lb/>q <lb/>Mouse <lb/>Human <lb/>Callus <lb/>Roots <lb/>Fly <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Data source <lb/>Macro F1 Score <lb/>Figure 1: Boxplots, displaying the estimated generalisation performance over 100 test partitions for the k-NN <lb/>transfer learning algorithm applied with (i) optimised class-specific weights (combined), (ii) only primary <lb/>data and (iii) only auxiliary data, for each dataset. <lb/>The k-NN transfer learning classifier uses optimised class weights to control the pro-<lb/>portion of primary to auxiliary neighbours to use in classification. One advantage of this <lb/>approach is the ability for the user to set class weights manually, allowing complete control <lb/></body>

			<page>16 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>over the amount of auxiliary data to incorporate. As previously described, the class weights <lb/>can be set through prior optimisation on the labelled training data. Figure 2 shows the de-<lb/>tailed results for the mouse dataset and the distribution of the 100 best weights selected over <lb/>100 rounds of optimisation are shown on the top left. We found the distribution of weights <lb/>in each dataset reflected closely the sub-cellular resolution in each experiment. For example, <lb/>in the E14TG2a mouse experiments the distribution of best weights identified for the endo-<lb/>plasmic reticulum (ER), mitochondria and chromatin niches are heavily skewed towards 1 <lb/>indicating that the proportion of neighbours to use in classification should be predominantly <lb/>primary. Note, as described in section 2.2.2 if the class weight is assigned to 1, then strictly <lb/>only neighbours in primary data are used in classification and similarly, if the class weight is <lb/>0 then all weight is given to the auxiliary data. If the weight falls between these two limits <lb/>the neighbours in both the primary and auxiliary data sources is considered. From examin-<lb/>ing the principal components analysis plot (PCA) (Figure 2, top right) we indeed found that <lb/>these organelles are well separated in the LOPIT experiment. Conversely, we found the that <lb/>the 40S ribosome overlaps somewhat with the nucleolus cluster (Figure 2, top right) which <lb/>is reflected in the best choice of class weights for these two niches; they are both assigned <lb/>best weights of 1/3 and their distribution of best weights is skewed towards 0 indicating <lb/>that more auxiliary data should be used to classify these sub-cellular classes. If we further <lb/>examine the class-F1 scores for these two sub-cellular niches (Figure 2, bottom) we indeed <lb/>find that including the auxiliary data in classification yields a significant improvement in <lb/>generalisation accuracy (p = 1.122e−16 for 40S ribosome (red) and p = 1.258e −10 , nucleolus <lb/>(pink)). We also found this to be the case for the proteasome, which is overlapping with <lb/>the cytosol. Biologically, this is expected as the proteasome is known to be localised in the <lb/>cytosol in this cell line. We found LOPIT alone did not distinguish between these two sub-<lb/>cellular niches in this particular experiment, however, the addition of auxiliary data from <lb/>the Gene Ontology resulted in a significant increase in classifier prediction (p = 2.108e −16 ) <lb/>as shown by the class-specific box plot in Figure 2, bottom (black). In this framework we are <lb/>able to resolve different niches in the data according to different data sources, as highlighted <lb/>in the class-specific box-plots in Supporting Figures 1 to 4. <lb/>Many experiments are specifically targeted towards resolving a particular organelle of <lb/>interest (e.g. the TGN in the roots dataset) which requires careful optimisation of the <lb/>LOPIT gradient. In such a setup sub-cellular niches other than the one of interest may not <lb/>be well-resolved which may simply be due to the fact that the gradient was not optimised <lb/>for maximal separation of all sub-cellular niches, but only one or a few particular organelles. <lb/>Such experiments in particular may benefit from the incorporation of auxiliary data. We <lb/>found that for the roots dataset all sub-cellular classes, except the TGN sub-compartment, <lb/>benefitted from including auxiliary data (Supporting Figure 3, bottom), highlighting the <lb/>advantage of using more than one source of information for sub-cellular protein classification. <lb/>The best weight for the TGN was found to be 1 (Supporting Figure 3, top left), as expected <lb/>and indicating high resolution in LOPIT for this class. <lb/>3.1.2 The SVM transfer learning classifier <lb/>Adapting Wu and Dietterich&apos;s classic application of transfer learning [6] we have implemented <lb/>a SVM transfer learning classifier that allows the incorporation of a second auxiliary data <lb/></body>

			<page>17 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>−2 <lb/>0 <lb/>2 <lb/>4 <lb/>−2 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>PC1 (40.28%) <lb/>PC2 (25.7%) <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• • <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>•• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• • <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>• <lb/>40S Ribosome <lb/>60S Ribosome <lb/>Cytosol <lb/>Endoplasmic reticulum <lb/>Lysosome <lb/>Mitochondrion <lb/>Nucleus − Chromatin <lb/>Nucleus − Nucleolus <lb/>Plasma membrane <lb/>Proteasome <lb/>unknown <lb/>Proteasome <lb/>a membrane <lb/>s − Nucleolus <lb/>s − Chromatin <lb/>itochondrion <lb/>Lysosome <lb/>mic reticulum <lb/>Cytosol <lb/>0S Ribosome <lb/>0S Ribosome <lb/>0 <lb/>1/3 <lb/>2/3 <lb/>1 <lb/>Classifier weight <lb/>40S Ribosome <lb/>60S Ribosome <lb/>Cytosol <lb/>Endoplasmic reticulum <lb/>Lysosome <lb/>Mitochondrion <lb/>Nucleus − Chromatin <lb/>Nucleus − Nucleolus <lb/>Plasma membrane <lb/>Proteasome <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>F1 score <lb/>Proteasome <lb/>Plasma membrane <lb/>Nucleus − Nucleolus <lb/>Nucleus − Chromatin <lb/>Mitochondrion <lb/>Lysosome <lb/>Endoplasmic reticulum <lb/>Cytosol <lb/>60S Ribosome <lb/>40S Ribosome <lb/>0 <lb/>1/3 <lb/>2/3 <lb/>1 <lb/>Classifier weight <lb/>Class <lb/>Figure 2: Top left: Bubble plot, displaying the distribution of the optimised class weights over the 100 test <lb/>partitions for the transfer learning algorithm applied to the E14TG2a mouse dataset. Top right: Principal <lb/>components analysis plot (first and second components, of the possible eight) of the E14TG2a mouse dataset, <lb/>showing the clustering of proteins according to their density gradient distributions. Bottom: Sub-cellular <lb/>class-specific box plots, displaying the estimated generalisation performance over 100 test partitions for the <lb/>transfer learning algorithm applied with (i) optimised class-specific weights (combined), (ii) only primary <lb/>data and (iii) only auxiliary data, for each sub-cellular class. <lb/>source to improve upon the the classification of experimental and condition-specific sub-<lb/>cellular localisation predictions. The method employs the use of two separate kernels, one <lb/></body>

			<page>18 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>for each data source. As described in section 2.2.4 to assess generalisation accuracy of our <lb/>classifier we employed the classic machine learning schema of partitioning our labelled data in <lb/>to training and testing sets, and used the testing sets to assess the strength of our classifiers. <lb/>This was repeated on 100 independent partitions for the the (i) SVM TL method, (ii) a <lb/>standard SVM trained on LOPIT alone, and (iii) a standard SVM trained on GO CC alone. <lb/>For the SVM TL experiments the resultant median macro-F1 scores for the mouse, hu-<lb/>man, callus, roots and fly datasets were 0.902, 0.868, 0.956, 0.875, 0.961, respectively, over <lb/>the 100 partitions. As per the k-NN TL, we found the macro-F1 scores for the SVM TL (Fig-<lb/>ure 3) was significantly higher than on profiles trained solely from only primary or auxiliary <lb/>alone; mouse (p = 4.474e −56 for primary alone and p = 6.313e −37 for auxiliary alone), human <lb/>(p = 7.325e −3 for primary alone and p = 1.071e −21 for auxiliary alone), callus (p = 0.004 and <lb/>p = 1.297e −92 ), roots (p = 1.725e −45 and p = 7.846e −25 ), and fly (p = 2.775e −3 for primary <lb/>alone, p = 4.325e −105 for auxiliary alone) data. This was also evident on the organellar level <lb/>as seen in Figure 4 and Supporting Figures 5 -8. <lb/>q <lb/>q q <lb/>q <lb/>q q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>q q <lb/>q <lb/>q <lb/>q q <lb/>q <lb/>q q <lb/>q <lb/>q q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>q <lb/>Mouse <lb/>Human <lb/>Callus <lb/>Roots <lb/>Fly <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Combined <lb/>Primary <lb/>Auxiliary <lb/>Data source <lb/>Macro F1 Score <lb/>Figure 3: Boxplots, displaying the estimated generalisation performance over 100 test partitions for the SVM <lb/>transfer learning algorithm applied with (i) optimised class-specific weights (combined), (ii) only primary <lb/>data and (iii) only auxiliary data, for each dataset. <lb/></body>

			<page>19 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>40S Ribosome <lb/>60S Ribosome <lb/>Cytosol <lb/>Endoplasmic reticulum <lb/>Lysosome <lb/>Mitochondrion <lb/>Nucleus − Chromatin <lb/>Nucleus − Nucleolus <lb/>Plasma membrane <lb/>Proteasome <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>0.6 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.7 <lb/>0.8 <lb/>0.9 <lb/>1.0 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.85 <lb/>0.90 <lb/>0.95 <lb/>1.00 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>Combined Primary Auxiliary <lb/>F1 score <lb/>Figure 4: Boxplots, displaying the estimated generalisation performance over 100 test partitions for the SVM <lb/>transfer learning algorithm applied with (i) optimised class-specific weights (combined), (ii) only primary <lb/>data and (iii) only auxiliary data, for the E14TG2a mouse dataset. <lb/></body>

			<page>20 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>3.2 Other auxiliary data sources <lb/>One of the advantages of the transfer learning framework is the flexibility to use different <lb/>types of information for both the primary and auxiliary data source. We demonstrate the <lb/>flexibility of this framework by testing other complementary sources of information as an <lb/>auxiliary data source. <lb/>3.2.1 The Human Protein Atlas <lb/>The sub-cellular Human Protein Atlas [67] provides protein expression patterns on a sub-<lb/>cellular level using immunofluorescently staining for human U-2 OS cells. As described in <lb/>2.1.2 we used the hpar Bioconductor package [68] to query the sub-cellular Human Protein <lb/>Atlas [67] (version 13, released on 11/06/2014). This auxiliary data, to be integrated with <lb/>our human LOPIT HEK293 experiment, was encoded as a binary matrix describing the <lb/>localisation of 670 proteins in 18 sub-cellular localisations supportively identified. Informa-<lb/>tion for 192 of the 381 labelled marker proteins were available. These 192 proteins covered <lb/>8 of the 10 known localisations in the LOPIT HEK293 experiment and were used in esti-<lb/>mated the classifier generalisation accuracy of the (i) the transfer learning approach, (ii) the <lb/>HEK293 primary LOPIT data and (iii) the HPA data, as described previously. As detailed <lb/>in the supplementary information (Supporting Figure 9), we observed a statistically signif-<lb/>icant improvement of our overall classification accuracy as well as several organelle-specific <lb/>results. <lb/>3.2.2 YLoc sequence and annotation features <lb/>Sequence and annotation features, as described in Table 1, that were used as input from the <lb/>computational classifier YLoc [61, 62] were selected as an auxiliary data source to comple-<lb/>ment the LOPIT E14TG2a mouse stem cell dataset. 34 sequence and annotation features <lb/>were selected using a correlation feature selection, as described in section 2.1.2. Using the <lb/>LOPIT mouse dataset as our primary data, and the 34 YLoc features as our auxiliary we <lb/>employed the standard protocol for testing classifier performance (1) using the k-NN trans-<lb/>fer learning with both data sources, (2) the primary data alone and (3) the auxiliary data <lb/>alone, as detailed in section 2.2.4. Although we did not observe a statistically significant <lb/>improvement using the auxiliary data in the transfer learning framework, we did see any <lb/>statistically significant disadvantage in combining information (Supporting Figure 10). Thus <lb/>we found that incorporating data from auxiliary sources in this framework does not dilute <lb/>any strong signals in the original experiment, demonstrating the flexibility of the classifier. <lb/>3.3 Biological application <lb/>We applied the two transfer learning classifiers to a real life scenario to (i) demonstrate <lb/>algorithm usage, and (ii) highlight the applicability of the method for predicting protein <lb/>localisation in MS-based spatial proteomics data over other single source classifiers. We <lb/>used the E14TG2a mouse dataset as our use case. The dataset contained density gradient <lb/>profiles for 1109 proteins, across 8 fractions, of which 387 proteins were labelled (i.e. identified <lb/>as known protein markers) distributed among 10 sub-cellular niches (the plasma membrane, <lb/></body>

			<page>21 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>endoplasmic reticulum, mitochondria, nucleolus, chromatin, 40S and 60S ribosomal subunits, <lb/>proteasome, lysosome and cytosol, see supporting table 1), the remaining 722 proteins were <lb/>unlabelled. We extracted the GO CC auxiliary data matrix for all proteins in the dataset <lb/>(as described in 2.1.2) and then applied the following four classifiers (1) k-NN (with LOPIT <lb/>data only), (2) k-NN TL (with LOPIT and GO CC data), (3) SVM (with LOPIT data <lb/>only) and (4) SVM TL (with LOPIT and GO CC data) for the prediction of the sub-cellular <lb/>localisation of the unlabelled proteins in the dataset. <lb/>As previously discussed, before applying any machine learning classifier one is required <lb/>to optimise any free algorithmic parameters on the training data as it is widely known that <lb/>wrongly set parameters can have adverse effects on the classification performance and suc-<lb/>cess of the learner. Following the standard protocol (as described in section 2.2.4) parameter <lb/>optimisation was conducted on the labelled training data using 100 rounds of stratified 80/20 <lb/>partitioning, in conjunction with 5-fold cross-validation in order to estimate the free param-<lb/>eters via a grid search, as implemented in the pRoloc package [64]. The best parameters <lb/>were found to be k = 5 for the k-NN classifier and for the k-NN TL classifier k P = 5, k A = 5 <lb/>and the best class weights were found θ = ( 1 <lb/>3 , 2 <lb/>3 , 2 <lb/>3 , 1, 1 <lb/>3 , 1, 1, 1 <lb/>3 , 2 <lb/>3 , 0) for the 40S ribosome, <lb/>60S ribosome, cytosol, endoplasmic reticulum, lysosome, mitochondria, nucleus -chromatin, <lb/>nucleolus, plasma membrane and proteasome, respectively. For the SVM classifier we found <lb/>the best cost to be C = 16 and γ = 10. For the SVM TL classifier we found C = 16, γ P = 1, <lb/>γ A = 0.1. Using these parameters with their associated algorithms we classified the 722 <lb/>unlabelled proteins in the dataset and obtained a classifier score for each protein. <lb/>In supervised machine learning the instances which one wishes to classify can only be <lb/>associated to the classes that were used in training. Thus, it is common when applying a <lb/>supervised classification algorithm, wherein the whole class diversity is not present in the <lb/>training data, to set a specific score cutoff on which to define new assignments, below which <lb/>classifications are set to unknown/unassigned. The pRoloc tutorial, which is found in the <lb/>set of accompanying vignettes in the pRoloc package [64], describes this procedure and how <lb/>to implement this in real practice. Deciding on a threshold in not trivial as classifier scores <lb/>are heavily dependent upon the classifier used and different sub-cellular niches can exhibit <lb/>different score distributions. <lb/>To validate our results and calculate classification thresholds based on a 5% false dis-<lb/>covery rate (FDR) for each of the four classifiers (i.e. k-NN, k-NN TL, SVM, SVM TL) we <lb/>compared the predicted localisations with the localisation of the same proteins found in the <lb/>highest resolution spatial map of mouse pluripotent embryonic stem cells to date 2 [81]. This <lb/>high resolution map was generated using hyperplexed LOPIT, a novel technique for robust <lb/>classification of protein localisation across the whole cell. The method uses an elaborate sub-<lb/>cellular fractionation scheme, enabled by the use of TMT 10-plex and application of a novel <lb/>MS data acquisition technique termed synchronous precursor selection MS3 (SPS)-MS 3 [82], <lb/>for high accuracy and precision of TMT quantification. The study used state-of-the-art data <lb/>analysis techniques [65, 64] combined with stringent manual curation of the data to provide <lb/>a robust map of the mouse pluripotent embryonic stem cell proteome. The authors also <lb/>provide a web interface to the data for exploration by the community through a dedicated <lb/></body>

			<note place="footnote">2 Currently under review <lb/></note>

			<page>22 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>online R shiny [83] application 3 . From examining the overlap between our new classifications <lb/>and the localisations in the high resolution mouse map we found 183 of our 722 unlabelled <lb/>proteins matched a high confidence localisation in the new dataset. Of the remaining, 347 of <lb/>our proteins were labelled as unknown in the mouse map (i.e. were assigned a low confidence <lb/>localisation in the experiment), and 192 proteins did not appear in the map. We used the <lb/>localisation of these 183 high confidence proteins as our gold standard on which to validate <lb/>our findings and set a false discovery rate for our predictions. <lb/>Figure 5 shows the score distributions for correct and incorrect assignments of the unas-<lb/>signed proteins in the dataset (as validated through the high resolution mouse pluripotent <lb/>embryonic stem cell map) and the distribution of the scores per classifier. Note, the scores <lb/>in Figure 5 are not a reflection of the classification power and the score distributions be-<lb/>tween the four different methods are not comparable to one another as they are calculated <lb/>using different techniques, as detailed in section 2.2. For both of the single source k-NN and <lb/>SVM classifiers there is a large overlap in the distribution of scores for correct and incorrect <lb/>assignments (Figure 5). It is desirable to have a distribution of scores that enables one to <lb/>choose a cutoff that minimises the false discovery rate. What is evident from examining the <lb/>score distributions of incorrect and correct assignments in Figure 5 is that by using transfer <lb/>learning we have increased the discrimination power of the classifier and thus lowered our <lb/>FDR. <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>knn <lb/>knn−TL <lb/>svm <lb/>svm−TL <lb/>Scores <lb/>outcome <lb/>correct <lb/>incorrect <lb/>Figure 5: Boxplots displaying the distribution of scores assigned to the unknown proteins in the mouse <lb/>dataset for the k-NN, k-NN transfer learning (TL) algorithm, a Support Vector Machine (SVM) and the <lb/>SVM TL classifiers. For each classifier the proteins have been split between those that have been classified <lb/>as incorrect or correct according the known protein localisations as found by a recent high resolution map <lb/>of the mouse proteome. <lb/>Using our knowledge of the correct/incorrect outcomes of these 183 previously unlabelled <lb/>proteins we calculated an appropriate threshold on which to classify all unlabelled proteins. <lb/>Using a FDR of 5% we found assignment thresholds for the SVM (0.85), SVM TL (0.785) <lb/></body>

			<note place="footnote">3 https://lgatto.shinyapps.io/christoforou2014 <lb/></note>

			<page>23 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>and k-NN TL (0.805) to classify the remaining unlabelled proteins. A FDR of 5% was not <lb/>possible with the k-NN classifier and the lowest achievable FDR was 15% which occurred <lb/>using the strictest threshold of 1 i.e. only when all 5 nearest neighbours agreed. Comparing <lb/>the classifications made from the single source classifiers to those made with the transfer <lb/>learning methods, we found in both cases we get many more assignments using the combined <lb/>transfer learning approaches compared to the single source methods using a fixed FDR of <lb/>5%, as discussed below. <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1.0 <lb/>SVM transfer learning <lb/>SVM <lb/>40S Ribosome <lb/>60S Ribosome <lb/>Cytosol <lb/>Endoplasmic reticulum <lb/>Lysosome <lb/>Mitochondrion <lb/>Nucleus − Chromatin <lb/>Nucleus − Nucleolus <lb/>Plasma membrane <lb/>Proteasome <lb/>Figure 6: Scatterplot displaying the scores for the SVM and SVM TL classifiers for the 183 proteins <lb/>validated by the hyperLOPIT mouse map (REF). Each point represents one protein and its associated <lb/>classifier scores. Filled circles highlight proteins that were assigned the same sub-cellular class with each <lb/>classifier, empty circles represent the instance when the two classifiers gave different results. The solid lines <lb/>show the classification boundaries for the two classifiers at a 5% FDR, above which proteins are classified to <lb/>the highlighted class, below these boundaries proteins are deemed low confidence and thus left unassigned. <lb/>Figure 6 shows the SVM and SVM TL scores assigned to each of the 183 validated <lb/>proteins. The sub-cellular class is highlighted by solid colours and an un-filled point on the <lb/>plot represents the case where the two classifiers disagreed on the sub-cellular localisation. <lb/>We found that the SVM TL classifier gave 70% more high confidence classifications with the <lb/>same 5% FDR threshold than the the single source SVM trained on primary data alone. All <lb/>proteins that were assigned to a sub-cellular niche with a high confidence score in both the <lb/>SVM and SVM TL (Figure 6, top right grid) were assigned to the same class. We also found <lb/>that many proteins outside of the high confidence threshold were assigned the same sub-<lb/>cellular class using both methods, as indicated by the abundance of solid points on the plot. <lb/>Of the total 722 previously unlabelled proteins we assigned high confidence localisations for <lb/>204 proteins using the SVM TL, and 176 proteins using the k-NN TL method, based on a <lb/>FDR of 5% (Supporting Tables 7 and 8). <lb/>By way of biological validation we investigated additional proteins gained using the SVM <lb/></body>

			<page>24 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>−2 <lb/>0 <lb/>2 <lb/>4 <lb/>−2 <lb/>−1 <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>PC1 (40.28%) <lb/>PC2 (25.7%) <lb/>40S Ribosome <lb/>60S Ribosome <lb/>Cytosol <lb/>Endoplasmic reticulum <lb/>Lysosome <lb/>Mitochondrion <lb/>Nucleus − Chromatin <lb/>Nucleus − Nucleolus <lb/>Plasma membrane <lb/>Proteasome <lb/>unknown <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>1 = GTR3_MOUSE <lb/>2 = SNTB2_MOUSE <lb/>3 = PAR6B_MOUSE <lb/>4 = ADA17_MOUSE <lb/>Figure 7: Principal components analysis plot (PCA) of the E14TG2a mouse stem cell dataset. Proteins <lb/>are clustered according to their density gradient distributions. Each point on the PCA plot represents one <lb/>protein. Filled circles are the original protein markers used in classification, hollow circles show new locations <lb/>as assigned by the SVM TL classifier. The 4 proteins GTR3 MOUSE, SNTB2 MOUSE, PAR6B MOUSE <lb/>and ADA17 MOUSE that were found in the SVM TL method and not in an SVM classification with LOPIT <lb/>only are highlighted. <lb/>TL method (Figure 6, bottom right grid) as novel assignments to one of these classes, the <lb/>plasma membrane, by searching through the literature for supporting empirical evidence. <lb/>For example, using the SVM TL method we found four proteins assigned only to the plasma <lb/>membrane with the SVM TL method (Figure 7) that were also assigned to the plasma <lb/>membrane in the recent high resolution mouse map (GTR3 MOUSE, SNTB2 MOUSE, <lb/>PAR6B MOUSE and ADA17 MOUSE). Dehydroascorbic acid transporter (GTR3 MOUSE) <lb/>is a multi-pass membrane protein which has been previously shown to be a plasma mem-<lb/>brane protein in studies isolating the cell surface glycoprotein in Jurkat cells [84]. Beta-2 <lb/>syntrophin or syntrophin 3 (SNTB2 MOUSE) is a phosphoprotein with PDZ domain through <lb/>which it interacts with ion channels and receptors. There are confounding reports of the sub-<lb/>cellular location of this peripheral protein. It associates with dystrophins and has no signal <lb/>sequence. It is found mostly in muscle fibres and brain [85], but to date, its role has not <lb/>been studied in mouse embryonic stem cells. Given its association with ion channels and <lb/>receptors, it is perfectly feasible that the steady location of this protein in stem cells is <lb/>plasma membrane. Partitioning defective 6 homolog beta (PAR6B MOUSE) is a peripheral <lb/>membrane protein thought to be in complex with E-cadherin, aPKC, and Par3 at the plasma <lb/>membrane [86], where is functions to guide GTP-bound Rho small GTPases to atypical pro-<lb/>tein kinase C proteins [87]. Disintegrin and metalloproteinase domain-containing protein 17 <lb/>(ADA17 MOUSE) is a single pass plasma membrane protein which functions to cleave the <lb/></body>

			<page>25 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>intracellular domain of various plasms membrane proteins including notch and TNF-alpha <lb/>[88]. It is therefore involved in the upstream events in several signalling pathways. It has a <lb/>17 amino acid N-terminal signal sequence suggestive of its function as a membrane protein. <lb/>The full list of localisation predictions for all proteins in the mouse E14TG2a dataset can <lb/>be found in the R data package pRolocdata. <lb/>3.4 A Comparison: KNN vs SVM <lb/>We compared the macro-and class-F1 scores from all experiments in 3.1 on the 5 datasets <lb/>used to assess the classifier performance of the k-NN TL and SVM TL methods. We found <lb/>that no single method systematically outperformed the other, as described further in section <lb/>5 of the supporting supplement. <lb/>When applying the SVM TL and k-NN TL classifiers to the unlabelled proteins in section <lb/>3.3 an analysis of the final assignments (as classified based on FDR of 5%) showed that there <lb/>was no contradiction in results. The predicted localisations were in high agreement and if not <lb/>assigned to the same class as the other classifier, proteins were found to labelled as unknown <lb/>i.e. were low confidence assignments (see Supporting Table 9). <lb/>4 Conclusion <lb/>In this study we have presented a flexible transfer learning framework for the integration of <lb/>heterogeneous data sources for robust supervised machine learning classification. We have <lb/>demonstrated the biological usage of the framework by applying these methods to the task of <lb/>protein localisation prediction from MS-based experiments. We further show the flexibility <lb/>of the framework by applying these methods to the five different spatial proteomics datasets, <lb/>from four different species, in conjunction with three different auxiliary data sources to <lb/>classify proteins to multiple sub-cellular compartments. We find the two different classifiers; <lb/>the k-NN TL and SVM TL, perform equally well and importantly both of these methods <lb/>outperform a single classifier trained on each single data source alone. We further applied <lb/>the algorithm to a real life use case, to classify a set of previously unknown proteins in a <lb/>spatial proteomics experiment on mouse embryonic stem cells, which was validated using <lb/>the most high resolution map of the mouse E14TG2a stem cell proteome to date [81]. We <lb/>find integrating data from a second data source directly in to classifier training and classifier <lb/>creation results in the assignment of proteins to organelles with high generalisation accuracy. <lb/>Finally, we find that using freely available data from repositories we can improve upon <lb/>the classification of experimental and condition-specific protein-organelle predictions in an <lb/>organelle specific manner. <lb/>To our knowledge, no other method has been developed to date that allows the incorpo-<lb/>ration of an auxiliary data source for the primary task of predicting sub-cellular localisation <lb/>in spatial proteomics experiments. In this study we have developed methods that not only <lb/>allow the inclusion of an auxiliary data source in localisation prediction, but we have created <lb/>a flexible framework allowing the use of many different types of auxiliary information, and <lb/>furthermore allows the user complete control over the weighting between data sources and <lb/>between specific classes. This is extremely important for the analysis of biological data in <lb/></body>

			<page>26 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<body>general, and spatial proteomics data in particular, as many experiments are targeted towards <lb/>resolving specific biologically relevant aspects (sub-cellular niches in spatial proteomics) and <lb/>thus users may wish to control the impact of auxiliary information for aspects that have <lb/>been specially targeted for analysis by the primary experimental method. In this context <lb/>the setting of weights manually in the k-NN transfer learning classifier allows users complete <lb/>power to explicitly choose whether to call upon an auxiliary data source or simply use data <lb/>from their own experiment, on an organelle-by-organelle basis. <lb/>The effectiveness of using databases as an auxiliary data source will depend greatly <lb/>on abundance and quality of annotation available for the species under investigation. For <lb/>example, human is a well-studied species and there is a large amount of information available <lb/>in the Gene Ontology and Human Protein Atlas. Furthermore, some organelles are easier to <lb/>enrich for and thus there exists much more information available to utilise as an auxiliary <lb/>source on a organelle by organelle basis. The transfer learning methods we present here allow <lb/>the inclusion of any type of auxiliary data, provided of course there is information available <lb/>for the proteins under investigation. <lb/>The integration of auxiliary data source is a double-edged sword. On the one hand, <lb/>it can shed light on (i) the primary classification task by reinforcing weak patterns or (ii) <lb/>complement the signal in the primary data. On the other hand however it is easy to di-<lb/>lute valuable signals in an expensive experiment by shadowing the uniqueness, and hence <lb/>biologically relevance of the experimental primary data when integration is not performed <lb/>with care. Thus one needs to be cautious with data integration in general and not overlook <lb/>the biological relevance of the primary data. Here, we provide a solution to this issue and <lb/>demonstrate that under this learning framework, one never can do worse than using primary <lb/>data alone: the k-NN transfer learning classifier uses optimised class-specific weights so as <lb/>not to penalise any strong signals in the primary, if no signal is found in the auxiliary, simi-<lb/>larly, the SVM transfer learning method uses optimised data-specific gamma parameters for <lb/>each data-specific kernel. <lb/>The transfer learning framework forms part of the open-source open-development Bio-<lb/>conductor pRoloc suite of computational methods available for organelle proteomics data <lb/>analysis. Moreover, as the pipeline utilises the formal Bioconductor classes different data <lb/>types, for example from gene expression technologies among others, can be easily used in this <lb/>framework. The integration of different data sources is one of major challenges in the data <lb/>intensive world of computational biology, and here we offer a flexible and powerful solution <lb/>to unify data obtained from different by complimentary techniques. <lb/></body>

			<div type="acknowledgement">Acknowledgements <lb/>LMB was supported by a BBSRC Tools and Resources Development Fund (Award <lb/>BB/K00137X/1). LG was supported by the European Union 7 th Framework Program <lb/>(PRIME-XS project, grant agreement number 262067) and a BBSRC Strategic Longer and <lb/>Larger grant (Award BB/L002817/1). DW and OK acknowledge funding from the European <lb/>Union (PRIME-XS, GA 262067) Deutsche Forschungsgemeinschaft (KO-2313/6-1). The au-<lb/>thors would also like to thank Dr Matthew Trotter from Celgene Institute Translational <lb/>Research Europe for his insightful comments and Dr. Dureid El-Moghraby from the High <lb/></div>

			<page>27 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<div type="acknowledgement">Performance Computing Service, University of Cambridge for his support <lb/></div>

			<listBibl>References <lb/>[1] T. Hey, S. Tansley, and K. Tolle. The Fourth Paradigm: Data-Intensive Scientific <lb/>Discovery. Microsoft, 2009. <lb/>[2] R. Spreafico, S. Mitchell, and A. Hoffmann. &quot;Training the 21st Century Immunologist&quot;. <lb/>In: Trends in Immunology 0 (2015). issn: 1471-4906. <lb/>[3] UniProt Consortium. &quot;UniProt: a hub for protein information&quot;. In: Nucleic Acids Res <lb/>43.Database issue (2015), pp. D204-12. <lb/>[4] M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler, J. M. Cherry, A. P. <lb/>Davis, K. Dolinski, S. S. Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-Tarver, <lb/>A. Kasarskis, S. Lewis, J. C. Matese, J. E. Richardson, M. Ringwald, G. M. Rubin, and <lb/>G. Sherlock. &quot;Gene ontology: tool for the unification of biology. The Gene Ontology <lb/>Consortium.&quot; In: Nat Genet 25.1 (2000), pp. 25-9. <lb/>[5] M. W. Libbrecht and W. S. Noble. &quot;Machine learning applications in genetics and <lb/>genomics&quot;. In: Nat Rev Genet 16.6 (2015), pp. 321-32. <lb/>[6] P. Wu and T. G. Dietterich. &quot;Improving SVM Accuracy by Training on Auxiliary Data <lb/>Sources&quot;. In: Proceedings of the 21st International Conference on Machine Learning <lb/>(ICML). 2004. <lb/>[7] M. Dreger. &quot;Subcellular proteomics.&quot; In: Mass Spectrom Rev 22.1 (2003), pp. 27-56. <lb/>[8] M. Hung and W. Link. &quot;Protein localization in disease and therapy.&quot; In: Journal of <lb/>cell science 124 (2011), pp. 3381-3392. <lb/>[9] L. Gatto, J. A. Vizcaíno, H. Hermjakob, W. Huber, and K. S. Lilley. &quot;Organelle pro-<lb/>teomics experimental designs and analysis.&quot; In: Proteomics 10.22 (2010), pp. 3957-<lb/>69. <lb/>[10] T. P. J. Dunkley, S. Hester, I. P. Shadforth, J. Runions, T. Weimar, S. L. Hanton, <lb/>J. L. Griffin, C. Bessant, F. Brandizzi, C. Hawes, R. B. Watson, P. Dupree, and K. S. <lb/>Lilley. &quot;Mapping the Arabidopsis organelle proteome.&quot; eng. In: Proc Natl Acad Sci <lb/>USA 103.17 (2006), pp. 6518-6523. <lb/>[11] L. J. Foster, C. L. d. Hoog, Y. Zhang, Y. Zhang, X. Xie, V. K. Mootha, and M. Mann. <lb/>&quot;A mammalian organelle map by protein correlation profiling.&quot; eng. In: Cell 125.1 <lb/>(2006), pp. 187-199. <lb/>[12] C. De Duve and H. Beaufay. A short history of tissue fractionation. 1981. <lb/>[13] P. G. Sadowski, T. P. J. Dunkley, I. P. Shadforth, P. Dupree, C. Bessant, J. L. Griffin, <lb/>and K. S. Lilley. &quot;Quantitative proteomic approach to study subcellular localization <lb/>of membrane proteins.&quot; eng. In: Nat Protoc 1.4 (2006), pp. 1778-1789. <lb/>[14] P. G. Sadowski, A. J. Groen, P. Dupree, and K. S. Lilley. &quot;Sub-cellular localization of <lb/>membrane proteins&quot;. In: Proteomics 8.19 (2008), pp. 3991-4011. <lb/></listBibl>

			<page>28 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<listBibl>[15] N. Nikolovski, D. Rubtsov, M. P. Segura, G. P. Miles, T. J. Stevens, T. P. Dunkley, <lb/>S. Munro, K. S. Lilley, and P. Dupree. &quot;Putative glycosyltransferases and other plant <lb/>golgi apparatus proteins are revealed by LOPIT proteomics.&quot; In: Plant Physiol 160.2 <lb/>(2012), pp. 1037-51. <lb/>[16] A. J. Groen, G. Sancho-Andrés, L. M. Breckels, L. Gatto, F. Aniento, and K. S. Lilley. <lb/>&quot;Identification of trans-golgi network proteins in Arabidopsis thaliana root tissue&quot;. In: <lb/>J Proteome Res 13.2 (2014), pp. 763-76. <lb/>[17] M. Tomizioli, C. Lazar, S. Brugière, T. Burger, D. Salvi, L. Gatto, L. Moyet, L. M. <lb/>Breckels, A.-M. Hesse, K. S. Lilley, D. Seigneurin-Berny, G. Finazzi, N. Rolland, and <lb/>M. Ferro. &quot;Deciphering thylakoid sub-compartments using a mass spectrometry-based <lb/>approach&quot;. In: Mol Cell Proteomics 13.8 (2014), pp. 2147-67. <lb/>[18] D. J. Tan, H. Dvinge, A. Christoforou, P. Bertone, A. A. Martinez, and K. S. Lilley. <lb/>&quot;Mapping organelle proteins and protein complexes in Drosophila melanogaster.&quot; In: <lb/>J Proteome Res 8.6 (2009), pp. 2667-78. <lb/>[19] M. Harner, C. Körner, D. Walther, D. Mokranjac, J. Kaesmacher, U. Welsch, J. Grif-<lb/>fith, M. Mann, F. Reggiori, and W. Neupert. &quot;The mitochondrial contact site complex, <lb/>a determinant of mitochondrial architecture&quot;. In: EMBO J 30.21 (2011), pp. 4356-70. <lb/>[20] J. S. Andersen, C. J. Wilkinson, T. Mayor, P. Mortensen, E. A. Nigg, and M. Mann. <lb/>&quot;Proteomic characterization of the human centrosome by protein correlation profiling.&quot; <lb/>eng. In: Nature 426.6966 (2003), pp. 570-574. <lb/>[21] A. Christoforou, C. Mulvey, L. M. Breckels, L. Gatto, and K. S. Lilley. &quot;Spatial Pro-<lb/>teomics: Practical Considerations for Data Acquisition and Analysis in Protein Sub-<lb/>cellular Localisation Studies&quot;. In: Quantitative Proteomics 1 (2014), p. 187. <lb/>[22] S. Wiese, T. Gronemeyer, R. Ofman, M. Kunze, C. P. Grou, J. A. Almeida, M. Eise-<lb/>nacher, C. Stephan, H. Hayen, L. Schollenberger, T. Korosec, H. R. Waterham, W. <lb/>Schliebs, R. Erdmann, J. Berger, H. E. Meyer, W. Just, J. E. Azevedo, R. J. A. Wan-<lb/>ders, and B. Warscheid. &quot;Proteomics characterization of mouse kidney peroxisomes by <lb/>tandem mass spectrometry and protein correlation profiling&quot;. In: Mol Cell Proteomics <lb/>6.12 (2007), pp. 2045-57. <lb/>[23] S. L. Hall, S. Hester, J. L. Griffin, K. S. Lilley, and A. P. Jackson. &quot;The organelle <lb/>proteome of the DT40 lymphocyte cell line.&quot; In: Mol Cell Proteomics 8.6 (2009), <lb/>pp. 1295-1305. <lb/>[24] L. Gatto, L. Breckels, T. Burger, D. Nightingale, A. Groen, C. Campbell, N. Nikolovski, <lb/>C. Mulvey, A. Christoforou, M. Ferro, and K. Lilley. &quot;A foundation for reliable spatial <lb/>proteomics data analysis.&quot; In: Mol Cell Proteomics 13.8 (2014), pp. 1937-52. <lb/>[25] M. W. B. Trotter, P. G. Sadowski, T. P. J. Dunkley, A. J. Groen, and K. S. Lilley. <lb/>&quot;Improved sub-cellular resolution via simultaneous analysis of organelle proteomics <lb/>data across varied experimental conditions&quot;. In: Proteomics 10.23 (2010), pp. 4213-<lb/>4219. issn: 1615-9861. <lb/></listBibl>

			<page>29 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<listBibl>[26] S. Ohta, J. C. Bukowski-Wills, L. Sanchez-Pulido, F. L. Alves, L. Wood, Z. A. Chen, <lb/>M. Platani, L. Fischer, D. F. Hudson, C. P. Ponting, T. Fukagawa, W. C. Earnshaw, <lb/>and J. Rappsilber. &quot;The protein composition of mitotic chromosomes determined using <lb/>multiclassifier combinatorial proteomics.&quot; In: Cell 142.5 (2010), pp. 810-21. <lb/>[27] M. Tardif, A. Atteia, M. Specht, G. Cogne, N. Rolland, S. Brugière, M. Hippler, M. <lb/>Ferro, C. Bruley, G. Peltier, O. Vallon, and L. Cournac. &quot;PredAlgo: a new subcellular <lb/>localization prediction tool dedicated to green algae.&quot; In: Mol Biol Evol 29.12 (2012), <lb/>pp. 3625-39. <lb/>[28] P. Du, T. Li, and X. Wang. &quot;Recent progress in predicting protein sub-subcellular <lb/>locations&quot;. In: Expert Rev Proteomics 8.3 (2011), pp. 391-404. <lb/>[29] X. Xiao, W.-Z. Lin, and K.-C. Chou. &quot;Recent advances in predicting protein classi-<lb/>fication and their applications to drug development&quot;. In: Curr Top Med Chem 13.14 <lb/>(2013), pp. 1622-35. <lb/>[30] A. K. Tiwari and R. Srivastava. &quot;A survey of computational intelligence techniques in <lb/>protein function prediction&quot;. In: Int J Proteomics 2014 (2014), p. 845479. <lb/>[31] Y. D. Cai, X. J. Liu, X. B. Xu, and K. C. Chou. &quot;Support vector machines for prediction <lb/>of protein subcellular location&quot;. In: Mol Cell Biol Res Commun 4.4 (2000), pp. 230-3. <lb/>[32] K. C. Chou. &quot;Prediction of protein cellular attributes using pseudo-amino acid com-<lb/>position&quot;. In: Proteins 43.3 (2001), pp. 246-55. <lb/>[33] F.-M. Li and Q.-Z. Li. &quot;Predicting protein subcellular location using Chou&apos;s pseudo <lb/>amino acid composition and improved hybrid approach&quot;. In: Protein Pept Lett 15.6 <lb/>(2008), pp. 612-6. <lb/>[34] H. Lin, H. Ding, F.-B. Guo, A.-Y. Zhang, and J. Huang. &quot;Predicting subcellular lo-<lb/>calization of mycobacterial proteins by using Chou&apos;s pseudo amino acid composition&quot;. <lb/>In: Protein Pept Lett 15.7 (2008), pp. 739-44. <lb/>[35] L. Nanni, S. Brahnam, and A. Lumini. &quot;High performance set of PseAAC and sequence <lb/>based descriptors for protein classification&quot;. In: J Theor Biol 266.1 (2010), pp. 1-10. <lb/>[36] J. Lin and Y. Wang. &quot;Using a novel AdaBoost algorithm and Chou&apos;s Pseudo amino <lb/>acid composition for predicting protein subcellular localization&quot;. In: Protein Pept Lett <lb/>18.12 (2011), pp. 1219-25. <lb/>[37] A. S. Mer and M. A. Andrade-Navarro. &quot;A novel approach for protein subcellular <lb/>location prediction using amino acid exposure&quot;. In: BMC Bioinformatics 14 (2013), <lb/>p. 342. <lb/>[38] K Nakai and P Horton. &quot;PSORT: a program for detecting sorting signals in proteins <lb/>and predicting their subcellular localization&quot;. In: Trends Biochem Sci 24.1 (1999), <lb/>pp. 34-6. <lb/>[39] S Hua and Z Sun. &quot;Support vector machine approach for protein subcellular localization <lb/>prediction&quot;. In: Bioinformatics 17.8 (2001), pp. 721-8. <lb/></listBibl>

			<page>30 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<listBibl>[40] J. L. Gardy, C. Spencer, K. Wang, M. Ester, G. E. Tusnády, I. Simon, S. Hua, K. <lb/>deFays, C. Lambert, K. Nakai, and F. S. L. Brinkman. &quot;PSORT-B: Improving protein <lb/>subcellular localization prediction for Gram-negative bacteria&quot;. In: Nucleic Acids Res <lb/>31.13 (2003), pp. 3613-7. <lb/>[41] S. Matsuda, J.-P. Vert, H. Saigo, N. Ueda, H. Toh, and T. Akutsu. &quot;A novel represen-<lb/>tation of protein sequences for prediction of subcellular location using support vector <lb/>machines&quot;. In: Protein Sci 14.11 (2005), pp. 2804-13. <lb/>[42] A. Pierleoni, P. L. Martelli, P. Fariselli, and R. Casadio. &quot;BaCelLo: a balanced subcel-<lb/>lular localization predictor&quot;. In: Bioinformatics 22.14 (2006), e408-16. <lb/>[43] P. Horton, K.-J. Park, T. Obayashi, N. Fujita, H. Harada, C. J. Adams-Collier, and K. <lb/>Nakai. &quot;WoLF PSORT: protein localization predictor&quot;. In: Nucleic Acids Res 35.Web <lb/>Server issue (2007), W585-7. <lb/>[44] O. Emanuelsson, S. Brunak, G. von Heijne, and H. Nielsen. &quot;Locating proteins in the <lb/>cell using TargetP, SignalP and related tools&quot;. In: Nat Protoc 2.4 (2007), pp. 953-71. <lb/>[45] S. Rastogi and B. Rost. &quot;Bioinformatics predictions of localization and targeting&quot;. In: <lb/>Methods Mol Biol 619 (2010), pp. 285-305. <lb/>[46] K. Wang, L.-L. Hu, X.-H. Shi, Y.-S. Dong, H.-P. Li, and T.-Q. Wen. &quot;PSCL: predicting <lb/>protein subcellular localization based on optimal functional domains&quot;. In: Protein Pept <lb/>Lett 19.1 (2012), pp. 15-22. <lb/>[47] G. A. Arango-Argoty, J. F. Ruiz-Muñoz, J. A. Jaramillo-Garzón, and C. G. Castellanos-<lb/>Domínguez. &quot;An adaptation of Pfam profiles to predict protein sub-cellular localiza-<lb/>tion in Gram positive bacteria&quot;. In: Conf Proc IEEE Eng Med Biol Soc 2012 (2012), <lb/>pp. 5554-7. <lb/>[48] L.-L. Hu, K.-Y. Feng, Y.-D. Cai, and K.-C. Chou. &quot;Using protein-protein interaction <lb/>network information to predict the subcellular locations of proteins in budding yeast&quot;. <lb/>In: Protein Pept Lett 19.6 (2012), pp. 644-51. <lb/>[49] J. Q. Jiang and M. Wu. &quot;Predicting multiplex subcellular localization of proteins using <lb/>protein-protein interaction network: a comparative study&quot;. In: BMC Bioinformatics 13 <lb/>Suppl 10 (2012), S20. <lb/>[50] P. Du and L. Wang. &quot;Predicting human protein subcellular locations by the ensem-<lb/>ble of multiple predictors via protein-protein interaction network with edge clustering <lb/>coefficients&quot;. In: PLoS One 9.1 (2014), e86879. <lb/>[51] W.-L. Huang, C.-W. Tung, S.-W. Ho, S.-F. Hwang, and S.-Y. Ho. &quot;ProLoc-GO: uti-<lb/>lizing informative Gene Ontology terms for sequence-based prediction of protein sub-<lb/>cellular localization&quot;. In: BMC Bioinformatics 9 (2008), p. 80. <lb/>[52] S. Mei, W. Fei, and S. Zhou. &quot;Gene ontology based transfer learning for protein sub-<lb/>cellular localization&quot;. In: BMC Bioinformatics 12 (2011), p. 44. <lb/>[53] S. Mei. &quot;Multi-kernel transfer learning based on Chou&apos;s PseAAC formulation for pro-<lb/>tein submitochondria localization&quot;. In: J Theor Biol 293 (2012), pp. 121-30. <lb/></listBibl>

			<page>31 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<listBibl>[54] S. Mei. &quot;Multi-label multi-kernel transfer learning for human protein subcellular local-<lb/>ization&quot;. In: PLoS One 7.6 (2012), e37716. <lb/>[55] M. T. Rosenstein, Z. Marx, L. Kaelbling, and T. Dietterich. &quot;Transfer or Not To <lb/>Transfer&quot;. In: NIPS-05 Workshop on Inductive Transfer: 10 Years Later. 2005. <lb/>[56] R. K. Ando and T. Zhang. &quot;A Framework for Learning Predictive Structures from <lb/>Multiple Tasks and Unlabeled Data&quot;. In: J. Mach. Learn. Res. 6 (Dec. 2005), pp. 1817-<lb/>1853. issn: 1532-4435. <lb/>[57] B. Bakker and T. Heskes. &quot;Task Clustering and Gating for Bayesian Multitask Learn-<lb/>ing&quot;. In: J. Mach. Learn. Res. 4 (Dec. 2003), pp. 83-99. issn: 1532-4435. <lb/>[58] J. Baxter. &quot;A Model of Inductive Bias Learning&quot;. In: J. Artif. Int. Res. 12.1 (Mar. <lb/>2000), pp. 149-198. issn: 1076-9757. <lb/>[59] L. Fagerberg, S. Strömberg, A. El-Obeid, M. Gry, K. Nilsson, M. Uhlen, F. Ponten, <lb/>and A. Asplund. &quot;Large-scale protein profiling in human cell lines using antibody-based <lb/>proteomics&quot;. In: J Proteome Res 10.9 (2011), pp. 4066-75. <lb/>[60] M. A. Hall. &quot;Correlation-based Feature Selection for Discrete and Numeric Class Ma-<lb/>chine Learning&quot;. In: Proceedings of the Seventeenth International Conference on Ma-<lb/>chine Learning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 -July <lb/>2, 2000. 2000, pp. 359-366. <lb/>[61] S. Briesemeister, J. Rahnenführer, and O. Kohlbacher. &quot;YLoc-an interpretable web <lb/>server for predicting subcellular localization&quot;. In: Nucleic Acids Res 38.Web Server <lb/>issue (2010), W497-502. <lb/>[62] S. Briesemeister, J. Rahnenführer, and O. Kohlbacher. &quot;Going from where to why-<lb/>interpretable prediction of protein subcellular localization&quot;. In: Bioinformatics 26.9 <lb/>(2010), pp. 1232-8. <lb/>[63] R. C. Gentleman, V. J. Carey, D. M. Bates, B. Bolstad, M. Dettling, S. Dudoit, B. <lb/>Ellis, L. Gautier, Y. Ge, J. Gentry, K. Hornik, T. Hothorn, W. Huber, S. Iacus, R. <lb/>Irizarry, F. Leisch, C. Li, M. Maechler, A. J. Rossini, G. Sawitzki, C. Smith, G. Smyth, <lb/>L. Tierney, J. Y. H. Yang, and J. Zhang. &quot;Bioconductor: open software development <lb/>for computational biology and bioinformatics.&quot; In: Genome Biol 5.10 (2004), pp. -80. <lb/>[64] L. Gatto, L. M. Breckels, S. Wieczorek, M. Burger, and K. S. Lilley. &quot;Mass-<lb/>spectrometry based spatial proteomics data analysis using pRoloc and pRolocdata&quot;. <lb/>In: Bioinformatics 30.9 (2104), pp. 1322-1324. <lb/>[65] L. M. Breckels, L. Gatto, A. Christoforou, A. J. Groen, K. S. Lilley, and M. W. <lb/>Trotter. &quot;The effect of organelle discovery upon sub-cellular protein localisation.&quot; In: <lb/>J Proteomics 88 (2013), pp. 129-40. <lb/>[66] C. J. A. Sigrist, L. Cerutti, E. de Castro, P. S. Langendijk-Genevaux, V. Bulliard, A. <lb/>Bairoch, and N. Hulo. &quot;PROSITE, a protein domain database for functional charac-<lb/>terization and annotation&quot;. In: Nucleic Acids Res 38.Database issue (2010), pp. D161-<lb/>6. <lb/></listBibl>

			<page>32 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<listBibl>[67] M. Uhlen, P. Oksvold, L. Fagerberg, E. Lundberg, K. Jonasson, M. Forsberg, M. <lb/>Zwahlen, C. Kampf, K. Wester, S. Hober, H. Wernerus, L. Björling, and F. Ponten. <lb/>&quot;Towards a knowledge-based Human Protein Atlas.&quot; In: Nat Biotechnol 28.12 (2010), <lb/>pp. 1248-50. <lb/>[68] L. Gatto. hpar: Human Protein Atlas in R. R package version 1.4.0. <lb/>[69] O. L. Mangasarian. &quot;Generalized Support Vector Machines&quot;. In: Advances in Large <lb/>Margin Classifiers. Ed. by A. J. Smola, P. Bartlett, B. Schölkopf, and D. Schuurmans. <lb/>MIT Press, Sept. 2000, pp. 135-146. <lb/>[70] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge <lb/>University Press, 2004. <lb/>[71] J. Mercer. &quot;Functions of positive and negative type and their connection with the <lb/>theory of integral equations&quot;. In: Philosophical Transactions of the Royal Society A <lb/>209 (1909), pp. 441-458. <lb/>[72] W. Wu, J. Xu, H. Li, and S. Oyama. Asymmetric Kernel Learning. Tech. rep. MSR-<lb/>TR-2010-85. Microsoft Research, June 2010. <lb/>[73] G. R. G. Lanckriet, ijl De Bie, N. Cristianini, M. I. Jordan, and W. S. Noble. &quot;A statis-<lb/>tical framework for genomic data fusion&quot;. In: Bioinformatics 20.16 (2004), pp. 2626-<lb/>2635. <lb/>[74] M. Gönen and E. Alpaydin. &quot;Multiple kernel learning algorithms&quot;. In: Journal of Ma-<lb/>chine Learning Research 12 (2011), pp. 2211-2268. <lb/>[75] C.-C. Chang and C.-J. Lin. &quot;LIBSVM: A Library for Support Vector Machines&quot;. In: <lb/>ACM Transactions on Intelligent Systems and Technology 2.3 (Apr. 2001). <lb/>[76] S. Knerr, L. Personnaz, and G. Dreyfus. &quot;Single-layer learning revisited: a stepwise <lb/>procedure for building and training a neural network&quot;. In: Neurocomputing: Algorithms, <lb/>Architectures and Applications (1990). <lb/>[77] K. Morik, P. Brockhausen, and T. Joachims. &quot;Combining statistical learning with a <lb/>knowledge-based approach-a case study in intensive care monitoring&quot;. In: Proceedings <lb/>of the International Conference on Machine Learning (ICML). 1999, pp. 268-277. <lb/>[78] J. C. Platt. &quot;Probabilities for SV Machines&quot;. In: Advances in Large Margin Classifiers. <lb/>Ed. by A. J. Smola, P. Bartlett, B. Schölkopf, and D. Schuurmans. MIT Press, Sept. <lb/>2000, pp. 61-74. <lb/>[79] H.-T. Lin, C.-J. Lin, and R. C. Weng. &quot;A note on Platt&apos;s probabilistic outputs for <lb/>support vector machines&quot;. In: Machine Learning 68 (2007), pp. 267-276. <lb/>[80] H. He. &quot;Learning from imbalanced data&quot;. In: IEEE Transactions on Knowledge and <lb/>Data Engineering 21.9 (2009), pp. 1263-1284. <lb/>[81] A. Christoforou, C. M. Mulvey, L. M. Breckels, P. C. Hayward, E. Geladaki, T. Hurrell, <lb/>T. Naake, L. Gatto, R. Viner, A. Martinez Arias, and K. S. Lilley. &quot;A draft map of <lb/>the mouse pluripotent stem cell spatial proteome&quot;. In: (under review). <lb/></listBibl>

			<page>33 <lb/></page>

			<note place="headnote">Transfer learning for spatial proteomics <lb/></note>

			<listBibl>[82] G. C. McAlister, D. P. Nusinow, M. P. Jedrychowski, M Wühr, E. L. Huttlin, B. <lb/>K. Erickson, R Rad, W Haas, and S. P. Gygi. &quot;MultiNotch MS3 enables accurate, <lb/>sensitive, and multiplexed detection of differential expression across cancer cell line <lb/>proteomes.&quot; In: Anal Chem 86.14 (2014), pp. 7150-8. <lb/>[83] W. Chang, J. Cheng, J. Allaire, Y. Xie, and J. McPherson. shiny: Web Application <lb/>Framework for R. R package version 0.11.1. 2015. <lb/>[84] B. Wollscheid, D. Bausch-Fluck, C. Henderson, R. O&apos;Brien, M. Bibel, R. Schiess, R. <lb/>Aebersold, and J. D. Watts. &quot;Mass-spectrometric identification and relative quantifi-<lb/>cation of N-linked cell surface glycoproteins&quot;. In: Nat Biotechnol 27.4 (2009), pp. 378-<lb/>86. <lb/>[85] S. H. Gee, R Madhavan, S. R. Levinson, J. H. Caldwell, R Sealock, and S. C. Froehner. <lb/>&quot;Interaction of muscle and brain sodium channels with multiple members of the syn-<lb/>trophin family of dystrophin-associated proteins&quot;. In: J Neurosci 18.1 (1998), pp. 128-<lb/>37. <lb/>[86] G Joberty, C Petersen, L Gao, and I. G. Macara. &quot;The cell-polarity protein Par6 links <lb/>Par3 and atypical protein kinase C to Cdc42&quot;. In: Nat Cell Biol 2.8 (2000), pp. 531-9. <lb/>[87] S. M. Garrard, C. T. Capaldo, L. Gao, M. K. Rosen, I. G. Macara, and D. R. Tomchick. <lb/>&quot;Structure of Cdc42 in a complex with the GTPase-binding domain of the cell polarity <lb/>protein, Par6&quot;. In: EMBO J 22.5 (2003), pp. 1125-33. <lb/>[88] C Brou, F Logeat, N Gupta, C Bessia, O LeBail, J. R. Doedens, A Cumano, P Roux, <lb/>R. A. Black, and A Israël. &quot;A novel proteolytic cleavage involved in Notch signaling: <lb/>the role of the disintegrin-metalloprotease TACE&quot;. In: Mol Cell 5.2 (2000), pp. 207-16. <lb/></listBibl>

			<page>34 </page>


	</text>
</tei>
