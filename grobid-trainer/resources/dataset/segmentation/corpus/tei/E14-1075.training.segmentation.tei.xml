<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_E14-1075"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, <lb/>Gothenburg, Sweden, April 26-30 2014. c <lb/> 2014 Association for Computational Linguistics <lb/></front>

			<front>Improving the Estimation of Word Importance for News Multi-Document <lb/>Summarization <lb/> Kai Hong <lb/> University of Pennsylvania <lb/>Philadelphia, PA, 19104 <lb/> hongkai1@seas.upenn.edu <lb/> Ani Nenkova <lb/> University of Pennsylvania <lb/>Philadelphia, PA, 19104 <lb/> nenkova@seas.upenn.edu <lb/> Abstract <lb/> We introduce a supervised model <lb/>for predicting word importance that <lb/>incorporates a rich set of features. Our <lb/>model is superior to prior approaches <lb/>for identifying words used in human <lb/>summaries. <lb/>Moreover we show <lb/>that an extractive summarizer using <lb/>these estimates of word importance is <lb/>comparable in automatic evaluation with <lb/>the state-of-the-art. <lb/></front> 
			
			<body>1 Introduction <lb/> In automatic extractive summarization, sentence <lb/>importance is calculated by taking into account, <lb/>among possibly other features, the importance <lb/>of words that appear in the sentence. In this <lb/>paper, we describe experiments on identifying <lb/>words from the input that are also included in <lb/>human summaries; we call such words summary <lb/> keywords. <lb/> We review several unsupervised <lb/>approaches for summary keyword identification <lb/>and further combine these, along with features <lb/>including position, part-of-speech, subjectivity, <lb/>topic categories, context and intrinsic importance, <lb/>in a superior supervised model for predicting word <lb/>importance. <lb/>One of the novel features we develop aims <lb/>to determine the intrinsic importance of words. <lb/>To this end, we analyze abstract-article pairs in <lb/>the New York Times corpus (Sandhaus, 2008) <lb/>to identify words that tend to be preserved in <lb/>the abstracts. We demonstrate that judging word <lb/>importance just based on this criterion leads to <lb/>significantly higher performance than selecting <lb/>sentences at random. Identifying intrinsically <lb/>important words allows us to generate summaries <lb/>without doing any feature computation on the <lb/>input, equivalent in quality to the standard baseline <lb/>of extracting the first 100 words from the latest <lb/>article in the input. Finally, we integrate the <lb/>schemes for assignment of word importance into <lb/>a summarizer which greedily optimizes for the <lb/>presence of important words. We show that our <lb/>better estimation of word importance leads to <lb/>better extractive summaries. <lb/> 2 Prior work <lb/> The idea of identifying words that are descriptive <lb/>of the input can be dated back to Luhn&apos;s earliest <lb/>work in automatic summarization (Luhn, 1958). <lb/>There keywords were identified based on the <lb/>number of times they appeared in the input, <lb/>and words that appeared most and least often <lb/>were excluded. Then the sentences in which <lb/>keywords appeared near each other, presumably <lb/>better conveying the relationship between the <lb/>keywords, were selected to form a summary. <lb/>Many successful recent systems also estimate <lb/>word importance. The simplest but competitive <lb/>way to do this task is to estimate the word <lb/>probability from the input (Nenkova and <lb/>Vanderwende, 2005). Another powerful method <lb/>is log-likelihood ratio test (Lin and Hovy, 2000), <lb/>which identifies the set of words that appear in <lb/>the input more often than in a background corpus <lb/>(Conroy et al., 2006; Harabagiu and Lacatusu, <lb/>2005). <lb/>In contrast to selecting a set of keywords, <lb/>weights are assigned to all words in the input <lb/>in the majority of summarization methods. <lb/>Approaches based on (approximately) optimizing <lb/>the coverage of these words have become widely <lb/>popular. Earliest such work relied on TF*IDF <lb/>weights (Filatova and Hatzivassiloglou, 2004), <lb/>later approaches included heuristics to identify <lb/>summary-worthy bigrams (Riedhammer et al., <lb/>2010). Most optimization approaches, however, <lb/>use TF*IDF or word probability in the input as <lb/>word weights (McDonald, 2007; Shen and Li, <lb/>2010; Berg-Kirkpatrick et al., 2011). <lb/>

			<page> 712 <lb/></page>

			Word weights have also been estimated by <lb/>supervised approaches, with word probability and <lb/>location of occurrence as typical features (Yih et <lb/>al., 2007; Takamura and Okumura, 2009; Sipos et <lb/>al., 2012). <lb/>A handful of investigations have productively <lb/>explored the mutually reinforcing relationship <lb/>between word and sentence importance, iteratively <lb/>re-estimating each in either supervised or <lb/>unsupervised framework (Zha, 2002; Wan et <lb/>al., 2007; Wei et al., 2008; Liu et al., 2011). <lb/>Most existing work directly focuses on predicting <lb/>sentence importance, with emphasis on the <lb/>formalization of the problem (Kupiec et al., 1995; <lb/>Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., <lb/>2010). There has been little work directly focused <lb/>on predicting keywords from the input that will <lb/>appear in human summaries. Also there has been <lb/>only a few investigations of suitable features <lb/>for estimating word importance and identifying <lb/>keywords in summaries; we address this issue by <lb/>exploring a range of possible indicators of word <lb/>importance in our model. <lb/> 3 Data and Planned Experiments <lb/> We carry out our experiments on two datasets from <lb/>the Document Understanding Conference (DUC) <lb/>(Over et al., 2007). DUC 2003 is used for training <lb/>and development, DUC 2004 is used for testing. <lb/>These are the last two years in which generic <lb/>summarization was evaluated at DUC workshops. <lb/>There are 30 multi-document clusters in DUC <lb/>2003 and 50 in DUC 2004, each with about 10 <lb/>news articles on a related topic. The task is <lb/>to produce a 100-word generic summary. Four <lb/>human abstractive summaries are available for <lb/>each cluster. <lb/>We compare different keyword extraction <lb/>methods by the F-measure  1  they achieve against <lb/>the gold-standard summary keywords. We do not <lb/>use stemming when calculating these scores. <lb/>In our work, keywords for an input are defined <lb/>as those words that appear in at least i of the <lb/>human abstracts, yielding four gold-standard sets <lb/>of keywords, denoted by G  i  . |G  i  | is thus the <lb/>cardinality of the set for the input. We only <lb/>consider the words in the summary that also <lb/>appear in the original input 2 , with stopwords <lb/> 
			
			<note place="footnote">1 2*precision*recall/(precision+recall) <lb/></note> 
			
			<note place="footnote">2 On average 26.3% (15.0% with stemming) of the words <lb/>in the four abstracts never appear in the input.<lb/></note>
			
			excluded 3 . Table 1 shows the average number of <lb/>unique content words for the respective keyword <lb/>gold-standard. <lb/>i <lb/>1 <lb/>2 <lb/>3 4 <lb/>Mean |G  i  | 102 32 15 6 <lb/>Table 1: Average number of words in G  i <lb/> For the summarization task, we compare results <lb/>using ROUGE (Lin, 2004). We report ROUGE-1, <lb/>-2, -4 recall, with stemming and without removing <lb/>stopwords. We consider ROUGE-2 recall as <lb/>the main metric for this comparison due to its <lb/>effectiveness in comparing machine summaries <lb/>(Owczarzak et al., 2012). All of the summaries <lb/>were truncated to the first 100 words by ROUGE 4 . <lb/>We use Wilcoxon signed-rank test to examine <lb/>the statistical significance as advocated by Rankel <lb/>et al. (2011) for both tasks, and consider <lb/>differences to be significant if the p-value is less <lb/>than 0.05. <lb/> 4 Unsupervised Word Weighting <lb/> In this section we describe three unsupervised <lb/>approaches of assigning importance weights to <lb/>words. <lb/>The first two are probability and <lb/>log-likelihood ratio, which have been extensively <lb/>used in prior work. We also apply a markov <lb/>random walk model for keyword ranking, similar <lb/>to Mihalcea and Tarau (2004). In the next <lb/>section we describe a summarizer that uses these <lb/>weights to form a summary and then describe <lb/>our regression approach to combine these and <lb/>other predictors in order to achieve more accurate <lb/>predictions for the word importance in Section 7. <lb/>The task is to assign a score to each word in the <lb/>input. The keywords extracted are thus the content <lb/>words with highest scores. <lb/> 4.1 Word Probability (Prob) <lb/> The frequency with which a word occurs in the <lb/>input is often considered as an indicator of its <lb/>importance. The weight for a word is computed <lb/>as p(w) =  c(w) <lb/>N  , where c(w) is the number of <lb/>times word w appears in the input and N is the <lb/>total number of word tokens in the input. <lb/> 
			
			<note place="footnote">3 We use the stopword list from the SMART system <lb/>(Salton, 1971), augmented with punctuation and symbols. <lb/></note> 
			
			<note place="footnote">4 ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n <lb/>4 -m -a -l 100 -x <lb/></note>

			<page> 713 <lb/></page>

			4.2 Log-likelihood Ratio (LLR) <lb/> The log-likelihood ratio test (Lin and Hovy, 2000) <lb/>compares the distribution of a word in the input <lb/>with that in a large background corpus to identify <lb/>topic words. We use the Gigaword corpus (Graff et <lb/>al., 2007) for background counts. The test statistic <lb/>has a χ  2  distribution, so a desired confidence level <lb/>can be chosen to find a small set of topic words. <lb/> 4.3 Markov Random Walk Model (MRW) <lb/> Graph methods have been successfully applied to <lb/>weighting sentences for generic (Wan and Yang, <lb/>2008; Mihalcea and Tarau, 2004; Erkan and <lb/>Radev, 2004) and query-focused summarization <lb/>(Otterbacher et al., 2009). <lb/>Here instead of constructing a graph with <lb/>sentences as nodes and edges weighted by <lb/>sentence similarity, we treat the words as vertices, <lb/>similar to Mihalcea and Tarau (2004). The <lb/>difference in our approach is that the edges <lb/>between the words are defined by syntactic <lb/>dependencies rather than depending on the <lb/>co-occurrence of words within a window of k. We <lb/>use the Stanford dependency parser (Marneffe et <lb/>al., 2006). In our approach, we consider a word <lb/> w more likely to be included in a human summary <lb/>when it is syntactically related to other (important) <lb/>words, even if w itself is not mentioned often. <lb/>The edge weight between two vertices is equal to <lb/>the number of syntactic dependencies of any type <lb/>between two words within the same sentence in <lb/>the input. The weights are then normalized by <lb/>summing up the weights of edges linked to one <lb/>node. <lb/>We apply the Pagerank algorithm (Lawrence <lb/>et al., 1998) on the resulting graph. We set the <lb/>probability of performing random jump between <lb/>nodes λ=0.15. The algorithm terminates when <lb/>the change of node weight between iterations is <lb/>smaller than 10  −4  for all nodes. Word importance <lb/>is equal to the final weight of its corresponding <lb/>node in the graph. <lb/> 5 Summary Generation Process <lb/> In this section, we outline how summaries <lb/>are generated by a greedy optimization system <lb/>which selects the sentence with highest weight <lb/>iteratively. This is the main process we use in all <lb/>our summarization systems. For comparison we <lb/>also use a summarization algorithm based on KL <lb/>divergence. <lb/> 5.1 Greedy Optimization Approach <lb/> Our algorithm extracts sentences by weighting <lb/>them based on word importance. The approach is <lb/>similar to the standard word probability baseline <lb/>(Nenkova et al., 2006) but we explore a range <lb/>of possibilities for assigning weights to individual <lb/>words. For each sentence, we calculate the <lb/>sentence weight by summing up the weights of <lb/>all words, normalized by the number of words in <lb/>the sentence. We sort the sentences in descending <lb/>order of their scores into a queue. To create a <lb/>summary, we iteratively dequeue one sentence, <lb/>check if the sentence is more than 8 words (as <lb/>in Erkan and Radev (2004)), then append it to <lb/>the current summary if it is non-redundant. A <lb/>sentence is considered non-redundant if it is not <lb/>similar to any sentences already in the summary, <lb/>measured by cosine similarity on binary vector <lb/>representations with stopwords excluded. We use <lb/>the cut-off of 0.5 for cosine similarity. This value <lb/>was tuned on the DUC 2003 dataset, by testing the <lb/>impact of the cut-off value on the ROUGE scores <lb/>for the final summary. Possible values ranged <lb/>from 0.1 to 0.9 with step of 0.1. <lb/> 5.2 KL Divergence Summarizer <lb/> The KLSUM summarizer (Haghighi and <lb/>Vanderwende, 2009) aims at minimizing the KL <lb/>divergence between the probability distribution <lb/>over words estimated from the summary and <lb/>the input respectively. This summarizer is a <lb/>component of the popular topic model approaches <lb/>(Daumé and Marcu, 2006; Celikyilmaz and <lb/>Hakkani-Tür, 2011; Mason and Charniak, 2011) <lb/>and achieves competitive performance with <lb/>minimal differences compared to a full-blown <lb/>topic model system. <lb/> 6 Global Indicators from NYT <lb/> Some words evoke topics that are of intrinsic <lb/>interest to people. Here we search for global <lb/>indicators of word importance regardless of <lb/>particular input. <lb/> 6.1 Global Indicators of Word Importance <lb/> We analyze a large corpus of original documents <lb/>and corresponding summaries in order to identify <lb/>words that consistently get included in or excluded <lb/>from the summary. In the 2004-2007 NYT corpus, <lb/>many news articles have abstracts along with the <lb/>original article, which makes it an appropriate <lb/>

			<page> 714 <lb/></page>

			Metric <lb/> Top-30 words <lb/> KL(A ∥ G)(w) photo(s), pres, article, column, reviews, letter, York, Sen, NY, discusses, drawing, op-ed, holds, Bush <lb/>correction, editorial, dept, city, NJ, map, corp, graph, contends, Iraq, John, dies, sec, state, comments <lb/> KL(G ∥ A)(w) <lb/> Mr, Ms, p.m., lot, Tuesday, CA, Wednesday, Friday, told, Monday, time, a.m., added, thing, Sunday <lb/>things, asked, good, night, Saturday, nyt, back, senator, wanted, kind, Jr., Mrs, bit, looked, wrote <lb/> P rA(w) <lb/> photo, photos, article, York, column, letter, Bush, state, reviews, million, American <lb/>pres, percent, Iraq, year, people, government, John, years, company, correction <lb/>national, federal, officials, city, drawing, billion, public, world, administration <lb/> Table 2: Top 30 words by three metrics from NYT corpus <lb/> resource to do such analysis. We identified <lb/> 160, 001 abstract-original pairs in the corpus. <lb/>From these, we generate two language models, <lb/>one estimated from the text of all abstracts (LM  A  ), <lb/>the other estimated from the corpus of original <lb/>articles (LM  G  ). We use SRILM (Stolcke, 2002) <lb/>with Ney smoothing. <lb/>We denote the probability of word w in LM  A  as <lb/> P r  A  (w), the probability in LM  G  as P r  G  (w), and <lb/>calculate the difference P r  A  (w)−P r  G  (w) and the <lb/>ratio P r  A  (w)/P r  G  (w) to capture the change of <lb/>probability. In addition, we calculate KL-like <lb/>weighted scores for words which reflect both the <lb/>change of probabilities between the two samples <lb/>and the overall frequency of the word. Here <lb/>we calculate both KL(A ∥ G) and KL(G ∥ <lb/> A). Words with high values for the former score <lb/>are favored in the summaries because they have <lb/>higher probability in the abstracts than in the <lb/>originals and have relatively high probability in <lb/>the abstracts. The later score is high for words that <lb/>are often not included in summaries. <lb/> KL(A ∥ G)(w) = P r  A  (w) · ln <lb/> P r  A  (w) <lb/> P r  G  (w) <lb/> KL(G ∥ A)(w) = P r  G  (w) · ln <lb/> P r  G  (w) <lb/> P r  A  (w) <lb/> Table 2 shows examples of the global <lb/>information captured from the three types <lb/>of scores—KL(A ∥ G), KL(G ∥ A) and <lb/> P r  A  (w)—listing the 30 content words with <lb/>highest scores for each type. Words that tend to <lb/>be used in the summaries, characterized by high <lb/> KL(A ∥ G) scores, include locations (York, NJ, <lb/>Iraq), people&apos;s names and titles (Bush, Sen, John), <lb/> some abbreviations (pres, corp, dept) and verbs of <lb/>conflict (contends, dies). On the other hand, from <lb/> KL(G ∥ A), we can see that it is unlikely for <lb/>writers to include courtesy titles (Mr, Ms, Jr.) and <lb/>relative time reference in summaries. The words <lb/>with high P r  A  (w) scores overlaps with those <lb/>ranked highly by KL(A ∥ G) to some extent, <lb/>but also includes a number of generally frequent <lb/>words which appeared often both in the abstracts <lb/>and original texts, such as million and percent. <lb/> 6.2 Blind Sentence Extraction <lb/> In later sections we include the measures of <lb/>global word importance as a feature of our <lb/>regression model for predicting word weights for <lb/>summarization. Before turning to that, however, <lb/>we report the results of an experiment aimed to <lb/>confirm the usefulness of these features. We <lb/>present a system,  BLIND,  which uses only weights <lb/>assigned to words by KL(A ∥ G) from NYT, <lb/>without doing any analysis of the original input. <lb/>We rank all non-stopword words from the input <lb/>according to this score. The top k words are given <lb/>weight 1, while the others are given weight 0. <lb/>
			
			The summaries are produced following the greedy <lb/>procedure described in Section 5.1. <lb/>Systems <lb/>R-1 <lb/>R-2 R-4 <lb/>RANDOM <lb/>30.32 4.42 0.36 <lb/>BLIND (80 keywords) 30.77 5.18 0.53 <lb/>BLIND (300 keywords) 32.91 5.94 0.61 <lb/>LASTESTLEAD <lb/>31.39 6.11 0.63 <lb/>FIRST-SENTENCE <lb/>34.26 7.22 1.21 <lb/>Table 3: <lb/>Blind sentence extraction system, <lb/>compared with three baseline systems (%) <lb/>Table 3 shows that the BLIND system has R-2 <lb/>recall of 0.0594 using the top 300 keywords, <lb/>significantly better than picking sentences from <lb/>the input randomly. It also achieves comparable <lb/>performance with the baseline in DUC 2004, <lb/>formed by selecting the first 100 words from <lb/>the latest article in the input (LASTESTLEAD). <lb/>However it is significantly worse than another <lb/>baseline of selecting the first sentences from the <lb/>input. Table 4 gives sample summaries generated <lb/>by these three approaches. These results confirm <lb/>that the information gleaned from the analysis <lb/>

			<page> 715 <lb/></page>

			Random Summary <lb/> It was sunny and about 14 degrees C(57 degrees F) in Tashkent on Sunday. The president is a strong person, and he has been <lb/>through far more difficult political situations, Mityukov said, according to Interfax. But Yeltsin&apos;s aides say his first term, <lb/>from 1991 to 1996, does not count because it began six months before the Soviet Union collapsed and before the current <lb/>constitution took effect. He must stay in bed like any other person, Yakushkin said. The issue was controversial earlier this <lb/>year when Yeltsin refused to spell out his intentions and his aides insisted he had the legal right to seek re-election. <lb/> NYT Summary from global keyword selection, KL(A ∥ G), k = 300 <lb/> Russia&apos;s constitutional court opened hearings Thursday on whether Boris Yeltsin can seek a third term. Yeltsin&apos;s growing <lb/>health problems would also seem to rule out another election campaign. The Russian constitution has a two-term limit for <lb/>presidents. Russian president Boris Yeltsin cut short a trip to Central Asia on Monday due to a respiratory infection that <lb/>revived questions about his overall health and ability to lead Russia through a sustained economic crisis. The upper house of <lb/>parliament was busy voting on a motion saying he should resign. The start of the meeting was shown on Russian television. <lb/> First Sentence Generated Summary <lb/> President Boris Yeltsin has suffered minor burns on his right hand, his press office said Thursday. President Boris Yeltsin&apos;s <lb/>doctors have pronounced his health more or less normal, his wife Naina said in an interview published Wednesday. President <lb/>Boris Yeltsin, on his first trip out of Russia since this spring, canceled a welcoming ceremony in Uzbekistan on Sunday <lb/>because he wasn&apos;t feeling well, his spokesman said. Doctors ordered Russian President Boris Yeltsin to cut short his Central <lb/>
			
			Asian trip because of a respiratory infection and he agreed to return home Monday, a day earlier than planned, officials said. <lb/> Table 4: Summary comparison by Random, Blind Extraction and First Sentence systems <lb/>of NYT abstract-original pairs encodes highly <lb/>relevant information about important content <lb/>independent of the actual text of the input. <lb/> 7 Regression-Based Keyword Extraction <lb/> Here we introduce a logistic regression model <lb/>for assigning importance weights to words in the <lb/>input. Crucially, this model combines evidence <lb/>from multiple indicators of importance. We have <lb/>at our disposal abundant data for learning because <lb/>each content word in the input can be treated as <lb/>a labeled instance. There are in total 32, 052 <lb/> samples from the 30 inputs of DUC 2003 for <lb/>training, 54, 591 samples from the 50 inputs of <lb/>DUC 2004 for testing. For a word in the input, <lb/>we assign label 1 if the word appears in at least <lb/>one of the four human summaries for this input. <lb/>Otherwise we assign label 0. <lb/> In the rest of this section, we describe the rich <lb/>variety of features included in our system. We also <lb/>analyze and discuss the predictive power of those <lb/>features by performing Wilcoxon signed-rank test <lb/>on the DUC 2003 dataset. There are in total 9, 261 <lb/> features used, among them 1, 625 are significant <lb/>(p-value &lt; 0.05). We rank these features in <lb/>increasing p-values derived from Wilcoxon test. <lb/>Apart from the widely used features of word <lb/>frequency and positions, some other less explored <lb/>features are highly significant. <lb/> 7.1 Frequency Features <lb/> We use the Probability, LLR chi-square statistic <lb/>value and MRW scores as features. Since prior <lb/>work has demonstrated that for LLR weights in <lb/>particular, it is useful to identify a small set of <lb/>important words and ignore all other words in <lb/>summary selection (Gupta et al., 2007), we use <lb/>a number of keyword indicators as features. For <lb/>these indicators, the value of feature is 1 if the <lb/>word is ranked within top k  i  , 0 otherwise. Here k  i <lb/> are preset cutoffs  5  . These cutoffs capture different <lb/>possibilities for defining the keywords in the input. <lb/>We also add the number of input documents that <lb/>contain the word as a feature. There are a total of <lb/> 100 features in this group, all of which are highly <lb/>significant, ranked among the top 200. <lb/> 7.2 Standard features <lb/> We now describe some standard features which <lb/>have been applied in prior work on summarization. <lb/> Word Locations: Especially in news articles, <lb/>sentences that occur at the beginning are often the <lb/>most important ones. In line with this observation, <lb/>we calculate several features related to the position <lb/>in which a word appears. We first compute <lb/>the relative positions for word tokens, where <lb/>the tokens are numbered sequentially in order of <lb/>appearance in each document in the input. The <lb/>relative position for one word token is therefore <lb/>its corresponding number, divided by total number <lb/>of tokens minus one in the document, e.g., 0 <lb/> for the first token, 1 for the last token. For <lb/>each word, we calculate its earliest first location, <lb/>latest last location, average location and average <lb/>first location for tokens of this word across all <lb/>documents in the input. In addition we have a <lb/>binary feature indicating if the word appears in the <lb/> 
			
			<note place="footnote">5  10, 15, 20, 30, 40, · · · , 190, 200, 220, 240, 260, 280, <lb/>300, 350, 400, 450, 500, 600, 700 (in total 33 values) <lb/></note> 
			
			<page>716 <lb/></page> 
			
			first sentence and the number of times it appears <lb/>in a first sentence among documents in one input. <lb/>There are 6 features in this group. All of them are <lb/>very significant, ranked within the top 100. <lb/> Word type: These features include Part of <lb/>Speech (POS) tags, Name Entity (NE) labels and <lb/>capitalization information. We use the Stanford <lb/>POS-Tagger (Toutanova et al., 2003) and Name <lb/>Entity Recognizer (Finkel et al., 2005). We have <lb/>one feature corresponding to each possible POS <lb/>and NE tag. The value of this feature is the <lb/>proportion of occurrences of the word with this <lb/>tag; in most cases only one feature gets a non-zero <lb/>value. We have two features which indicate if <lb/>one word has been capitalized and the ratio of its <lb/>capitalized occurrences. <lb/>Most of the NE features (6 out of 8) are <lb/>significant: there are more Organizations and <lb/> Locations but fewer Time and Date words in the <lb/>human summaries. Of the POS tags, 11 out of 41 <lb/> are significant: there are more nouns (NN, NNS, <lb/>NNPS); fewer verbs (VBG, VBP, VB) and fewer <lb/>cardinal numbers in the abstracts compared to the <lb/>input. Capitalized words also tend to be included <lb/>in human summaries. <lb/> KL: Prior work has shown that having estimates <lb/>of sentence importance can also help in estimating <lb/>word importance (Wan et al., 2007; Liu et al., <lb/>2011; Wei et al., 2008). The summarizer based <lb/>on KL-divergence assigns importance to sentences <lb/>directly, in a complex function according to the <lb/>word distribution in the sentence. Therefore, <lb/>we use these summaries as potential indicators <lb/>of word importance. We include two features <lb/>here, the first one indicates if the word appears <lb/>in a KLSUM summary of the input, as well as <lb/>a feature corresponding to the number of times <lb/>the word appeared in that summary. Both of the <lb/>features are highly significant, ranked within the <lb/>top 200. <lb/> 7.3 NYT-weights as Features <lb/> We include features from the relative rank of <lb/>a word according to KL(A ∥ G), KL(G ∥ <lb/> A), P r  A  (w)−P r  G  (w), P r  A  (w)/P r  G  (w) and <lb/> P r  A  (w), derived from the NYT as described in <lb/>Section 6. If the rank of a word is within top-k <lb/>or bottom-k by one metric, we would label it as <lb/> 1, where k is selected from a set of pre-defined <lb/>values  6  . We have in total 70 features in this <lb/> 
			
			<note place="footnote">6 100, 200, 500, 1000, 2000, 5000, 10000 in this case. <lb/></note> 
			
			category, of which 56 are significant, 47 having <lb/>a p-value less than 10  −7  . The predictive power of <lb/>those global indicators are only behind the features <lb/>which indicates frequency and word positions. <lb/> 7.4 Unigrams <lb/> This is a binary feature corresponding to each <lb/>of the words that appeared at least twice in the <lb/>training data. The idea is to learn which words <lb/>from the input tend to be mentioned in the human <lb/>summaries. There are in total 8, 691 unigrams, <lb/>among which 1, 290 are significant. Despite the <lb/>high number of significant unigram features, most <lb/>of them are not as significant as the more general <lb/>ones we described so far. It is interesting to <lb/>compare the significant unigrams identified in the <lb/>DUC abstract/input data with those derived from <lb/>the NYT corpus. Unigrams that tend to appear in <lb/>DUC summaries include president, government, <lb/>political. We also find the same unigrams among <lb/>the top words from NYT corpus according to <lb/> KL(A ∥ G) . As for words unlikely to appear in <lb/>summaries, we see Wednesday, added, thing, etc, <lb/>which again rank high according to KL(G ∥ A). <lb/> 7.5 Dictionary Features: MPQA and LIWC <lb/> Unigram features are notoriously sparse. To <lb/>mitigate the sparsity problem, we resort to <lb/>more general groupings to words according to <lb/>salient semantic and functional categories. We <lb/>employ two hand-crafted dictionaries, MPQA for <lb/>subjectivity analysis and LIWC for topic analysis. <lb/>The MPQA dictionary (Wiebe and Cardie, <lb/>2005) contains words with different polarities <lb/>(positive, neutral, negative) and intensities (strong, <lb/>weak). The combinations correspond to six <lb/>features. It turns out that words with strong <lb/>polarity, either positive or negative, are seldomly <lb/>included in the summaries. Most strikingly, <lb/>the p-value from significance test for the strong <lb/>negative words is less than 10  −4  —these words <lb/>are rarely included in summaries. There is no <lb/>significant difference on weak polarity categories. <lb/>Another dictionary we use is LIWC (Tausczik <lb/>and Pennebaker, 2007), which contains manually <lb/>constructed dictionaries for multiple categories <lb/>of words. The value of the feature is 1 for <lb/>one word if the word appears in the particular <lb/>dictionary for the category. 34 out of 64 LIWC <lb/>features are significant. Interesting categories <lb/>which appear at higher rate in summaries include <lb/>events about death, anger, achievements, money <lb/> 
			
			<page>717 <lb/></page> 
			
			and negative emotions. Those that appear at lower <lb/>rate in the summaries include auxiliary verbs, hear, <lb/>pronouns, negation, function words, social words, <lb/>swear, adverbs, words related to families, etc. <lb/> 7.6 Context Features <lb/> We use context features here, based on the <lb/>assumption that context importance around a word <lb/>affects the importance of this word. For context <lb/>we consider the words before and after the target <lb/>word. We extend our feature space by calculating <lb/>the weighted average of the feature values of the <lb/>context words. For word w, we denote L  w  as the <lb/>set of words before w, R  w  as the set of words <lb/>after w. We denote the feature for one word as <lb/> w.f  i  , the way of calculating the newly extended <lb/>word-before feature w.l  f  i  could be written as: <lb/> w.l  f  i  = <lb/> ∑ <lb/> i <lb/> p(w  l  ) · w  l  .f  i  , ∀w  l  ∈ L  w <lb/> Here p(w  l  ) is the probability word w  l  appears <lb/>before w among all words in L  w  . <lb/>For context features, we calculate the weighted <lb/>average of the most widely used basic features, <lb/>including frequency, location and capitalization <lb/>for surrounding contexts. There are in total <lb/> 220 features of this kind, among which 117 are <lb/>significant, 74 having a p-value less than 10  −4  . <lb/> 8 Experiments <lb/> The performance of our logistic regression model <lb/>is evaluated on two tasks: keyword identification <lb/>and extractive summarization. We name our <lb/>system REGSUM. <lb/> 8.1 Regression for Keyword Identification <lb/> For each input, we define the set of keywords <lb/>as the top k words according to the scores <lb/>generated from different models. We compare <lb/>our regression system with three unsupervised <lb/>systems: PROB, LLR, MRW. To show the <lb/>effectiveness of new features, we compare our <lb/>results with a regression system trained only <lb/>on word frequency and location related features <lb/>described in Section 7. Those features are the <lb/>ones standardly used for ranking the importance <lb/>of words in recent summarization works (Yih et <lb/>al., 2007; Takamura and Okumura, 2009; Sipos et <lb/>al., 2012), and we name this system REGBASIC. <lb/>Figure 1 shows the performance of systems <lb/>when selecting the 100 words with highest weights <lb/>Figure 1: <lb/>Precision, Recall and F-score of <lb/>keyword identification, 100 words selected, G  1  as <lb/>gold-standard <lb/>as keywords. Each word from the input that <lb/>appeared in any of the four human summaries is <lb/>considered as a gold-standard keyword. Among <lb/>the unsupervised approaches, word probability <lb/>identifies keywords better than LLR and MRW <lb/>by at least 4% on F-score. REGBASIC does not <lb/>give better performance at keyword identification <lb/>compared with PROB, even though it includes <lb/>location information. Our system gets 2.2% <lb/> F-score improvement over PROB, 5.2% over <lb/>REGBASIC, and more improvement over the <lb/>other approaches. All of these improvements are <lb/>statistically significant by Wilcoxon test. <lb/>Table 5 shows the performance of keyword <lb/>identification for different G  i  and different <lb/>number of keywords selected. The regression <lb/>system has no advantage over PROB when <lb/>identifying keywords that appeared in all of the <lb/>four human summaries. However our system <lb/>achieves significant improvement for predicting <lb/>words that appeared in more than one or two <lb/>human summaries.  7 <lb/> 8.2 Regression for Summarization <lb/> We now show that the performance of extractive <lb/>summarization can be improved by better <lb/>estimation of word weights. We compare our <lb/>regression system with the four models introduced <lb/>in Section 8.1. We also include PEER-65, the best <lb/>system in DUC-2004, as well as KLSUM for <lb/>comparison. Apart from these, we compare our <lb/>model with two state-of-the-art systems, including <lb/>the submodular approach (SUBMOD) (Lin and <lb/> 
			
			<note place="footnote">7 We also apply a weighted keyword evaluation approach, <lb/>similar to the pyramid method for summarization. Still <lb/>our system shows significant improvement over the others. <lb/>See https://www.seas.upenn.edu/~hongkai1/regsum.html for <lb/>details. <lb/></note>

			<page> 718 <lb/></page>

			G  i  #words PROB LLR MRW REGBASIC REGSUM <lb/> G  1 <lb/> 80 <lb/>43.6 <lb/>37.9 <lb/>38.9 <lb/>39.9 <lb/>45.7 <lb/> G  1 <lb/> 100 <lb/>44.3 <lb/>38.7 <lb/>39.2 <lb/>41.0 <lb/>46.5 <lb/> G  1 <lb/> 120 <lb/>44.6 <lb/>38.5 <lb/>39.2 <lb/>40.9 <lb/>46.4 <lb/> G  2 <lb/> 30 <lb/>47.8 <lb/>44.0 <lb/>42.4 <lb/>47.4 <lb/>50.2 <lb/> G  2 <lb/> 35 <lb/>47.1 <lb/>43.3 <lb/>42.1 <lb/>47.0 <lb/>49.5 <lb/> G  2 <lb/> 40 <lb/>46.5 <lb/>42.4 <lb/>41.8 <lb/>46.4 <lb/>49.2 <lb/> G  3 <lb/> 10 <lb/>51.2 <lb/>46.2 <lb/>43.8 <lb/>46.9 <lb/>50.2 <lb/> G  3 <lb/> 15 <lb/>51.4 <lb/>47.5 <lb/>43.7 <lb/>49.8 <lb/>52.9 <lb/> G  3 <lb/> 20 <lb/>49.7 <lb/>47.6 <lb/>42.5 <lb/>49.3 <lb/>51.5 <lb/> G  4 <lb/> 5 <lb/>50.0 <lb/>48.8 <lb/>44.9 <lb/>43.6 <lb/>45.1 <lb/> G  4 <lb/> 6 <lb/>51.4 <lb/>46.9 <lb/>43.7 <lb/>45.2 <lb/>47.6 <lb/> G  4 <lb/> 7 <lb/>50.9 <lb/>48.2 <lb/>43.7 <lb/>45.8 <lb/>47.8 <lb/>Table 5: Keyword identification F-score (%) for different G  i  and different number of words selected. <lb/>Bilmes, 2012) and the determinantal point process <lb/>(DPP) summarizer (Kulesza and Taskar, 2012). <lb/>The summaries were kindly provided by the <lb/>authors of these systems (Hong et al., 2014). <lb/>As can been seen in Table 6, our system <lb/>outperforms PROB, LLR, MRW, PEER-65, <lb/>KLSUM and REGBASIC. These improvements <lb/>are significant on ROUGE-2 recall. Interestingly, <lb/>although the supervised system REGBASIC which <lb/>uses only frequency and positions achieve <lb/>low performance in keyword identification, the <lb/>summaries it generates are of high quality. The <lb/>inclusion of position features negatively affects the <lb/>performance in summary keyword identification <lb/>but boosts the weights for the words which appear <lb/>close to the beginning of the documents, which is <lb/>helpful for identifying informative sentences. By <lb/>including other features we greatly improve over <lb/>REGBASIC in keyword identification. Similarly <lb/>here the richer set of features results in better <lb/>quality summaries. <lb/>We also examined the ROUGE-1, -2, -4 <lb/>recall compared with the SUBMOD and DPP <lb/>summarizers  8  . There is no significant difference <lb/>on R-2 and R-4 recall compared with these <lb/>two state-of-the-art systems. DPP performed <lb/>significantly better than our system on R-1 recall, <lb/>but that system is optimizing on R-1 F-score in <lb/>training. Overall, our conceptually simple system <lb/>is on par with the state of the art summarizers and <lb/>points to the need for better models for estimating <lb/>word importance. <lb/> 
			
			<note place="footnote">8 The results are slightly different from the ones reported <lb/>in the original papers due to the fact that we truncated to 100 <lb/>words, while they truncated to 665 bytes. <lb/></note> 
			
			System <lb/>R-1 <lb/>R-2 R-4 <lb/>PROB <lb/>35.14 8.17 1.06 <lb/>LLR <lb/>34.60 7.56 0.83 <lb/>MRW <lb/>35.78 8.15 0.99 <lb/>REGBASIC 37.56 9.28 1.49 <lb/>KL <lb/>37.97 8.53 1.26 <lb/>PEER-65 <lb/>37.62 8.96 1.51 <lb/>SUBMOD <lb/>39.18 9.35 1.39 <lb/>DPP <lb/>39.79 9.62 1.57 <lb/> REGSUM 38.57 9.75 1.60 <lb/> Table 6: System performance comparison (%) <lb/> 9 Conclusion <lb/> We presented a series of experiments which <lb/>show that keyword identification can be improved <lb/>in a supervised framework which incorporates <lb/>a rich set of indicators of importance. We <lb/>also show that the better estimation of word <lb/>importance leads to better extractive summaries. <lb/>Our analysis of features related to global <lb/>importance, sentiment and topical categories <lb/>reveals rather unexpected results and confirms that <lb/>word importance estimation is a worthy research <lb/>direction. Success in the task is likely to improve <lb/>sophisticated summarization approaches too, as <lb/>well as sentence compression systems which use <lb/>only crude frequency related measures to decide <lb/>which words should be deleted from a sentence. 9 <lb/>
		</body>

		<back>
			
			<div type="acknowledgement">9 The work is partially funded by NSF CAREER award <lb/> IIS 0953445. <lb/></div>

			<page> 719 <lb/></page>

			<listBibl>References <lb/> Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. <lb/>2011. Jointly learning to extract and compress. In <lb/> Proceedings of ACL-HLT, pages 481–490. <lb/>Asli Celikyilmaz and Dilek Hakkani-Tur. <lb/>2010. <lb/>A hybrid hierarchical model for multi-document <lb/>summarization. In Proceedings of ACL, pages <lb/>815–824. <lb/>Asli Celikyilmaz and Dilek Hakkani-Tür. <lb/> 2011. <lb/>Discovery of topically coherent sentences for <lb/>extractive summarization. <lb/>In Proceedings of <lb/>ACL-HLT, pages 491–499. <lb/>John M. Conroy, Judith D. Schlesinger, and Dianne P. <lb/>O&apos;Leary. 2006. Topic-focused multi-document <lb/>summarization using an approximate oracle score. <lb/>In Proceedings of COLING/ACL, pages 152–159. <lb/>Hal Daumé, III and Daniel Marcu. 2006. Bayesian <lb/>query-focused summarization. In Proceedings of <lb/>ACL, pages 305–312. <lb/>Gunes Erkan and Dragomir R. Radev. 2004. Lexrank: <lb/>graph-based lexical centrality as salience in text <lb/>summarization. Journal of Artificial Intelligence <lb/>Research, 22(1):457–479. <lb/>Elena Filatova and Vasileios Hatzivassiloglou. 2004. <lb/>A formal model for information selection in <lb/>multi-sentence text extraction. In Proceedings of <lb/>COLING. <lb/> Jenny Rose Finkel, Trond Grenager, and Christopher <lb/>Manning. <lb/>2005. <lb/>Incorporating non-local <lb/>information into information extraction systems by <lb/>gibbs sampling. In Proceedings of ACL, pages <lb/>363–370. <lb/>D. Graff, J. Kong, K. Chen, and K. Maeda. 2007. <lb/>English gigaword third edition. Linguistic Data <lb/>Consortium, Philadelphia, PA. <lb/> Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. <lb/>2007. Measuring importance and query relevance <lb/>in topic-focused multi-document summarization. In <lb/> Proceedings of ACL, pages 193–196. <lb/>Aria Haghighi and Lucy Vanderwende. <lb/>2009. <lb/>Exploring content models for multi-document <lb/>summarization. In Proceedings of HLT-NAACL, <lb/> pages 362–370. <lb/>Sanda Harabagiu and Finley Lacatusu. 2005. Topic <lb/>themes for multi-document summarization. <lb/>In <lb/> Proceedings of SIGIR 2005, pages 202–209. <lb/>Kai Hong, John M. Conroy, Benoit Favre, Alex <lb/>Kulesza, Hui Lin, and Ani Nenkova. 2014. A <lb/>repositary of state of the art and competitive baseline <lb/>summaries for generic news summarization. In <lb/> Proceedings of LREC, May. <lb/>Alex Kulesza and Ben Taskar. 2012. Determinantal <lb/>point processes for machine learning. Foundations <lb/>and Trends in Machine Learning, 5(2–3). <lb/>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. <lb/>A trainable document summarizer. In Proceedings <lb/>of SIGIR, pages 68–73. <lb/>Page Lawrence, Brin Sergey, Rajeev Motwani, and <lb/>Terry Winograd. 1998. The pagerank citation <lb/>ranking: Bringing order to the web. Technical <lb/>report, Stanford University. <lb/>Hui Lin and Jeff Bilmes. 2012. Learning mixtures <lb/>of submodular shells with application to document <lb/>summarization. In UAI, pages 479–490. <lb/>Chin-Yew Lin and Eduard Hovy. <lb/>2000. <lb/>The <lb/>automated acquisition of topic signatures for text <lb/>summarization. In Proceedgins of COLING, pages <lb/>495–501. <lb/>Chin-Yew Lin. <lb/>2004. <lb/>Rouge: A package for <lb/>automatic evaluation of summaries. <lb/>In Text <lb/>Summarization Branches Out: Proceedings of the <lb/>ACL-04 Workshop, pages 74–81. <lb/>Marina Litvak, Mark Last, and Menahem Friedman. <lb/>2010. A new approach to improving multilingual <lb/>summarization using a genetic algorithm. <lb/>In <lb/> Proceedings of ACL, pages 927–936. <lb/>Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised <lb/>framework for keyword extraction from meeting <lb/>transcripts. Transactions on Audio Speech and <lb/>Language Processing, 19(3):538–548. <lb/>H. P. Luhn. <lb/>1958. <lb/>The automatic creation of <lb/>literature abstracts. IBM Journal of Research and <lb/>Development, 2(2):159–165, April. <lb/>M. Marneffe, B. Maccartney, and C. Manning. 2006. <lb/>Generating Typed Dependency Parses from Phrase <lb/>Structure Parses. In Proceedings of LREC-06, pages <lb/>449–454. <lb/>Rebecca Mason and Eugene Charniak. <lb/>2011. <lb/>Extractive multi-document summaries should <lb/>explicitly not contain document-specific content. <lb/>In Proceedings of the Workshop on Automatic <lb/>Summarization for Different Genres, Media, and <lb/>Languages, pages 49–54. <lb/>Ryan McDonald. 2007. A study of global inference <lb/>algorithms in multi-document summarization. In <lb/> Proceedings of ECIR, pages 557–564. <lb/>Rada Mihalcea and Paul Tarau. 2004. Textrank: <lb/>Bringing order into text. In Proceedings of EMNLP, <lb/> pages 404–411. <lb/>Ani Nenkova and Lucy Vanderwende. 2005. The <lb/>impact of frequency on summarization. Technical <lb/>report, Microsoft Research. <lb/> 
			
			<page>720 <lb/></page> 
			
			Ani Nenkova, Lucy Vanderwende, and Kathleen <lb/>McKeown. 2006. A compositional context sensitive <lb/>multi-document summarizer: exploring the factors <lb/>that influence summarization. In Proceedings of <lb/>SIGIR, pages 573–580. <lb/>Jahna Otterbacher, Günes Erkan, and Dragomir R. <lb/>Radev. <lb/>2009. <lb/>Biased lexrank: Passage <lb/>retrieval using random walks with question-based <lb/>priors. Information Processing and Management, <lb/> 45(1):42–54. <lb/>Paul Over, Hoa Dang, and Donna Harman. 2007. Duc <lb/>in context. Inf. Process. Manage., 43(6):1506–1520. <lb/>Karolina Owczarzak, John M. Conroy, Hoa Trang <lb/>Dang, and Ani Nenkova. 2012. An assessment <lb/>of the accuracy of automatic evaluation in <lb/>summarization. In NAACL-HLT 2012: Workshop <lb/>on Evaluation Metrics and System Comparison for <lb/>Automatic Summarization, pages 1–9. <lb/>Peter Rankel, John Conroy, Eric Slud, and Dianne <lb/>O&apos;Leary. 2011. Ranking human and machine <lb/>summarization systems. In Proceedings of EMNLP, <lb/> pages 467–473. <lb/>Korbinian Riedhammer, Benoˆ t Favre, and Dilek <lb/>Hakkani-Tür. <lb/>2010. <lb/>Long story short -<lb/>global unsupervised models for keyphrase based <lb/>meeting summarization. Speech Communication, <lb/> 52(10):801–815. <lb/>G. Salton. 1971. The SMART Retrieval System: <lb/>Experiments in Automatic Document Processing. <lb/> Prentice-Hall, Inc., Upper Saddle River, NJ, USA. <lb/>Evan Sandhaus. 2008. The new york times annotated <lb/>corpus. Linguistic Data Consortium, Philadelphia, <lb/>PA. <lb/> Chao Shen and Tao Li. 2010. Multi-document <lb/>summarization via the minimum dominating set. In <lb/> Proceedings of Coling, pages 984–992. <lb/>Ruben Sipos, Pannaga Shivaswamy, and Thorsten <lb/>Joachims. <lb/>2012. <lb/>Large-margin learning of <lb/>submodular summarization models. In Proceedings <lb/>of EACL, pages 224–233. <lb/>Andreas Stolcke. 2002. SRILM – an extensible <lb/>language modeling toolkit. <lb/>In Proceedings of <lb/>ICSLP, volume 2, pages 901–904. <lb/>Hiroya Takamura and Manabu Okumura. 2009. Text <lb/>summarization model based on maximum coverage <lb/>problem and its variant. In Proceedings of EACL, <lb/> pages 781–789. <lb/>Yla R Tausczik and James W Pennebaker. 2007. <lb/>The Psychological Meaning of Words: LIWC and <lb/>Computerized Text Analysis Methods. Journal of <lb/>Language and Social Psychology, 29:24–54. <lb/>Kristina Toutanova, Dan Klein, Christopher D. <lb/>Manning, and Yoram Singer. 2003. Feature-rich <lb/>part-of-speech tagging with a cyclic dependency <lb/>network. In Proceedings of the NAACL-HLT, pages <lb/>173–180. <lb/>Xiaojun Wan and Jianwu Yang. 2008. Multi-document <lb/>summarization using cluster-based link analysis. In <lb/> Proceedings of SIGIR, pages 299–306. <lb/>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. <lb/>2007. Towards an iterative reinforcement approach <lb/>for simultaneous document summarization and <lb/>keyword extraction. In Proceedings of ACL, pages <lb/>552–559. <lb/>Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He. 2008. <lb/>Query-sensitive mutual reinforcement chain and <lb/>its application in query-oriented multi-document <lb/>summarization. In Proceedings of SIGIR, pages <lb/>283–290. <lb/>Janyce Wiebe and Claire Cardie. 2005. Annotating <lb/>expressions of opinions and emotions in language. <lb/>language resources and evaluation. In Language <lb/>Resources and Evaluation (formerly Computers and <lb/>the Humanities), page 1(2). <lb/>Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, <lb/>and Hisami Suzuki. <lb/>2007. <lb/>Multi-document <lb/>summarization by maximizing informative <lb/>content-words. In Proceedings of IJCAI, pages <lb/>1776–1782. <lb/>Hongyuan Zha. 2002. Generic summarization and <lb/>keyphrase extraction using mutual reinforcement <lb/>principle and sentence clustering. In Proceedings <lb/>of SIGIR, pages 113–120. <lb/></listBibl>

			<page> 721 </page>

		</back>
	</text>
</tei>
