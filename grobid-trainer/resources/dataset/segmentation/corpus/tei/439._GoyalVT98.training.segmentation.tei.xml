<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<page>16 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<body>Quantized Overcomplete Expansions in <lb/>: <lb/>Analysis, Synthesis, and Algorithms <lb/>Vivek K Goyal, Student Member, IEEE, Martin Vetterli, Fellow, IEEE, and Nguyen T. Thao, Member, IEEE <lb/>Abstract-Coefficient quantization has peculiar qualitative ef-<lb/>fects on representations of vectors in IR N with respect to over-<lb/>complete sets of vectors. These effects are investigated in two <lb/>settings: frame expansions (representations obtained by forming <lb/>inner products with each element of the set) and matching pursuit <lb/>expansions (approximations obtained by greedily forming linear <lb/>combinations). In both cases, based on the concept of consistency, <lb/>it is shown that traditional linear reconstruction methods are <lb/>suboptimal, and better consistent reconstruction algorithms are <lb/>given. The proposed consistent reconstruction algorithms were in <lb/>each case implemented, and experimental results are included. <lb/>For frame expansions, results are proven to bound distortion <lb/>as a function of frame redundancy r and quantization step size <lb/>for linear, consistent, and optimal reconstruction methods. Taken <lb/>together, these suggest that optimal reconstruction methods will <lb/>yield O(1=r 2 ) mean-squared error (MSE), and that consistency <lb/>is sufficient to insure this asymptotic behavior. A result on the <lb/>asymptotic tightness of random frames is also proven. Applica-<lb/>bility of quantized matching pursuit to lossy vector compression <lb/>is explored. Experiments demonstrate the likelihood that a linear <lb/>reconstruction is inconsistent, the MSE reduction obtained with <lb/>a nonlinear (consistent) reconstruction algorithm, and generally <lb/>competitive performance at low bit rates. <lb/>Index Terms-Consistent reconstruction, frames, matching <lb/>pursuit, MSE bounds, optimal reconstruction, overcomplete <lb/>representations, quantization, source coding. <lb/>I. INTRODUCTION <lb/>L INEAR transforms and expansions are the fundamental <lb/>mathematical tools of signal processing. Yet the prop-<lb/>erties of linear expansions in the presence of coefficient <lb/>quantization are not yet fully understood. These properties are <lb/>most intricate when signal representations are with respect <lb/>to redundant, or overcomplete, sets of vectors. This paper <lb/>considers the effects of quantization in overcomplete finite <lb/></body>

			<front>Manuscript received March 15, 1996; revised May 15, 1997. This work <lb/>was supported in part by the U.S. Advanced Research Projects Agency <lb/>through a National Defense Science and Engineering Graduate Fellowship, the <lb/>U.S. National Science Foundation under Grant MIP-93-21302, and the Swiss <lb/>National Science Foundation under Grant 2100-043136.95. The material in <lb/>this paper was presented in part at the IEEE Data Compression Conference, <lb/>Snowbird, UT, March 28-30, 1995, and at the IEEE International Conference <lb/>on Acoustics, Speech, and Signal Processing, Atlanta, GA, May 7-10, 1996. <lb/>V. K. Goyal is with the Department of Electrical Engineering and Computer <lb/>Sciences, University of California, Berkeley, CA 94720 USA. <lb/>M. Vetterli is with the Laboratoire de Communications Audiovisuelles, <lb/>Département d&apos; Électricité, École Polytechnique Fédérale de Lausanne, CH-<lb/>1015 Lausanne, Switzerland, and the Department of Electrical Engineering <lb/>and Computer Sciences, University of California, Berkeley, CA 94720 USA. <lb/>N. T. Thao is with the Department of Electrical and Electronic Engineering, <lb/>The Hong Kong University of Science and Technology, Clear Water Bay, <lb/>Kowloon, Hong Kong. <lb/>Publisher Item Identifier S 0018-9448(98)00080-7. <lb/></front>

			<body>Fig. 1. Block diagram of reconstruction from quantized frame expansion. <lb/>linear expansions. Both fixed and adaptive basis methods are <lb/>studied. Although it represents an input vector as a linear <lb/>combination of elements from a representation set, the adaptive <lb/>basis method is in fact a nonlinear mapping. While many <lb/>other issues are explored, the unifying theme is that consistent <lb/>reconstruction methods [1] give considerable improvement <lb/>over linear reconstruction methods. <lb/>Consider the expansion-quantization-reconstruction sce-<lb/>nario depicted in Fig. 1. A vector <lb/>C is left-multiplied <lb/>by a matrix <lb/>C <lb/>of rank <lb/>to get <lb/>C . The <lb/>transformed source vector is scalar quantized, i.e., quantized <lb/>with a quantizer which acts separably on each component of , <lb/>to get . As shown in Section II-A.2, this type of representation <lb/>arises naturally in simple oversampled A/D conversion. In <lb/>general, this sort of representation may be desirable when <lb/>many coarse measurements can be made easily, but precise <lb/>measurements are difficult to make. How can one best estimate <lb/>from ? How does the quality of the estimate depend on <lb/>the properties of , in particular its number of rows ? These <lb/>are the fundamental questions addressed in Section II. <lb/>To put this in a solid framework, we review the basic <lb/>properties of frames and prove a new result on the tight-<lb/>ness of random frames. We then show that the quality of <lb/>reconstruction can be improved by using deterministic prop-<lb/>erties of quantization (consistent reconstruction), as opposed <lb/>to considering quantization to be the addition of noise that <lb/>is independent in each dimension. The relationship between <lb/>the redundancy of the frame and the minimum possible recon-<lb/>struction error is explored. <lb/>Without sophisticated coding, a nonadaptive overcomplete <lb/>expansion can be a very inefficient representation. In the <lb/>context of Fig. 1, coding <lb/>may be an inefficient way to <lb/>represent . But could we get a good representation if we <lb/>could choose a few components of <lb/>a posteriori which <lb/>best describe ? This question is related to adaptive basis <lb/>techniques described in Section III. <lb/>In Section III, the use of a greedy successive approximation <lb/>algorithm for finding sparse linear representations with respect <lb/>to an overcomplete set is studied. This algorithm, called <lb/>matching pursuit (MP) [2], has recently been applied to image <lb/>coding [3], [4] and video coding [5], [6], which inherently <lb/></body>

			<front>0018-9448/98$10.00 © 1998 IEEE <lb/></front>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>17 <lb/></page>

			<body>require coarse coefficient quantization. However, to the best <lb/>of our knowledge, the present work is the first to describe <lb/>the qualitative effects of coefficient quantization in matching <lb/>pursuit. In particular, as in Section II, we will find that <lb/>reconstruction can be improved by consistent reconstruction <lb/>techniques. <lb/>Except where noted, we consider vectors in a finite dimen-<lb/>sional Hilbert space <lb/>or C . For <lb/>, we use <lb/>the inner product <lb/>and the norm derived from <lb/>the inner product through <lb/>. <lb/>is used <lb/>to denote the Normal distribution with mean and covariance <lb/>matrix . The term squared error (SE) is used for the square <lb/>of the norm of the difference between a vector and an estimate <lb/>of the vector. The term mean-squared error (MSE) is reserved <lb/>for the ensemble average of SE or expected SE. <lb/>II. NONADAPTIVE EXPANSIONS <lb/>This section describes frames, which provide a general <lb/>framework for studying nonadaptive linear transforms. Frames <lb/>were introduced by Duffin and Schaeffer [7] in the context <lb/>of nonharmonic Fourier series. Recent interest in frames has <lb/>been spurred by their utility in analyzing discrete wavelet <lb/>transforms [8]-[10] and time-frequency decompositions [11]. <lb/>We are motivated by a desire to understand quantization effects <lb/>and efficient representations. <lb/>Section II-A begins with definitions and examples of frames. <lb/>It concludes with a theorem on the tightness of random <lb/>frames and a discussion of that result. Section II-B begins <lb/>with a review of reconstruction from exactly known frame <lb/>coefficients. The remainder of the section gives new results <lb/>on reconstruction from quantized frame coefficients. Most <lb/>previous work on frame expansions is predicated either on <lb/>exact knowledge of coefficients or on coefficient degradation <lb/>by white additive noise. For example, Munch [11] considered <lb/>a particular type of frame and assumed the coefficients were <lb/>subject to a stationary noise. This paper, on the other hand, is <lb/>in the same spirit as [1], and [12]-[15] in that it utilizes the <lb/>deterministic qualities of quantization. <lb/>A. Frames <lb/>1) Definitions and Basics: This subsection is largely adapt-<lb/>ed from [10, ch. 3]. Some definitions and notations have been <lb/>simplified because we are limiting our attention to <lb/>or C . <lb/>Let <lb/>, where <lb/>is a countable index set. <lb/>is called a frame if there exist <lb/>and <lb/>such <lb/>that for all <lb/>(1) <lb/>and <lb/>are called the frame bounds. The cardinality of <lb/>is denoted by <lb/>. The lower bound in (1) is equivalent to <lb/>requiring that <lb/>spans . Thus a frame will always have <lb/>. Also notice that one can choose <lb/>whenever <lb/>. We will refer to <lb/>as the <lb/>redundancy of the frame. A frame <lb/>is called a tight frame <lb/>if the frame bounds can be taken to be equal. It is easy to <lb/>verify that if is a tight frame with <lb/>for all <lb/>, <lb/>then <lb/>. <lb/>Given a frame <lb/>in , the associated frame <lb/>operator is the linear operator from <lb/>to C defined by <lb/>(2) <lb/>Since <lb/>is finite-dimensional, this operation is a matrix <lb/>multiplication where is a matrix with th row equal to . <lb/>Using the frame operator, (1) can be rewritten as <lb/>(3) <lb/>where <lb/>is the <lb/>identity matrix. (The matrix inequality <lb/>means that <lb/>is a positive semidefinite <lb/>matrix.) In this notation, <lb/>if and only if <lb/>is a <lb/>tight frame. From (3) we can immediately conclude that the <lb/>eigenvalues of <lb/>lie in the interval <lb/>; in the tight <lb/>frame case, all of the eigenvalues are equal. This gives a <lb/>computational procedure for finding frame bounds. Since it <lb/>is conventional to assume <lb/>is chosen as large as possible <lb/>and is chosen as small as possible, we will sometimes take <lb/>the minimum and maximum eigenvalues of <lb/>to be the <lb/>frame bounds. Note that it also follows from (3) that <lb/>is invertible because all of its eigenvalues are nonzero, and <lb/>furthermore <lb/>(4) <lb/>The dual frame of is defined as <lb/>, where <lb/>(5) <lb/>is itself a frame with frame bounds <lb/>and <lb/>. <lb/>Since <lb/>, any vector <lb/>can be written as <lb/>(6) <lb/>for some set of coefficients <lb/>. If <lb/>, <lb/>may <lb/>not be unique. We refer to (6) as a redundant representation <lb/>even though it is not necessary that more than <lb/>of the &apos;s <lb/>be nonzero. <lb/>2) Example: The question of whether a set of vectors form <lb/>a frame is not very interesting in a finite-dimensional space; <lb/>any finite set of vectors which span the space form a frame. <lb/>Thus if <lb/>vectors are chosen randomly with a circularly <lb/>symmetric distribution on , they almost surely form a frame. 1 <lb/>So in some sense, it is easier to find a frame than to give an <lb/>example of a set of vectors which do not form a frame. In <lb/>this section we give a single example of a structured family <lb/>of frames. We will prove certain properties of these frames in <lb/>Section II-B4. <lb/>Oversampling of a periodic, bandlimited signal can be <lb/>viewed as a frame operator applied to the signal, where the <lb/>frame operator is associated with a tight frame. If the samples <lb/>are quantized, this is exactly the situation of oversampled A/D <lb/></body>

			<note place="footnote">1 An infinite set in a finite-dimensional space can form a frame only if the <lb/>norms of the elements decay appropriately, for otherwise a finite upper frame <lb/>bound will not exist. <lb/></note>

			<page>18 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<body>conversion [1]. Let <lb/>, with odd. <lb/>Define a corresponding continuous-time signal by <lb/>(7) <lb/>where <lb/>. Any real-valued, -periodic, band-<lb/>limited, continuous-time signal can be written in this form. <lb/>Let <lb/>. Define a sampled version of <lb/>by <lb/>and let <lb/>Then we have <lb/>, where <lb/>with <lb/>(8) <lb/>Using the orthogonality properties of sine and cosine, it is <lb/>easy to verify that <lb/>, so <lb/>is an operator <lb/>associated with a tight frame. Pairing terms and using the <lb/>identity <lb/>, we find that each row of <lb/>has norm <lb/>. Dividing by <lb/>normalizes the frame and <lb/>results in a frame bound equal to the redundancy ratio . Also <lb/>note that is the oversampling ratio with respect to the Nyquist <lb/>sampling frequency. <lb/>3) Tightness of Random Frames: Tight frames constitute <lb/>an important class of frames. As we will see in Section II-<lb/>B1, since a tight frame is self-dual, it has some desirable <lb/>reconstruction properties. These extend smoothly to nearly <lb/>tight frames, i.e., frames with <lb/>close to one. Also, for <lb/>a tight frame (1) reduces to something similar to Parseval&apos;s <lb/>equality. Thus a tight frame operator scales the energy of <lb/>an input by a constant factor . Furthermore, it is shown in <lb/>Section II-B4 that some properties of &quot;typical&quot; frame operators <lb/>depend only on the redundancy. This motivates our interest <lb/>in the following theorem. <lb/>Theorem 1. Tightness of Random Frames: Let <lb/>be a sequence of frames in <lb/>such that <lb/>is generated by <lb/>choosing <lb/>vectors independently with a uniform distribution <lb/>on the unit sphere in <lb/>. Let <lb/>be the frame operator <lb/>associated with <lb/>. Then, in the mean squared sense, <lb/>elementwise as <lb/>Proof: See Appendix I-A. <lb/>Theorem 1 shows that a sequence of random frames with <lb/>increasing redundancy will approach a tight frame. Note <lb/>that although the proof in Appendix I-A uses an unrelated <lb/>strategy, the constant <lb/>is intuitive: If <lb/>is a tight <lb/>frame with normalized elements, then we have <lb/>because the frame bound equals the redundancy of <lb/>the frame. Numerical experiments were performed to confirm <lb/>this behavior and observe the rate of convergence. Sequences <lb/>of frames were generated by successively adding random <lb/>vectors (chosen according to the appropriate distribution) to <lb/>Fig. 2. Normalized frame bounds for random frames in IR 4 . <lb/>existing frames. Results shown in Fig. 2 are averaged results <lb/>for 1000 sequences of frames in <lb/>. Fig. 2 shows that <lb/>and <lb/>converge to <lb/>and that <lb/>converges to one. <lb/>B. Reconstruction from Frame Coefficients <lb/>One cannot rightly call a frame expansion a &quot;signal repre-<lb/>sentation&quot; without considering the viability of reconstructing <lb/>the original signal. This is the problem that we address <lb/>presently. <lb/>In Section II-B1, we review the basic properties of recon-<lb/>structing from (unquantized) frame coefficients. This material <lb/>is adapted from [10]. The subsequent sections consider the <lb/>problem of reconstructing an estimate of an original signal <lb/>from quantized frame coefficients. Classical methods are lim-<lb/>ited by the assumption that the quantization noise is white. Our <lb/>approach uses deterministic qualities of quantization to arrive <lb/>at the concept of consistent reconstruction [1]. Consistent <lb/>reconstruction methods yield smaller reconstruction errors than <lb/>classical methods. <lb/>1) Unquantized Case: Let <lb/>be a frame and assume the <lb/>notation of Section II-A1. In this subsection, we consider the <lb/>problem of recovering from <lb/>. Let <lb/>C be the frame operator associated with . It can be shown <lb/>[10, Proposition 3.2.3] that <lb/>. Thus a possible <lb/>reconstruction formula is given by <lb/>This formula is reminiscent of reconstruction from a discrete <lb/>Fourier transform (DFT) representation, in which case <lb/>In the DFT and inverse DFT, one set of vectors plays the <lb/>roles of both and because it is a tight frame in C . Other <lb/>reconstruction formulas are possible; for details the reader is <lb/>referred to [10, Sec. 3.2]. <lb/></body>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>19 <lb/></page>

			<body>2) Classical Method: We now turn to the question of re-<lb/>constructing when the frame coefficients <lb/>are <lb/>degraded in some way. Any mode of degradation is possible, <lb/>but the most practical situations are additive noise due to <lb/>measurement error or quantization. We are most interested in <lb/>the latter case because of its implications for efficient storage <lb/>and transmission of information. <lb/>Suppose we wish to approximate given <lb/>, where <lb/>C is a zero-mean noise, uncorrelated with . The key <lb/>to finding the best approximation is that <lb/>is <lb/>an -dimensional subspace of C . Hence the component of <lb/>perpendicular to <lb/>should not hinder our approximation, <lb/>and the best approximation is the projection of <lb/>onto <lb/>. By [10, Proposition 3.2.3], this approximation is <lb/>given by <lb/>(9) <lb/>Furthermore, because the component of <lb/>orthogonal to <lb/>does not contribute, we expect <lb/>to be smaller than <lb/>. The following proposition makes this <lb/>more precise. <lb/>Proposition 1. Noise Reduction in Linear Reconstruction: <lb/>Let <lb/>be a frame of unit-norm vectors with associated <lb/>frame operator <lb/>and let <lb/>, where the <lb/>&apos;s are independent random variables with mean zero and <lb/>variance . Then the MSE of the classical reconstruction (9) <lb/>satisfies <lb/>MSE <lb/>Proof: See Appendix I-B. <lb/>Corollary 1: If the frame in Proposition 1 is tight <lb/>MSE <lb/>Now consider the case where the degradation is due to <lb/>quantization. Let <lb/>and <lb/>, where <lb/>is <lb/>a frame operator. Suppose <lb/>, where <lb/>is a scalar quantization function, i.e., <lb/>where <lb/>, <lb/>, is a scalar quantization <lb/>function. One approach to approximating given is to treat <lb/>the quantization noise <lb/>as random, independent in each <lb/>dimension, and uncorrelated with . These assumptions make <lb/>the problem tractable using statistical techniques. The problem <lb/>reduces to the previous problem, and <lb/>is the best <lb/>approximation. Strictly speaking, however, the assumptions <lb/>on which this reconstruction is based are not valid because <lb/>is a deterministic quantity depending on , with interplay <lb/>between the components. <lb/>3) Consistent Reconstruction: <lb/>Definition 1. Consistency [1]: Let <lb/>. Let <lb/>and <lb/>. If <lb/>then <lb/>is called a consistent <lb/>estimate of from . An algorithm that produces consistent <lb/>estimates is called a consistent reconstruction algorithm. An <lb/>estimate that is not consistent is said to be inconsistent. <lb/>Fig. 3. Illustration of consistent reconstruction. <lb/>The essence of consistency is that is a consistent estimate <lb/>if it is compatible with the observed value of , i.e., it is <lb/>possible that is exactly equal to . In the case of quantized <lb/>frame expansions <lb/>, and one can give a geometric <lb/>interpretation. <lb/>induces a partitioning of <lb/>, which in <lb/>turn induces a partitioning of <lb/>through the inverse map <lb/>of <lb/>. A consistent estimate is simply one that falls in <lb/>the same partition region as the original signal vector. These <lb/>concepts are illustrated for <lb/>and <lb/>in Fig. 3. <lb/>The ambient space is <lb/>. The cube represents the partition <lb/>region in <lb/>containing <lb/>and has codebook value <lb/>. The plane is <lb/>and hence is the subspace within <lb/>which any unquantized value must lie. The intersection of the <lb/>plane with the cube gives the shaded triangle within which a <lb/>consistent estimate must lie. Projecting to <lb/>, as in the <lb/>classical reconstruction method, removes the out-of-subspace <lb/>component of <lb/>. As illustrated, this type of reconstruction <lb/>is not necessarily consistent. Further geometric interpretations <lb/>of quantized frame expansions are given in Appendix II. <lb/>With no assumptions on <lb/>other than that the partition <lb/>regions be convex, a consistent estimate can be determined <lb/>using the projection onto convex sets (POCS) algorithm [16]. <lb/>In this case, that implies generating a sequence of estimates <lb/>by alternately projecting on <lb/>and <lb/>. <lb/>When is a scalar quantizer and each component quantizer <lb/>is uniform, a linear program can be used to find consistent <lb/>estimates. For <lb/>denote the quantization <lb/>stepsize in the th component by <lb/>. For notational con-<lb/>venience, assume that the reproduction values lie halfway <lb/>between decision levels. Then for each , <lb/>. <lb/>To obtain a consistent estimate, for each we must have <lb/>. Expanding the absolute value, we find <lb/>the constraints <lb/>and <lb/>where <lb/>, and the inequalities are <lb/>elementwise. These inequalities can be combined into <lb/>(10) <lb/></body>

			<page>20 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<body>TABLE I <lb/>ALGORITHM FOR CONSISTENT RECONSTRUCTION <lb/>FROM A QUANTIZED FRAME EXPANSION <lb/>1. Form <lb/>F = F <lb/>0F and y = <lb/>1 <lb/>2 1 + <lb/>ŷ <lb/>1 <lb/>2 1 0 <lb/>ŷ : <lb/>2. Pick an arbitrary cost function c 2 IR N . <lb/>3. Use a linear programming method to find x to minimize c T x subject <lb/>to F x y. <lb/>The formulation (10) shows that can be determined through <lb/>linear programming [17]. The feasible set of the linear program <lb/>is exactly the set of consistent estimates, so an arbitrary cost <lb/>function can be used. This is summarized in Table I. <lb/>A linear program always returns a corner of the feasible <lb/>set [17, Sec. 8.1], so this type of reconstruction will not be <lb/>close to the centroid of the partition cell. Since the cells are <lb/>convex, one could use several cost functions to (presumably) <lb/>get different corners of the feasible set and average the results. <lb/>Another approach is to use a quadratic cost function equal to <lb/>the distance from the projection estimate given by (9). Both <lb/>of these methods will reduce the MSE by a constant factor. <lb/>They do not change the asymptotic behavior of the MSE as <lb/>the redundancy is increased. <lb/>4) Error Bounds for Consistent Reconstruction: In orthog-<lb/>onal representations, it is well understood that under very <lb/>general conditions, the MSE is <lb/>for small . For frame <lb/>expansions, how does the MSE depend on , for large , and <lb/>how does it depend on the reconstruction method? The MSE <lb/>obtained with any reconstruction method depends in general <lb/>on the distribution of the source. The evidence suggests that <lb/>any consistent reconstruction algorithm is essentially optimal, <lb/>in a sense made clear by the following propositions, and gives <lb/>MSE. <lb/>Proposition 2. MSE Lower Bound: Let be a random vari-<lb/>able with probability density function with support on a <lb/>bounded subset of <lb/>. Consider any set of quantized frame <lb/>expansions of for which <lb/>Unless <lb/>is degenerate in a way which allows for exact <lb/>reconstruction, any reconstruction algorithm will yield an MSE <lb/>that can be lower-bounded by <lb/>, where is a coefficient <lb/>independent of and a function of , , the diameter <lb/>of <lb/>, and the maximum density value . <lb/>Proof: See Appendix I-C. <lb/>Proposition 3. Squared-Error Upper Bound-DFT Case: <lb/>Fix a quantization stepsize <lb/>. For a sequence of <lb/>quantized frame expansions of a fixed <lb/>followed <lb/>by consistent reconstruction, the squared error can be upper-<lb/>bounded by an <lb/>expression under the following con-<lb/>ditions: <lb/>i) <lb/>odd: The frame operators are as in (8) and <lb/>or <lb/>ii) <lb/>even: The frame operators are as in (8) with the first <lb/>column removed and <lb/>. <lb/>Proof: See Appendix A-D. <lb/>Conjecture 1. MSE Upper Bound: Under very general con-<lb/>ditions, for any set of quantized frame expansions, any algo-<lb/>rithm that gives consistent estimates will yield an MSE that <lb/>can be upper-bounded by an <lb/>expression. <lb/>For this sort of general upper bound to hold, some sort of <lb/>nondegeneracy condition is required because we can easily <lb/>construct a sequence of frames with increasing for which <lb/>the frame coefficients give no additional information as is <lb/>increased. For example, we can start with an orthonormal <lb/>basis and increase by adding copies of vectors already in <lb/>the frame. Putting aside such pathological cases, simulations <lb/>for quantization of a source uniformly distributed on <lb/>support this conjecture. Simulations were performed with three <lb/>types of frame sequences: <lb/>I. A sequence of frames corresponding to oversampled <lb/>A/D conversion, as given by (8). This is the case in <lb/>which we have proven an <lb/>SE upper bound. <lb/>II. For <lb/>, , and , Hardin, Sloane, and Smith <lb/>have numerically found arrangements of up to 130 <lb/>points on -dimensional unit spheres that maximize <lb/>the minimum Euclidean norm separation [18]. <lb/>III. Frames generated by randomly choosing points on the <lb/>unit sphere according to a uniform distribution. <lb/>Simulation results are given in Fig. 4. The dashed, dotted, <lb/>and solid curves correspond to frame types I, II, and III, <lb/>respectively. The data points marked with &apos;s correspond <lb/>to using a linear program based on (10) to find consistent <lb/>estimates. The data points marked with &apos;s correspond to <lb/>classical reconstruction. The important characteristics of the <lb/>graph are the slopes of the curves. Note that <lb/>MSE <lb/>corresponds to a slope of 3.01 dB/octave and <lb/>MSE <lb/>corresponds to a slope of 6.02 dB/octave. The consistent <lb/>reconstruction algorithm exhibits <lb/>MSE for each of <lb/>the types of frames. The classical method exhibits <lb/>MSE behavior, as expected. It is particularly interesting to <lb/>note that the performance with random frames is as good as <lb/>with either of the other two types of frames. <lb/>Note that in light of Theorem 1, it may be useful to try to <lb/>prove Conjecture 1 only for tight frames. <lb/>5) Rate-Distortion Tradeoffs: We have asserted that opti-<lb/>mal reconstruction techniques give an MSE proportional to <lb/>, and the <lb/>MSE for orthogonal representations <lb/>extends to the frame case as well. Thus there are two ways to <lb/>reduce the MSE by approximately a factor of four: double <lb/>or halve . Our discussion has focused on expected distortion <lb/>without concern for rate, and there is no reason to think that <lb/>these options each have the same effect on the rate. <lb/>As the simplest possible case, suppose a frame expansion is <lb/>stored (or transmitted) as <lb/>-bit numbers, for a total rate of <lb/>bits per sample. Doubling gives <lb/>-bit numbers, <lb/>for a total rate of <lb/>bits per sample. On the other hand, <lb/>halving <lb/>results in <lb/>-bit numbers for a rate of <lb/>only <lb/>bits per sample. This example suggests that <lb/></body>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>21 <lb/></page>

			<body>Fig. 4. Experimental results for reconstruction from quantized frame expan-<lb/>sions. Shows O(1=r 2 ) MSE for consistent reconstruction and O(1=r) MSE <lb/>for classical reconstruction. <lb/>halving <lb/>is always the better option, but a few comments <lb/>are in order. One caveat is that in some situations, doubling <lb/>and halving may have very different costs. For example, the <lb/>much higher cost of halving <lb/>than of doubling is a major <lb/>motivating factor for oversampled A/D conversion. Also, if <lb/>is doubled, storing the result as <lb/>-bit values is far from <lb/>the best thing to do. This is because many of the <lb/>additional <lb/>numbers give little or no information on . This is discussed <lb/>further in Appendix II. <lb/>III. ADAPTIVE EXPANSIONS <lb/>Transform coding theory, as introduced in [19] and analyzed <lb/>in detail in [20], is predicated on fine quantization approx-<lb/>imations and assuming that signals are Gaussian. For most <lb/>practical coding applications, these assumptions do not hold, <lb/>so the wisdom of maximizing coding gain-which leads to <lb/>the optimality of the Karhunen-Loève transform-has been <lb/>questioned. More fundamentally, we can leave the usual <lb/>framework of static orthogonal transform coding and consider <lb/>the application of adaptive and nonlinear transforms. <lb/>The matching pursuit algorithm [2], described in Section <lb/>III-A, has both adaptive and nonlinear aspects. Given a source <lb/>vector and a frame <lb/>, it produces an approximate <lb/>signal representation <lb/>. It is adaptive in the <lb/>sense that the &apos;s depend on , yet it can be considered <lb/>nonadaptive because it is time-invariant for transforming a <lb/>sequence of source vectors. On the other hand, it has a linear <lb/>nature because it produces a linear representation, but it is <lb/>nonlinear because it does not satisfy additivity. 2 <lb/>The matching pursuit algorithm is a greedy algorithm for <lb/>choosing a subset of the frame and finding a linear combination <lb/></body>

			<note place="footnote">2 The usage of additivity is not obvious. Clearly if x 1 n01 <lb/>i=0 i &apos; k and <lb/>x 2 n01 <lb/>i=0 i &apos; k , then <lb/>x 1 + x 2 <lb/>n01 <lb/>i=0 <lb/>( i + i )&apos; k : <lb/>But in general the expansions of x 1 , x 2 , and x 1 + x 2 would not use the same <lb/>k i &apos;s; for this reason the transform is nonlinear. <lb/></note>

			<body>of that subset that approximates a given signal vector. The <lb/>use of a greedy algorithm is justified by the computational <lb/>intractability of finding the optimal subset of the original frame <lb/>[21, ch. 2]. In our finite-dimensional setting, this is very similar <lb/>to the problem of finding sparse approximate solutions to linear <lb/>systems. In that context, this greedy heuristic is well-known <lb/>and performance bounds have been proven [22]. <lb/>Quantization of coefficients in matching pursuit leads to <lb/>many interesting issues; some of these are discussed in Section <lb/>III-B. Along with exploring general properties of matching <lb/>pursuit, we are interested in its application to compressing <lb/>data vectors in <lb/>. A vector compression method based on <lb/>matching pursuit is described in Section III-C. <lb/>A. Matching Pursuit <lb/>1) Algorithm: Let <lb/>be a frame such <lb/>that <lb/>for all . is called the dictionary. Matching <lb/>pursuit (MP) is an algorithm to represent <lb/>by a linear <lb/>combination of elements of . In the first step of the algorithm, <lb/>is selected such that <lb/>is maximized. Then can <lb/>be written as its projection onto <lb/>and a residue <lb/>The algorithm is iterated by treating <lb/>as the vector to be <lb/>best approximated by a multiple of <lb/>. At step <lb/>, <lb/>is <lb/>chosen to maximize <lb/>and <lb/>Identifying <lb/>, we can write <lb/>(11) <lb/>Hereafter, we will denote <lb/>by . Note that the out-<lb/>put of a matching pursuit expansion is not only the coefficients <lb/>, <lb/>but also the indices <lb/>, <lb/>For storage <lb/>and transmission purposes, we will have to account for the <lb/>indices. <lb/>Matching pursuit was introduced to the signal processing <lb/>community in the context of time-frequency analysis by Mal-<lb/>lat and Zhang [2]. Mallat and his coworkers have uncovered <lb/>many of its properties [21], [23], [24]. <lb/>2) Discussion: Since <lb/>is determined by projection, <lb/>. Thus we have the &quot;energy conservation&quot; <lb/>equation <lb/>(12) <lb/>This fact, the selection criterion for , and the fact that <lb/>spans , can be combined for a simple convergence proof <lb/>for finite-dimensional spaces. In particular, the energy in the <lb/>residue is strictly decreasing until is exactly represented. <lb/>Even in a finite-dimensional space, matching pursuit is not <lb/>guaranteed to converge in a finite number of iterations. This <lb/>is a serious drawback when exact (or very precise) signal <lb/>expansions are desired, especially since an algorithm which <lb/>picks dictionary elements jointly would choose a basis from <lb/>the dictionary and get an exact expansion in steps. One way <lb/></body>

			<page>22 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<body>to speed convergence is to use an orthogonalized version of <lb/>MP which at each step modifies the dictionary and chooses <lb/>a dictionary element perpendicular to all previously chosen <lb/>dictionary elements. Since orthogonalized matching pursuit <lb/>does not converge significantly faster than the nonorthogo-<lb/>nalized version for a small number of iterations [6], [21], <lb/>[25], nonorthogonalized matching pursuit is not considered <lb/>hereafter. <lb/>Matching pursuit has been found to be useful in source <lb/>coding for two (related) reasons: The first reason-which was <lb/>emphasized in the original Mallat and Zhang paper [2]-has <lb/>been termed flexibility; the second is that the nonlinear approx-<lb/>imation framework allows greater energy compaction than a <lb/>linear transform. <lb/>MP is often said to have flexibility to differing signal <lb/>structures. The archetypal illustration is that a Fourier basis <lb/>provides a poor representation of functions well localized <lb/>in time, while wavelet bases are not well suited to rep-<lb/>resenting functions whose Fourier transforms have narrow, <lb/>high-frequency support [2]. The implication is that MP, with <lb/>a dictionary including a Fourier basis and a wavelet basis, <lb/>would avoid these difficulties. <lb/>Looking at the energy compaction properties of MP gives <lb/>a more extensive view of the potential of MP. Energy com-<lb/>paction refers to the fact that after an appropriately chosen <lb/>transform, most of the energy of a signal can be captured <lb/>by a small number of coefficients. In orthogonal transform <lb/>coding, getting high-energy compaction is dependent on de-<lb/>signing the transform based on knowledge of source statistics; <lb/>for fine quantization of a stationary Gaussian source the <lb/>Karhunen-Loève Transform is optimal [26]. Although both <lb/>produce an approximation for a source vector which is a linear <lb/>combination of basis elements, orthogonal transform coding <lb/>contrasts sharply with MP in that the basis elements are chosen <lb/>a priori and hence at best one can make the optimum basis <lb/>choice on average. In MP, a subset of the dictionary is chosen <lb/>in a per vector manner, so much more energy compaction is <lb/>possible. <lb/>To illustrate the energy compaction property of MP, con-<lb/>sider the following situation. A <lb/>source is to be <lb/>transform coded. Because the components of the source are <lb/>uncorrelated, no orthogonal transform will give energy com-<lb/>paction; so in the linear coding case, <lb/>coefficients will <lb/>capture <lb/>of the signal energy. A -step MP expansion will <lb/>capture much more of the energy. Fig. 5 shows the results <lb/>of a simulation with <lb/>. The plot shows the fraction <lb/>of the signal energy in the residual when one-to four-term <lb/>expansions are used. The dictionaries are generated randomly <lb/>according to a uniform distribution on the unit sphere. For <lb/>a corresponding number of terms, the energy compaction is <lb/>much better with MP than with a linear transform. Notice <lb/>in particular that this is true even if the dictionary is not <lb/>overcomplete <lb/>, in which case MP has no more <lb/>&quot;flexibility&quot; than an orthogonal basis representation. <lb/>B. Quantized Matching Pursuit <lb/>Define quantized matching pursuit (QMP) to be a modified <lb/>version of matching pursuit which incorporates coefficient <lb/>Fig. 5. Comparison of energy compaction properties for coding of a <lb/>N(0; I 8 ) source. With a k-term orthogonal expansion, the residual has <lb/>(8 0 k)=8 of the energy (&apos;s). The residual energy is much less with MP <lb/>(solid curves). <lb/>quantization. In particular, the inner product <lb/>is quantized to <lb/>prior to the computation of the <lb/>residual <lb/>. The quantized value is used in the residual <lb/>calculation: <lb/>. The use of the quantized <lb/>value in the residual calculation reduces the propagation of the <lb/>quantization error to subsequent iterations. <lb/>Although QMP has been applied to low bit rate compression <lb/>problems [5], [6], [25], which inherently require coarse coef-<lb/>ficient quantization, little work has been done to understand <lb/>the qualitative effects of coefficient quantization in matching <lb/>pursuit. In this section we explore some of these effects. The <lb/>relationship between quantized matching pursuit and other <lb/>vector quantization (VQ) methods is discussed in Section III-<lb/>B.1. The issue of consistency in these expansions is explored <lb/>in Section III-B.2. The potential lack of consistency shows <lb/>that even though matching pursuit is designed to produce a <lb/>linear combination to estimate a given source vector, optimal <lb/>reconstruction in the presence of coefficient quantization re-<lb/>quires a nonlinear algorithm. (Such an algorithm is presented <lb/>in Section III-C.2.) In Section III-B.3, a detailed example <lb/>on the application of QMP to quantization of an <lb/>-valued <lb/>source is presented. This serves to illustrate the concepts from <lb/>Section III-B.2 and demonstrate the potential for improved <lb/>reconstruction using consistency. <lb/>1) Relationship to Other Vector Quantization Methods: A <lb/>single iteration of matching pursuit is very similar to shape-<lb/>gain VQ, which was introduced in [27]. In shape-gain VQ, <lb/>a vector <lb/>is separated into a gain, <lb/>and a <lb/>shape, <lb/>. A shape is chosen from a shape codebook <lb/>to maximize <lb/>. Then a gain is chosen from a gain <lb/>codebook <lb/>to minimize <lb/>. The similarity is <lb/>clear with <lb/>corresponding to and <lb/>corresponding to the <lb/>quantizer for <lb/>, the only differences being that in MP one <lb/>maximizes the absolute value of the correlation and thus the <lb/>gain factor can be negative. Obtaining a good approximation <lb/>in shape-gain VQ requires that <lb/>forms a dense subset of <lb/>the unit sphere in <lb/>. The area of the unit sphere increases <lb/></body>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>23 <lb/></page>

			<body>exponentially with , making it difficult to use shape-gain VQ <lb/>in high-dimensional spaces. A multiple iteration application of <lb/>matching pursuit can be seen as a cascade form of shape-gain <lb/>VQ. <lb/>2) Consistency: We have thus far discussed only signal <lb/>analysis (or encoding) using QMP and not synthesis (recon-<lb/>struction) from a QMP representation. To the best of our <lb/>knowledge, all previous work with QMP has used <lb/>(13) <lb/>which results from simply using quantized coefficients in <lb/>(11) and setting the final residual to zero. Computing this <lb/>reconstruction has very low complexity, but its shortcoming <lb/>is that it disregards the effects of quantization; hence it can <lb/>produce inconsistent estimates. <lb/>Suppose iterations of QMP are performed with the dic-<lb/>tionary <lb/>and denote the output by <lb/>QMP <lb/>(14) <lb/>Denote the output of QMP (with the same dictionary and <lb/>quantizers) applied to by <lb/>By the definition of consistency (Section II-B3), <lb/>is a <lb/>consistent estimate of if and only if <lb/>and <lb/>for <lb/>. <lb/>We now develop a description of the set of consistent <lb/>estimates of through simultaneous linear inequalities. For <lb/>notational convenience, we assume uniform scalar quantization <lb/>of the coefficients with stepsize <lb/>and midpoint reconstruc-<lb/>tion. 3 The selection of <lb/>implies <lb/>(15) <lb/>For each element of <lb/>, (15) specifies a pair of half-<lb/>space constraints with boundary planes passing through the <lb/>origin. An example of such a constraint in <lb/>is shown in <lb/>Fig. 6(a). If <lb/>is the vector with the solid arrowhead (chosen <lb/>from all of the marked vectors), the source vector must lie <lb/>in the hatched area. For <lb/>, the intersection of these <lb/>constraints is two infinite convex polyhedral cones situated <lb/>symmetrically with their apexes at the origin. The value of <lb/>gives the constraint <lb/>(16) <lb/>This specifies a pair of planes, perpendicular to <lb/>, between <lb/>which must lie. Constraints (15) and (16) are illustrated in <lb/>Fig. 6(b) for <lb/>. The vector with the solid arrowhead was <lb/>chosen among all the marked dictionary vectors as <lb/>. Then <lb/></body>

			<note place="footnote">3 Ambiguities on partition cell boundaries due to arbitrary tie-breaking-in <lb/>both dictionary element selection and nearest neighbor scalar quantiza-<lb/>tion-are ignored. <lb/></note>

			<body>(a) <lb/>(b) <lb/>Fig. 6. (a) Illustration of consistency constraint (15) in IR 2 . (b) Illustration <lb/>of consistency constraints (15) and (16) in IR 3 . <lb/>the quantization of <lb/>implies that the source vector lies in <lb/>the volume shown. <lb/>At the <lb/>st step, the selection of gives the constraints <lb/>(17) <lb/>This defines <lb/>pairs of linear half-space constraints with <lb/>boundaries passing through <lb/>. As before, these <lb/>define two infinite pyramids situated symmetrically with their <lb/>apexes at <lb/>. Then <lb/>gives <lb/>(18) <lb/>This again specifies a pair of planes, now perpendicular to <lb/>, between which must lie. <lb/>By being explicit about the constraints as above, we see <lb/>that, except in the case that <lb/>for <lb/>some , the partition cell defined by (14) is convex. 4 Thus by <lb/>using an appropriate projection operator, one can find a strictly <lb/>consistent estimate from any initial estimate. The partition cells <lb/>are intersections of cells of the form shown in Fig. 6(b). <lb/>Notice now that contrary to what would be surmised from <lb/>(13), <lb/>gives some information on the signal even if <lb/>. <lb/>The experiments in Section III-C3 show that when <lb/>, <lb/>it tends to be inefficient in a rate-distortion sense to store or <lb/>transmit . If we know that <lb/>but do not know the value <lb/>of , then (17) and (18) reduce to <lb/>(19) <lb/>Experiments were performed to demonstrate that (13) often <lb/>gives inconsistent estimates and to assess how the probability <lb/>of an inconsistent estimate depends on the dictionary size <lb/>and the quantization. We present here results for an <lb/>-<lb/>valued source with the <lb/>distribution. The consistency <lb/>of reconstruction was checked for two iteration expansions <lb/>with dictionaries generated randomly according to a uniform <lb/></body>

			<note place="footnote">4 The &quot;hourglass&quot; cell that results from 0 2 [ i 01=2; i +1=2] does not <lb/>make consistent reconstruction more difficult, but is intuitively undesirable in <lb/>a rate-distortion sense. <lb/></note>

			<page>24 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<body>Fig. 7. Probability that (13) gives an inconsistent reconstruction for two <lb/>iteration expansions of an IR 4 -valued source. <lb/>distribution on the unit sphere. Dictionary sizes of <lb/>were used. The quantization was uniform with <lb/>reconstruction points <lb/>. The results are shown in <lb/>Fig. 7. The probability of inconsistency goes to zero for very <lb/>coarse quantization and goes to one for fine quantization. <lb/>The dependence on dictionary size and lack of monotonicity <lb/>indicate complicated geometric factors. Similar experiments <lb/>with different sources and dictionaries were reported in [28]. <lb/>As noted earlier, the cells of the partition generated by QMP <lb/>are convex or the union of two convex cells that share one <lb/>point. This fact allows the computation of consistent estimates <lb/>through the method of alternating projections [16]. One would <lb/>normally start with an initial estimate given by (13). Given an <lb/>estimate , the algorithm given in Table II performs the one <lb/>&quot;most needed&quot; projection; namely, the first projection needed <lb/>in enforcing (15)-(18). Among the possible projections in <lb/>enforcing (17), the one corresponding to the largest deviation <lb/>from consistency is performed. For notational convenience and <lb/>concreteness, we assume again uniform quantization with <lb/>Steps 5 and 6 could easily be adjusted for a general quantizer. <lb/>In a broadly applicable special case, the inequalities (15)-<lb/>(18) can be manipulated into a set of elementwise inequalities <lb/>suitable for reconstruction using linear or quadratic <lb/>programming, where <lb/>and are <lb/>and <lb/>, <lb/>respectively, and <lb/>and depend only on the QMP output. <lb/>This formulation is possible when each QMP iteration either <lb/>a) uses a quantizer with zero as a decision point; or b) uses <lb/>a quantizer which maps a symmetric interval to zero, and the <lb/>value of <lb/>is discarded when <lb/>. <lb/>Consider first the case where <lb/>has zero as a decision <lb/>point. For notational convenience, we will assume the decision <lb/>points and reconstruction values are given by <lb/>and <lb/>, respectively, but all that is actually <lb/>necessary is that the quantized coefficient <lb/>reveals the sign <lb/>of the unquantized coefficient . Denote <lb/>by , and <lb/>TABLE II <lb/>PROJECTION ALGORITHM FOR CONSISTENT <lb/>RECONSTRUCTION FROM A QMP REPRESENTATION <lb/>1. Set c = 0. This is a counter of the number of steps of QMP that <lb/>x <lb/>is consistent with. <lb/>2. Let <lb/>x = x 0 <lb/>c01 <lb/>i=0 <lb/>i D k <lb/>where it is understood that the summation is empty for c = 0 <lb/>3. Find &apos; 2 D that maximizes jh&apos;; xij. If &apos; = &apos; k , go to Step 5; else <lb/>go to Step 4. <lb/>4. (x is not consistent with k c .) Let <lb/>&apos;k = sgn (h&apos; k ; xi)&apos; k <lb/>and <lb/>&apos; = sgn (h&apos;; xi)&apos;: <lb/>Let <lb/>x = x 0 h&apos; k 0 &apos;; xi( &apos;k 0 &apos;) <lb/>the orthogonal projection of x onto the set described by (17). Terminate <lb/>5. (x is consistent with k c .) If <lb/>h&apos; k ; xi 2 [ c 0 1 <lb/>2 1; c + 1 <lb/>2 1) <lb/>go to Step 7; else go to Step 6. <lb/>6. (x is not consistent with c .) Let <lb/>=sgn(h&apos; k ; xi0 c ) <lb/>1 min h&apos; k ; xi0 c+ 1 <lb/>2 ; h&apos; k ; xi0 c0 1 <lb/>2 : <lb/>Let x = x0&apos; k , the orthogonal projection of x onto the set described <lb/>by (18). Terminate. <lb/>7. (x is consistent with c.) Increment c. If c = p, terminate (x is <lb/>consistent); else go to Step 2. <lb/>furthermore define the following <lb/>matrices: <lb/>First, write (17) as <lb/>where is shorthand for <lb/>. Combining the <lb/>nontrivial inequalities gives <lb/>Expanding the absolute value one can obtain <lb/>(20) <lb/>Writing (18) first as <lb/>one easily obtains <lb/>(21) <lb/></body>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>25 <lb/></page>

			<body>On the other hand, if <lb/>maps an interval <lb/>to zero and <lb/>is not coded, then (19) leads similarly to the <lb/>inequalities <lb/>(22) <lb/>A formulation of the form <lb/>is obtained by stacking <lb/>inequalities (20)-(22) appropriately. <lb/>3) An Example in <lb/>: Consider quantization of an <lb/>-<lb/>valued source. Assume that two iterations will be performed <lb/>with the four-element dictionary <lb/>Even if the distribution of the source is known, it is difficult to <lb/>find analytical expressions for optimal quantizers. (The issue <lb/>of optimal quantizer design is considered for the case of a <lb/>source with a uniform distribution on <lb/>in [28, Sec. <lb/>3.3.2].) Since we wish to use fixed, untrained quantizers, we <lb/>will use uniform quantizers for <lb/>and . It will generally <lb/>be true that <lb/>, so it makes sense for the quantization <lb/>step sizes for <lb/>and <lb/>to be equal. <lb/>The partitions generated by matching pursuit are very intri-<lb/>cate. In Fig. 8, the heavy lines show the partitioning of the first <lb/>quadrant when zero is a quantizer reconstruction value, i.e., <lb/>the quantizer reconstruction points are <lb/>and decision <lb/>points are <lb/>for some quantization stepsize . 5 <lb/>The dotted lines show boundaries that are created by choice <lb/>of <lb/>but, depending on the reconstruction method, might <lb/>not be important because <lb/>. In this partition, <lb/>most of the cells are squares, but there are also some smaller <lb/>cells. The fraction of cells that are not square goes to zero <lb/>as <lb/>. <lb/>This quantization of <lb/>gives concrete examples of the <lb/>inconsistency resulting from using (13). The linear recon-<lb/>struction points are indicated in Fig. 8 by &apos;s. The light <lb/>line segments connect these to the corresponding optimal 6 <lb/>reconstruction points. Such a line segment crossing a cell <lb/>boundary indicates a case of (13) giving an inconsistent <lb/>estimate. <lb/>C. Lossy Vector Coding with Quantized Matching Pursuit <lb/>This section explores the efficacy of using QMP as an <lb/>algorithm for lossy compression of vectors in <lb/>. In order <lb/>to reveal qualitative properties most clearly, very simple <lb/>dictionaries and synthetic sources are used in the experiments. <lb/>Experiments with other dictionaries and sources appear in <lb/>[28]. We do not explore the design of a dictionary or scalar <lb/>quantizers for a particular application. Dictionary structure has <lb/>a great impact on the computational complexity of QMP as <lb/>demonstrated, for example, in [29]. <lb/></body>

			<note place="footnote">5 The partition is somewhat different when the quantizer has different <lb/>decision points, e.g., f(m + 1 <lb/>2 )1g m2 [28, Sec. 3.3.2]. The ensuing <lb/>conclusions are qualitatively unchanged. <lb/>6 Optimality is with respect to a uniform source distribution. <lb/></note>

			<body>Fig. 8. Partitioning of first quadrant of IR 2 by matching pursuit with <lb/>four-element dictionary (heavy lines). Linear reconstruction points (&apos;s) are <lb/>connected to optimal reconstruction points (2&apos;s) by light line segments. <lb/>For simplicity, rate and distortion are measured by sample <lb/>entropy and MSE per component, respectively. The sources <lb/>used are multidimensional Gaussian with zero mean and <lb/>independent components. The inner product quantization is <lb/>uniform with midpoint reconstruction values at <lb/>. <lb/>Furthermore, the quantization stepsize <lb/>is constant across <lb/>iterations. This is consistent with equal weighting of error in <lb/>each direction. <lb/>1) Basic Experimental Results: In the first experiment, <lb/>and the dictionary was composed of <lb/>maximally spaced points on the unit sphere [18]. Rate was <lb/>measured by summing the (scalar) sample entropies of , <lb/>, where is the number <lb/>of iterations. The results are shown in Fig. 9. The three <lb/>dotted curves correspond to varying <lb/>from <lb/>to , while <lb/>reconstructing according to (13). The points along each dotted <lb/>curve are obtained by varying . Notice that the number <lb/>of iterations that minimizes the distortion depends on the <lb/>available rate. The solid curve is the convex hull of these R-D <lb/>operating points (converted to a decibel scale). In subsequent <lb/>graphs, only this convex hull performance is shown. <lb/>2) Improved Reconstruction Using Consistency: Continu-<lb/>ing the experiment above, the degree of improvement obtained <lb/>by using a consistent reconstruction algorithm was ascertained. <lb/>Using consistent reconstruction gives the performance shown <lb/>by the dashed curve in Fig. 9. Notice that there is no im-<lb/>provement at low bit rates because consistency is not an issue <lb/>for a single-iteration expansion. The improvement increases <lb/>monotonically with the bit rate. <lb/>3) An Effective Stopping Criterion: Regardless of the re-<lb/>construction method, the coding results shown in Fig. 9 are <lb/>far from satisfactory, especially at low bit rates. For a -step <lb/>expansion, the &quot;baseline&quot; coding method is to apply entropy <lb/>codes (separately) to <lb/>, <lb/>. This <lb/>coding places a rather large penalty of roughly <lb/>bits <lb/></body>

			<page>26 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<body>Fig. 9. Performance comparison between reconstruction based on (13) and <lb/>consistent reconstruction. N = 4 and the dictionary is composed of M = 11 <lb/>maximally spaced points on the unit sphere [18]. <lb/>Fig. 10. Performance comparison between a fixed number of iterations and a <lb/>simple stopping criterion. N = 4 and the dictionary is composed of M = 11 <lb/>maximally spaced points on the unit sphere [18]. <lb/>on each iteration, i.e., this many bits must be spent in addition <lb/>to the coding of the coefficient. In particular, the minimum <lb/>achievable bit rate is about <lb/>. <lb/>Assume that the same scalar quantization function is used at <lb/>each iteration and that the quantizer maps a symmetric interval <lb/>to zero. Based on a few simple observations, we can devise <lb/>a simple alternative coding method which greatly reduces the <lb/>rate. The first observation is that if <lb/>, then <lb/>for <lb/>all <lb/>because the residual remains unchanged. Secondly, <lb/>if <lb/>, then carries relatively little information. Thus we <lb/>propose that a) <lb/>be used as a stopping criterion which <lb/>causes a block to be terminated even if the maximum <lb/>of iterations has not been reached and b) <lb/>be considered <lb/>conceptually to come after , so <lb/>is not coded if <lb/>. <lb/>Simulations were performed with the same source, dictio-<lb/>nary, and quantizers as before to demonstrate the improvement <lb/>due to the use of a stopping criterion. The results, shown in <lb/>Fig. 10, indicate a sizable improvement at low bit rates. <lb/>4) Further Explorations: Having established the merits of <lb/>consistent reconstruction and the stopping criterion of Section <lb/>Fig. 11. Performance of QMP as the dictionary size is varied (solid curves, <lb/>labeled by M) compared to the performance of independent uniform quanti-<lb/>zation of each sample (dotted curve). <lb/>III-C3, we now explore the effects of varying the size of the <lb/>dictionary. Again the source is independent and identically <lb/>distributed (i.i.d.) Gaussian in blocks of <lb/>samples, <lb/>and dictionaries generated randomly according to a uniform <lb/>distribution on the unit sphere were used. Fig. 11 shows the <lb/>performance of QMP with <lb/>(solid curves); <lb/>and of independent uniform scalar quantization followed by <lb/>entropy coding (dotted curve). The performance of QMP <lb/>improves as <lb/>is increased and exceeds that of independent <lb/>uniform scalar quantization at low bit rates. This result high-<lb/>lights the advantage of a nonlinear transform, since no linear <lb/>transform would give any coding gain for this source. 7 <lb/>In the final experimental investigation, we consider the <lb/>lowest complexity instance of QMP. This occurs when the <lb/>dictionary is an orthonormal set. In this case, QMP reduces to <lb/>nothing more than a linear transform followed by sorting by <lb/>absolute value and quantization. Here we code an i.i.d. Gauss-<lb/>ian source with block sizes <lb/>. The results <lb/>shown in Fig. 12 indicate that even in this computationally <lb/>simple case without a redundant dictionary, QMP performs <lb/>well at low bit rates. An interesting phenomenon is revealed: <lb/>is best at high bit rates and <lb/>is best at low bit <lb/>rates; no larger value of <lb/>is best at any bit rate. <lb/>5) A Few Possible Variations: The experiments of the pre-<lb/>vious subsections are the tip of the iceberg in terms of possible <lb/>design choices. To conclude our discussion of source coding, <lb/>a few possible variations are presented along with plausibility <lb/>arguments for their application. <lb/>An obvious area to study is the design of dictionaries. For <lb/>static, untrained dictionaries, issues of interest include not only <lb/>R-D performance, but also storage requirements, complexity <lb/>of inner product computation, and complexity of largest inner <lb/>product search. <lb/>There is no a priori reason to use the same dictionary <lb/>at every iteration. Given a iteration estimate, the entropy <lb/></body>

			<note place="footnote">7 We are not advocating the use of random dictionaries. Slightly better <lb/>performance is expected with an appropriately chosen fixed dictionary. <lb/>8 Of course, N = 1 gives independent uniform scalar quantization of each <lb/>sample. <lb/></note>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>27 <lb/></page>

			<body>Fig. 12. Performance of QMP with an orthogonal basis dictionary as the <lb/>block size N is varied. <lb/>of <lb/>becomes a limiting factor in adding the results of an <lb/>additional iteration. To reduce this entropy, it might be useful <lb/>to use coarser dictionaries as the iterations proceed. Another <lb/>possibility is to adapt the dictionary by augmenting it with <lb/>samples from the source. (Dictionary elements might also be <lb/>deleted or adjusted.) The decoder would have be aware of <lb/>changes in the dictionary, but depending on the nature of the <lb/>adaptation, this may come without a rate penalty. <lb/>The experimental results that have been presented are based <lb/>on entropy coding each independently of the indices, which <lb/>are in turn coded separately; there are other possibilities. Joint <lb/>entropy coding of indices was explored in [28] and [30]. Also, <lb/>conditional entropy coding could exploit the likelihood of <lb/>consecutively chosen dictionary vectors being orthogonal or <lb/>nearly orthogonal. <lb/>Finally, for a broad class of source distributions, the distri-<lb/>butions of the &apos;s will have some common properties because <lb/>they are similar to order statistics. For example, the probability <lb/>density of <lb/>will be small near zero. This could be exploited <lb/>in quantizer design. <lb/>IV. CONCLUSIONS <lb/>This paper has considered the effects of coefficient quantiza-<lb/>tion in overcomplete expansions. Two classes of overcomplete <lb/>expansions were considered: fixed (frame) expansions and <lb/>expansions that are adapted to each particular source sample, <lb/>as given by matching pursuit. In each case, the possible <lb/>inconsistency of linear reconstruction was exhibited, compu-<lb/>tational methods for finding consistent estimates were given, <lb/>and the distortion reduction due to consistent reconstruction <lb/>was experimentally assessed. <lb/>For a quantized frame expansion with redundancy , it was <lb/>proven that any reconstruction method will give MSE that <lb/>can be lower-bounded by an <lb/>expression. Backed by <lb/>experimental evidence and a proof of a restricted case, it was <lb/>conjectured that any reconstruction method that gives consis-<lb/>tent estimates will have an MSE that can be upper-bounded <lb/>by an <lb/>expression. Taken together, these suggest that <lb/>optimal reconstruction methods will yield <lb/>MSE, and <lb/>that consistency is sufficient to insure this asymptotic behavior. <lb/>Experiments on the application of quantized matching pur-<lb/>suit as a vector compression method demonstrated good low <lb/>bit rate performance when an effective stopping criterion was <lb/>used. Since it is a successive approximation method, matching <lb/>pursuit may be useful in a multiresolution framework, and the <lb/>inherent hierarchical nature of the representation is amenable <lb/>to unequal error protection methods for transmission over <lb/>noisy channels. Because of the dependencies between outputs <lb/>of successive iterations, MP might also work well coupled <lb/>with adaptive and/or universal lossless coding. <lb/>APPENDIX I <lb/>PROOFS <lb/>A. Theorem 1 <lb/>Let <lb/>. The corresponding frame operator is <lb/>given by <lb/>. Thus the <lb/>th element <lb/>of <lb/>is given by <lb/>where <lb/>is the th component of <lb/>. <lb/>First consider the diagonal elements <lb/>. Since for a <lb/>fixed the random variables <lb/>, <lb/>are i.i.d., and <lb/>have zero mean, we find that <lb/>(23) <lb/>where <lb/>and <lb/>[31, Sec. 8-1]. For <lb/>the off-diagonal elements <lb/>(24) <lb/>(25) <lb/>Noting that <lb/>and <lb/>are independent of , (23) shows that <lb/>as <lb/>, so <lb/>in <lb/>the mean-squared sense [31, Sec. 8-4]. Similarly, (24) and (25) <lb/>show that for <lb/>, <lb/>in the mean-squared <lb/>sense. This completes the proof, provided <lb/>. <lb/>We now derive explicit formulas (depending on ) for , <lb/>, and <lb/>. For notational convenience, we omit <lb/>the subscript and use subscripts to identify the components <lb/>of the vector. To compute expectations, we need an expression <lb/>for the joint probability density of <lb/>. Denote <lb/>the -dimensional unit sphere (centered at the origin) by <lb/>. <lb/></body>

			<page>28 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<div type="annex">Since is uniformly distributed on <lb/>, the probability density <lb/>function (p.d.f.) of is given by <lb/>(26) <lb/>where <lb/>is the surface area of <lb/>. Using spherical coordi-<lb/>nates, <lb/>is given by <lb/>(27) <lb/>Using (26), we can make the following calculation: <lb/>where <lb/>is a differential area element <lb/>(28) <lb/>(29) <lb/>In this calculation, (28) results from using spherical coordi-<lb/>nates and (29) follows from substituting (27) and canceling <lb/>like terms. The final simplification is due to a standard <lb/>integration formula [32, eq. (323)]. Similar calculations give <lb/>and, for <lb/>, <lb/>B. Proposition 1 <lb/>Subtracting <lb/>from <lb/>gives <lb/>Then we can calculate <lb/>MSE <lb/>(30) <lb/>(31) <lb/>where (30) results from evaluating expectations using the <lb/>conditions on , and (31) uses (5). From (4) we can derive <lb/>which simplifies to <lb/>(32) <lb/>because of the normalization of the frame. Combining (31) <lb/>and (32) completes the proof. <lb/>C. Proposition 2 <lb/>Let us consider a given reconstruction algorithm. It maps <lb/>every possible discrete vector of <lb/>into a vector <lb/>of <lb/>. The reconstruction algorithm thus approximates any <lb/>input vector <lb/>by <lb/>where <lb/>. The reconstruction MSE is thus <lb/>. The mapping <lb/>is a vector <lb/>quantizer of <lb/>. For each discrete vector <lb/>, it <lb/>maps all vectors of the subset <lb/>of <lb/>into the <lb/>single vector <lb/>of <lb/>. According to the terminology <lb/>in vector quantization [26], <lb/>is a cell of the <lb/>partition defined by the vector quantizer <lb/>in <lb/>, and <lb/>is the corresponding code vector. <lb/>Let be the number of cells that can be found in the region <lb/>. It was proved by Zador [33], [34] (see also [35]) that there <lb/>exists a coefficient <lb/>which only depends on <lb/>and , <lb/>such that, for <lb/>large enough <lb/>(33) <lb/>To obtain a lower bound in terms of , let us calculate an <lb/>upper bound on in terms of . From (2) and the definition <lb/>of , we have <lb/>, where <lb/>. For each <lb/>, <lb/>is <lb/>a mapping from <lb/>to . Because <lb/>we have <lb/>(34) <lb/>Consider a fixed <lb/>. Because is a uniform <lb/>scalar quantizer of step size <lb/>Thus <lb/>is the subset of <lb/>delimited by the two <lb/>hyperplanes of equations <lb/>and <lb/></div>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>29 <lb/></page>

			<div type="annex">, respectively. These two parallel hyperplanes are <lb/>perpendicular to the vector ; the distance between them is <lb/>. Because <lb/>has its values on a discrete set <lb/>of equidistant points separated by <lb/>, the set of all possible <lb/>subsets <lb/>forms a partition of <lb/>whose cell boundaries <lb/>are formed by parallel and equidistant hyperplanes. This type <lb/>of partition was studied in [14] and is called a &quot;hyperplane <lb/>wave partition.&quot; The number <lb/>represents the density of <lb/>hyperplanes, or the number of hyperplanes per unit length in <lb/>their orthogonal direction. The vector <lb/>is called the density vector of the hyperplane wave partition. <lb/>Thanks to (34), we see that the partition induced by <lb/>is obtained by intersecting <lb/>hyperplane wave partitions. It <lb/>was shown in [14, Theorem A.7] that the number of cells <lb/>induced by such a partition in a region of diameter <lb/>can be <lb/>upper-bounded as <lb/>(35) <lb/>where <lb/>. In our case, <lb/>. <lb/>Writing <lb/>, we have <lb/>(36) <lb/>By combining (33), (35), and (36), we obtain <lb/>where <lb/>D. Proposition 3 <lb/>The proof is based on establishing the hypotheses of the <lb/>following lemma: <lb/>Lemma 1: Assume <lb/>defined in (7) has at least <lb/>quantization threshold crossings (QTC&apos;s) and consider <lb/>sampling at a rate of <lb/>samples per period. Then there exist <lb/>constants <lb/>and <lb/>depending only on <lb/>such that <lb/>for all <lb/>, whenever <lb/>and <lb/>have the <lb/>same quantized sampled versions <lb/>Proof: This is a version of [1, Theorem 4.1] for real-<lb/>valued signals. <lb/>The following lemma gives a rough estimate which allows <lb/>us to relate signal amplitude to signal power. 9 <lb/>Lemma 2: Among zero-mean, periodic signals with power <lb/>, the minimum possible peak-to-peak amplitude is <lb/>. <lb/>9 We use the standard notion of power; for y(t) with period T : <lb/>1=T <lb/>T <lb/>jy(t)j 2 dt: <lb/>Fig. 13. One period of the signal used in the proof of Lemma 2. <lb/>Proof: We will construct a signal <lb/>with power of <lb/>minimum peak-to-peak amplitude. For convenience, let <lb/>. <lb/>Without loss of generality, we can assume that , <lb/>, <lb/>such that <lb/>for <lb/>and <lb/>for <lb/>. Then, <lb/>to have minimum amplitude for given power, <lb/>must be <lb/>piecewise-constant as in Fig. 13, with <lb/>and <lb/>. <lb/>The mean and power constraints can be combined to give <lb/>. Under this constraint, the amplitude <lb/>is uniquely <lb/>minimized by <lb/>. <lb/>This final lemma relates the peak-to-peak amplitude of a <lb/>continuous signal to its quantization threshold crossings: <lb/>Lemma 3: A continuous, periodic signal with peak-to-peak <lb/>amplitude <lb/>which is subject to uniform quantization <lb/>with stepsize <lb/>has at least <lb/>quantization threshold <lb/>crossings per period. <lb/>Proof: Consider first a signal <lb/>with peak-to-peak <lb/>amplitude . The &quot;worst case&quot; is for <lb/>with <lb/>, and <lb/>for <lb/>and <lb/>to lie at quantization thresholds. <lb/>In this case, the most we can guarantee is <lb/>&quot;increasing&quot; <lb/>QTC&apos;s and <lb/>&quot;decreasing&quot; QTC&apos;s per period. If the peak-<lb/>to-peak amplitude exceeds , this worst case cannot happen, <lb/>and we get at least <lb/>QTC&apos;s. <lb/>Proof of the Proposition: <lb/>i) <lb/>odd: Quantized frame expansion of with <lb/>frame <lb/>vectors is precisely equivalent to quantized sampling of <lb/>with <lb/>samples per period (see Section II-A2). <lb/>Denote the quantized frame expansion of <lb/>and the <lb/>corresponding continuous-time signal by <lb/>and <lb/>, <lb/>respectively. It is easy to verify that the average time-<lb/>domain SE <lb/>is the same as the vector SE <lb/>. Let <lb/>. Then <lb/>is a zero-mean -periodic signal <lb/>with power <lb/>, which by hypothesis <lb/>is greater than <lb/>. Applying Lemma 2 we <lb/>conclude that <lb/>has peak-to-peak amplitude greater <lb/>than <lb/>. Since <lb/>has precisely the same <lb/>peak-to-peak amplitude as <lb/>, we can apply Lemma <lb/>3 to <lb/>to conclude that <lb/>has at least <lb/>QTC&apos;s. Applying Lemma 1 with <lb/>completes the <lb/>proof. <lb/></div>

			<page>30 <lb/></page>

			<note place="headnote">IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998 <lb/></note>

			<div type="annex">ii) <lb/>even: We need only make slight adjustments from <lb/>the previous case. Let <lb/>and define <lb/>and <lb/>correspondingly. Again the <lb/>average time-domain SE <lb/>is the same as the vector SE <lb/>. The power of <lb/>equals <lb/>. Applying Lemmas <lb/>2 and 3 implies that <lb/>has at least <lb/>QTC&apos;s. We apply Lemma 1, this time with <lb/>to match the form of (7), to complete the proof. <lb/>Note that the bounds in the hypotheses of the proposition <lb/>are not tight. This is evidenced in particular by the fact <lb/>that the bound in Lemma 2 is not attainable by bandlimited <lb/>signals. For example, for <lb/>the minimum peak-to-peak <lb/>amplitude is <lb/>and for <lb/>the minimum is <lb/>, compared to the bound of <lb/>. Because <lb/>of Gibbs&apos; phenomenon, the bound is not even asymptotically <lb/>tight, but a more complicated lemma would serve no purpose <lb/>here. <lb/>APPENDIX II <lb/>FRAME EXPANSIONS AND HYPERPLANE WAVE PARTITIONS <lb/>This appendix gives an interpretation of frame coefficients <lb/>as measurements along different directions. Given a frame <lb/>, the th component of <lb/>is <lb/>. <lb/>Thus is a measurement of along . We can thus interpret <lb/>as a vector of <lb/>&quot;measurements&quot; of in directions specified <lb/>by . Notice that in the original basis representation of , <lb/>we have <lb/>measurements of with respect to the directions <lb/>specified by the standard basis. Each of the <lb/>measurements <lb/>is needed to fix a point in <lb/>. On the other hand, the <lb/>measurements given in have only <lb/>degrees of freedom. <lb/>Now let us suppose <lb/>is scalar-quantized to give <lb/>by <lb/>rounding each component to the nearest multiple of . Since <lb/>specifies the measurement of a component parallel to <lb/>, <lb/>specifies an <lb/>-dimensional hyperplane <lb/>perpendicular to <lb/>. Thus quantization of <lb/>gives a set of <lb/>parallel hyperplanes spaced by , called a hyperplane single <lb/>wave. The <lb/>hyperplane single waves give a partition with <lb/>a particular structure called a hyperplane wave partition [14]. <lb/>Examples of hyperplane wave partitions are shown in Fig. 14. <lb/>In each diagram, a set of vectors comprising a frame in <lb/>is <lb/>shown superimposed on the hyperplane wave partition induced <lb/>by quantized frame expansion with that frame. <lb/>We can now interpret increasing the redundancy of a frame <lb/>as increasing the number of directions in which is measured. <lb/>It is well known that MSE is proportional to <lb/>. Section II-<lb/>B4 presents a conjecture that MSE is proportional to <lb/>. <lb/>This conjecture can be recast as saying that, asymptotically, <lb/>(a) <lb/>(b) <lb/>Fig. 14. Examples of hyperplane wave partitions in IR 2 . (a) M = 3. (b) <lb/>M = 5. <lb/>increasing directional resolution is as good as increasing <lb/>coefficient resolution. <lb/>In Section II-B5 it was mentioned that coding each com-<lb/>ponent of separately is inefficient when <lb/>. This can <lb/>be explained by reference to Fig. 14. Specifying <lb/>and <lb/>defines a parallelogram within which lies. Then there are <lb/>a limited number of possibilities for . (In Fig. 14(a), there <lb/>are exactly two possibilities. In Fig. 14(b), there are three or <lb/>four possibilities.) Then with , , and specified, there are <lb/>yet fewer possibilities for . If this is exploited fully in the <lb/>coding, the bit rate should only slightly exceed the logarithm <lb/>of the number of partition cells. <lb/></div>

			<div type="acknowledgement">ACKNOWLEDGMENT <lb/>The authors wish to thank Dr. D. Taubman for asking the <lb/>first author whether consistency is an issue in matching pursuit <lb/>and Dr. T. Kalker for providing some of the building blocks <lb/>for the simulation software. An anonymous reviewer made <lb/>several valuable comments. <lb/></div>

			<listBibl>REFERENCES <lb/>[1] N. T. Thao and M. Vetterli, &quot;Reduction of the MSE in R-times <lb/>oversampled A/D conversion from O(1=R) to O(1=R 2 ),&quot; IEEE Trans. <lb/>Signal Processing, vol. 42, pp. 200-203, Jan. 1994. <lb/>[2] S. G. Mallat and Z. Zhang, &quot;Matching pursuits with time-frequency <lb/>dictionaries,&quot; IEEE Trans. Signal Processing, vol. 41, pp. 3397-3415, <lb/>Dec. 1993. <lb/>[3] F. Bergeaud and S. Mallat, &quot;Matching pursuit of images,&quot; in Proc. IEEE <lb/>Int. Conf. on Image Processing (Washington, DC, Oct. 1995), vol. I, pp. <lb/>53-56. <lb/>[4] V. K Goyal and M. Vetterli, &quot;Dependent coding in quantized matching <lb/>pursuit,&quot; in Proc. SPIE Conf. on Visual Communication and Image <lb/>Processing (San Jose, CA, Feb. 1997), vol. 3024, pp. 2-14. <lb/>[5] R. Neff, A. Zakhor, and M. Vetterli, &quot;Very low bit rate video coding <lb/>using matching pursuits,&quot; in Proc. SPIE Conf. on Visual Communication <lb/>and Image Processing (Chicago, IL, Sept. 1994), vol. 2308, pp. 47-60. <lb/>[6] M. Vetterli and T. Kalker, &quot;Matching pursuit for compression and <lb/>application to motion compensated video coding,&quot; in Proc. IEEE Int. <lb/>Conf. on Image Processing (Austin, TX, Nov. 1994), vol. 1, pp. <lb/>725-729. <lb/>[7] R. J. Duffin and A. C. Schaeffer, &quot;A class of nonharmonic Fourier <lb/>series,&quot; Trans. Amer. Math. Soc., vol. 72, pp. 341-366, 1952. <lb/>[8] C. Heil and D. Walnut, &quot;Continuous and discrete wavelet transforms,&quot; <lb/>SIAM Rev., vol. 31, pp. 628-666, 1989. <lb/>[9] I. Daubechies, &quot;The wavelet transform, time-frequency localization and <lb/>signal analysis,&quot; IEEE Trans. Inform. Theory, vol. 36, pp. 961-1005, <lb/>Sept. 1990. <lb/>[10] <lb/>, Ten Lectures on Wavelets. Philadelphia, PA: Soc. Industrial <lb/>and Appl. Math., 1992. <lb/></listBibl>

			<note place="headnote">GOYAL et al.: QUANTIZED OVERCOMPLETE EXPANSIONS IN <lb/></note>

			<page>31 <lb/></page>

			<listBibl>[11] N. J. Munch, &quot;Noise reduction in tight Weyl-Heisenberg frames,&quot; IEEE <lb/>Trans. Inform. Theory, vol. 38, pp. 608-616, Mar. 1992. <lb/>[12] N. T. Thao, &quot;Deterministic analysis of oversampled A/D conversion and <lb/>sigma-delta modulation, and decoding improvements using consistent <lb/>estimates,&quot; Ph.D. dissertation, Columbia Univ., New York, 1993. <lb/>[13] N. T. Thao and M. Vetterli, &quot;Deterministic analysis of oversampled A/D <lb/>conversion and decoding improvement based on consistent estimates,&quot; <lb/>IEEE Trans. Signal Processing, vol. 42, pp. 519-531, Mar. 1994. <lb/>[14] <lb/>, &quot;Lower bound on the mean-squared error in oversampled <lb/>quantization of periodic signals using vector quantization analysis,&quot; <lb/>IEEE Trans. Inform. Theory, vol. 42, pp. 469-479, Mar. 1996. <lb/>[15] Z. Cvetković and M. Vetterli, &quot;Error analysis in oversampled A/D <lb/>conversion and quantization of Weyl-Heisenberg frame expansions,&quot; <lb/>submitted to IEEE Trans. Inform. Theory, 1996. <lb/>[16] D. C. Youla, &quot;Mathematical theory of image restoration by the method <lb/>of convex projections,&quot; in Image Recovery: Theory and Application, H. <lb/>Stark, Ed. New York: Academic, 1987. <lb/>[17] G. Strang, Introduction to Applied Mathematics. Cambridge, MA: <lb/>Wellesley-Cambridge, 1986. <lb/>[18] R. H. Hardin, N. J. A. Sloane, and W. D. Smith, &quot;Library of <lb/>best ways known to us to pack n points on sphere so that <lb/>minimum separation is maximized&quot; [Online]. Availabe WWW: <lb/>http://www.research.att.com/ njas/packings. <lb/>[19] H. P. Kramer and M. V. Mathews, &quot;A linear coding for transmitting <lb/>a set of correlated signals,&quot; IRE Trans. Inform. Theory, vol. IT-23, pp. <lb/>41-46, Sept. 1956. <lb/>[20] J.-Y. Huang and P. M. Schultheiss, &quot;Block quantization of correlated <lb/>Gaussian random variables,&quot; IEEE Trans. Commun., vol. COM-11, pp. <lb/>289-296, Sept. 1963. <lb/>[21] G. Davis, &quot;Adaptive nonlinear approximations,&quot; Ph.D. dissertation, New <lb/>York Univ., Sept. 1994. <lb/>[22] B. K. Natarajan, &quot;Sparse approximate solutions to linear systems,&quot; SIAM <lb/>J. Comput., vol. 24, no. 2, pp. 227-234, Apr. 1995. <lb/>[23] Z. Zhang, &quot;Matching pursuit,&quot; Ph.D. dissertation, New York Univ., <lb/>1993. <lb/>[24] G. Davis, S. Mallat, and Z. Zhang, &quot;Adaptive time-frequency approx-<lb/>imations with matching pursuits,&quot; New York Univ., Tech. Rep. 657, <lb/>Mar. 1994. <lb/>[25] T. Kalker and M. Vetterli, &quot;Projection methods in motion estimation <lb/>and compensation,&quot; in Proc. IS &amp; T/SPIE (San Jose, CA, Feb. 1995), <lb/>vol. 2419, pp. 164-175. <lb/>[26] A. Gersho and R. M. Gray, Vector Quantization and Signal Compression. <lb/>Boston, MA: Kluwer, 1992. <lb/>[27] A. Buzo, A. H. Gray, Jr., R. M. Gray, and J. D. Markel, &quot;Speech coding <lb/>based upon vector quantization,&quot; IEEE Trans. Acoust. Speech Signal <lb/>Process., vol. ASSP-28, pp. 562-574, Oct. 1980. <lb/>[28] V. K. Goyal, &quot;Quantized overcomplete expansions: Analysis, synthesis <lb/>and algorithms,&quot; UC-Berkeley/ERL, Tech. Rep. M95/57, July 1995. <lb/>[29] M. Goodwin, &quot;Matching pursuit with damped sinusoids,&quot; in Proc. <lb/>IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (Munich, <lb/>Germany, Apr. 1997), pp. 2037-2040. <lb/>[30] V. K. Goyal, M. Vetterli, and N. T. Thao, &quot;Quantization of overcomplete <lb/>expansions,&quot; in Proc. IEEE Data Compression Conf., J. A. Storer and <lb/>M. Cohn, Eds. Snowbird, UT: IEEE Comp. Soc. Press, Mar. 1995, <lb/>pp. 13-22. <lb/>[31] A. Papoulis, Probability, Random Variables, and Stochastic Processes, <lb/>3rd ed. New York: McGraw-Hill, 1994. <lb/>[32] S. M. Selby, Ed., Standard Mathematical Tables, 18th ed. Boca Raton, <lb/>FL: CRC, 1970. <lb/>[33] P. L. Zador, &quot;Development and evaluation of procedures for quantizing <lb/>multivariate distributions,&quot; Ph.D. dissertation, Stanford Univ., Stanford, <lb/>CA, 1963. <lb/>[34] <lb/>, &quot;Asymptotic quantization error of continuous signals and the <lb/>quantization dimension,&quot; IEEE Trans. Inform. Theory, vol. IT-28, pp. <lb/>139-148, Mar. 1982. <lb/>[35] J. H. Conway and N. J. A. Sloane, Sphere Packings, Lattices and Groups. <lb/>New York: Springer-Verlag, 1988. </listBibl>


	</text>
</tei>
