<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_C12-1051"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Proceedings of COLING 2012: Technical Papers, pages 833–848, <lb/>COLING 2012, Mumbai, December 2012. <lb/> Flexible Structural Analysis of Near-Meet-Semilattices for <lb/>Typed Unification-based Grammar Design <lb/> Rouz beh FARAH M AN D Ger al d P EN N <lb/> University of Toronto <lb/> {rouzbeh,gpenn}@cs.toronto.edu <lb/> Abstract <lb/> We present a new method for directly working with typed unification grammars in which <lb/>type unification is not well-defined. This is often the case, as large-scale HPSG grammars <lb/>now usually have type systems for which many pairs do not have least upper bounds. Our <lb/>method yields a unification algorithm that compiles quickly and yet is nearly as fast during <lb/>parsing as one that requires least upper bounds. The method also provides a natural naming <lb/>convention for unification results in cases where no user-defined type exists. <lb/> Keywords: HPSG, typed feature structures, unification-based grammar. <lb/></front>

			<page> 833 <lb/></page>

			<body> 1 Introduction <lb/> In Head-driven Phrase Structure Grammars (HPSG, Pollard and Sag, 1994), most of the <lb/>on-line computation time is spent performing unifications. HPSG type systems are typically <lb/>large and involve subtyping, so computing a unification involves computing a join, the <lb/>least informative type that is subsumed by both of the argument feature structures&apos; types. 1 <lb/> Early TFS-based parsing systems like ALE (Carpenter and Penn, 1996) required that every <lb/>pair of unifiable types have a least common supertype, i.e., joins are uniquely defined <lb/>wherever they need to be defined. Type hierarchies that have this requirement are called <lb/> meet semi-lattices (MSLs) (Davey and Priestley, 2002), because the existence of a meet for <lb/>every pair of types is sufficient to guarantee the existence of a join for every unifiable pair <lb/>of types when the type hierarchy is finite. Later HPSG parsing systems such as the LKB <lb/>(Copestake and Flickinger, 2000) and PET (Callmeier, 2000) eliminated this restriction, <lb/>and it is now extremely rare to find a decent-sized HPSG with an MSL type hierarchy. The <lb/>LKB, at least in some early versions, used an on-line caching algorithm to add joins to the <lb/>user-defined type system when necessary. PET adds all of the necessary joins in advance, <lb/>during a grammar compilation stage. In either case, the time it costs to perform these <lb/>repairs is proportional to the number of new joins that must be added. It therefore makes <lb/>some sense to strive to add the least number of types necessary. <lb/>While modern HPSG type systems are never MSLs in practice, they are always almost MSLs, <lb/>in a sense that can be made mathematically precise, which we do here for the first time. <lb/>Converting an HPSG type system to an MSL would be horrendously slow if it were not the <lb/>case that HPSGs were almost MSLs already, because an exponential number of types need <lb/>to be added in the worst case. There is another cost hidden in this method, furthermore, at <lb/>least when we strive to add the least number of extra types necessary. In this case, naming <lb/> these types is also a challenge. Figure 1(a), for example, shows an input type hierarchy <lb/>that is not an MSL. In Figure 1(b), it has been converted into an MSL by adding one extra <lb/>type. Ideally, we would like to name this type in a manner that reflects its usage in the <lb/>grammar. Two possibilities that are immediately evident are  e ∨ f  or  a ∧ b ∧ c ∧ d.  Neither <lb/>of these are ideal because they do not attest to a precise pair of types that the system <lb/>needed to unify during parsing, and which therefore gave rise to the need for this extra type. <lb/>This is extremely useful for debugging understanding feature structure outputs with large <lb/>grammars. Grammar writers instantly know what user-defined types had been combined. <lb/>The key to the alternative introduced here is that it actually costs less time in practice to <lb/>compute with a larger type system provided that joins are added on-line when necessary. <lb/>This is achieved by doing a small amount of extra compilation off-line — small in practice <lb/>again because of a specific mathematical property that practical HPSGs possess -and using <lb/>the data structure that it creates to add many more types during parsing than the absolute <lb/>minimum necessary for MSL-hood. In the limit, our method creates a type hierarchy more <lb/>like a conjunctive lattice, but it only adds types as it needs them. Figure 1(c) is what it <lb/>creates from Figure 1(a). From the standpoint of naming conventions, this type system is <lb/>ideal, because every added type is named for the set (pair or larger) of types that required <lb/>it. <lb/>

			<note place="footnote"> 1 Some of the HPSG community draw these subtypes below their supertypes in graphical representations of type <lb/>hierarchies, but still use the name join, even though the correct term is meet in this orientation. We will also use <lb/>the term, join, but will depict type hierarchies in the opposite orientation, in which more specific subtypes appear <lb/>above their more general supertypes. <lb/></note>

			<page> 834 <lb/></page>

			(a) <lb/> (b) <lb/>(c) <lb/>Figure 1: (a) A non-MSL type hierarchy, (b) its MSL completion based on the Dedekind-MacNeille <lb/>completion, and (c) its conjunctive lattice. <lb/> Section 2 discusses the mathematical background necessary to understand the conversion <lb/>that yields Figure 1(b). Section 3 presents a precise way of appreciating what we mean by a <lb/> &quot; near-meet-semilattice, &quot; through quantifying the size and number of a special kind of subset <lb/>of types called prime sets. Section 4 then introduces another special kind of subset that can <lb/>be used to efficiently compute the prime sets of a type hierarchy. Section 4.1 describes how <lb/>joins are computed with the new method, and what the compiler computes to achieve this. <lb/>Section 6 then presents an evaluation which shows that the new method has nearly the <lb/>same performance at run-time (5.4% slower) as parsing with precompiled MSLs directly, <lb/>but without the compile-time cost of the latter. We assert that in the context of grammar <lb/>design, 5% is worth less compilation time along with the better naming conventions that <lb/>naturally ensue from this method. Irrespective of compiling and parsing speed, the formal <lb/>treatment of this subject also provides a more refined theory for analyzing the structure of <lb/>type hierarchies in large-scale grammars than a binary MSL/non-MSL distinction. <lb/>

			<page> 835 <lb/></page>

			2 Dedekind-MacNeille Completions <lb/> Let  P  be a partially ordered set, and  S ⊆ P.  The set of upper bounds of  S,  written  S u  , is the <lb/>set of all  x ∈ P  such that for all  s ∈ S, s ≤  P x.  The lower bounds,  S l  , are all of the  x ∈ P <lb/> such that for all  s ∈ S, x ≤  P s.  The Dedekind-MacNeille completion of  P  (DM(P)) is the set <lb/>of all  A ⊆ P  such that  A ul  = A.  All of the singleton subsets of  P  are included in  DM (P),  so <lb/>informally we can say that  P  is included in  DM (P). DM (P)  is the smallest set that both <lb/>includes  P  and is an MSL (Davey and Priestley, 2002). <lb/>The computer science literature on lattice theory has tended to emphasize  DM (P)  as either <lb/>a theoretical device for understanding (but not computing) joins even when they are not <lb/>present (e.g., Aït-Kaci et al., 1989), or a goal to aim for when adding joins incrementally, <lb/>each as it becomes necessary (e.g., Bertet et al., 1997). Using  DM (P)  is supposed to provide <lb/>us with some sort of reassurance about the maximum amount of extra work that must <lb/>be performed to embed  P  into an MSL, at least if the results are pre-compiled or cached. <lb/>Because  |DM (P)|  is exponential in  |P|  in the worst case, however, this does not provide <lb/>much solace by itself. What matters in the end is the actual amount of overhead accrued for <lb/>working with a non-MSL type hierarchy. This is true even if  DM (P)  is not used. <lb/>That overhead can be paid either all at once during an initial compilation stage, or in small <lb/>amounts over time, with the hope that some completion types will never be needed by the <lb/>user&apos;s queries. Obviously, we do not want to compute any completion larger than  DM (P) <lb/> at compile-time if we can help it. The prospect of incrementally computing completion <lb/>types in a larger lattice is still available, however, as long as the overhead is still acceptable <lb/>in practice. It is also worth noting that the incremental algorithm of Bertet et al. (1997) <lb/>includes a line which stops execution for every type that is added and prompts the user <lb/>to name it. The problem of naming in the Dedekind-MacNeille completion extends well <lb/>beyond applications to computational linguistics. <lb/> 3 Prime Sets <lb/> What remains then is to find a new source of reassurance about on-line cost that depends <lb/>upon structural properties of the input type hierarchy rather than upon the size of the <lb/>worst-case lattice using a given completion strategy. The best structural property of all is for <lb/> P  to be an MSL already. In this case,  DM (P)  consists only of singleton sets plus possibly a <lb/>new top-most element (which is usually discarded anyway), and so  |DM (P)| ≤ |P| + 1. <lb/> Paradoxically, even if  P  were not an MSL, finding the upper bound of a set of arguments <lb/> S ⊆ P  in many cases is as easy as combining every type of  S  in succession, according to <lb/>some arbitrary linear ordering of its elements. At each step, we compute the join of the <lb/>current element and a running accumulator type, or fail if they have no upper bounds in <lb/>common. That is still linear time in  |S|,  even if non-linear in other variables, as is the case <lb/>when  P  is an MSL. It is difficult to imagine doing any better in this particular dimension. <lb/>The trouble is that this linear-time incremental method does not always work, because of <lb/>subsets of  S  (or  P)  that we will call prime sets.  2 . <lb/> Definition 1. Let P be a partially ordered set, and S ⊆ P. S is an anti-chain iff for all x, y ∈ S, <lb/> neither x ≤  P y nor y ≤  P x. <lb/>

			<note place="footnote"> 2 These should not be confused with prime ideals or filters nor the prime elements that generate them (see <lb/>section I-3 of Gierz et al., 2003) <lb/></note>

			<page> 836 <lb/></page>

			Definition 2. Let  P be a partially ordered set, and S ⊆ P. S is a prime set of P iff  |S| &gt; 1, S is <lb/>an anti-chain, and, for all non-empty T ⊆ S, T has a join iff  |T | = |S|  or  |T | = 1. <lb/> Proposition 3. A partial order is a meet-semi-lattice iff its maximal prime sets are of size 2. <lb/> No non-trivial proper subset of a prime set has a join. When  |S| &gt; 2 and  S  is prime, <lb/>considering pairs of elements in succession will not work for any linear ordering of  S.  In <lb/>Figure 2(a), for example,  S = {a, b, c}  is a prime set because it has a join and none of its <lb/>2-subsets does. One needs to consider all three at once to see the join. <lb/>Candidate sets  S  of size larger than 2 are important because they naturally result from <lb/>delaying the computation of joins. A very natural strategy for performing unification in a <lb/>non-MSL is to use joins where they exist, and the union of the sets of types to be unified <lb/>otherwise, hoping that the latter will eventually be resolved to a join by the addition of <lb/>some other element later in the computation. This strategy implicitly uses the conjunctive <lb/>lattice of  P  to support its computations, but does not compute it. It is also the strategy that <lb/>we will adopt. The only thing we need to remember is that we should never refer to a set or <lb/>subset of types when that set has a join in  P  that we could refer to instead. That amounts to <lb/>using the user&apos;s names for conjunctions vis-a-vis the subtyping relation where they exist, <lb/>and explicit conjunctions elsewhere. Operationally, it reduces unification to searching for <lb/>prime sets and prime subsets. <lb/>In a nutshell, the reason that  DM (P)  can be exponentially larger than  P  is that the prime <lb/>sets of  P  can grow in size linearly with  |P|,  as will be proven in the next section. Better still, <lb/>tabulating the number of prime sets of each size indicates how &quot; MSL-like &quot; a type hierarchy <lb/>is. It gives us a spectral view of the basic subsets that every large set decomposes into during <lb/>unification. <lb/>While most large HPSGs are not MSLs in practice, they do not avail themselves of the <lb/>possibility of having large prime sets either. This is evident in their spectra, and it means <lb/>that it is relatively inexpensive to enumerate all of the prime sets. <lb/> 4 Pseudo-prime Sets <lb/> A close variant of prime sets turns out to be even more useful: <lb/> Definition 4.  S ⊆ P is a pseudo-prime set of P iff  |S| &gt; 1, S is consistent, and, for all non-empty <lb/> T ⊆ S, T has a join iff  |T | = 1. <lb/> A pseudo-prime set is as close as a set with no least upper bound can get to being a prime <lb/>set. The number of pseudo-prime sets is also an upper bound on the potential number of <lb/>new types added by a completion, because every completion type in  DM (P)  corresponds to <lb/>the set of upper bounds of some pseudo-prime set (Penn, 2000, though stated in different <lb/>terms). <lb/>In Figure 2(a),  {a, b}, {a, c}  and  {b, c}  are all pseudo-prime sets, for example. <lb/>Pseudo-prime sets stand in a very special relationship to prime sets and to each other: <lb/> Proposition 5. Every k-subset of a pseudo-prime set with k &gt; 1  is a pseudo-prime set. <lb/> Proposition 6. Every proper k-subset of a prime set with k &gt; 1  is a pseudo-prime set. <lb/> This means that we do not have to search through all <lb/>  |P| <lb/> k <lb/> possible  k-subsets  of  P  to <lb/>find the size-k prime sets of  P.  Instead, we can merely focus on the size  k − 1 pseudo-primes, <lb/>

			<page> 837 <lb/></page>

			Data: A finite poset of types  P = 〈T, ⊑〉 <lb/> Result: All the pseudo-prime  PsPrime(P)  and prime sets  Prime(P)  of  P <lb/> 1 begin <lb/>2 <lb/> init:  Prime(P)  ←  {},  PsPrime(P)  ←  {} ; <lb/> 3 <lb/>forall the consistent types t 1 , t 2 ∈ T  do <lb/>4 <lb/> Mins  ←  FindMinimal({t  1 , t 2 } u  ); <lb/> 5 <lb/>if  Mins  has only one element then <lb/> // There is a least upper bound <lb/> 6 <lb/> Add  {t 1 , t 2 }  to  Prime(P)  ; <lb/> 7 <lb/>else//  There is no join <lb/> 8 <lb/> Add  {t 1 , t 2 }  to  PsPrime(P)  ; <lb/> 9 <lb/>end <lb/>10 <lb/>end <lb/>11 <lb/>if if there is no set in  PsPrime(P)  with size 2 then <lb/>12 <lb/>return  PsPrime(P)  and  Prime(P)  ; <lb/> 13 <lb/>else//  P  is not an MSL: search for primes and pseudo-primes of size greater than 2 <lb/> 14 <lb/> size  := 2 ; <lb/> 15 <lb/>repeat <lb/>16 <lb/>forall the  pp 1 , pp 2 ∈  PsPrime(P)  do <lb/>17 <lb/> Cand  ← pp 1 ∪ pp 2 ; <lb/> 18 <lb/>if  |  Cand  |  is  size  + 1  then <lb/>19 <lb/> Mins  ←  FindMinimal(Cand); <lb/> 20 <lb/>if  Mins  has only one element then <lb/>21 <lb/> Prime(P)+  =  Cand; <lb/> 22 <lb/>else <lb/>23 <lb/> PsPrime(P)+  =  Cand; <lb/> 24 <lb/>end <lb/>25 <lb/>end <lb/>26 <lb/>end <lb/>27 <lb/> size  =  size  + 1; <lb/> 28 <lb/>until  (there  is no pseudo-prime set of  size); <lb/> 29 <lb/>return  PsPrime(P)  and  Prime(P)  ; <lb/> 30 <lb/>end <lb/>31 end <lb/> Algorithm 1:  Algorithm for finding prime and pseudo-prime sets of a poset. <lb/> one of which must live inside every prime set. The size  k − 1 pseudo-primes in turn can <lb/>be constructed from  k − 2-pseudo-primes, etc., with the 2-pseudo-primes being the pairs <lb/>of types that attest to the non-MSLhood of  P.  This recursive search procedure is given in <lb/>Algorithm 1. Any procedure that enumerates prime sets or pseudo-prime sets is exponential <lb/>in the worst case, but this algorithm makes efficient use of the relationship between primes <lb/>and pseudo-primes to achieve efficiency when the largest prime sets are of low cardinality. <lb/>Pseudo-primes are also the key to bounding the size of prime sets: <lb/> Proposition 7. The maximum size attainable by a prime subset of a poset P is  ⌊ |P|−1 <lb/> 2 <lb/> ⌋. <lb/> Proof. Let  Ψ  be any prime subset of  P.  Let  n = |P|  and  p = |Ψ|.  Consider the  p  distinct <lb/>subsets of  Ψ  of size  p − 1 (i.e.  K  1 , ..., K p  ⊆ Ψ  such that  |K  1  | = ... = |K  p  | = p − 1). Let <lb/> m = | <lb/> 1≤i≤p  µ(K  i  )|  where  µ(S) = {x ∈ S u  | ∄ u ∈ S u  s.t  u ≤ x},  the set of all minimal upper-<lb/>bounds of  S.  For all  i, |µ(K  i  )| ≥ 2 because  K  1 , ..., K p  are all pseudo-prime sets. For the same <lb/>reason,  |µ(K  i  ) ∩ µ(K  j  )| ≤ 1 when  i 񮽙 = j,  because then  K i  ∪ K j  = Ψ.  In fact,  µ(K  1  ), ..., µ(K  p  ) <lb/> are either all pairwise disjoint or all agree on the same one element, so there are only two <lb/>cases: <lb/> Case A: <lb/> i  µ(K  i  ) =  Then  P  must have at least  p + m  elements and one extra element for <lb/>the join of  Ψ  that must exist above all the  m  elements in <lb/> 1≤i≤p  µ(K  i  ).  So, (i)  n ≥ p + m + 1. <lb/> Conservatively, we must choose  p  disjoint antichains of size at least 2 from the  m  elements, <lb/>thus we have (ii)  p ≤ ⌊  m <lb/> 2 <lb/> ⌋.  Therefore, from (i) and (ii), if  m  is even then  n ≥ 2p + 1, and if <lb/> m  is odd then  n ≥ 3p + 2. Since 2p + 1 ≤ 3p + 2 ≤ n,  then  p ≤  n−1 <lb/> 2 <lb/> . <lb/>

			<page> 838 <lb/></page>

			Case B: <lb/> i  µ(K  i  ) 񮽙 = .  In this case,  n ≥ p + m  and  p ≤ m − 1, subtracting for the one upper <lb/>bound that all of the  K i  have in common. Therefore,  n ≥ p + m ≥ 2p + 1 and consequently <lb/> p ≤  n−1 <lb/> 2 <lb/> . <lb/>There are also simple examples in which this bound can be attained, so this is tight. <lb/>A spectral decomposition of pseudo-primes is also somewhat interesting because of its <lb/>relationship to the size of  DM (P),  and to prime sets — the largest prime set cannot be <lb/>more than one element larger than the largest pseudo-prime, but in practice it is much less. <lb/>Figure 3 shows the decompositions for both primes and pseudo-primes for both a 1999 <lb/>pre-release of the English Resource Grammar (ERG Flickinger, 1999) and Berligram (Müller, <lb/>2007). The first thing that we can observe is the very small size of all of the pseudo-primes <lb/>and primes in both grammars — at most 9 in the ERG, and 6 in Berligram. It took 27 <lb/>seconds 3 to find all the primes and pseudo-primes in the ERG, and less than 1 second to <lb/>find them in Berligram. By contrast, it takes the LKB 4 seconds to compute  DM (P)  for <lb/>the ERG. We can also see a greater difference between the largest prime and the largest <lb/>pseudo-prime in the ERG (9-4=5) than we can in Berligram (6-4=2). Pseudo-primes lay the <lb/>groundwork for primes, so when a prime set nevertheless does not occur, this is significant. <lb/>Finally, we can observe that, although the ERG is nearly 10 times the size of Berligram in its <lb/>total number of types, it has about 25 times as many primes of cardinality 3 or greater, and <lb/>of the same maximum size (4). <lb/>A large number of primes, a skewed distribution of primes towards small cardinalities, a <lb/>large number of pseudo-primes, and a large difference between the size and/or number of <lb/>pseudo-primes and primes are all strong indicators that a type hierarchy is more MSL-like <lb/>when interpreted relative to the total number of types. The total number of types is not a <lb/>good indicator, nor is &quot; join density, &quot; which is common to formal concept analysis (Besson <lb/>et al., 2005). 4 The (relative to  P)  size of  DM (P)  is not bad, but it fails to indicate just how <lb/>far the tails of these distributions extend. A type hierarchy with 10 6 pseudo-primes of size 2 <lb/>is more MSL-like than one with 10 5 pseudo-primes of size 9, and simple counts of  |DM (P)| <lb/> often cannot tell the difference, especially with high join densities. <lb/>One conclusion to draw from this particular comparison is obviously that the ERG is more <lb/>MSL-like, which implies that it is more conservative in its structure, and would be easier <lb/>to compute with than many other potential 3414-type hierarchies. On the other hand, the <lb/>sheer number of types may make finding the right part of the type hierarchy to modify more <lb/>difficult than it needed to have been. Another way to look at it is that Berligram makes <lb/>more efficient use of the amount of information that a 434-type hierarchy can carry. There <lb/>is more information embedded in its structure relative to its size, but that information may <lb/>make it more difficult to predict the consequences of type unification than in some other <lb/>hierarchies of its size. The ERG and Berligram do not have the same number of types, nor <lb/>

			<note place="footnote"> 3 All the timing results reported in this paper were obtained on an Intel R <lb/> -based system with a 3.6 GHz <lb/>Xeon TM processor and 3 GB of RAM running the Ubuntu 6.06.2 Linux operating system. <lb/> 4 Join density, in terms of our partial orders, is calculated as the ratio of the size of the extension of the <lb/>(transitively and reflexively closed) subtype relation to the square of  |P|.  The join density of the ERG is .001; that <lb/>of Berligram is .006. <lb/></note>

			<page> 839 <lb/></page>

			do they attempt to account for the same constructions, nor even the same language, so we <lb/>can only speculate here. But prime sets and pseudo-primes can illuminate these issues. <lb/> 4.1 Building an Automaton-based Index <lb/> The final application of prime and pseudo-prime sets considered here is to unification itself. <lb/>As mentioned above, it is possible to maintain argument sets of types from a non-MSL as <lb/>argument sets even after unification, provided that all of the prime subsets are replaced <lb/>with their joins. For the unifier not to perform this replacement is tantamount to it simply <lb/>handing back a candidate pair of types from an MSL ununified. <lb/>If we use topologically sorted lists (as induced by the subtype relation) as our representations <lb/>of prime sets, pseudo-prime sets and argument sets, then Propositions 5 and 6 will allow us <lb/>to use a simple automaton-based index to perform this replacement. The index has a linear <lb/>number of states relative to the number and size of the pseudo-primes, because each state <lb/>corresponds to a prefix of one or more pseudo-primes in topological order. It has two kinds <lb/>of edges: suffix edges and redex edges, and both kinds of edges are labelled with elements of <lb/>pseudo-prime or prime sets. <lb/>All of the pseudo-prime sets can be arranged into a tree such that every path through the <lb/>tree from the root to any node corresponds to a pseudo-prime or a singleton set. Paths from <lb/>the root to a leaf correspond to maximal pseudo-primes — no pseudo-prime contains them. <lb/>These trees are connected with suffix edges, each of which is labelled with the  k  th element <lb/>that extends a  k − 1-length prefix of one or more pseudo-primes to a  k-length  prefix. On top <lb/>of this skeleton, we add the redex edges, which map a pseudo-prime prefix,  X  , that contains <lb/>a  k − 1-length prefix of a size-k prime set to the result of replacing that prime set within  X <lb/> by its join, transitively closed under all possible further replacements. The redex edge is <lb/>labelled with the  k  th element that completes the prime set. <lb/>The index for Figure 2(a), for example, is shown in Figure 2(b). Its pseudo-prime sets <lb/>are:  {{a, b}  12 , {a, c}  13 , {a, z}  14 , {b, z}  15 , {b, c}  16 , {d, z}  17 , {c, z}  18 , {g, z}  19 , { f , z}  20 , {e, z}  21 , <lb/> {h, z}  22 , {a, b, z}  23 , {a, c, z}  24 , {b, c, z}  25 ; and its prime sets are:  {a, f }, {b, g}, {d, c}, {d, g}, <lb/> {d, f }, {d, e}, {g, f }, {g, e}, { f , e}, {a, b, c}.  5 Suppose  {a, b, c, z}  is of interest. The automaton <lb/>consumes  a  and  b,  which is a pseudo-prime (12) and the prefix of the pseudo-prime, <lb/> {a, b, z}  23 , but upon seeing  c  traverses a redex edge that reduces the prime set  {a, b, c}  to its <lb/>join,  e,  which is a singleton and therefore not subject to further replacement. Consuming  z <lb/> next leads to the pseudo-prime,  {e, z}  21 . So if the union of the argument sets provided to <lb/>the unifier is  {a, b, c, z},  what the unifier should return is  {e, z},  which should be interpreted <lb/>as the conjunctive type,  e&amp;z.  This is as close to the grammar writer&apos;s idioms as we can come <lb/>in describing this result, given that the type hierarchy is not an MSL. <lb/>Having constructed the automaton directly from the pseudo-prime and prime sets, it can <lb/>then be pruned by eliminating states that correspond to singletons with no outgoing suffix <lb/>edges and no outgoing or incoming redex edges. Singleton sets clearly do not need to be <lb/>fed to the automaton. In the example in Figure 2(b), states 9, 10 and 11 would be pruned. <lb/>Some statistics for the automata constructed for the ERG and Berligram are presented in <lb/>

			<note place="footnote"> 5 These sets are sorted according to the following topological order (subscripts are topological ordinals): <lb/> ⊥  0  &lt; a  1  &lt; b  2  &lt; d  3  &lt; c  4  &lt; g  5  &lt; f  6  &lt; e  7  &lt; h  8  &lt; z  9  &lt; y  10  &lt; x  11 . <lb/></note>

			<page> 840 <lb/></page>

			Table 1. Our implementation of the automaton compiler has been written in Prolog and <lb/> Before pruning <lb/>After pruning <lb/>ERG <lb/>Berligram <lb/>ERG <lb/>Berligram <lb/># of suffix edges <lb/>9508 <lb/>1425 <lb/>7058 <lb/>1205 <lb/># of redex edges <lb/>2296 <lb/>554 <lb/>2296 <lb/>554 <lb/>Total # of edges <lb/>11804 <lb/>1979 <lb/>9354 <lb/>1759 <lb/>Total # of states <lb/>8967 <lb/>1314 <lb/>6517 <lb/>1094 <lb/> Table 1: Statistics related to the constructed automaton-based index for type hierarchies of the ERG <lb/>(Flickinger, 1999) and Berligram (Müller, 2007). <lb/> generates the automaton as Prolog clauses, too, which are then also compiled. The two <lb/>stages of compilation together consumed 30 milliseconds for Berligram and 2.2 seconds for <lb/>the ERG, which is well within the range of compilers that add joins on-line. Further speed <lb/>improvements are doubtlessly possible, since the automaton can in principle be constructed <lb/>incrementally as Algorithm 1 is being executed. <lb/> 5 Unification <lb/> Having constructed the automaton directly from pseudo-prime and prime sets, we are <lb/>implicitly assuming that every input set to the automaton is an anti-chain. The algorithm <lb/>for unification, given as Algorithm 2, shows how the result should look. <lb/> Data:  P = 〈T, ⊑  P  〉  ; <lb/>two sets of types  A, B ⊆ T  ; <lb/>the automaton-based index   P  ; <lb/> ψ:  the topology of  P  used for construction of   P  ; <lb/> Result: minimal upper bound of  A  and  B  or failure <lb/> 1 begin <lb/> 2 <lb/> Feed  ←  AntiChainReduce(P,  ψ  ,A  ∪ B); <lb/> 3 <lb/> if  Feed  has only one element then <lb/> 4 <lb/> return  Feed; <lb/> 5 <lb/> else <lb/> 6 <lb/> AutResult  ←  Aut(start  state of   P ,  Feed); <lb/> 7 <lb/> return  AutResult; <lb/> 8 <lb/> end <lb/> 9 end <lb/> Algorithm 2:  The algorithm for unification of two sets of types. <lb/> AntiChainReduce  reduces an input argument set to the topologically sorted, maximally <lb/>specific anti-chain that contains it. This can be accomplished in quadratic time as a function <lb/>of the size of the input set, simply by considering every pair of types and replacing the pair <lb/>with the higher of the two if they are ordered. <lb/> 6 Parsing Evaluation <lb/> To evaluate how this unification algorithm performs in practice, we evaluated three systems <lb/>on a common grammar and corpus of sentences. The grammar was the ERG, and the <lb/>corpus, called the FUSE corpus, has 2354 sentences, ranging from 1 to 50 words in length, <lb/>192 of which are ungrammatical according to the ERG. All three of the systems that we <lb/>

			<page> 841 <lb/></page>

			tested had memory allocation problems while parsing this corpus, so every sentence that <lb/>resulted in at least one system running out of memory was excluded, leaving a remainder of <lb/>1727 sentences, 159 of which were ungrammatical. 6 We also imposed a maximum of 8000 <lb/>chart edges after which parsing was terminated. The excluded sentences were those that <lb/>exceeded the memory available to one of the processes with that cap in place. The three <lb/>systems were: <lb/> (1) ALE version 4.0 beta, running on the Dedekind-MacNeille completion of an ALE port <lb/>of a 1999 pre-release version of the ERG, with 45 rules, 155 features, 1314 lexical entries, <lb/>no lexical rules and 3412 types, plus a most general type,  ⊥,  plus a built-in type for strings. <lb/> DM (P)  added another 893 types to the system. We compiled ALE with SICStus Prolog <lb/>3.12.10 (compact code). 7 <lb/> (2) the LKB, version as at March, 2009: This system allows non-MSL grammars as input, <lb/>but it automatically computes the Dedekind-MacNeille completion on their type systems at <lb/>compile-time, naming newly added types with a number. The 1999 pre-release of the ERG <lb/>no longer runs on the LKB because of non-backwards-compatible changes in the system <lb/>over the last 10 years. Porting an LKB grammar to ALE is very tricky because the LKB&apos;s <lb/>input syntax is very heavily overloaded. So instead of porting the current ERG to ALE, we <lb/>reported our existing port of the 1999 pre-release of the ERG back to this version of the LKB. <lb/>We have verified that these two ports generate exactly the same edges on the non-excluded <lb/>sentences of this corpus. We compiled the LKB with Allegro Common Lisp Enterprise 8.0. <lb/> (3) an experimental system, obtained by replacing ALE 4.0&apos;s type unifier with the one <lb/>described in this paper. This system generates the same object code as ALE 4.0 when the <lb/>input grammar&apos;s type system is an MSL. We ran this on the 1999 pre-release of the ERG <lb/>without computing the Dedekind-MacNeille completion, which thus results in different <lb/>object code. <lb/>The results are shown in Figure 6. These are log-linear graphs, so the roughly even separa-<lb/>tions attest to a nearly constant factor of speed-up, not a constant difference in milliseconds. <lb/>The LKB is approximately 1.82 times slower than ALE 4.0, and the experimental version <lb/>implemented for this study is approximately 1.054 times slower than ALE 4.0. Thus the <lb/>experimental system produces a run-time parser that is 5.4% slower than ALE after having <lb/>compiled out the Dedekind-MacNeille completion in advance, but is still more than 40% <lb/>faster than the LKB, a commonly used system that caches them on-line. <lb/>The figure of 5.4% obtains with an experimental system that does not cache the results <lb/>of prime-set reduction on previously seen argument sets. Caching is best implemented by <lb/>caching not only previously seen argument sets that are consistent, but also previously seen <lb/>argument sets that are inconsistent. As shown in Figure 6(d), however, the difference is <lb/>not large. In the latter case, one obtains a system that is an average of 5.7% slower than <lb/>parsing with precompiled MSLs on a first pass of parsing, but only 2.7% slower on a second <lb/>pass through the same corpus of sentences (for which all of the cache entries are in place). <lb/>We used the built-in SICStus  term_hash/2  predicate to hash argument sets, which gave us <lb/>a perfect hash (one set per hash). Here again, a comparison with the LKB is informative in <lb/>that argument set caching behaves surprisingly like the LKB&apos;s lexicon caching: while we <lb/>

			<note place="footnote"> 6 The test sentences and results are available at  http://www.cs.toronto.edu/ ˜ rouzbeh/resources.html. <lb/> 7 The MSL-ERG in ALE syntax is available at  http://www.cs.toronto.edu/ ˜ rouzbeh/resources.html. <lb/></note>

			<page> 842 <lb/></page>

			might naturally expect to see a steady stream of new words throughout the parsing of a <lb/>corpus of this size, we also see a steady stream of new argument sets, even though the <lb/>number of types and their relative ordering are held constant throughout the experimental <lb/>parsing runs. As a result, it would take a much larger sample to witness a convergence of <lb/>the second pass with the first. This suggests that argument set caching is only worthwhile <lb/>over much longer timespans of use with the same type system. <lb/> 7 Conclusion <lb/> We introduced two new mathematical constructions, prime sets and pseudo-prime sets, and <lb/>showed that they provide a reasonable alternative to the Dedekind-MacNeille completion, <lb/>by providing a means for manipulating conjunctive sets of types at very low overhead. These <lb/>sets provide better naming conventions for newly added types than any implementation <lb/>based on Dedekind-MacNeille could hope to do because they effectively allow for a trace <lb/>of partial unifications of a set of arguments. Prime and pseudo-prime sets also form the <lb/>basis of a more refined method for analyzing the upper bounds of a type system than simply <lb/>calling it an MSL or non-MSL. This is of independent value to grammar designers. <lb/>We have not yet looked at the frequency distributions of primes and pseudo-primes encoun-<lb/>tered during parsing with the ERG over the FUSE corpus, for example. Both our compilation <lb/>strategy and the spectral decomposition method would benefit from taking this information <lb/>into account, because some portions of a completed lattice are clearly more important than <lb/>others, simply as a result of certain types being used more often in parsing representative <lb/>corpus input than others. <lb/>Some additional work on the combinatorial properties of prime and pseudo-prime sets also <lb/>remains. Although we have identified a tight upper bound on the possible sizes of prime <lb/>sets, the only known bounds on the numeracy of prime and pseudo-prime sets are based on <lb/>the classical problem (the so-called Dedekind Problem), of finding the number of anti-chains <lb/>of a given poset. The field had settled on a fairly stable bound until recently (Korshunov <lb/>and Shmulevich, 2000). <lb/>

			<page> 843 <lb/></page>

			(a) <lb/> (b) <lb/> Figure 2: (a) A non-MSL type hierarchy and (b) its automaton-based index). The redex edges are <lb/>depicted as dotted arcs; if those eliminated from the automaton, a suffix tree is obtained. <lb/>

			<page> 844 <lb/></page>

			(a) <lb/> (b) <lb/> Figure 3: Number and size of the prime and pseudo-prime sets of (a) the English Resource Grammar <lb/>(3412 types in total), and (b) the German Berligram (434 types). The horizontal and vertical axes <lb/>show the number and size of the sets, respectively. <lb/>

			<page> 845 <lb/></page>

			(a) <lb/> (b) <lb/> 850 <lb/> 900 <lb/>950 <lb/>1000 <lb/>1050 <lb/>1100 <lb/>1150 <lb/>10 <lb/> 3 <lb/> LKB−cached−lexicon <lb/>Experimental ALE <lb/>MSL−restricted ALE <lb/>ALE(success+failure) <lb/>1850 <lb/>1900 <lb/>1950 <lb/>2000 <lb/>2050 <lb/>10 <lb/> 3.4 <lb/> 10 <lb/> 3.5 <lb/> Sentence Number <lb/>MSL−restricted <lb/>cached success+failure <lb/>success+failure <lb/>cached success only <lb/>success only <lb/> (c) <lb/>(d) <lb/>Figure 4: Evaluation of the  MSL-restricted ALE, Experimental ALE  and the  LKB  on FUSE. The <lb/> LKB  caches the lexicon as it parses, so each sentence was parsed twice in succession. The parsing time <lb/>of the first parse is given in (a) and that of the second in (b). Figure (c) is a close-up of a portion of <lb/>Figure (b), along with Experimental ALE plus caching of consistent and inconsistent argument sets <lb/>(first pass). Figure (d) shows the effect of caching in close-up detail. <lb/></body>

			<page> 846 <lb/></page>

		<back>
			<listBibl> References <lb/> Aït-Kaci, H., Boyer, R., Lincoln, P., and Nasr, R. (1989). Efficient implementation of lattice <lb/>operations. ACM Trans. Program. Lang. Syst., 11(1):115–146. <lb/>Bertet, K., Morvan, M., and Nourine, L. (1997). Lazy completion of a partial order to the <lb/>smallest lattice. <lb/>Besson, J., Robardet, C., Boulicaut, J.-F., and Rome, S. (2005). Constraint-based concept <lb/>mining and its application to microarray data analysis. Intell. Data Anal., 9(1):59–82. <lb/>Callmeier, U. (2000). Pet – a platform for experimentation with efficient HPSG processing <lb/>techniques. Nat. Lang. Eng., 6(1):99–107. <lb/>Carpenter, B. and Penn, G. (1996). Efficient parsing of compiled typed attribute value <lb/>logic grammars. In Bunt, H. and Tomita, M., editors, Recent Advances in Parsing Technology, <lb/> pages 145–168. Kluwer Academic Publishers, Dordrecht, Boston, London. <lb/>Copestake, A. and Flickinger, D. (2000). An open-source grammar development envi-<lb/>ronment and broad-coverage English grammar using HPSG. In In Proceedings of LREC <lb/>2000. <lb/> Davey, B. and Priestley, H. (2002). Introduction to Lattices and Order. Cambridge University <lb/>Press. <lb/>Flickinger, D. (1999). LinGo, the English Resource Grammar. <lb/>Gierz, G., Hofmann, K. H., Keimel, K., Lawson, J. D., Mislove, M., and Scott, D. S. <lb/>(2003). Continuous Lattices and Domains, volume 93 of Encyclopedia of Mathematics and <lb/>its Applications. Cambridge. <lb/>Korshunov, A. D. and Shmulevich, I. (2000). On the distribution of the number of monotone <lb/>boolean functions relative to the number of lower units. Discrete Math., 257(2-3):463–479. <lb/>Müller, S. (2007). Berligram: German grammar based on Head-driven Phrase Structure <lb/>Grammar: Eine Einführung. <lb/>Penn, G. (2000). The Algebraic Structure of Attributed Type Signatures. PhD thesis, School <lb/>of Computer Science, Carnegie Mellon University. <lb/>Pollard, C. and Sag, I. (1994). Head-Driven Phrase Structure Grammar. Chicago University <lb/>Press, Chicago, Illinois. <lb/></listBibl>

			<page> 847 </page>

		</back>
	</text>
</tei>
