<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Enhancing safety and efficiency in automated container terminals: <lb/>Route planning for hazardous material AGV using LSTM neural <lb/>network and Deep Q-Network <lb/>Fei Li 1, † , Junchi Cheng 2, † , Zhiqi Mao 3,✉ , Yuhao Wang 3 , Pingfa Feng 1 <lb/>1 Department of Mechanical Engineering, Tsinghua University, Beijing 100084, China <lb/>2 Information Science and Technology College, Dalian Maritime University, Dalian 116000, China <lb/>3 School of Vehicle and Mobility, Tsinghua University, Beijing 100084, China <lb/>Received: January 17, 2024; Revised: March 6, 2024; Accepted: March 18, 2024 <lb/>© The Author(s) 2024. This is an open access article under the terms of the Creative Commons Attribution 4.0 International License <lb/>(http://creativecommons.org/licenses/by/4.0/). <lb/>ABSTRACT: As the proliferation and development of automated container terminal continue, the issues of efficiency and <lb/>safety become increasingly significant. The container yard is one of the most crucial cargo distribution centers in a terminal. <lb/>Automated Guided Vehicles (AGVs) that carry materials of varying hazard levels through these yards without compromising <lb/>on the safe transportation of hazardous materials, while also maximizing efficiency, is a complex challenge. This research <lb/>introduces an algorithm that integrates Long Short-Term Memory (LSTM) neural network with reinforcement learning <lb/>techniques, specifically Deep Q-Network (DQN), for routing an AGV carrying hazardous materials within a container yard. <lb/>The objective is to ensure that the AGV carrying hazardous materials efficiently reaches its destination while effectively <lb/>avoiding AGVs carrying non-hazardous materials. Utilizing real data from the Meishan Port in Ningbo, Zhejiang, China, the <lb/>actual yard is first abstracted into an undirected graph. Since LSTM neural network can efficiently conveys and represents <lb/>information in long time sequences and do not causes useful information before long time to be ignored, a two-layer LSTM <lb/>neural network with 64 neurons per layer was constructed for predicting the motion trajectory of AGVs carrying non-<lb/>hazardous materials, which are incorporated into the map as background AGVs. Subsequently, DQN is employed to plan <lb/>the route for an AGV transporting hazardous materials, aiming to reach its destination swiftly while avoiding encounters with <lb/>other AGVs. Experimental tests have shown that the route planning algorithm proposed in this study improves the level of <lb/>avoidance of hazardous material AGV in relation to non-hazardous material AGVs. Compared to the method where <lb/>hazardous material AGV follow the shortest path to their destination, the avoidance efficiency was enhanced by 3.11%. This <lb/>improvement demonstrates potential strategies for balancing efficiency and safety in automated terminals. Additionally, it <lb/>provides insights for designing avoidance schemes for autonomous driving AGVs, offering solutions for complex operational <lb/>environments where safety and efficient navigation are paramount. <lb/>KEYWORDS: container yard, route planning, hazardous material Automated Guided Vehicle (AGV), Long Short-Term <lb/>Memory (LSTM), Deep Q-Network (DQN) <lb/></front>

			<body>1 Introduction <lb/>The container yard within a terminal serves a critical function as a <lb/>transitional storage and coordination area. Here, goods destined <lb/>for both unloading from and loading onto ships are temporarily <lb/>stored, forming a vital buffer in the logistical chain. The <lb/>operational dynamics of this yard, where Automated Guided <lb/>Vehicles (AGVs) and cranes diligently operate, are integral to the <lb/>overall efficiency of the terminal&apos;s functions. The correlation <lb/>between the yard&apos;s operational efficiency and the terminal&apos;s overall <lb/>performance has been underscored in numerous studies <lb/>(Gunawardhana et al., 2021; Li et al., 2021, 2023; Skaf et al., 2021; <lb/>Tan et al., 2024; Xiang et al., 2022). Therefore, a substantial <lb/>number of scholars have dedicated their efforts to enhancing work <lb/>efficiency and striving for practicality (Wang et al., 2018; Zhao et <lb/>al., 2020; Zhen et al., 2012; Zhu et al., 2012). <lb/>Initial studies primarily focused on optimizing the routing of <lb/>AGVs within the yard and enhancing the coordination between <lb/>AGVs and cranes for seamless operation (Chen et al., 2004; Zhen, <lb/>2016; Zhu, 2012). With the advent and continuous evolution of <lb/>artificial intelligence and autonomous AGV technology, the <lb/>concept of unmanned and intelligent AGVs has become a pivotal <lb/>area of research. Chen et al. (2022) posited that the future of <lb/>terminal operations lies in automation, particularly emphasizing <lb/>the role of driverless AGVs not only within the terminal but also <lb/>in transporting cargo beyond terminals. Filom et al. (2022) <lb/>highlighted the growing utilization of machine learning in AGV <lb/>route planning within terminals, indicating a significant trend <lb/>towards technological integration in logistics management. <lb/>Despite these advancements, the diverse nature of cargo <lb/>transported by AGVs, particularly with regard to hazardous <lb/>materials, is often overlooked in existing research. Different <lb/>cargoes, especially hazardous chemicals, necessitate distinct <lb/>logistical considerations. For instance, AGV transporting <lb/></body>

			<front> † Fei Li and Junchi Cheng contributed equally to this work. <lb/>✉ Corresponding author. <lb/>E-mail: mzq16@tsinghua.org.cn <lb/>Journal of Intelligent and Connected Vehicles <lb/>2024, 7(1): 64-77 <lb/>https://doi.org/10.26599/JICV.2023.9210041 <lb/>Research Article <lb/>J Intell Connect Veh 2024, 7(1): 64-77 <lb/></front>

			<body>hazardous materials must maintain a safe distance from those <lb/>carrying general cargo, even if they belong to the same type of <lb/>AGV. This differentiation in route planning is crucial not only for <lb/>operational efficiency but also for ensuring safety and security <lb/>within the terminal environment. <lb/>However, most existing research does not adequately address <lb/>the separate route planning required for AGVs carrying hazardous <lb/>materials. There remains a gap in ensuring that these AGVs <lb/>carrying hazardous materials are kept at a safe distance from <lb/>AGVs carrying non-hazardous materials, while still fulfilling both <lb/>their transport objectives. This aspect is critical for maintaining <lb/>high security standards in terminal operations and warrants <lb/>further investigation and development in the field of terminal <lb/>logistics and management. <lb/>In this study, we aim to develop an innovative model for the <lb/>strategic path planning of AGV transporting hazardous materials <lb/>within container yards. The primary objective of this model is to <lb/>ensure the expedient and safe routing of an AGV carrying <lb/>hazardous materials (hereafter referred to as H-AGV) within <lb/>terminal environments, particularly focusing on their navigation <lb/>in proximity to AGVs transporting non-hazardous materials <lb/>(referred to as N-AGVs). This model is designed to facilitate the <lb/>H-AGV, ensuring their timely arrival at designated locations while <lb/>maintaining a safe distance from operating N-AGVs, thereby <lb/>enhancing safety and efficiency in terminal operations. The <lb/>proposed model is based on real data from Meishan Port, Ningbo <lb/>Zhoushan Port, Zhejiang, China. The logic block diagram of the <lb/>proposed model is shown in Fig. 1, and the methodology of this <lb/>study is outlined as follows: <lb/>Firstly, this work involves precise mapping of the yard <lb/>environment at Meishan Port, which is part of Ningbo Zhoushan <lb/>Port in Zhejiang, China. This mapping is achieved by using the <lb/>specific longitude and latitude coordinates of the container yard, <lb/>which provides a detailed and accurate representation of the <lb/>operational area. At the same time, the map used in this work has <lb/>been reflected and discretized relative to the reality. <lb/>Secondly, we propose an AGV trajectory rolling prediction <lb/>model based on Long Short-Term Memory (LSTM) network. <lb/>This network is employed to analyze the historical trajectory data, <lb/>specifically the latitude and longitude coordinates of N-AGVs <lb/>within the Meishan port. The output from this analysis is used to <lb/>predict the future paths of these N-AGVs. In the context of our <lb/>model, these predicted paths are then integrated into the yard <lb/>map, playing a role similar to background elements in <lb/>cinematography, providing a dynamic context for the movement <lb/>of H-AGV. <lb/>Thirdly, we raise reinforcement learning algorithms to <lb/>determine the optimal routes for H-AGV. The H-AGV is assigned <lb/>random start and endpoint locations on the map. The <lb/>reinforcement learning framework is tasked with generating a <lb/>trajectory that not only ensures the quickest possible route to the <lb/>destination but also adheres to the safety criterion of maintaining a <lb/>safe distance from N-AGVs throughout the journey. <lb/>This study is organized as follows. In Section 2, related literature <lb/>is reviewed. Section 3 provides a comprehensive overview of the <lb/>map drawing process, along with the foundational assumptions of <lb/>the proposed model and definitions of key symbols employed <lb/>throughout the study. Section 4 focuses on elucidating how the <lb/>LSTM network predicts the trajectories of N-AGVs within <lb/>terminal environment. Section 5 is dedicated to exploring the <lb/>application of reinforcement learning in training the paths of <lb/>H-AGV. Section 6 presents the outcomes of this project. It <lb/>includes a thorough analysis and comparison of experimental <lb/>results, providing insights into the efficacy of the proposed model <lb/>in practical scenarios. Section 7 discusses the limitations of the <lb/>experiment and offers suggestions for future experiments. Section 8 <lb/>synthesizes the findings of the study, offering a concise summary <lb/>of the key conclusions drawn from the research. It also discusses <lb/>the implications of these findings and potential avenues for future <lb/>research in this field. <lb/>2 Literature review <lb/>This work primarily focuses on two crucial aspects. Firstly, it <lb/>addresses the configuration of background AGV movement, <lb/>specifically emphasizing the development of a motion design for <lb/>N-AGVs that strikes a balance between realism and <lb/>computational simplicity. Secondly, it involves the design of the <lb/>H-AGV&apos;s movement strategy to ensure that it can reach its <lb/>endpoint while avoiding the N-AGVs as efficiently as possible. In <lb/>this section, we have reviewed some highly relevant and state-of-<lb/>the-art studies. <lb/>2.1 AGV trajectory prediction <lb/>For the first part, concerning the design of N-AGVs&apos; motion <lb/>trajectories, the dataset utilized in this study consists of historical <lb/>trajectory latitude and longitude coordinates of numerous N-<lb/>AGVs. Regarding AGV trajectory prediction, current research <lb/>predominantly utilizes operational research methods (Pei et al., <lb/>2023; Qu et al., 2022) and deep learning (Zhu et al., 2022). <lb/>Notably, deep learning neural network, initially developed for <lb/>processing language and text, have demonstrated considerable <lb/>History of <lb/>longitude <lb/>and latitude <lb/>coordinates <lb/>for N-AGVs <lb/>Longitude <lb/>and latitude <lb/>coordinates <lb/>for port yard <lb/>LSTM <lb/>neural <lb/>network <lb/>Set the <lb/>background <lb/>for the <lb/>training <lb/>of H-AGV <lb/>Use reinforcement learning <lb/>to plan a trajectory for H-AGV <lb/>H-AGV reaches <lb/>its destination <lb/>while avoiding <lb/>N-AGVs <lb/>Fig. 1 Logic block diagram of the model for plan the trajectory of AGV carrying hazardous materials. <lb/></body>

			<note place="headnote">Enhancing safety and efficiency in automated container terminals: Route planning for hazardous material AGV using LSTM neural ... <lb/></note>

			<page>65 <lb/></page>

			<note place="footnote">https://doi.org/10.26599/JICV.2023.9210041 <lb/></note>

			<body>effectiveness in AGV trajectory prediction and path planning (Qu <lb/>et al., 2023). <lb/>Liu et al. (2023) put forward their concept based on the <lb/>mechanism that models developed for processing natural language <lb/>treat human language as sequences of words constrained by <lb/>specific grammatical rules. Following their concept, they proposed <lb/>that any entity which can be mapped into a learnable sequence, <lb/>not just natural language, could be interpreted through a language <lb/>model. For instance, in the context of delivery problems, they <lb/>treated delivery routes as &quot;sentences&quot; and delivery points as <lb/>&quot;words&quot;. By applying language models to optimize delivery paths, <lb/>they achieved superior results. This approach exemplifies the <lb/>innovative application of natural language processing techniques <lb/>in non-linguistic domains like logistics and route planning. Fang <lb/>et al. (2022) developed a deep learning-based LSTM model for <lb/>intelligent AGV collision avoidance trajectory planning, using the <lb/>coordinates of AGVs at the time of collision as inputs. After <lb/>experimentation, it was found that this LSTM model exhibited <lb/>superior fitting accuracy compared to traditional planning models, <lb/>demonstrating greater potential for broader application. This <lb/>advancement underscores the effectiveness of LSTM network in <lb/>complex, dynamic scenarios like real-time AGV trajectory <lb/>planning and collision avoidance. Wang et al. (2022) employed an <lb/>Seq2Seq model to predict AGV trajectories and trained an LSTM <lb/>model with analogous inputs similarly. They compared the <lb/>performance of these two models in AGV trajectory prediction <lb/>and found that the LSTM model exhibited superior predictive <lb/>accuracy. However, the results show that the model&apos;s performance <lb/>deteriorated over time, with an increase of Root Mean Squared <lb/>Error (RMSE), especially as the prediction horizon extended. This <lb/>observation highlights a common challenge in time-series <lb/>predictions using LSTM models, where longer-term predictions <lb/>tend to lose accuracy due to the accumulation of errors over time. <lb/>Yang et al. (2022) experimented with different improved LSTM <lb/>model, i.e., Social Long Short-Term Memory (S-LSTM), training <lb/>them using the same historical latitude and longitude coordinates <lb/>of AGVs. They predicted the coordinates for the next 5 s for 250 <lb/>sets of data, calculating the RMSE value of the predictions every <lb/>second. According to their findings, while these models produced <lb/>similar prediction results, all of them exhibited an increase in <lb/>prediction error over time. This trend underscores a common <lb/>limitation in time-series forecasting models: As the prediction <lb/>horizon extends, the accuracy tends to decrease, a challenge <lb/>particularly pertinent in dynamic systems like AGV trajectory <lb/>prediction. <lb/>These research findings align with our objectives and <lb/>demonstrate through existing experiments that models like LSTM, <lb/>originally designed for natural language processing, are also <lb/>effective in AGV path planning. Moreover, they also highlight a <lb/>crucial consideration: when LSTM models are used for prediction <lb/>and output multiple sets of data simultaneously, the earlier <lb/>outputs tend to have smaller errors, while the errors in later <lb/>outputs increase progressively. This insight is valuable for our <lb/>work, as it underscores the need for careful management and <lb/>interpretation of LSTM model predictions, particularly when <lb/>planning over longer time horizons or for complex, dynamic <lb/>environments like traffic systems. <lb/>2.2 AGV route planning <lb/>An H-AGV needs to avoid continuously moving N-AGVs, all <lb/>within the confines of a structured map with strong regularity, and <lb/>reinforcement learning methods are extensively utilized in this <lb/>domain. Reinforcement learning, with its ability to learn optimal <lb/>strategies through trial and error in dynamic environments, is <lb/>particularly well-suited for scenarios like this (Han et al., 2023; Sun <lb/>et al., 2022), where the H-AGV must navigate in real-time while <lb/>adapting to the movements of N-AGVs. This approach allows the <lb/>model to learn and update its strategies based on the immediate <lb/>outcomes of its actions, making it ideal for complex situations <lb/>such as traffic and obstacle avoidance in autonomous AGV <lb/>navigation. <lb/>Zhang and Wu (2023) proposed an autonomous AGV <lb/>behavior decision model based on an ensemble of deep <lb/>reinforcement learning approaches. This model integrates three <lb/>foundational network models: Deep Q-Learning Network (DQN), <lb/>Double DQN (DDQN), and Dueling Double DQN (Dueling <lb/>DDQN). The model was tested and its generalizability was verified <lb/>in a highway simulation environment across scenarios with one-<lb/>way three, four, and five lanes. The interactions of AGVs during <lb/>five driving behaviors (i.e., lane changing to the left, lane keeping, <lb/>lane changing to the right, accelerating in the same lane, and <lb/>decelerating) were examined. It was found that deep learning can <lb/>effectively control AGV interactions over short time periods, <lb/>highlighting its potential in managing complex, real-time <lb/>vehicular behaviors and interactions. Li et al. (2022) investigated <lb/>the application of Deep Reinforcement Learning in planning AGV <lb/>trajectories during continuous lane changes amidst multiple-AGV <lb/>interactions on highways. They found that the use of the DQN <lb/>model led to enhanced outcomes in aspects such as collision <lb/>avoidance and maintaining AGV speed. This research highlights <lb/>the efficacy of DQN in managing complex driving scenarios, <lb/>demonstrating its capability to optimize vehicular movements in <lb/>dynamic, multi-agent environments where quick adaptation and <lb/>decision-making are crucial. Beyond its effectiveness in fine-<lb/>tuning the positional relationships among multiple targets within a <lb/>limited scope, reinforcement learning also plays a significant role <lb/>in scenarios involving the large-scale movement of multiple <lb/>AGVs. In such contexts, reinforcement learning is adept at <lb/>handling the complexities of larger, more dynamic environments <lb/>where AGVs interact extensively. Instead of considering its <lb/>concrete shape, AGVs are abstracted into a point. This capability <lb/>is crucial for tasks like coordinating the movements of a fleet, <lb/>managing traffic flow, or optimizing routes in real-time, <lb/>demonstrating the versatility and robustness of reinforcement <lb/>learning in diverse and expansive vehicular systems (Dai et al., <lb/>2023; Ding et al., 2022). Cai et al. (2023) conducted research on <lb/>the interactions between AGVs and pedestrians at major traffic <lb/>intersections. They studied, on a relatively large scale, how AGVs <lb/>navigate around pedestrians and other AGVs. Their focus was on <lb/>enabling AGVs to choose the shortest possible path through <lb/>intersections while maintaining as high a speed as is safely <lb/>possible. This research is significant in understanding and <lb/>optimizing the complex dynamics at busy intersections, with the <lb/>goal of enhancing traffic flow efficiency while ensuring the safety <lb/>of all road users, including pedestrians. <lb/>These large-scale studies involving multi-objective and multi-<lb/>action reinforcement learning scenarios are similar to our work <lb/>and provide valuable insights for our project. Our work will also <lb/>take place in a larger area with numerous AGVs having extensive <lb/>mobility. Each AGV can be abstracted as a point. From the <lb/>existing literature, it is evident that the assumptions and <lb/>constraints on the movement patterns of these points significantly <lb/></body>

			<page>66 <lb/></page>

			<note place="headnote">Li F, Cheng J C, Mao Z Q, et al. <lb/></note>

			<note place="footnote">J Intell Connect Veh 2024, 7(1): 64-77 <lb/></note>

			<body>impact the computational demands of reinforcement learning. <lb/>This aspect should be a focal point in our subsequent work. It <lb/>suggests that careful consideration of how movement patterns are <lb/>modeled and constrained can greatly influence the efficiency and <lb/>effectiveness of the reinforcement learning algorithms employed <lb/>in this study. <lb/>2.3 Contributions <lb/>Drawing from the aforementioned literature, this study adapts and <lb/>innovates upon the methods of deep learning, particularly LSTM, <lb/>and reinforcement learning for predicting AGV trajectories and <lb/>managing inter-AGV relationships. Our approach integrates <lb/>insights from these fields, leveraging the strengths of LSTM for <lb/>accurate short-term movement forecasting and the dynamic <lb/>decision-making capabilities of reinforcement learning. By doing <lb/>so, our work not only builds upon existing methodologies but also <lb/>introduces novel improvements and adaptations, which is suitable <lb/>for complex, large-scale vehicular environments. The main <lb/>contributions of this paper are listed as follows: <lb/>1) The trajectories of N-AGVs are not arbitrarily created; <lb/>rather, they are derived from a LSTM neural network training <lb/>process. This method ensures a high degree of realism in the <lb/>generated paths, as they closely mimic the actual movement <lb/>patterns observed in terminal operations. In response to the <lb/>aforementioned issue where LSTM models generate larger errors <lb/>for later data points when predicting AGV trajectories with a <lb/>single output of multiple data sets, we propose a rolling prediction <lb/>method to address this problem. The rolling prediction approach <lb/>involves continuously updating the predictions as new data <lb/>becomes available, rather than relying on a single, long-term <lb/>forecast. This method helps to mitigate the error accumulation in <lb/>LSTM predictions over extended periods. <lb/>2) The project is grounded in authentic data from real-world <lb/>terminal operations. This reliance on genuine terminal data <lb/>imbues our approach with a level of realism and applicability that <lb/>is often lacking in theoretical models. By utilizing authentic data, <lb/>our model is not only more accurate but also possesses a universal <lb/>applicability, making it a potentially valuable tool for a wide range <lb/>of terminal logistics scenarios. The discretization of coordinates <lb/>through graph theory, on the other hand, allows for a more <lb/>structured and analyzable representation of the data, aiding in the <lb/>identification of patterns and relationships between AGVs more <lb/>effectively. This combination of techniques aims to enhance the <lb/>accuracy and usability of AGV trajectory predictions in complex, <lb/>dynamic environments. <lb/>3) Our primary focus is on delineating and contrasting the path <lb/>planning methodologies for H-AGV and N-AGVs. In this study, <lb/>we introduce an innovative solution specifically tailored for the <lb/>path planning of H-AGV. This solution is designed to address the <lb/>unique challenge posed by the transportation of hazardous <lb/>materials within the terminal environment, ensuring both safety <lb/>and efficiency. <lb/>3 Mapping and problem formulation <lb/>3.1 Cartographic abstractions <lb/>Prior to delving into the modeling process, it is essential to provide <lb/>a geographical context for our data source, Meishan Port. This <lb/>port is geographically located at approximately 121.9842°E <lb/>longitude and 29.7726°N latitude. Its layout is captured in Fig. 2, <lb/>which presents a satellite image from Google Earth software. Fig. 2 <lb/>reveals that the container yard is predominantly rectangular, <lb/>stretching in a northeast-southwest direction. The yard&apos;s <lb/>dimensions are substantial, measuring approximately 3,000 m in <lb/>length and 600 m in width. <lb/>The internal roadway network of the Meishan port exhibits a <lb/>complex grid structure, consisting of 12 roadways aligned in a <lb/>northwest-southeast direction and 9 roadways in a southwest-<lb/>northeast direction. These roadways intersect orthogonally, <lb/>forming a total of 108 junctions. For the purposes of simplifying <lb/>our model, each roadway is represented as a linear segment devoid <lb/>of width, and each intersection is approximated as a point. The <lb/>latitude and longitude coordinates for these intersection points <lb/>have been precisely determined. <lb/>To facilitate the development of the LSTM neural network and <lb/>the reinforcement learning model, a simplified map of the harbor <lb/>yard is constructed through the following two-step process. <lb/>Initially, a Cartesian coordinate system is introduced to facilitate <lb/>the map&apos;s construction. This system is employed instead of <lb/>directly utilizing the latitude and longitude coordinates of the <lb/>intersections, allowing for a more streamlined representation. <lb/>Subsequently, the map is rotated around the origin of this newly <lb/>established Cartesian coordinate system. This rotation is <lb/>strategically executed to make roadways either horizontally or <lb/>vertically on the new map, thereby simplifying the representation <lb/>for modeling purposes. <lb/>(0,0) <lb/>(x ′′ <lb/>m , y ′′ <lb/>n ) <lb/>m <lb/>n <lb/>Through meticulous observation, it has been ascertained that <lb/>the latitude of all intersection points in the Meishan port exceeds <lb/>29.75°N, and their longitudes surpass 121.98°E. Considering the <lb/>yard&apos;s dimensions, approximately 3,000 m in length and 600 m in <lb/>width, and its roughly rectangular shape, the impact of the Earth&apos;s <lb/>curvature on this area is deemed negligible for the purposes of our <lb/>study. Thus, for the sake of simplification, this assumption is <lb/>adopted in our modeling approach. In this study, we establish a <lb/>Cartesian coordinate system to represent the yard&apos;s layout. The <lb/>intersection point at 29.75°N and 121.98°E is designated as the <lb/>origin <lb/>of this system, with the 121.98°E meridian forming <lb/>the Y-axis and the 29.75°N parallel constituting the X-axis. The <lb/>longitude and latitude of each intersection point are initially <lb/>denoted as <lb/>, where &apos; &apos; and &apos; &apos; represent the sequential <lb/>numbering of the roadways from left to right and top to bottom <lb/>700 m <lb/>29.7726°N, 121.9842°E <lb/>Fig. 2 Meshan port satellite photo from Google Earth software. <lb/></body>

			<note place="headnote">Enhancing safety and efficiency in automated container terminals: Route planning for hazardous material AGV using LSTM neural ... <lb/></note>

			<page>67 <lb/></page>

			<note place="footnote">https://doi.org/10.26599/JICV.2023.9210041 <lb/></note>

			<body>m ∈ {1,2, . . . , 12} <lb/>n ∈ {1,2, . . . , 9} <lb/>x ′ <lb/>m = x ′′ <lb/>m -121.98°y ′ <lb/>n = y ′′ <lb/>n -29.75°( <lb/>x ′ <lb/>m , y ′ <lb/>n ) <lb/>( <lb/>and <lb/>), respectively. To adapt <lb/>these geographical coordinates to our coordinate system, we <lb/>subtract 121.98°E from each longitude and 29.75°N from each <lb/>latitude (i.e., <lb/>and <lb/>). The <lb/>resulting differences are then assigned as the new coordinates <lb/>in the Cartesian frame. <lb/>According to the two-dimensional orientation methods, Eq. (1) <lb/>is applied: <lb/>{ <lb/>xm = x ′ <lb/>m cosθ -y ′ <lb/>n sinθ <lb/>yn = x ′ <lb/>m sinθ + y ′ <lb/>n cosθ <lb/>(1) <lb/>θ <lb/>θ <lb/>θ = -42°w <lb/>here is the angle it rotates. If the rotation is clockwise, will be <lb/>negative. Here a rotation of <lb/>was chosen. It is important <lb/>to note that the selection of this angle is not governed by a <lb/>stringent rule; rather, it is chosen for its practical utility. <lb/>The resultant map, post-rotation, is depicted in Fig. 3a. An <lb/>observation of this map reveals that the distances between each <lb/>intersection point are not uniform. To facilitate the deployment <lb/>and movement simulation of N-AGVs and H-AGV on this map, <lb/>and to streamline training and analysis processes, we have opted <lb/>for a discrete movement model. In this model, the movement of <lb/>AGVs is analogous to checkers, with jumps from one <lb/>predetermined point to another on the mapped roads. These fixed <lb/>points are essential for creating a realistic and manageable <lb/>simulation environment. <lb/>To be more realistic, the distance between these points should <lb/>be similar and reasonable. It is therefore necessary to artificially <lb/>place such points between the intersections where AGVs are likely <lb/>to pass. In order to distinguish between these points, they will be <lb/>referred to jump points, while the intersection points at road <lb/>intersections will be called intersection points as before. The <lb/>intersection points refer to the actual road intersections, while <lb/>jump points are the artificially created points to facilitate AGVs <lb/>movement simulation. <lb/>The density and spacing of jump points have a direct impact on <lb/>the simulation&apos;s realism and computational load. Denser jump <lb/>points create a more continuous and realistic AGV movement but <lb/>computational demands increase later. Conversely, fewer and <lb/>more widely spaced jump points result in a more discrete <lb/>movement pattern but reduce computational requirements. <lb/>Balancing these factors, we determined an optimal total of 936 <lb/>points, which corresponds to an estimated real-world distance <lb/>ranging from 40 to 50 m between each point in the physical yard. <lb/>Fig. 3b illustrates the new map, clearly marked with both <lb/>intersection points and the strategically placed jump points. This <lb/>setup ensures that the deployment and movement of H-AGV and <lb/>N-AGVs are confined to these defined points, facilitating a <lb/>controlled and realistic simulation of vehicular movement within <lb/>the container yard. <lb/>3.2 Utilizing of map <lb/>After the map is created, for ease of management, each point on <lb/>the map is assigned a unique ID, starting from 0. This process <lb/>gives each of the 936 points in the graph a distinct identifier. <lb/>Additionally, the coordinates of each point on the map are <lb/>recorded and directly correlated with their respective IDs. This <lb/>system of assigning unique IDs and corresponding coordinates to <lb/>each point allows for precise tracking and efficient management of <lb/>spatial data within the map. Points that are directly connected to <lb/>any given point by dashed lines are referred to as its neighbor <lb/>points (hereafter referred to as n-points). For instance, an <lb/>intersection point at a T-junction would have three neighbor <lb/>points, reflecting the three directions of possible movement from <lb/>that junction. Similarly, an intersection point at a crossroads <lb/>would have four neighbor points, corresponding to the four <lb/>connected directions. However, for a jump point, which <lb/>presumably represents a simpler connection like a straight line, <lb/>there would only be two neighbor points. <lb/>For the subsequent movement of AGVs within the graph, there <lb/>are specific rules and requirements that need to be followed： <lb/>1) AGVs are abstracted as points on the map. The key rule is <lb/>that they can only appear at the coordinates corresponding to one <lb/>of the 936 predefined points on the map. <lb/>2) The starting points f AGVs are defined as s-points. Besides, <lb/>we define the points where the AGVs are now on as current <lb/>points (i.e., c-points) and the AGVs just left as history point (i.e., h-<lb/>points). It is noted that h-points are not all the points that the <lb/>AGVs have passed through, but the only one point they left just <lb/>after the jump). <lb/>3) Any N-AGV initially located at an s-point (which can also be <lb/>referred to as a c-point) has options to either jump to any of the n-<lb/>points connected to its current s-point or remain stationary at its <lb/>current s-point/c-point. However, once a N-AGV makes its first <lb/>jump, its options for subsequent moves, regardless of its current <lb/>position (including if it returns to its original s-point), are limited <lb/>to only two choices: It can either stay stationary at a c-point or <lb/>jump to any n-point that is not an h-point. <lb/>4) In the map with 936 points, it is permitted for more than one <lb/>AGV to occupy or stay at any given point simultaneously. <lb/>5) The process of an AGV jumping from one point to another <lb/>is instantaneous, but not all AGVs jump synchronously. This <lb/>reflects real-world conditions where AGVs with higher speeds in <lb/>the simulation would jump between points more quickly, whereas <lb/>slower AGVs would make their jumps more slowly. <lb/>6) When the simulation starts, N-AGVs are theoretically <lb/>designed to operate continuously for an indefinite period. This <lb/>design ensures that the H-AGV has sufficient time to navigate <lb/>towards its randomly assigned destination. <lb/>(a) <lb/>(b) <lb/>Fig. 3 Newly built map (a) with only intersections points marked; (b) with <lb/>intersections points and marked jump points. <lb/></body>

			<page>68 <lb/></page>

			<note place="headnote">Li F, Cheng J C, Mao Z Q, et al. <lb/></note>

			<note place="footnote">J Intell Connect Veh 2024, 7(1): 64-77 <lb/></note>

			<body>7) In the model, there are multiple N-AGVs and only one <lb/>H-AGV. This assumption is made to simplify the scenario to a <lb/>certain extent. Once the AGVs start operating in simulation, the <lb/>number of H-AGV and N-AGVs are fixed-they cannot increase <lb/>or decrease in quantity, nor can they disappear and reappear. <lb/>The design and assumptions outlined above aim to closely <lb/>mimic real-world AGV motions while ensuring the feasibility of <lb/>subsequent calculations. <lb/>4 N-AGV setting and LSTM training <lb/>4.1 General idea and goal of LSTM training <lb/>Based on the existing literature presented in Section 2 and the <lb/>rules for AGV operation outlined in Section 3.2, the decision was <lb/>made to first use an LSTM neural network to predict the <lb/>trajectories of N-AGVs based on their historical latitude and <lb/>longitude coordinates. These predicted trajectories are then <lb/>discretized onto the created map. To address the issue of LSTM <lb/>producing larger errors in later predictions when outputting <lb/>multiple sets simultaneously, and to handle the need to predict an <lb/>indefinite number of N-AGV trajectories, a rolling prediction <lb/>method is proposed in this study. Additionally, it is important to <lb/>clarify that the prediction of N-AGVs&apos; trajectories and their <lb/>deployment on the map is primarily to provide a dynamic <lb/>background for subsequent H-AGV path planning. Therefore, the <lb/>primary focus for the prediction and mapping of N-AGVs&apos; <lb/>trajectories should be on their reasonableness and realism, rather <lb/>than achieving extremely high prediction accuracy. This approach <lb/>recognizes the practical need for creating a realistic and dynamic <lb/>simulation environment that effectively supports the main <lb/>objective of optimizing H-AGV&apos;s path planning. <lb/>4.2 Data preprocessing and network frame <lb/>The dataset used in this work consists of historical coordinate data <lb/>for N-AGVs provided by Meishan Port. It contains a total of <lb/>207,165 rows and four columns. The first column contains the <lb/>names of different N-AGV (the name of an AGV takes up <lb/>multiple rows, each corresponding to different other data, and all <lb/>rows of the same car are grouped together, not mixed), the second <lb/>column contains timestamps of each AGVs&apos; data (the timestamp <lb/>for each car is incremental), the third column lists the longitude <lb/>coordinates of the N-AGV at that time, and the fourth column <lb/>lists the latitude coordinates. The sampling frequency of the data is <lb/>1 Hz and each N-AGV has several hundred to several thousand <lb/>sets of latitude and longitude coordinates over continuous time <lb/>periods. The dataset covers a total of 556 N-AGVs. For the <lb/>purposes of model testing and validation, 38 sets of data with <lb/>fewer coordinate points were selected as the test set, 104 sets as the <lb/>validation set, and the remaining 414 sets as the training set. Given <lb/>that all N-AGVs exhibit similar movement patterns, and with the <lb/>aim for the model to learn common features across all N-AGVs, <lb/>the latitude and longitude coordinates from the 414 N-AGV <lb/>groups are used as input to train a single model. This approach is <lb/>designed to help the model generalize across different AGVs and <lb/>conditions. <lb/>Combined with previous relevant studies (Fang, 2022; Yang, <lb/>2022), the structural design of our model is shown in Fig. 4. The <lb/>number of layers is set to be 2 and the number of hidden units in <lb/>each LSTM layer is set to 64. The outputs of the model are the <lb/>longitude and latitude coordinates. <lb/>4.3 Network training and results <lb/>The LSTM model employs a toolkit provided by Pytorch. For the <lb/>Historical latitude <lb/>coordinate <lb/>Historical longitude <lb/>coordinate <lb/>Predicted longitude <lb/>coordinate <lb/>Predicted latitude <lb/>coordinate <lb/>64 units <lb/>2 layers <lb/>64 units <lb/>Fig. 4 Structure of the LSTM model. <lb/></body>

			<note place="headnote">Enhancing safety and efficiency in automated container terminals: Route planning for hazardous material AGV using LSTM neural ... <lb/></note>

			<page>69 <lb/></page>

			<note place="footnote">https://doi.org/10.26599/JICV.2023.9210041 <lb/></note>

			<body>training set, a batch size of 64 is set, while for the validation set, a <lb/>smaller batch size of 32 is used. Learning rate is set to 0.001, and <lb/>an Adam optimizer is used to adjust parameters. The loss function <lb/>is Mean Squared Error (MSE). Verification frequency is once <lb/>every 10 epochs. The convergence process of LSTM model <lb/>training is shown in Fig. 5 shows that the number of training <lb/>iterations can be set to 250. The nature of LSTM network tends to <lb/>cause an increase in error for later outputs, we focus on MSE of <lb/>the first 10, 20, 30, 40 and 50 sets of predictions respectively, which <lb/>has been calculated and showed in Table 1. <lb/>Train loss <lb/>Test loss <lb/>1e-6 Train vs. test loss over epochs (extended) <lb/>5 <lb/>4 <lb/>3 <lb/>2 <lb/>1 <lb/>0 <lb/>Loss <lb/>0 <lb/>50 <lb/>100 <lb/>Epoch <lb/>150 <lb/>200 <lb/>250 <lb/>Fig. 5 LSTM network model training convergence process. <lb/>Table 1 Results of LSTM network <lb/>Set <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>MSE <lb/>0.339 <lb/>0.346 <lb/>0.401 <lb/>0.456 <lb/>0.498 <lb/>4.4 Rolling prediction <lb/>N <lb/>i <lb/>[i -49, i] <lb/>i + 1 <lb/>[i -48, i + 1] <lb/>i + 2 <lb/>To address the issue of increasing errors in later predictions when <lb/>LSTM outputs multiple sets of data at once, and to meet the <lb/>requirement for the model to make indefinite predictions, a rolling <lb/>prediction method is proposed in this study. Use to represent <lb/>the historical trajectory points of any N-AGV. to represent <lb/>the last historical trajectory points (i.e., the longitude and latitude <lb/>coordinate with the latest We use 50 sets of data from <lb/>as the input to make the prediction a warm start. Then, <lb/>after the model predicts the <lb/>as a result, we then input <lb/>as the new set of data to the model and get <lb/>as a <lb/>result, and so on for the rest of the process. This rolling prediction <lb/>approach effectively creates a moving window that shifts forward <lb/>with each new prediction. By always incorporating the most <lb/>recent prediction into the input window for the next prediction, <lb/>the model continually adapts to the latest and minimize the <lb/>accumulation of errors. <lb/>4.5 Real world coordinate mapping to the map <lb/>i + 1 <lb/>i + 1 <lb/>i + 2 <lb/>The data predicted by the LSTM are real coordinates in practice, <lb/>which need to be transformed by Eq. (1) to get into the coordinate <lb/>system we set up. Next, we need to discretize these predicted <lb/>coordinates and &quot;drag&quot; them to the 936 points we set up <lb/>beforehand. First of all, the discretization and prediction are done <lb/>at the same time, i.e., every time a coordinate is predicted, it will be <lb/>dragged to one of the 936 points. When an AGV predicts <lb/>, <lb/>the Euclidean distances between <lb/>and all 936 possible points <lb/>in the graph will be computed. Once the nearest jump point or <lb/>intersection point is found, the ID of this point is recorded and <lb/>regards that point as the AGV&apos;s s-point (as well as its c-point <lb/>now). Next, when the model predicts <lb/>, it calculates the <lb/>i + 2 <lb/>i + 3 <lb/>i + 2 <lb/>i + 3 <lb/>i + 3 <lb/>Euclidean distance from <lb/>to the c-point and all its n-points, <lb/>selects the point with the closest Euclidean distance as the new c-<lb/>point and records the ID of this point. There will be two cases, one <lb/>is that there is no movement, and then when the model predicts <lb/>, it repeats the above operation after predicting <lb/>. In the <lb/>other case, if there is a movement, i.e., there is a new c-point and <lb/>the left c-point becomes h-point, then after the model predicts <lb/>, it will compute the Euclidean distances from <lb/>to the c-<lb/>point and to all n-points except h-point, and select the nearest <lb/>point as the new c-point and record the ID of this point. The <lb/>subsequent process is carried out in an analogous manner. This <lb/>process will be done through the Networkx library in python. It <lb/>will allow the predicted coordinates to be mapped to the map in a <lb/>timely manner. And there is a record of the IDs of the points that <lb/>N-AGVs passes through. When the LSTM model predicts the <lb/>coordinates of multiple AGVs at the same time, since it uses the <lb/>same prediction method and inputs the same amount of data, we <lb/>can assume that the model generates the predicted data at the <lb/>same rate for each AGV trajectory, so that the number of IDs <lb/>recorded for each AGV in the same time period after mapping is <lb/>also the same. If the predicted sets of coordinates are close in <lb/>distance to each other, then it appears intuitively on the map that <lb/>AGV is generating stagnation. Conversely, if the predicted sets of <lb/>coordinates are farther apart, then it appears visually on the map <lb/>that AGV is in continuous motion. This is a good response to the <lb/>speed of the AGV and is more realistic. <lb/>5 Methodology of the reinforcement learning <lb/>5.1 Introduction of DQN <lb/>Reinforcement learning (RL) has a very wide range of applications <lb/>in AGV trajectory planning (Han, 2023; Zhang and Wu, 2023). <lb/>Moreover, since the map for AGV operations contains 936 points, <lb/>using traditional Q-Learning would result in a substantial <lb/>computational load during training. Therefore, in this context, <lb/>DQN will be employed to plan the trajectory of the H-AGV. <lb/>Deep Q-Learning method interacts with the environment <lb/>through a policy-target structure, selecting appropriate actions as <lb/>policy outputs. The process of interaction with the environment is <lb/>illustrated in Fig. 6. <lb/>t <lb/>st <lb/>at <lb/>rt <lb/>st+1 <lb/>t + 1 <lb/>st+1 <lb/>rt <lb/>The policy-target structure of DQN is composed of two neural <lb/>networks, namely the policy network and the target network. The <lb/>process of DQN interacting with the environment can be <lb/>described as follows. At time , the policy network estimates the Q <lb/>values of all executable actions based on the input state from the <lb/>environment and executes the action with the highest Q value. <lb/>The environment then provides feedback in the form of a reward <lb/>based on the executed action. Subsequently, the state of the <lb/>environment updates to the state <lb/>at time <lb/>. The target <lb/>network extracts information about the state <lb/>after the action is <lb/>executed, as well as the obtained feedback reward , and <lb/>recalculates the Q value of the optimal action chosen by the policy <lb/>network. The loss function compares the Q value calculated by the <lb/>target network with the Q value estimated by the policy network. <lb/>It updates the policy network so that the Q values estimated by the <lb/>policy network during training progressively approximate the true <lb/>Q values. This enables the selection of the optimal action based on <lb/>the state information of the environment. <lb/>To define the Q value of an action, let us first define the reward <lb/></body>

			<page>70 <lb/></page>

			<note place="headnote">Li F, Cheng J C, Mao Z Q, et al. <lb/></note>

			<note place="footnote">J Intell Connect Veh 2024, 7(1): 64-77 <lb/></note>

			<body>rt <lb/>t <lb/>t <lb/>fed back by the environment after executing an action at step . <lb/>When selecting actions, the DQN strategy considers not only the <lb/>immediate reward obtained but also its future impact. Therefore, <lb/>the reward fed back by the environment after executing an action <lb/>is defined as the cumulative reward. The cumulative reward <lb/>obtained after executing an action at step is the sum of the <lb/>current reward at that moment and the discounted future rewards, <lb/>which is represented as <lb/>Rt = <lb/>∞ <lb/>∑ <lb/>i=0 <lb/>β i rt+i <lb/>(2) <lb/>rt+i <lb/>t + i <lb/>(0, 1] <lb/>where <lb/>represents the reward obtained at step <lb/>; β is the <lb/>discount factor, indicating the impact of future moments in the <lb/>cumulative reward. β belongs to the interval <lb/>. <lb/>(a1, a2, a3, . . . , an) <lb/>Rt <lb/>t <lb/>st <lb/>an <lb/>DQN, through interaction with the environment, trains to <lb/>select the optimal action based on different environmental states. <lb/>The executable actions are predefined as <lb/>. Let <lb/>represents the cumulative reward obtained after executing an <lb/>action. Thus, at step , under the state , the Q value of action <lb/>is <lb/>Q (st, an) = E (Rt|s = st, a = at) <lb/>(3) <lb/>st <lb/>The expected value of the cumulative reward of the action <lb/>under state is the Q value of the action. <lb/>The policy network and target network feature implicit fitting <lb/>functions. Therefore, it is only necessary to define their inputs and <lb/>outputs. The policy network estimates the Q values of all actions <lb/>based on the current state and selects the optimal action. The <lb/>policy network is expressed as <lb/>Vactor (ωa) = maxE (Q (st, a1) , Q (st, a2) , . . . , Q (st, an)) (4) <lb/>ωa <lb/>where represents the parameters of the policy network. <lb/>st <lb/>Equation (4) indicates that the policy network, under state , <lb/>estimates the Q values of all actions, and outputs the action with <lb/>the highest Q By substituting Eq. (2) into Eq. (3), we obtain: <lb/>Q (st, ax) = E <lb/>( <lb/>rt + β <lb/>∞ <lb/>∑ <lb/>i=0 <lb/>β i rt+1+i|s = st, a = ax <lb/>) <lb/>= E[rt + maxβ(Q (st+1, a1) , Q (st+1, a2) , . . . , <lb/>Q (st+1, an))|s = st, a = ax] <lb/>(5) <lb/>Equation (5) indicates that the Q value of an action can be <lb/>composed of the reward value obtained at the current moment <lb/>and the Q value of the optimal action in the next moment. <lb/>The target network calculates the Q value of an action based on <lb/>the generated reward, therefore the Q value calculated by the <lb/>target network is closer to the real value than that estimated by the <lb/>policy network. The output of the target network is defined as <lb/>Vcritic (ωc) = rt + βmaxE(Q (st+1, a1) , <lb/>Q (st+1, a2) , . . . , Q (st+1, an)) <lb/>(6) <lb/>ωc <lb/>where represents the parameters of the target network. <lb/>Equation (6) suggests that the target network calculates the Q <lb/>value of the current action by considering the Q value of the <lb/>optimal action in the next moment and the reward obtained after <lb/>executing the current action. <lb/>By inputting the Q values calculated by the target network and <lb/>estimated by the policy network into the loss function, and based <lb/>on the output of the loss function, the policy network is updated <lb/>through gradient descent. This process ensures that the Q values <lb/>output by the policy network continually approximate the true Q <lb/>values (i.e., target Q value), thereby achieving the goal of selecting <lb/>the optimal action under different states. <lb/>The loss function is defined as <lb/>L(ω) = maxE (Vcritic(ω c ) -Vactor(ωa)) 2 <lb/>= maxE{[rt + βmaxE(Q(st+1, a1), Q(st+1, a2), . . . , <lb/>Q(st+1, an))] -[maxE(Q(st, a1), <lb/>Q(st, a2), . . . , Q(st, an))]} 2 <lb/>(7) <lb/>s1 <lb/>a1 <lb/>r1 <lb/>s2 <lb/>(s1, a1, r 1 , s2) <lb/>D <lb/>Through its unique policy-target structure interacting with the <lb/>environment, there is no need to establish an accurate model of <lb/>the environment, endowing it with the ability to autonomously <lb/>explore optimal strategies. Furthermore, during the training of the <lb/>DQN neural network, it is necessary to use experience replay to <lb/>enhance the training efficiency of the model. Experience replay <lb/>involves storing data; for example, if an agent in state performs <lb/>action and receives a reward , subsequently transitioning to <lb/>state , then the experience replay pool will save this tuple <lb/>. During training, a certain number of data samples <lb/>(represented as ) are randomly selected from the experience <lb/>replay pool for training. This method continuously optimizes the <lb/>network model. <lb/>Action <lb/>Policy <lb/>network <lb/>Environment <lb/>Parameter replication <lb/>Gradient descent <lb/>Loss function <lb/>Estimated <lb/>Q value <lb/>Target <lb/>Q value <lb/>DQN <lb/>Random sampling <lb/>Target <lb/>network <lb/>Experiment <lb/>replay <lb/>pool <lb/>State and reward <lb/>Fig. 6 Interaction of DQN with the environment. <lb/></body>

			<note place="headnote">Enhancing safety and efficiency in automated container terminals: Route planning for hazardous material AGV using LSTM neural ... <lb/></note>

			<page>71 <lb/></page>

			<note place="footnote">https://doi.org/10.26599/JICV.2023.9210041 <lb/></note>

			<body>5.2 Experiment settings <lb/>The experimental design mainly consists of two parts. Firstly, <lb/>some detailed parameters of DQN will be introduced during the <lb/>training. Secondly, the design of the four key elements of <lb/>reinforcement learning will be individually discussed in this <lb/>experiment: action space, state space, reward design, and <lb/>termination conditions. <lb/>The policy network and the target network are initialized at the <lb/>beginning. Subsequently, we collect data over a certain number of <lb/>steps. The position ID, next action, reward and the next position <lb/>ID of the H-AGV are stored in the experience replay buffer as the <lb/>current state. Next, training module is used for iterative training of <lb/>the policy network and target network, utilizing random sampling <lb/>from the experience replay buffer. The training continues until all <lb/>networks progressively converge. In this experiment, the policy <lb/>network and the target network have identical structures; however, <lb/>there is a distinction in their model parameters during the training <lb/>process. The main parameter configurations for training are <lb/>presented in Table 2. <lb/>2 Parameters for DQN training <lb/>Parameter <lb/>Symbol <lb/>Value <lb/>Buffer size <lb/>N <lb/>2,000 <lb/>Learning rate <lb/>η <lb/>10 -4 <lb/>Batch size <lb/>B <lb/>256 <lb/>Discount factor <lb/>β <lb/>0.99 <lb/>Exploration initial rate <lb/>ε i <lb/>1 <lb/>Exploration final rate <lb/>ε f <lb/>0.05 <lb/>Target update interval <lb/>C <lb/>200 <lb/>Max grad norm <lb/>-<lb/>10 <lb/>Here, the meaning of each parameter is explained. The buffer <lb/>size is the size of the replay buffer, which controls the amount of <lb/>memory used to store and resample past experiences. The learning <lb/>rate is the rate at which the model&apos;s weights are updated. Affects <lb/>the step size in the optimization algorithm. The batch size is the <lb/>number of samples used in each training iteration. <lb/>ε <lb/>ε <lb/>The discount factor, which determines the present value of <lb/>future rewards. The exploration initial rate and exploration final <lb/>rate are the values of at the beginning and the end. is a <lb/>probability parameter, meaning what are the chances that in <lb/>training DQN, the H-AGV will choose a random action on <lb/>certain state. It will decrease in the training process. The target <lb/>update interval is the interval of updates to the target network. The <lb/>max grad norm is the maximum norm for gradient clipping. <lb/>In order to better understand the training process of the model, <lb/>a piece of pseudo-code Algorithm 1 is shown below: <lb/>The Action space design, State space design, Reward design, <lb/>and Termination conditions design of reinforcement learning <lb/>used in this work are introduced as follows. <lb/>1) Action space design <lb/>For the H-AGV, its movement is discrete, capable of moving <lb/>only by jumping among the 936 points on the map. It can move <lb/>from any c-point to its any n-point. An additional note is that, <lb/>since the H-AGV carries hazardous materials, reflecting real-world <lb/>scenarios, we allow it to perform reversing actions. This means <lb/>that the H-AGV can jump back from a c-point to an h-point, <lb/>whereas the N-AGVs cannot. Therefore, summarizing above, the <lb/>action space is jumping towards the up, down, left, and right <lb/>directions. However, for certain c-points, there may be no n-<lb/>points in the up, down, left, or right directions. Therefore, these <lb/>directions, which do not offer a viable path for jumping, are <lb/>effectively masked in the system. To provide more details about <lb/>this process, if the H-AGV is currently at a jump point, it can only <lb/>jump to one of the two n-points up and down, or to one of the <lb/>two n-points to the left and right. If it is at a T-intersection point, <lb/>it can jump to any of the three n-points. If it is at a crossroad <lb/>intersection point, then it can jump to the four n-points in the up, <lb/>down, left, and right directions. Additionally, it is worth noting <lb/>that to simplify the design of subsequent rewards, the H-AGV will <lb/>be set to be unable to remain stationary at a point. This means that <lb/>it must move continuously. <lb/>2) State space design <lb/>The state space refers to the collection of all possible states in an <lb/>environment. For this project, since the movement of both N-<lb/>AGV and H-AGV are discrete, confined to 936 points on the <lb/>map, the state space is mainly a collection of different states of <lb/>points on the map, including five states. The first state is the <lb/>historical trajectory of the H-AGV. When the H-AGV is in <lb/>operation, the last four points that it has passed are marked. The <lb/>second state is the directional predicted points for H-AGV, i.e., <lb/>when the H-AGV is at a certain c-point, the n-point that is with <lb/>the shortest minimum Euclidean distance to the destination <lb/>among all its n-points (except h-point) will be labeled, which <lb/>represents the direction of the destination. The third state is the <lb/>DQN with experience replay <lb/></body>

			<page>72 <lb/></page>

			<note place="headnote">Li F, Cheng J C, Mao Z Q, et al. <lb/></note>

			<note place="footnote">J Intell Connect Veh 2024, 7(1): 64-77 <lb/></note>

			<body>current location of all N-AGVs on the map, specifically the ID of <lb/>the points where these AGVs are situated. These points are <lb/>recorded. The fourth state is the destination of the H-AGV. This <lb/>point is unique on the map, and the location of the destination <lb/>point is specially recorded. The fifth state is all the points and the <lb/>connecting dotted lines on the map, which make up the entire <lb/>undirected graph. <lb/>3) Reward design <lb/>The design of the environment&apos;s reward incorporates two <lb/>components: positive rewards and negative rewards, the latter <lb/>commonly known as penalties. For simplicity, both will be <lb/>collectively referred to as &quot;rewards&quot;. Their values can be both <lb/>positive and negative. These rewards account for various factors <lb/>that influence the scenario. <lb/>rd <lb/>rd = 0.3 <lb/>rd = -0.2 <lb/>rd = 0 <lb/>rd <lb/>Path and distance reward: For the H-AGV, one of its most <lb/>important tasks is to move from the starting point to the <lb/>destination via the shortest path. Due to the relatively simple <lb/>structure of the map, the shortest path between any two points on <lb/>the graph can be easily found using Python&apos;s Networkx library. <lb/>The reward received by the AGV is denoted by the symbol . The <lb/>first scenario is that the H-AGV predicts the next point it will <lb/>move to from the c-point, and simultaneously calculates the <lb/>shortest path to the destination with c-point as the starting point <lb/>using the Networkx library. If the predicted next point <lb/>with the second point of the calculated shortest path, then a <lb/>reward <lb/>will be given. Considering that the H-AGV needs <lb/>to avoid N-AGVs during its movement, which may lead to <lb/>deviations from the shortest path, and at the same time, we do not <lb/>the H-AGV to engage in meaningless movements that take it <lb/>further away from the destination, a balance is introduced using <lb/>the variable &quot;distance&quot;. &quot;Distance&quot; refers to the Euclidean distance <lb/>from the current c-point to the destination. &quot;Path&quot; is the shortest <lb/>route calculated from c-point to the destination. The second <lb/>scenario, which is the worst case, occurs when the next point the <lb/>H-AGV is about to jump to from c-point has a greater &quot;distance&quot; <lb/>to the destination than c-point, and the point about to jump is also <lb/>not on the shortest path corresponding to c-point. In this case, a <lb/>reward of <lb/>will be incurred. Apart from the two <lb/>scenarios mentioned above, all other situations are classified as the <lb/>third scenario. In these cases, no reward is given, i.e., <lb/>. <lb/>Therefore, based on the above discussion, can be represented by <lb/>Eq. (8) : <lb/>rd = <lb/> <lb/> <lb/> <lb/>0.3 <lb/>shortest path <lb/>-0.2 <lb/>not shortest path ∩ distance increase <lb/>0 <lb/>other <lb/>(8) <lb/>This design of the path and distance reward is a simplification <lb/>of the real-world scenario, intended to ensure that the H-AGV, <lb/>while avoiding N-AGVs and aiming to reach its destination via <lb/>the shortest path, and receives a reward as close to zero as possible. <lb/>This approach balances the need for efficient routing with the <lb/>practical necessity of avoiding obstacles. <lb/>Cover reward: The cover reward is designed to encourage the H-<lb/>AGV to avoid N-AGVs. Its mechanism is as follows: At any given <lb/>step, the current position of the H-AGV is denoted as the c-point. <lb/>The c-points surrounding are its n-points. Each of these n-points <lb/>has its own surrounding n-points. Thus, for a particular step, we <lb/>record the n-points of the H-AGV&apos;s c-points as well as the n-<lb/>points of these n-points. These points&apos; IDs are then compared <lb/>with the IDs of the c-points of all N-AGVs on the map at that <lb/>moment. If there is a match (the times of total matches occurred <lb/>in the experiment will be referred to as total number of overlaps <lb/>later), it indicates that at least one N-AGV is near the H-AGV. If, <lb/>n <lb/>n <lb/>rcov = -0.4n <lb/>in a particular step, such matches occur (indicating the presence <lb/>of N-AGVs near the H-AGV), a reward is given, calculated as <lb/>. This reward system effectively incentivizes the H-<lb/>AGV to maintain a distance from N-AGVs, enhancing its <lb/>navigational efficiency and safety. <lb/>rdis = -1 <lb/>Duplication reward: The H-AGV might encounter a situation <lb/>where, especially near the edges and corners of the map, it <lb/>repeatedly jumps between two or several points, leading to an <lb/>inability to escape this loop. To monitor the H-AGV&apos;s movement <lb/>from the starting point (s-point) to any c-point, all passed point <lb/>IDs are tracked. If the ID of any point appears three times, a <lb/>significant negative reward of <lb/>is assigned. This <lb/>mechanism effectively prevents the H-AGV from repeatedly <lb/>jumping back and forth between a few points. However, there is a <lb/>degree of randomness involved. The AGV might coincidentally <lb/>pass the same point three times on its way to the destination <lb/>without being stuck in a loop. Although possible, the probability <lb/>of this occurring is relatively low. So, we ignore this situation. <lb/>rt = -0.1 <lb/>Step reward: To encourage the H-AGV to reach its destination <lb/>from the starting point as quickly as possible, with the fewest <lb/>number of steps, a negative reward is set for each jump the H-<lb/>AGV makes. The negative reward is <lb/>per jump. This <lb/>incentivizes minimizing the number of steps taken to complete the <lb/>route. <lb/>rarr = 4 <lb/>Destination reward: The destination reward represents the final <lb/>reward for the H-AGV: upon reaching its destination, it receives a <lb/>significant reward of <lb/>. This large reward is intended to <lb/>strongly incentivize the H-AGV to efficiently navigate to its <lb/>destination. <lb/>4) Termination conditions design <lb/>There are three termination conditions for the system, all of <lb/>which are about H-AGV. Firstly, when the ID of the H-AGV&apos;s c-<lb/>point equals the ID of the destination point that we initially <lb/>designed, i.e., when the H-AGV reaches the destination, the <lb/>process will terminate. Secondly, if the H-AGV has been operating <lb/>for a long time and the number of actions it takes exceeds a <lb/>certain threshold, but it still not reaches the destination, a <lb/>termination will occur. The threshold is set to be 200 steps of <lb/>jumping for The second type is for special situations. <lb/>Thirdly, termination occurs when the H-AGV moves outside of <lb/>the 936 points, meaning that it exits the map following a certain <lb/>action. Since the H-AGV is directly generated at a specific point <lb/>on the map, unlike the N-AGVs, which has its actual path <lb/>predicted and then mapped, the exceeding situation can arise with <lb/>the H-AGV. <lb/>6 Results of the experiment <lb/>6.1 DQN training results <lb/>To establish clear terminology for the study, AGV movement is <lb/>referred to as &quot;step&quot; (i.e., when an AGV jumps from a point to <lb/>another, that is one step), whereas each iteration during the <lb/>training process is termed as &quot;time&quot;. <lb/>Fig. 7 illustrates the changes in the loss of the policy network. <lb/>As observed from the graph, there is an initial fluctuation in the <lb/>loss values. However, after approximately 380,000 training <lb/>iterations, the loss gradually approaches zero. This trend indicates <lb/>a stabilization and improvement in the policy network&apos;s <lb/>performance as the training progresses. Fig. 8 represents the <lb/>number of steps required for the H-AGV to travel from the <lb/>starting point to the destination in each episode. The graph shows <lb/>that, as the number of training iterations increases, the required <lb/></body>

			<note place="headnote">Enhancing safety and efficiency in automated container terminals: Route planning for hazardous material AGV using LSTM neural ... <lb/></note>

			<page>73 <lb/></page>

			<note place="footnote">https://doi.org/10.26599/JICV.2023.9210041 <lb/></note>

			<body>number of steps gradually decreases. This trend indicates that the <lb/>H-AGV is becoming more and more efficient, able to reach its <lb/>destination faster as the training progresses. Fig. 9 depicts the <lb/>curve of changes in average rewards. The average reward value is <lb/>calculated as the ratio of the total reward obtained at the end of <lb/>each episode to the total number of steps taken by the H-AGV. It <lb/>is evident from the graph that the average reward curve gradually <lb/>increases as training progresses. Correspondingly, Fig. 8 indicates <lb/>a downward trend in the total number of steps required by the H-<lb/>AGV in each episode. This implies that, following training, the <lb/>total reward obtained in each episode increases, signifying <lb/>enhanced efficiency and performance of the H-AGV. <lb/>6.2 Simulation results <lb/>The entire process is simulated using a Python-based platform, as <lb/>illustrated in Fig. 10. In this simulation, the blue star represents the <lb/>H-AGV, and its historical four points are connected by green lines <lb/>and marked on the map. The black dots represent the N-AGVs, <lb/>and the green solid square indicates the destination of the H-<lb/>AGV. On the right side of the simulation, various parameters, <lb/>including the rewards during operation, are monitored in real-<lb/>time. <lb/>During experimentation, the system is first initialized by setting <lb/>up 10 N-AGVs with randomly generated starting points. Then, <lb/>one H-AGV is set up with randomly determined starting and <lb/>destination points. The simulation runs, tracking the number of <lb/>steps the H-AGV takes. If the H-AGV reaches its destination in <lb/>no more than 200 steps, the total number of steps taken, the total <lb/>reward received at the end, and the total number of overlaps are <lb/>recorded. If the H-AGV does not reach its destination within 200 <lb/>steps, the simulation is forcibly terminated, and the total reward <lb/>and total number of overlaps at that point are recorded. <lb/>After conducting a total of 1,500 experiments, the H-AGV <lb/>successfully reached its destination within 200 steps on 1,237 <lb/>occasions, accounting for 82.47% of all trials. Among these <lb/>successful instances, 63.21% had a total overlap count of zero. The <lb/>results indicate favorable outcomes from the experiment. We also <lb/>plot the step vs. total overlap number scatter diagrams as shown in <lb/>Fig. 11. The data used for the illustration is derived from 400 <lb/>randomly sampled instances where the H-AGV successfully <lb/>reached its destination. The graph reveals that in nearly half of <lb/>these experiments, the H-AGV was able to reach the destination <lb/>with a total number of overlaps of zero. Moreover, it generally <lb/>reaches the destination within 100 steps. This observation <lb/>underscores the effectiveness of the routing strategy in efficiently <lb/>guiding the H-AGV to its destination while minimizing <lb/>interactions with other AGVs. <lb/>0.35 <lb/>0.30 <lb/>0.25 <lb/>0.20 <lb/>0.15 <lb/>0.10 <lb/>0.05 <lb/>0 <lb/>Loss <lb/>0 <lb/>5 0 , 0 0 0 <lb/>1 0 0 , 0 0 0 <lb/>1 5 0 , 0 0 0 <lb/>2 0 0 , 0 0 0 <lb/>2 5 0 , 0 0 0 3 0 0 , 0 0 0 3 5 0 , 0 0 0 4 0 0 , 0 0 0 <lb/>Times <lb/>Loss of policy network <lb/>Fig. 7 Loss of policy network during training. <lb/>200 <lb/>180 <lb/>160 <lb/>140 <lb/>120 <lb/>100 <lb/>80 <lb/>60 <lb/>Step <lb/>0 <lb/>5 0 , 0 0 0 <lb/>1 0 0 , 0 0 0 <lb/>1 5 0 , 0 0 0 <lb/>2 0 0 , 0 0 0 <lb/>2 5 0 , 0 0 0 <lb/>3 0 0 , 0 0 0 <lb/>3 5 0 , 0 0 0 <lb/>4 0 0 , 0 0 0 <lb/>Times <lb/>Number of steps for each episode <lb/>Fig. 8 Number of steps for each episode. <lb/>0 <lb/>-10 <lb/>-20 <lb/>-30 <lb/>-40 <lb/>-50 <lb/>-60 <lb/>-70 <lb/>Reward <lb/>0 <lb/>5 0 , 0 0 0 1 0 0 , 0 0 0 1 5 0 , 0 0 0 2 0 0 , 0 0 0 2 5 0 , 0 0 0 3 0 0 , 0 0 0 3 5 0 , 0 0 0 4 0 0 , 0 0 0 <lb/>Times <lb/>Average reward <lb/>Fig. 9 Average reward for each episode. <lb/>H-AGV <lb/>r_exc: 0.00, r_timeout: 0.00 <lb/>r_t: -0.10, r_total: -0.80 <lb/>r_dir: -1.00, r_arr: 0.00 <lb/>r_dis: 0.00, r-path: 0.30 <lb/>done:False, action: 2 <lb/>start_id: 860, des_id: 707 <lb/>topo: [1 1 0 0] <lb/>prev_a_prob: 0.00 0.00 0.53 0.47 <lb/>act0: 2 act1: 3 act2: 2 act3: 3 act4: 2 <lb/>r_act0: 2 r_act1: 1 r_act2: -1 r_act3: -1 <lb/>t-5: 782, t-4: 781 <lb/>t-3: 782, t-2: 781 <lb/>t-1: 782, <lb/>Fig. 10 Python-based stimulation platform. <lb/></body>

			<page>74 <lb/></page>

			<note place="headnote">Li F, Cheng J C, Mao Z Q, et al. <lb/></note>

			<note place="footnote">J Intell Connect Veh 2024, 7(1): 64-77 <lb/></note>

			<body>6.3 Comparison <lb/>To validate the effectiveness of the experiment, a control group <lb/>was established. The initialization of the control group is identical <lb/>to that of the experimental group previously mentioned. However, <lb/>in the control group, the H-AGV navigates solely based on the <lb/>shortest path to the destination calculated using the Networkx <lb/>library. The total number of overlaps in this scenario is recorded. <lb/>Since the H-AGV in the control group, which follows the shortest <lb/>path, cannot exceed 200 steps in its operation, a comparison will <lb/>be made using the results of 400 randomly conducted runs from <lb/>the control group against those results of the experimental group <lb/>mentioned above. The results of control group are shown in <lb/>Fig. 12, while the results of both experimental group and control <lb/>group are shown in Fig. 13. <lb/>It is important to note a key consideration: Since the starting <lb/>points and destinations of the H-AGV are randomly generated in <lb/>each experiment, they may be relatively close or far apart. <lb/>Therefore, for a more accurate assessment of the experiment&apos;s <lb/>effectiveness, we utilize the ratio of the total number of overlaps to <lb/>the total number of steps taken. It is found that the experimental <lb/>group had an average step count of 41.81 and an average total <lb/>number of overlaps of 2.45. In contrast, the control group has an <lb/>average step count of 37.78 and an average total number of <lb/>overlaps of 3.39. The average ratio for the experimental group is <lb/>5.860%, while for the control group, it is 8.973%. The average ratio <lb/>for the control group is 3.11%, which is higher than that of the <lb/>experimental group. This difference suggests that, through <lb/>training, the H-AGV is able to effectively avoid N-AGVs. <lb/>7 Discussion <lb/>This experiment proposes an algorithm that combines machine <lb/>learning and reinforcement learning to accomplish the task of <lb/>route planning for AGVs carrying hazardous materials within a <lb/>container yard. The reinforcement learning method introduced <lb/>has been successful in enabling AGVs carrying hazardous <lb/>materials to avoid those transporting regular materials. While this <lb/>approach can provide guidance for the trajectories of AGVs <lb/>carrying hazardous materials, there are still areas needing <lb/>improvement, particularly in model assumptions and network <lb/>design. These aspects represent potential avenues for further <lb/>refinement to enhance the algorithm&apos;s effectiveness and <lb/>applicability in real-world scenarios. <lb/>Regarding model assumptions, the abstraction of the real-world <lb/>map used in our study is relatively simplistic. Additionally, the <lb/>design decision that AGVs carrying hazardous materials are <lb/>unable to halt possesses certain drawbacks. In terms of algorithm <lb/>application, the LSTM neural network exhibits suboptimal <lb/>prediction performance, and the reinforcement learning approach <lb/>involves cumbersome and challenging parameter tuning. <lb/>Therefore, future research should focus on in-depth studies into <lb/>more accurate modeling of real-world container yards, and on <lb/>how to effectively design reward systems and network structures. <lb/>This would involve refining the map abstraction to better reflect <lb/>real-world complexities, exploring alternative neural network <lb/>models or architectures that could yield better predictive accuracy, <lb/>and devising more efficient and robust methods for parameter <lb/>tuning in reinforcement learning. These advancements could <lb/>significantly enhance the practicality and effectiveness of the <lb/>algorithm in real-world container yard management scenarios. <lb/>8 Conclusions <lb/>The method developed in this research, which employs LSTM to <lb/>construct the environment of background AGVs and DQN to <lb/>plan the route of AGVs carrying hazardous materials, has <lb/>successfully achieved avoidance of regular cargo AGVs in the <lb/>container yard area to a certain extent. Compared to the <lb/>straightforward approach of using the shortest path, the method <lb/>proposed in this study has improved avoidance efficiency by <lb/>3.11%. The method provides valuable insights for the operation <lb/>efficiency and safety of automated container terminals. Moreover, <lb/>it offers a novel perspective for path planning involving mutual <lb/>avoidance among autonomous AGVs within a certain area. This <lb/>approach could be particularly beneficial in settings where safety <lb/>and efficiency are paramount, requiring dynamic real-time <lb/>decision-making for AGV navigation. It should also be noted that <lb/>since DQN no longer relies on the traditional Q-value table, it has <lb/>a very strong processing capability for more complex <lb/>reinforcement learning tasks in action space and state space, and <lb/>thus has a wide range of applications in related motion planning. <lb/>30 <lb/>25 <lb/>20 <lb/>15 <lb/>10 <lb/>5 <lb/>0 <lb/>Total overlap num. <lb/>Step vs. total overlap num. for experimental group <lb/>0 <lb/>25 50 75 100 <lb/>Step <lb/>125 150 175 200 <lb/>Fig. 11 Step vs. total number of overlaps for experimental group. <lb/>20 <lb/>15 <lb/>10 <lb/>5 <lb/>0 <lb/>Total overlap num. <lb/>Step vs. total overlap num. for control group <lb/>0 <lb/>10 20 30 40 <lb/>Step <lb/>50 60 70 80 <lb/>Fig. 12 Step vs. total number of overlaps for control group. <lb/>30 <lb/>25 <lb/>20 <lb/>15 <lb/>10 <lb/>5 <lb/>0 <lb/>Total overlap num. <lb/>Total overlap num. for exp. group and control group <lb/>0 <lb/>25 50 75 100 <lb/>Step <lb/>125 150 175 200 <lb/>Exp. group <lb/>Control group <lb/>Fig. 13 Total number of overlaps for experimental group and control group. <lb/></body>

			<note place="headnote">Enhancing safety and efficiency in automated container terminals: Route planning for hazardous material AGV using LSTM neural ... <lb/></note>

			<page>75 <lb/></page>

			<note place="footnote">https://doi.org/10.26599/JICV.2023.9210041 <lb/></note>

			<div type="availability">Replication and data sharing <lb/>The data and codes that support the findings of this study are <lb/>available at https://cloud.tsinghua.edu.cn/d/fcd08e5336704c7fafe8. <lb/></div>

			<div type="acknowledgement">Acknowledgements <lb/>We sincerely thank Meishan Port in Ningbo, Zhejiang for <lb/>providing the data essential for our research. Our gratitude also <lb/>extends to the Emerging Transportation Solutions (ETS) Lab at <lb/>Tsinghua University for their invaluable technical support and <lb/>assistance. <lb/></div>

            <div type="annex">Declaration of competing interest <lb/>The authors have no competing interests to declare that are <lb/>relevant to the content of this article. <lb/></div>

            <listBibl>References <lb/>Cai, P., He, J., Li, Y., 2023. Hybrid cooperative intersection management <lb/>for connected automated vehicles and pedestrians. J Intell Connect <lb/>Veh, 6, 91-101. <lb/>Chen, P., Fu, Z., Lim, A., Rodrigues, B., 2004. Port yard storage optimiza-<lb/>tion. IEEE Trans Automat Sci Eng, 1, 26-37. <lb/>Chen, R., Meng, Q., Jia, P., 2022. Container port drayage operations and <lb/>management: Past and future. Transp Res Part E Logist Transp Rev, <lb/>159, 102633. <lb/>Dai, C., Zong, C., Zhang, D., Li, G., Chuyo, K., Zheng, H., et al., 2023. <lb/>Human-like lane-changing trajectory planning algorithm for human-<lb/>machine conflict mitigation. J Intell Connect Veh, 6, 46-63. <lb/>Ding, H., Li, W., Xu, N., Zhang, J., 2022. An enhanced eco-driving stra-<lb/>tegy based on reinforcement learning for connected electric vehicles: <lb/>Cooperative velocity and lane-changing control. J Intell Connect Veh, <lb/>5, 316-332. <lb/>Fang, L., Guan, Z. W., Wang, T., Gong, J. F., Du, F., 2022. Collision <lb/>avoidance model and its validation for intelligent vehicles based on <lb/>deep learning LSTM. J Automot Saf Energy, 13, 104-111. <lb/>Filom, S., Amiri, A. M., Razavi, S., 2022. Applications of machine learn-<lb/>ing methods in port operations -A systematic literature review. <lb/>Transp Res Part E Logist Transp Rev, 161, 102722. <lb/>Gunawardhana, J. A., Perera, H. N., Thibbotuwawa, A., 2021. Rule-based <lb/>dynamic container stacking to optimize yard operations at port ter-<lb/>minals. Marit Transp Res, 2, 100034. <lb/>Han, L., Zhang, H., Fang, R. Y., Liu, G. P., Zhu, C. S., Chi, R. F., 2023. <lb/>Global path planning strategy based on an improved deep reinforce-<lb/>ment learning. J Automot Saf Energy, 14, 202-211. <lb/>Li, H., Peng, J., Wang, X., Wan, J., 2021. Integrated resource assignment <lb/>and scheduling optimization with limited critical equipment con-<lb/>straints at an automated container terminal. IEEE Trans Intell Trans-<lb/>port Syst, 22, 7607-7618. <lb/>Li, N., Sheng, H., Wang, P., Jia, Y., Yang, Z., Jin, Z., 2023. Modeling cate-<lb/>gorized truck arrivals at ports: Big data for traffic prediction. IEEE <lb/>Trans Intell Transport Syst, 24, 2772-2788. <lb/>Li, S., Wei, C., Wang, Y., 2022. Combining decision making and trajec-<lb/>tory planning for lane changing using deep reinforcement learning. <lb/>IEEE Trans Intell Transport Syst, 23, 16110-16136. <lb/>Liu, Y., Wu, F., Liu, Z., Wang, K., Wang, F., Qu, X., 2023. Can language <lb/>models be used for real-world urban-delivery route optimization? <lb/>Innovation, 4, 100520. <lb/>Pei, M., Zhu, H., Ling, J., Hu, Y., Yao, H., Zhong, L., 2023. Empowering <lb/>highway network: Optimal deployment and strategy for dynamic <lb/>wireless charging lanes. Commun Transport Res, 3, 100106. <lb/>Qu, X., Lin, H., Liu, Y., 2023. Envisioning the future of transportation: <lb/>Inspiration of ChatGPT and large models. Commun Transport Res, <lb/>3, 100103. <lb/>Qu, X., Wang, S., Niemeier, D., 2022. On the urban-rural bus transit sys-<lb/>tem with passenger-freight mixed flow. Commun Transport Res, 2, <lb/>100054. <lb/>Skaf, A., Lamrous, S., Hammoudan, Z., Manier, M. A., 2021. Integrated <lb/>quay crane and yard truck scheduling problem at port of Tripoli-<lb/>Lebanon. Comput Ind Eng, 159, 107448. <lb/>Sun, Y., Chu, Y., Xu, T., Li, J., Ji, X., 2022. Inverse reinforcement learning <lb/>based: Segmented lane-change trajectory planning with considera-<lb/>tion of interactive driving intention. IEEE Trans Veh Technol, 71, <lb/>11395-11407. <lb/>Tan, C., Qin, T., He, J., Wang, Y., Yu, H., 2024. Yard space allocation of <lb/>container port based on dual cycle strategy. Ocean Coast Manag, 247, <lb/>106915. <lb/>Wang, K., Qian, Y., An, T., Zhang, Z., Zhang, J., 2022. LSTM-based pre-<lb/>diction method of surrounding vehicle trajectory. In: 2022 Interna-<lb/>tional Conference on Artificial Intelligence in Everything (AIE), <lb/>100-105. <lb/>Wang, K., Zhen, L., Wang, S., Laporte, G., 2018. Column generation for <lb/>the integrated berth allocation, quay crane assignment, and yard <lb/>assignment problem. Transp Sci, 52, 812-834. <lb/>Xiang, X., Liu, C., Lee, L. H., Chew, E. P., 2022. Performance estimation <lb/>and design optimization of a congested automated container termi-<lb/>nal. IEEE Trans Automat Sci Eng, 19, 2437-2449. <lb/>Yang, Z., Liu, D., Ma, L., 2022. Vehicle trajectory prediction based on <lb/>LSTM network. In: 2022 International Conference on Artificial Intel-<lb/>ligence and Computer Information Technology (AICIT), 1-4. <lb/>Zhang, X. F., Wu, L., 2023. Behavior decision-making model for <lb/>autonomous vehicles based on an ensemble deep reinforcement <lb/>learning. J Automot Saf Energy, 14, 472-479. <lb/>Zhao, G., Liu, J., Tang, L., Zhao, R., Dong, Y., 2020. Model and heuristic <lb/>solutions for the multiple double-load crane scheduling problem in <lb/>slab yards. IEEE Trans Automat Sci Eng, 17, 1307-1319. <lb/>Zhen, L., 2016. Modeling of yard congestion and optimization of yard <lb/>template in container ports. Transp Res Part B Methodol, 90, 83-<lb/>104. <lb/>Zhen, L., Lee, L. H., Chew, E. P., Chang, D. F., Xu, Z. X., 2012. A compar-<lb/>ative study on two types of automated container terminal systems. <lb/>IEEE Trans Automat Sci Eng, 9, 56-69. <lb/>Zhu, J., Easa, S., Gao, K., 2022. Merging control strategies of connected <lb/>and autonomous AGVs at freeway on-ramps: A comprehensive <lb/>review. J Intell Connect Veh, 5, 99-111. <lb/>Zhu, W., Qin, H., Lim, A., Zhang, H., 2012. Iterative deepening A* algo-<lb/>rithms for the container relocation problem. IEEE Trans Automat Sci <lb/>Eng, 9, 710-722. <lb/>76 <lb/>Li F, Cheng J C, Mao Z Q, et al. <lb/>J Intell Connect Veh 2024, 7(1): 64-77 <lb/></listBibl>

			<div type="annex">Fei Li is pursing the Ph.D. degree of engineer-<lb/>ing at Tsinghua University. He is a senior Engi-<lb/>neer. His research interests include Cloud <lb/>Computing, Big Data, AI, Digital Twin, and <lb/>technical application in the area of transporta-<lb/>tion and logistics, equipment manufacturing. <lb/>He has hosted and participated in Ningbo <lb/>Meishan Port project as chief solution architect. <lb/>Junchi Cheng now pursing the M.S. degree in <lb/>Electronic Engineering in Dalian Maritime <lb/>University, China. He is interested in machine <lb/>learning and is studying at the Emerging Trans-<lb/>portation Solutions Lab in the School of Vehi-<lb/>cle and Mobility in Tsinghua University. <lb/>Zhiqi Mao received the B.S. degree in Elec-<lb/>tronic Information Science and Technology <lb/>from the Department of Electronic Engineering, <lb/>Tsinghua University, Beijing, China, in 2020 <lb/>and received the M.S. degree from the Depart-<lb/>ment of Automation, Tsinghua University, Bei-<lb/>jing, China in 2023. His current research inter-<lb/>ests include reinforcement learning, auto-<lb/>nomous driving, and operation research <lb/>Yuhao Wang received the B.S. degree in Tra-<lb/>ffic Engineering from Southeast University, <lb/>China, in 2018, and the M.S. degree in Trans-<lb/>port from Imperial College London in 2019. He <lb/>obtained the Ph.D. degree from The Hong <lb/>Kong Polytechnic University in 2023, where he <lb/>conducted research on intelligent transport sys-<lb/>tems and smart pavements. He is currently a <lb/>postdoctoral fellow at the Emerging Trans-<lb/>portation Solutions Lab in the School of Vehi-<lb/>cle and Mobility in Tsinghua University. <lb/>Pingfa Feng is now a Professor in the Depart-<lb/>ment of Mechanical Engineering of Tsinghua <lb/>University. His research interests include inte-<lb/>lligent manufacturing, ultrasonic machining, <lb/>high speed cutting, performance analysis, and <lb/>optimization of machine tools. <lb/></div>

			<note place="headnote">Enhancing safety and efficiency in automated container terminals: Route planning for hazardous material AGV using LSTM neural ... <lb/></note>

			<page>77 <lb/></page>

			<note place="footnote">https://doi.org/10.26599/JICV.2023.9210041 </note>


	</text>
</tei>
