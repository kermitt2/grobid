<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title level="a">Finding Cross-Rule Optimization Bugs in Datalog Engines</title>
        <author>
          <persName>
            <forename>Chi</forename>
            <surname>Zhang</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Linzhang</forename>
            <surname>Wang</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Manuel</forename>
            <surname>Rigger</surname>
          </persName>
        </author>
      </titleStmt>
      <editionStmt>
        <edition>
          <date when="2025-10-30T10:48:25.827997Z">30.10.2025 10:48:25</date>
          <title>grobid.training.segmentation [default]</title>
          <idno type="fileref">10.1145$1$3649815</idno>
        </edition>
      </editionStmt>
      <publicationStmt>
        <publisher>Association for Computing Machinery (ACM)</publisher>
        <availability>
          <licence target="https://creativecommons.org/licenses/by/4.0/"/>
        </availability>
        <date type="publication">2024</date>
        <idno type="DOI">10.1145/3649815</idno>
      </publicationStmt>
      <sourceDesc>
        <bibl>Chi Zhang, Linzhang Wang, Manuel Rigger. (2024). Finding Cross-Rule Optimization Bugs in Datalog Engines. Proceedings of the ACM on Programming Languages, 8(OOPSLA1), 110-136. DOI: 10.1145/3649815</bibl>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <appInfo>
        <application version="1.0" ident="pdf-tei-editor" type="editor">
          <ref target="https://github.com/mpilhlt/pdf-tei-editor"/>
        </application>
        <application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-10-30T10:48:25.827997Z" type="extractor">
          <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
          <label type="revision">eb7768b</label>
          <label type="flavor">default</label>
          <label type="variant-id">grobid.training.segmentation</label>
          <ref target="https://github.com/kermitt2/grobid"/>
        </application>
      </appInfo>
    </encodingDesc>
    <revisionDesc>
      <change when="2025-10-30T10:48:25.827997Z" status="draft">
        <desc>Generated with createTraining API</desc>
      </change>
    </revisionDesc>
  </teiHeader>
  <text xmlns="http://www.tei-c.org/ns/1.0" xml:lang="en">
    <front>Compiling Recurrences over Dense and Sparse Arrays <lb/>SHIV SUNDRAM, Stanford University, USA <lb/>MUHAMMAD USMAN TARIQ, Stanford University, USA <lb/>FREDRIK KJOLSTAD, Stanford University, USA <lb/>We present a framework for compiling recurrence equations into native code. In our framework, users specify <lb/>a system of recurrences, the types of data structures that store inputs and outputs, and scheduling commands <lb/>for optimization. Our compiler then lowers these specifications into native code that respects the dependencies <lb/>in the recurrence equations. Our compiler can generate code over both sparse and dense data structures, <lb/>and determines if the recurrence system is solvable with the provided scheduling primitives. We evaluate <lb/>the performance and correctness of the generated code on several recurrences, from domains as diverse <lb/>as dense and sparse matrix solvers, dynamic programming, graph problems, and sparse tensor algebra. We <lb/>demonstrate that the generated code has competitive performance to hand-optimized implementations in <lb/>libraries. However, these handwritten libraries target specific recurrences, specific data structures, and specific <lb/>optimizations. Our system, on the other hand, automatically generates implementations from recurrences, <lb/>data formats, and schedules, giving our system more generality than library approaches. <lb/>CCS Concepts: • Software and its engineering → Domain specific languages; Source code generation. <lb/>Additional Key Words and Phrases: recurrences, sparse tensor algebra, linear algebra, dynamic programming <lb/>ACM Reference Format: <lb/>Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad. 2024. Compiling Recurrences over Dense and <lb/>Sparse Arrays. Proc. ACM Program. Lang. 8, OOPSLA1, Article 103 (April 2024), 26 pages. https://doi.org/10. <lb/>1145/3649820 <lb/></front>

    <body>1 INTRODUCTION <lb/>Recurrences compute a value of a sequence based on previous values in the same sequence. Recur-<lb/>rences are used to express dynamic programs, graph analysis, linear solvers, and matrix factorization. <lb/>Algorithms as diverse as sequence alignment, Dijkstra&apos;s algorithm, triangular matrix solves, and <lb/>even Cholesky factorization can all be expressed as recurrence equations. A simple example of a <lb/>recurrence is a running sum of an array where = -1 + . The computed value at each <lb/>depends on previously computed values. Due to this dependency, there is a limited number of <lb/>ways that values can be computed. For example, one cannot compute the values backwards with <lb/>stepping from down to 0. <lb/>Several optimized library implementations of these operations exist. However, recurrence im-<lb/>plementations form a large design space across recurrences (e.g., the Viterbi and Floyd-Warshall <lb/>algorithms), the choice of data structures (e.g., dense and sparse arrays/tensors), and different <lb/>optimizations. Where a library implementation does not exist, programmers must write and hand-<lb/>optimize their own kernels. Programmers are thus faced with the challenge of implementing and <lb/>optimizing a complex algorithm across diverse data structures while respecting dependencies. <lb/></body>

    <front>Authors&apos; addresses: Shiv Sundram, shiv1@stanford.edu, Stanford University, P.O. Box 1212, Stanford, California, USA; <lb/>Muhammad Usman Tariq, Stanford University, Stanford, USA; Fredrik Kjolstad, Stanford University, Stanford, USA, kjostad@ <lb/>stanford.edu. <lb/>© 2024 Copyright held by the owner/author(s). <lb/>ACM 2475-1421/2024/4-ART103 <lb/>https://doi.org/10.1145/3649820 <lb/>Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/>This work is licensed under a Creative Commons Attribution 4.0 International License. <lb/></front>

    <page>103:2 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>Table 1. Features of different recurrences. A ✔denotes the recurrence contains that feature. <lb/>Algorithm <lb/>Type <lb/>Recurrence <lb/>Dependencies <lb/>Multiple <lb/>Equations <lb/>Sparsity Masks <lb/>Timestep <lb/>Variables <lb/>Prefix Sum <lb/>dynamic <lb/>✔ <lb/>Cholesky <lb/>direct solver <lb/>✔ <lb/>✔ <lb/>✔ <lb/>✔ <lb/>Triangular Solve <lb/>direct solver <lb/>✔ <lb/>✔ <lb/>✔ <lb/>Fused Tri Solve <lb/>direct solver <lb/>✔ <lb/>✔ <lb/>✔ <lb/>✔ <lb/>QR <lb/>direct solver <lb/>✔ <lb/>✔ <lb/>✔ <lb/>✔ <lb/>LU <lb/>direct solver <lb/>✔ <lb/>✔ <lb/>✔ <lb/>✔ <lb/>Floyd Warshall <lb/>dynamic/graph <lb/>✔ <lb/>✔ <lb/>Viterbi <lb/>dynamic/graph <lb/>✔ <lb/>✔ <lb/>✔ <lb/>Needleman-Wunsch <lb/>dynamic <lb/>✔ <lb/>Gauss-Seidel <lb/>iterative solver <lb/>✔ <lb/>✔ <lb/>SpMV <lb/>tensor algebra <lb/>✔ <lb/>✔ <lb/>SDDMM <lb/>tensor algebra <lb/>✔ <lb/>✔ <lb/>Table 1 highlights the complexity of the space by listing several recurrence equations across several <lb/>domains. Recurrence dependencies, multiple interleaved equations, and sparsity combine to make <lb/>it hard to implement and optimize recurrences. <lb/>Existing approaches to general recurrence programming systems do not capture this full design <lb/>space. Prior work on systems for solving general recurrence equations, like the Dyna language <lb/>[Eisner et al. 2004], use dynamic runtime analysis to track output dependencies and determine <lb/>the order in that to calculate outputs. These approaches are thus not as efficient as handwritten <lb/>code. Furthermore, the Dyna approach does not generalize across data structures and also does not <lb/>support optimization decisions like controlling the loop order. And other approaches to compiling <lb/>numerical code to dense and sparse tensors, such as TACO [Kjolstad et al. 2017], SparseTIR [Ye <lb/>et al. 2023], and MLIR SparseTensor [Bik et al. 2022], do not support recurrences. <lb/>We describe how to generate imperative code from recurrence equations, along with separate <lb/>descriptions of data structures and loop nest ordering. Based on these specifications, our compiler <lb/>generates bespoke code for CPUs. The major contributions of this paper are: <lb/>• A simple recurrence language that extends tensor index notation (Section 4). <lb/>• A simple dependency checking model (Section 5 and Section 6). <lb/>• An algorithm that lowers recurrences to loop nests that iterate over abstract arrays (Section 6). <lb/>• Optimizations for recurrence programs, including loop fusion and auto-parallelization (Sec-<lb/>tion 8). <lb/>To evaluate these ideas, we developed a compiler for recurrences called RECUMA (RECUrrence <lb/>computation MAchine). RECUMA lowers recurrences to loop nests over abstract arrays, checking <lb/>dependencies and optimizing loop order during construction. It then uses ideas from prior work on <lb/>sparse tensor algebra compilation [Kjolstad et al. 2019, 2017] to lower loop nests over abstract arrays <lb/>to imperative C code over dense and sparse data structures, although we have extended these ideas <lb/>to support triangular loops. We demonstrate RECUMA&apos;s ability to generate code with different <lb/>loop nest orders as well as to generate code over different types of dense and sparse data structures, <lb/>and show that these choices affect performance. Moreover, RECUMA includes a set of program <lb/>optimization strategies that can be shared across different recurrence problems. We show that the <lb/>performance of the generated code is competitive with handwritten implementations from existing <lb/>libraries, including CXSparse [Davis 2006a,b] for sparse matrix decompositions, Parasail [Daily <lb/>2016] for sequence-alignment, Boost [Boost 2002] for Floyd-Warshall, and PolyBench [Pouchet 2016] <lb/>for Gauss-Seidel. Since tensor algebra equations are recurrence equations without dependencies, <lb/>we also show that tensor algebra kernels generated by RECUMA have performance competitive <lb/>with those generated by TACO [Kjolstad et al. 2017]. <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:3 <lb/></page>

    <body>for j: <lb/>for k&lt;j: <lb/>for i&gt;j: <lb/>for i&gt;j: <lb/>for i: <lb/>for j&lt;i: <lb/>for k&lt;j: <lb/>ijk Cholesky <lb/>jki Cholesky <lb/>Fig. 1. Pseudocode for two Cholesky implementations with different loop ordering ( and <lb/>). The arrows <lb/>highlight three of the differences between the variants. Loops with no bounds implicitly iterate from [0,N). <lb/>2 CHOLESKY DECOMPOSITION EXAMPLE <lb/>To illustrate the requirements of a recurrence compiler and the complexity it must capture, we use <lb/>the Cholesky decomposition as a running example. The Cholesky decomposition is used to solve a <lb/>linear system involving a symmetric positive definite matrix. For any symmetric positive definite <lb/>matrix A, the decomposition computes a lower triangular matrix L such that <lb/>= . The matrix <lb/>L can then be used to solve linear systems of the form <lb/>= (to compute ) through a sequence <lb/>of two triangular solves. The Cholesky decomposition is expressible as a system of two mutually <lb/>dependent recurrence equations that together define the matrix , where previously computed <lb/>values of are used to compute subsequent entries: <lb/>= <lb/>-<lb/>: &lt; &lt; <lb/>= <lb/>-<lb/>: &lt; <lb/>Each recurrence equation expresses the values of the components of as a function of both <lb/>and itself. The recurrences have constraints on the index variables that describe what components <lb/>of the result that the equation computes and what components of the operands that it uses to <lb/>compute them. The constraints specify that the first recurrence calculates only the lower triangular <lb/>elements of the output ( where &lt; ), while the second recurrence calculates the diagonal <lb/>entries ( ). The upper triangular elements are not calculated by these equations and thus default <lb/>to zeros. Computed values in the equations depend on other values computed in the same equation <lb/>as well as values computed in the other equation. That is, diagonal components rely on the values <lb/>of non-diagonal components, and vice versa. The equations are therefore inter-dependent and their <lb/>computation must be interleaved. Any valid Cholesky decomposition implementation must respect <lb/>both intra-recurrence and inter-recurrence dependencies. <lb/>Implementing a Cholesky decomposition with different loop orderings leads to significantly <lb/>different code structure. This is in contrast to basic linear algebra expressions (e.g., matrix multipli-<lb/>cation) in which rearranging the loop headers is sufficient to implement different loop orderings. <lb/>To illustrate, the left of Figure 1 shows pseudocode for the commonly-used up-looking Cholesky <lb/>decomposition. We denote this variant as an <lb/>Cholesky decomposition following the nesting <lb/>order of the loops. The <lb/>loop ordering, also known as the Cholesky-Banachiewicz algorithm, is <lb/>useful when the matrices and are stored row-major, to ensure efficient data structure accesses. <lb/>If and are column-major, however, it is better to use a loop nest with a <lb/>loop ordering, shown <lb/>on the right of Figure 1. Notice the drastic differences between the two forms: <lb/>Loop Headers The and loop bounds are different due to the recurrence constraints. In the <lb/>first loop nest iterates from 0 to , while in the second loop nest it iterates from + 1 to . <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:4 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>f o r ( i n t i =0 ; i &lt;N ; i ++ ) { <lb/>/ / n o t shown : l o a d s p a r s e row i o f A i n t o a d e n s e a r r a y t m p A i j and i n i t t h e d e n s e <lb/>t e m p o r a r y a r r a y t m p L i j t o b u i l d t h e o u t p u t row i o f L <lb/>d o u b l e L i i = 0 ; <lb/>f o r ( i n t j =0 ; j &lt; i ; j ++ ) { <lb/>d o u b l e t L i j = 0 ; <lb/>f o r ( i n t p _ k _ L j k = L 1 _ p o s [ j ] ; p _ k _ L j k &lt; L 1 _ p o s [ j + 1 ] ; p _ k _ L j k ++ ) { <lb/>d o u b l e L j k = L _ v a l s [ p _ k _ L j k ] ; <lb/>d o u b l e L i k = t m p L i j _ v a l s [ L j k _ c r d ] ; <lb/>t L i j += L i k * L j k ; <lb/>} <lb/>d o u b l e A i j = t m p A i j _ v a l s [ j ] ; <lb/>d o u b l e L j j = L _ v a l s [ L 1 _ p o s [ j ] -1 ] ; <lb/>d o u b l e L i j = ( A i j -t L i j ) / L j j ; <lb/>t m p L i j _ v a l s [ j ] = L i j ; <lb/>L i i += L i j * L i j ; <lb/>} <lb/>d o u b l e A i i = t m p A i j _ v a l s [ i ] ; <lb/>t m p L i j _ v a l s [ i ] = L i i = s q r t ( A i i -L i i ) ; <lb/>/ / n o t shown : c o m p r e s s t m p L i j i n t o new row o f o u t p u t L , s e t L _ 1 c r d and L 1 _ p o s <lb/>} <lb/>Fig. 2. Sparse <lb/>Cholesky Factorization <lb/>Diagonal Elements The calculations of the diagonal elements <lb/>in the first loop nest are <lb/>reformulated as calculations of <lb/>in the second nest. <lb/>Number of Loops The two implementations have different numbers of for loops. Unlike the <lb/>version, the <lb/>version has two loops over , the first of is which is the inner most loop <lb/>and performs a summation. The second loop is a middle loop and scales this summation by <lb/>the inverse of . These two loops cannot be fused. <lb/>Therefore, unlike a matrix multiply kernel where an <lb/>loop nest can be converted into a <lb/>loop nest by simply permuting the order of the loop headers, switching the loop iteration order in <lb/>a recurrence system may require more changes. Moreover, there are six possible Cholesky loop <lb/>orderings corresponding to the different permutations of , , and , all of which are useful in <lb/>different scenarios and which have notable differences from each other. Despite these differences, <lb/>the six variants share a common recurrence equation formulation. Similar recurrences systems <lb/>can be defined for the LU decomposition, QR decomposition, triangular solve, as well as dynamic <lb/>programming and graph problems outside linear algebra (e.g., Table 1). <lb/>Finally, when a large portion of the entries of are zeros, we need to generate implementations <lb/>of the six Cholesky variants that use sparse data structures. Figure 2 shows an implementation of <lb/>the simpler <lb/>Cholesky variant where the matrices are stored in compressed sparse row (CSR) <lb/>data structures. This code is more complicated than the dense version but the above principles still <lb/>apply. Moreover, the pseudocode in Figure 1 still applies as long as we view the matrices as logical <lb/>and abstract arrays for which we have not yet introduced physical data structures. In this paper we <lb/>describe how to compile recurrences to code that operate on both dense and sparse data structures. <lb/>3 OVERVIEW <lb/>Our recurrence compiler takes in a declarative system of recurrence equations, dense/sparse data <lb/>structure descriptions, and a schedule describing optimizations such as loop ordering. The compiler <lb/>generates an imperative C implementation that computes the recurrence sequence. The compiler <lb/>ensures that dependencies are observed, that the code adheres to the user-specified loop ordering, <lb/>and that the code operates on the user-specified data structures. <lb/>Figure 3 shows the process of compiling declarative recurrence expressions to an imperative C <lb/>implementation through two lowering passes. These passes move from the recurrences equations <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:5 <lb/></page>

    <body>Recurrence Equations <lb/>Recurrence Index Notation <lb/>: <lb/>: <lb/>Schedule <lb/>ordering=ijk <lb/>omp=k <lb/>... <lb/>C code <lb/>Storage <lb/>L:[Dense(0), Compressed(1)] <lb/>A: [Dense(0), Compressed(1)] <lb/>Lower to <lb/>Recurrence Index <lb/>Notation <lb/>Lower to C <lb/>Generate Fragments <lb/>Build &amp; Sort DAG <lb/>Figure 2 <lb/>for i: <lb/>for j&lt;i: <lb/>for_all k&lt;j: <lb/>Fusion <lb/>Parallelization <lb/>Fig. 3. An overview of the recurrence compilation workflow. Green boxes are user-provided inputs to the <lb/>compiler. Purple boxes are intermediate and final implementations produced by the lowering passes. <lb/>to an imperative IR we call recurrence index notation (RIN), and finally to imperative C code. The <lb/>figure illustrates the process for a dense Cholesky decomposition with an <lb/>loop ordering. <lb/>The output of the first lowering stage is in recurrence index notation, an imperative IR that <lb/>describes a program&apos;s loop structure and placement of compute statements. All data accesses in RIN <lb/>are to logical/abstract arrays. This allows the compiler to reason about the desired program&apos;s loops, <lb/>computations, and dependencies without regard for how the tensors are stored. Therefore, loop <lb/>reordering, dependency analysis, placement of compute statements within loop nests, and low-level <lb/>program optimizations (e.g., loop fusion and auto-parallelization) can be carried out without the <lb/>need to manage low-level constructs related to sparse code, such as while loops, if statements, and <lb/>indirect memory accesses. <lb/>Recurrences are lowered to RIN by a new lowering algorithm. In this process, each recurrence <lb/>equation (or a sub-expression of the equation) is placed as a statement into a location in an <lb/>imperative loop nest. These expressions are greedily placed into the first location of a loop nest <lb/>where the expression&apos;s dependencies are satisfied. The core component of this algorithm is its <lb/>ability to reason about, for each iteration and location in the loop nest, what outputs have already <lb/>been computed and what needs to be computed next. If it is not possible to generate a program that <lb/>respects both the desired loop order and the dependencies of the recurrences, then the compiler <lb/>reports an error. Once the RIN is fully formed, it is possible to apply user specified optimizations to <lb/>it. Loop fusion, however, happens automatically in the RIN generation stage. <lb/>The final pass lowers RIN into C code. We use standard techniques utilized by tensor algebra <lb/>compilers like TACO, COMET [Tian et al. 2021], and MLIR SparseTensor [Bik et al. 2022] for <lb/>generating sparse loops, augmented to support triangular loop bounds. The final lowering pass is <lb/>the only pass that requires information about the data structures in which the tensors are stored. <lb/>Deferring the data structure selection to after loop optimization is an important design point that <lb/>drastically simplifies the dependency handling, RIN generation, and loop optimization. The final <lb/>code can use dense arrays or sparse data structures, which can be used to represent graphs, sparse <lb/>matrices, and sparse arrays. <lb/>4 RECURRENCE LANGUAGE <lb/>Our language for recurrences, which we used in the Cholesky example, extends the common <lb/>mathematical notation used to describe recurrence equations. The language consists of a set of <lb/>equations that describe how different parts of a tensor are computed. Tensors are indexed by index <lb/>variables, and scalar expressions can be summed over reduction indices. The language thus extends <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:6 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>the tensor index notation [Ricci and Levi-Civita 1900] that is used by many tensor algebra compilers. <lb/>For example, in both our recurrence language and in tensor index notation, matrix multiplication <lb/>is expressed as <lb/>= <lb/>. However, the recurrence language has four features that extends <lb/>tensor index notation: <lb/>Recurrences The core feature of a recurrence equation is that components of the result ar-<lb/>ray can be computed based on previously computed components of the same array, thus <lb/>introducing dependencies. The same array (i.e., tensor) may thus appear on both the left and <lb/>right-hand sides of a recurrence equation. <lb/>Constraints Constraints bound the domain of one index variable in terms of another index <lb/>variable. Constraints can be equalities or inequalities of index variables (e.g., = or &lt; ). <lb/>Multiple equations A recurrence problem may consist of multiple recurrence equations that <lb/>compute disjoint parts of the result array. Furthermore, these recurrences can be mutually <lb/>dependent; result components computed in one recurrence equation may be used to calculate <lb/>components in another equation, and vice versa. <lb/>Timestep Variables Timestep variables can be used to describe iterative computation in time. <lb/>The first three features were notably used in the Cholesky equations in Section 2. The fourth <lb/>feature, timestep variables, is used in algorithms that have to iteratively compute a result, such as <lb/>the Floyd-Warshall algorithm. Each program in our language is thus a set of recurrence equations, <lb/>in which each equation has its own constraints over index variables. <lb/>4.1 Expressing Recurrence Equations <lb/>The grammar of the recurrence language is given in Figure 4. A recurrence consists of a recurrence <lb/>equation that connects indexed elements of a result tensor to expressions required to compute <lb/>them. Each tensor element is accessed by a <lb/>, which denotes an element indexed by a <lb/>sequence of <lb/>variables, which may optionally be offset by constants. A single tensorAccess <lb/>forms the left-hand side of a recurrence. A recurrence with an optional list of constraints is a <lb/>, a set of which makes a program. <lb/>The subscripts of a tensorAccess are the index variables used to access the corresponding tensor <lb/>element. The index variables in a tensorAccess subscript may each be offset by an integer constant. <lb/>This is illustrated by the Fibonacci equation = -1 + -2 , where the indexing expressions on the <lb/>right-hand side are -1 and -2. While our indexing expressions do not include the full spectrum <lb/>of affine indexing expressions, they are sufficient to express many important recurrences. <lb/>iVar ::= string <lb/>index :: = iVar &apos;+&apos; constantInt <lb/>| constantInt <lb/>tensor ::= string <lb/>tensorAccess ::= <lb/> * <lb/>+ <lb/>expr ::= tensorAccess <lb/>| <lb/>expr <lb/>| constant <lb/>| &apos;-&apos; expr <lb/>| expr &apos;+&apos; expr <lb/>| expr expr <lb/>| ... <lb/>recurrence ::= tensorAccess &apos;=&apos; expr <lb/>constraint ::= iVar &apos;&lt;&apos; index <lb/>| iVar &apos;&lt;=&apos; index <lb/>| iVar &apos;=&apos; index <lb/>constraints :: = constraint (&apos;,&apos; constraint )* <lb/>constrainedRecurrence ::= recurrence &apos;:&apos; constraints <lb/>program ::= constrainedRecurrence (&apos;,&apos; constrainedRecurrence)* <lb/>Fig. 4. Grammar for the recurrence language <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:7 <lb/></page>

    <body>A recurrence equation&apos;s right-hand side consists of scalar arithmetic operations and summations, <lb/>which operate on tensorAccesses and constants. Supported binary operations include addition, <lb/>subtraction, multiplication, min, max, but can be expanded with other linear or non-linear [Henry <lb/>et al. 2021] operations as needed. Similarly, unary operations include square roots and negations. <lb/>Each recurrence equation is assigned a set of constraints defined over index variables. The language <lb/>also includes an operator denoting a summation across an index variable for a given expression. <lb/>Such reductions can, however, be built on top of any commutative and associative binary operation. <lb/>Finally, some recurrences represent iterative algorithms that iterate over a timestep , and <lb/>this variable may be used as an indexing variable when accessing an element in the output data <lb/>structure. Accordingly, a tensorAccess includes an optional superscript that may contain a timestep <lb/>variable. The Floyd-Warshall all-pairs shortest paths algorithm, for example, is defined as <lb/>= <lb/>( -1 , -1 + -1 ), in which denotes the current timestep. <lb/>denotes the distance between <lb/>nodes and at the th timestep. Separating timestep variables from other index variables is <lb/>useful; the algorithm does not require the output data structure to store the distances from every <lb/>intermediate timestep , since the algorithm only requires knowing the last -1th timestep to <lb/>make progress. can thus be implemented as a two-dimensional (instead of three-dimensional) <lb/>tensor containing <lb/>. Timestep variables give the programming system enough information to <lb/>instantiate these optimizations, while simultaneously being intuitive to the programmer. <lb/>4.2 Denoting Schedule and Storage <lb/>A schedule tells a compiler how to optimize code [Ragan-Kelley et al. 2013] and can also be applied <lb/>to compilers for sparse operations [Senanayake et al. 2020]. The main component of a schedule is <lb/>an ordered list of index variables denoting loop ordering. As shown, a Cholesky decomposition can <lb/>be implemented as a triply nested loop. If the user wants to generate a program in which the outer <lb/>loop iterates over , the middle over , and the inner over , then the user will simply provide a <lb/>loop ordering to the compiler. <lb/>The final component is the description of the data structures of the result and operand tensors. <lb/>Like in prior work on sparse tensor algebra compilation [Chou et al. 2018; Kjolstad et al. 2017], <lb/>each tensor can have multiple dimensions and each dimension must have a level format (Dense or <lb/>Compressed). The Dense keyword states that in a particular dimension, each element is stored in <lb/>the data structure, including zero entries. The Compressed keyword denotes that for a particular <lb/>dimension, only nonzero elements are stored. These level formats can be composed to form common <lb/>sparse formats like compressed sparse rows (CSR) or compressed sparse columns (CSC). To generate <lb/>a Cholesky decomposition over a CSR matrix, the matrix&apos;s associated storage type is [Dense(0), <lb/>Compressed(1)]. The 0 before 1 indicates that rows are stored before columns. In a column-major <lb/>matrix, dimension 1 thus is stored before 0. <lb/>By separating specifications of equations, constraints, schedules, and storage info, the user can <lb/>then concisely express a variety of recurrences over dense tensors, sparse tensors, and graphs, <lb/>which are ultimately either dense or sparse matrices. For a Cholesky decomposition using CSR <lb/>matrices, the recurrences, storage, and loop ordering are expressed to RECUMA in Python as so: <lb/>r e c 1 = &quot; L ( i , j ) = ( A ( i , j ) -Sum { k } ( L ( i , k ) * L ( j , k ) ) ) / L ( j , j ) : [ k&lt; j , j &lt; i ] &quot; <lb/>r e c 2 = &quot; L ( i , j ) = s q r t ( A ( i , j ) -Sum { k } ( L ( i , k ) * L ( j , k ) ) ) : [ k&lt; j , j = i ] &quot; <lb/>s c h e d u l e [ &quot; o r d e r i n g &quot; ] = &quot; i j k &quot; <lb/>s t o r a g e [ &quot;A&quot; ] = S t o r a g e ( [ Dense ( 0 ) , Compressed ( 1 ) ] ) <lb/>s t o r a g e [ &quot; L &quot; ] = S t o r a g e ( [ Dense ( 0 ) , Compressed ( 1 ) ] ) <lb/>program = Program ( [ r e c 1 , r e c 2 ] , s c h e d u l e , s t o r a g e ) # g e n e r a t e C program <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:8 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>for i: <lb/>for j&lt;i: <lb/>for_all k&lt;j: <lb/>Fig. 5. A dependency graph orders the placement of recurrences in the abstract loops of the <lb/>Cholesky. <lb/>Green nodes are inputs, blue nodes are le -hand sides of fragments, white nodes are outputs that are also <lb/>dependencies, implicitly calculated by blue nodes in previous iterations. <lb/>5 RECURRENCE DEPENDENCIES <lb/>In recurrence equations, there are dependencies between computed values. For example, in the <lb/>prefix sum = + -1 , the th element depends on all previously computed elements. The values, <lb/>therefore, cannot be calculated in an arbitrary order. When a user provides a loop ordering, it imposes <lb/>a constraint on the order in which to calculate outputs. By 1) identifying these dependencies in the <lb/>recurrence equations, 2) representing the dependencies in a dependency graph, and 3) reasoning <lb/>about how loop ordering affects dependency management, we can construct the foundation of an <lb/>algorithm for checking dependencies while lowering recurrences to imperative code. <lb/>5.1 Dependency Graphs <lb/>In a recurrence, a dependency of an element is any previous element in the recurrence sequence <lb/>needed to calculate it. The elements calculated by a recurrence equation directly depend on all the <lb/>elements accessed in the operands on the equation&apos;s right-hand side. As long as no such dependen-<lb/>cies are inverted (meaning all values are calculated before their use) then a loop order correctly <lb/>computes the results. In the first equation of the Cholesky example, <lb/>= ( <lb/>-<lb/>)/ , <lb/>the result <lb/>depends on <lb/>, , <lb/>, and . We model these dependencies with a directed acyclic <lb/>graph in which each tensorAccess expression is a node and each dependency is an edge. Figure 5 <lb/>shows an example dependency graph for the Cholesky factorization. <lb/>Before identifying dependencies in a recurrence system, recurrences are broken into smaller, <lb/>fragment recurrences. A fragment is a recurrence in which a summation expression (including <lb/>the inner expression) is placed in a separate recurrence from the expression encapsulating the full <lb/>summation. In our Cholesky example, the two input recurrences are thus expanded to the four <lb/>recurrences shown in the center column of Figure 5. A fragment for a summation calculates a partial <lb/>value of the final output, and we name the partial value by taking the final output&apos;s tensorAccess <lb/>name and preceding it with a (for temporary). The subsequent code generation stage places the <lb/>fragment recurrences into the loop instead of the original recurrences. This allows subexpressions <lb/>from different input recurrences to be interleaved within a single fused loop nest. <lb/>For each fragment recurrence, we draw an edge in the dependency graph from each operand to <lb/>the result. Figure 5 shows how the dependency graph (left) is used to order the fragment recurrences <lb/>(center), which are then placed into the imperative loop nests of the RIN (right). <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:9 <lb/></page>

    <body>ijk Cholesky <lb/>outermost/ <lb/>slowest loop <lb/>innermost/fastest loop <lb/>j <lb/>i <lb/>i <lb/>k <lb/>k <lb/>j <lb/>k <lb/>k <lb/>i <lb/>i <lb/>j <lb/>j <lb/>jik Cholesky <lb/>ikj Cholesky <lb/>j <lb/>i <lb/>i <lb/>k <lb/>k <lb/>j <lb/>scattered writes <lb/>in inner loop (red) <lb/>reduce to scalar <lb/>in inner loop (red) <lb/>previously calculated <lb/>rows (green) <lb/>previously calculated <lb/>values on current row (blue) <lb/>Fig. 6. Comparing dataflow and dependencies of <lb/>, <lb/>, and <lb/>Cholesky decompositions. Solid arrows <lb/>illustrate the direction a tensorAccess moves along as the corresponding loop iteration variable increases. <lb/>Calculating <lb/>requires that <lb/>and <lb/>are calculated. Arrows with an empty head denote dependencies. <lb/>For a <lb/>program, when calculating <lb/>, all previous columns must already be calculated (green). For an <lb/>program, all previous rows (green) and all previous entries in the same row (blue) must already be calculated. <lb/>In <lb/>Cholesky, the inner loop (triple head arrows) notably iterates over both rows and columns while <lb/>sca ering writes (red) across a row. <lb/>is a partially computed value of , which then becomes read as <lb/>and <lb/>. <lb/>5.2 Dependencies and Loop Ordering <lb/>Any code generation algorithm for recurrences must manage dependencies so that the final code <lb/>does not violate them. The treatment of dependencies depends both on the recurrence equations <lb/>and on the user-specified loop ordering. For some recurrences, certain loop orderings cannot be <lb/>made to respect dependencies and are therefore illegal. And in general, different loop orders will <lb/>have different loop-carried dependencies. <lb/>Data structure decisions, however, do not affect the loop dependencies we describe here. It is <lb/>true that accessing a data structure in the wrong order carries a high overhead, but it does not <lb/>affect correctness. The reason is that both dense arrays and sparse data structure can be randomly <lb/>accessed instead of traversed. But where a dense array can be accessed in constant time, a sparse <lb/>data structure may require a search. For example, randomly accessing an row-major CSR sparse <lb/>data structure in column-major order requires an (log ) search if it is ordered and an ( ) search <lb/>if it is unordered. Our compiler techniques can generate such searches and can therefore generate <lb/>code for any RIN loop nest that passes the dependency checks. Alternatively, a compiler can report <lb/>an error before generating C code if a data structure is traversed in the wrong direction, which is a <lb/>simple check that is orthogonal to the dependencies we treat in this paper. <lb/>The effect of different loop orders on dependency handling is exemplified by comparing three <lb/>different loop orders of the running Cholesky example. Figure 6 visualizes the dependencies of <lb/>the (fragment) recurrence equation <lb/>= <lb/>in three different loop orders of Cholesky <lb/>decomposition. Here, <lb/>is a partially computed value of , which then becomes accessed as <lb/>and <lb/>in subsequent iterations. The left figure shows a <lb/>Cholesky in which the output is <lb/>calculated column-by-column. The dependency graph shows that calculating <lb/>requires having <lb/>already calculated the two dependencies <lb/>and <lb/>. In the <lb/>loop order, results are calculated one <lb/>column at a time, such that the ℎ iteration of the outer loop computes the ℎ column. And the <lb/>Cholesky constraints state that &lt; , so we know that column , which includes both dependencies, <lb/>is already calculated by the time we get to calculating column . In the visual language of the figure, <lb/>the fact that both dependencies are computed when we get to <lb/>is shown by them being situated <lb/>in the green area of previously computed columns. <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:10 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>The middle figure shows an <lb/>Cholesky in which the output is calculated one row at a time, <lb/>such that the ℎ iteration of the outer loop computes the ℎ row. The dependencies of <lb/>are still <lb/>and <lb/>, which are in rows and respectively. When calculating <lb/>in row , the constraint <lb/>&lt; only guarantees that <lb/>, in row , is already calculated. The other dependency, , is not <lb/>satisfied by this constraint as it lies in the row that we are currently trying to calculate. However, <lb/>we can satisfy the dependency <lb/>by assuming that we calculate the elements of row in increasing <lb/>order. Thus, because &lt; , we know that the th element in the current row has been calculated <lb/>by the time we are calculating the th element. <lb/>The right figure shows an <lb/>Cholesky in which L is computed row-by-row. Like in <lb/>and <lb/>Cholesky, results depend on both values from previously computed rows and previously computed <lb/>elements on the same row. However, because the loop, which indexes into the result, is nested <lb/>inside the loop, the <lb/>Cholesky scatters results down the part of the row after the current <lb/>value of . <lb/>The dependencies have implications for parallelization. For example, they imply that the outer <lb/>loop in the <lb/>program has a loop carried dependency. That is, when calculating an entry in <lb/>column , we must have already calculated all previous columns until column . Meanwhile, both <lb/>the outer loop and the middle loop in the <lb/>program have a loop carried dependency. That is, <lb/>when calculating an entry in row we must have calculated all previous rows up to . Additionally, <lb/>when calculating the th entry of <lb/>within a row, we must have already calculated all previous <lb/>entries in the row until the th one. A loop with a loop carried dependency cannot in general be <lb/>trivially parallelized. 1 In <lb/>Cholesky, there is a loop carried dependency in the outer and middle <lb/>loop, so only the inner loop can be parallelized. In <lb/>Cholesky, only the outer loop contains a <lb/>loop carried dependency, meaning both the middle and inner loop can be parallelized. <lb/>The lowering algorithm from recurrence equations to RIN, which we describe in Section 6, <lb/>manages recurrence dependencies with inductive assumptions that formalize the above intuition. <lb/>The lowering algorithm incrementally constructs a proof by induction during lowering by making <lb/>assumptions about what is computed at each lowering step. For example, for <lb/>Cholesky, the <lb/>lowering algorithm makes an assumption that a statement over is true for all &lt; , and then uses <lb/>this assumption to prove that the same statement is true for . If both this inductive hypothesis <lb/>and the base case are true, then the statement is always true. Similarly, if we can calculate the th <lb/>column of by assuming that all columns &lt; are already calculated, then we can always satisfy <lb/>the dependencies necessary to calculate every entry in . <lb/>6 RECURRENCE EQUATION LOWERING <lb/>Recurrence equation lowering lowers recurrences into the imperative recurrence index notation <lb/>(RIN) IR. This IR represents a program in which the recurrence results are iteratively calculated <lb/>in an abstract loop nest where physical data structures have not yet been specified. The lowering <lb/>algorithm generates the user-defined loop order, thus simplifying dependency checking and avoiding <lb/>the complexity of loop reordering in the face of fragmented loops such as those of the <lb/>Cholesky <lb/>in Figure 1. Recurrence lowering is based on three principles that form the steps of the algorithm: <lb/>(1) Recurrences can be decomposed into smaller fragments that put summation statements in <lb/>the their own recurrence. <lb/>(2) RIN can then be generated by placing the fragment recurrences into a loop nest one at a time, <lb/>such that dependencies between fragments are not violated. Dependencies can be modeled <lb/>with a dependency graph that encodes the order in which to place statements. <lb/></body>

    <note place="footnote">1 Exceptions include tree-based parallelization schemes for reductions and prefix sums. <lb/></note>

	<note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:11 <lb/></page>

    <body># 1) generates minimal fragements 2) sorts fragments 3) places each fragment into RIN <lb/>program , making inductive assumptions when necessary <lb/>def lower ( input_recurrencs , loop_ordering ) : <lb/>fragment_recurrences = generateFragments ( input_recurrences ) <lb/>sorted_fragments = topologicalSort ( fragment_recurrences ) <lb/>assumes rin_program = {} ,{} # no inductive assumptions yet , RIN initially empty <lb/>for statement in sorted_fragments : <lb/>tryPlacing ( statement , rin_program ) <lb/># try making inductive assumption , if that lets us place statement <lb/>while statement . notPlaced () and canMakeAssumption ( loop_ordering , assumes ) : <lb/>assumes = makeAssumption ( loop_ordering , assumes ) <lb/>tryPlacing ( statement , rin_program , assumes ) <lb/>if statement . notPlaced () : <lb/>return fail loop_ordering is impossible <lb/>return rin_program <lb/># greedily place statement into first location where dependencies are satistfied <lb/>def tryPlacing ( statement , program , assumptions ) : <lb/># function defined by Equation 2 <lb/>location = PlacementLocation ( statement , statement . dependencies , rin_program ) <lb/>if location : <lb/>location . place ( statement ) # inserts readiness statement <lb/>Fig. 7. Pseudocode for RIN generation. Not shown is 1. the process for generating empty loop headers at a <lb/>potential location for placing a fragment and 2. the check that index variables used in a fragment are in scope <lb/>at the potential location. Both of these happen implicitly when iterating over valid locations in the program <lb/>(3) When determining where to place a statement, what results have already been calculated <lb/>can be inductively determined at any location and any iteration of the nested loop. The <lb/>inductive assumptions are necessary when a statement depends on results computed by the <lb/>same statement in earlier loop iterations. <lb/>The lowering algorithm follows these steps to generate an RIN program and terminates once all <lb/>fragments have been placed. It reports an error if the loop order violates a dependency. Pseudocode <lb/>for the placement algorithm is shown in Figure 7 and described in the following sections. <lb/>6.1 Recurrence Index Notation <lb/>The recurrence index notation (RIN) consists of abstract loop nests that iterate over index variables <lb/>to compute result values in scalar statements. The loops are abstract in that they iterate over <lb/>and compute with abstract tensors, meaning specific physical data structures have not yet been <lb/>introduced. This abstraction impacts scalar compute statements and loop bounds. The scalar <lb/>statements access the logical tensors through tensor accesses. And the loop bounds are given as set <lb/>expressions that describe how to co-iterate over the relevant tensors. Moreover, the loop bounds <lb/>contain constraints such as &lt; to express triangular iteration spaces. Like in prior work [Kjolstad <lb/>et al. 2017], accesses to and iteration over physical data structures are introduced only when <lb/>generating low-level imperative code (Section 7). <lb/>Two examples of RIN loop nests were given in Figure 1 and the grammar is shown in Figure 8. The <lb/>if statements and while loops often found in sparse codes are absent in RIN; all loops are for or forall <lb/>loops (in which a for loop has loop-carried dependencies, and a forall does not). Thus, a recurrence <lb/>program that requests dense data structures and an otherwise equivalent recurrence program that <lb/>requests sparse data structures will have the exact same RIN. Finally, the set expressions and ranges <lb/>of the loop bounds are left implicit and are inferred from tensors and statements in bodies of the <lb/>loops during low-level code generation. For example, the range of a loop is inferred from the tensor <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:12 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>assign ::= tensorAccess &apos;=&apos; expr <lb/>| tensorAccess &apos;+=&apos; expr <lb/>expr ::= expr &apos;+&apos; expr <lb/>| expr expr <lb/>| &apos;-&apos; expr <lb/>| ... <lb/>tensorAccess ::= tensor index * <lb/>index + <lb/>index ::= iVar <lb/>| iVar &apos;+&apos; constantInt <lb/>| constantInt <lb/>stmt ::= assign <lb/>| for <lb/>| forall <lb/>for ::= &apos;for&apos; loopHeader &apos;:&apos; stmt+ <lb/>forall ::= &apos;forall&apos; loopHeader &apos;:&apos; stmt+ <lb/>loopHeader ::= iVar &apos;&lt;&apos; index &apos;:&apos; <lb/>| iVar &apos;&gt;&apos; index &apos;:&apos; <lb/>| index &apos;&lt;&apos; iVar &apos;&lt;&apos; index &apos;:&apos; <lb/>| iVar &apos;:&apos; <lb/>RIN ::= stmt+ <lb/>Fig. 8. Grammar for Recurrence Index Notation <lb/>dimensions it indexes. The set expression for iterating over a statement containing a multiplication <lb/>expression is the intersection of its operands&apos; iteration spaces [Kjolstad et al. 2017]. <lb/>6.2 Generate Fragment Equations <lb/>Recurrence fragments are generated by traversing the recurrence bottom-up and placing each <lb/>summation operation in its own recurrence. Because fragment recurrences do not compute final <lb/>results, they instead store their partial sums to temporaries. To disambiguate a temporary from a <lb/>result, the temporary is denoted by the corresponding tensor name being preceded by the letter <lb/>(e.g., ). The temporary is a partial computation of a value that will eventually reside in the output <lb/>tensor. The Cholesky decomposition, for example, will decompose each of its two input equations <lb/>into two fragments, since each equation contains one summation. The four resulting recurrences <lb/>for the Cholesky decompositions are shown in Figure 5. <lb/>6.3 Determine Placement Order <lb/>The placement algorithm only places a fragment into the loop at a location where the fragment&apos;s <lb/>dependencies are already calculated. There is thus a limited set of correct orderings in which <lb/>fragments can be placed into the imperative loop, and these orderings can be inferred from the <lb/>directed acyclic dependency graph over all fragments&apos; tensorAccesses. This graph is generated <lb/>right before the placement stage. <lb/>In a dependency graph, a directed edge → (meaning tensor access B depends on the values <lb/>from tensor access A) exists if there exists a recurrence equation with as the result and as <lb/>an operand. As described in Section 5.1, the tensor accesses across all fragments form a directed <lb/>acyclic graph, in which each tensor access is a node and each dependency is a directed edge. <lb/>Placement order is then determined by a topological sort of the fragments. For example, in an <lb/>Cholesky decomposition, the first placed equation is <lb/>= <lb/>because of its position in <lb/>the dependency graph in Figure 5; of the four nodes referring to the fragment results, it is the only <lb/>one in the dependency graph not dependent on any other. <lb/>, on the other hand, cannot be placed <lb/>first. As shown in Figure 5, it depends on , which itself is dependent on <lb/>. <lb/>6.4 Greedily Place Equations into an RIN Loop Nest <lb/>The placement algorithm starts with an empty loop nest created from the recurrence&apos;s index vari-<lb/>ables and a user-specified loop order. The algorithm proceeds by iterating through the dependency <lb/>graph in topological order and placing each fragment into a location within the RIN. <lb/>As fragments are placed into the loop, the compiler keeps track of what outputs are already <lb/>computed at each location in the loop via internal state. This state is a table that maps program <lb/>locations to outputs, indicating that a certain subsection of the output can be considered computed <lb/>at this location in the RIN. If a compute statement fully calculating an output is present at a certain <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:13 <lb/></page>

    <body>location, the compiler tracks that this output is ready directly after this location. This state is <lb/>modified whenever fragments are placed and whenever inductive assumptions are made. Inductive <lb/>assumptions preemptively enforce that an output will be calculated by a certain program location, <lb/>even if the corresponding compute statements have not yet been inserted into the RIN. The compiler <lb/>tracks these assumptions with the same mechanism, and modifies that table accordingly. While <lb/>this state is not embedded within RIN, we annotate Figure 9 with readiness markers that illustrate <lb/>readiness information internal to the compiler. In the Cholesky example, for instance, a marker <lb/>illustrating that <lb/>is considered calculated after the marker&apos;s location would be <lb/>( , ). <lb/>Let be a collection of locations in a RIN loop nest, ordered according to program order. At any <lb/>particular location ∈ , the set of outputs and temporaries that have already been computed at <lb/>can be calculated with the following equation. Here, <lb/>refers to the internal table the compiler <lb/>keeps about which output is considered calculated at location . Thus, <lb/>( ) records the set <lb/>of all outputs considered computed by the RIN at line . <lb/>( ) = { <lb/>∈ <lb/>( )} <lb/>(1) <lb/>Therefore, the set of already calculated outputs at point is the set union of what is considered <lb/>ready at each program location until , inclusively. In other words, it is determined by collecting <lb/>all readiness info from an in-order traversal over RIN&apos;s AST, terminating at location . <lb/>For a statement with a set of dependencies , the placement algorithm chooses a program <lb/>point in the set of program points in the RIN in which to place . Assuming program points in <lb/>are ordered according to the program order, this location can be determined using Equation 1 <lb/>calculating <lb/>( ). <lb/>( , , ) = min <lb/>∀ ∈ <lb/>. . <lb/>⊂ <lb/>( ) <lb/>(2) <lb/>In other words, each statement is greedily placed into the first location in the RIN loop nest where <lb/>the statement&apos;s dependencies are computed. When a statement is placed in the loop nest, the <lb/>compiler internally records that the statement&apos;s result has been computed at the statement program <lb/>point following the statement. <lb/>We show several steps of the placement process for <lb/>Cholesky in Figure 9. For each step, green <lb/>readiness markers show the lowering algorithm&apos;s record of what values have been computed at <lb/>each program point. The placement algorithm terminates when all fragments have been placed, in <lb/>which case the IR is fully formed, or when it it not possible to place a statement anywhere. In the <lb/>latter case, the given loop-ordering is illegal and the compiler returns an error. <lb/>A statement may depend on a value in the output calculated by itself in an earlier loop iteration. <lb/>This is a direct result of having dependencies amongst outputs within a recurrence. Due to this cyclic <lb/>dependency, placing this statement into the loop is not possible without a prescient guarantee about <lb/>what outputs have been calculated for each iteration and location in the loop. These guarantees <lb/>are made in the form of inductive assumptions. When these assumptions are made, additional <lb/>readiness info is recorded by the compiler. This is shown in stage b of Figure 9, in which green <lb/>readiness markers encoding these guarantees are placed into the loop nest. <lb/>An assumption guarantees that for a given loop iteration variable , the subsequence of output <lb/>elements that can be considered as already calculated at the start of iteration is a function of . In <lb/>an <lb/>Cholesky decomposition, this could mean that at the beginning of iteration of the outer <lb/>loop, all rows of the output up to row (exclusively) are already calculated. Each assumption can <lb/>be thought of as a strong inductive hypothesis. In this hypothesis, let be a tensor access from <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:14 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>f o r a l l i <lb/>f o r a l l j &lt; i <lb/>f o r a l l k&lt; j <lb/>(a) No Cholesky statements can be placed unless <lb/>inductive assumptions have been made <lb/>f o r i <lb/>r e a d y L ( : i , : ) <lb/>f o r j &lt; i <lb/>r e a d y L ( i , : j ) <lb/>f o r a l l k&lt; j <lb/>+= <lb/>r e a d y t L ( i , j ) <lb/>(b) Inductive assumptions made over and , allowing <lb/>placement of ( , ) <lb/>f o r i <lb/>r e a d y L ( : i , : ) <lb/>f o r j &lt; i <lb/>r e a d y L ( i , : j ) <lb/>f o r a l l k&lt; j <lb/>+= <lb/>r e a d y t L ( i , j ) <lb/>= ( <lb/>-<lb/>)/ <lb/>r e a d y L ( i , j ) <lb/>(c) Place ( , ) <lb/>f o r i <lb/>r e a d y L ( : i , : ) <lb/>f o r j &lt; i <lb/>r e a d y L ( i , : j ) <lb/>f o r a l l k&lt; j <lb/>+= <lb/>r e a d y t L ( i , j ) <lb/>= ( <lb/>-<lb/>)/ <lb/>r e a d y L ( i , j ) <lb/>+= <lb/>r e a d y t L ( i , i ) <lb/>= <lb/>( <lb/>-<lb/>) <lb/>(d) Place remaining fragments <lb/>Fig. 9. Steps of the placement algorithm for generating RIN of an <lb/>Cholesky decomposition. Green text <lb/>show readiness markers that are not part of IR but internal to the compiler <lb/>a fragment&apos;s left-hand side, in which is a tensor and is an ordered list of indices. Let be an <lb/>index variable such that ∈ . Let be another list of index variables, and let represent the index <lb/>of an index variable in or . Elements in or can use an indexing colon, where a single colon : <lb/>represents all potential indices in a dimension, and : represents all potential indices less than . <lb/>If an inductive assumption is made over , then at the beginning of iteration in the loop over , <lb/>all outputs encapsulated by <lb/>have been computed. Here, is defined as: <lb/>= <lb/>     <lb/>   <lb/> <lb/>: <lb/>if = <lb/>if an assumption already exists over a parent loop over <lb/>: <lb/>otherwise <lb/>The strong inductive hypothesis guarantees that at the beginning of loop iteration , all slices <lb/>of until the th slice are already computed. As with strong induction, this hypothesis can only <lb/>become true if all statements computing values in the th slice of are placed within iteration <lb/>of this loop, such that the th iteration fully calculates the th slice. In RIN, this corresponds to <lb/>converting the for loop over to a forall loop. This conversion implies three things: <lb/>(1) The guarantee that all slices of A until slice are calculated before iteration of that loop. <lb/>(2) The restriction that the current loop over must be the last loop over that writes to slice <lb/>of . Computations writing to this slice can now only exist within this loop over , or the <lb/>aforementioned guarantee will be broken, rendering the inductive hypothesis false. <lb/>(3) The introduction of a loop-carried dependency in this loop with respect to , which will <lb/>affect whether the loop can be parallelized <lb/>For example, as shown in Figure 6, in an <lb/>Cholesky decomposition computing , an assump-<lb/>tion over would indicate that all rows of L up to row (shown in green) are already computed at <lb/>the start of iteration in the loop over . Using the formula for , we now consider (: , :) as ready <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:15 <lb/></page>

    <body>in this loop. The algorithm then records this readiness info internally, which we visualize in stage <lb/>b of Figure 9 by placing a readiness marker <lb/>(: , :) in the first location within the loop over <lb/>. For the current fragment we are trying to place <lb/>+= <lb/>, the guarantee that all rows up to <lb/>are calculated satisfies the dependency over <lb/>, an element in row . The fragment&apos;s constraint <lb/>guarantees that &lt; , so <lb/>can now be considered as already computed. The algorithm is then <lb/>forced to place all statements calculating row of L within this loop iteration . This is shown by <lb/>the RIN in the top row of Figure 9, which illustrates what happens during this assumption. <lb/>Figure 9 also illustrates how after an initial assumption is made about in the <lb/>Cholesky <lb/>RIN, an additional, nested assumption is made over the middle loop . The formula for shows <lb/>that this new assumption inherits the outer assumption over . The algorithm then records the <lb/>new readiness information corresponding to this assumption, illustrated by placing the marker <lb/>( , : ) at the beginning of iteration for loop . The readiness marker means that at the <lb/>beginning of iteration in the loop over , all elements in row until the th element are computed. <lb/>This is visualized by the middle diagram in Figure 6. For the fragment we are trying to place, <lb/>+= <lb/>, the constraint guarantees that &lt; , so the dependency <lb/>is now considered <lb/>computed at this program point. As the other dependency <lb/>was satisfied by the first assumption <lb/>over , both dependencies are now satisfied in the loop over , allowing us to place the statement as <lb/>shown in stage b of Figure 9. <lb/>A similar sequence of events will occur for all other loop orderings. In a <lb/>Cholesky ordering <lb/>where the outer loop iterates over , an assumption over would guarantee that at the beginning of <lb/>iteration in loop , all columns of output up to column are already computed. This assumption <lb/>gives a guarantee about columns because indexes the column dimension in . <lb/>If the algorithm succeeds, then the inductive assumption is justified. We can make assumptions <lb/>over index variables in the order given by the loop ordering, until the number of assumptions <lb/>made matches the number of dimensions of the highest dimensional output. In this case, we cannot <lb/>make more than the two assumptions over loops over and , as is only two dimensional. Finally, <lb/>if the input recurrences contain no recurrence dependencies amongst outputs, then inductive <lb/>assumptions are not required. Tensor algebra expressions (e.g., matrix multiply), for example, lack <lb/>dependencies, as computation of outputs is embarrassingly parallel, so the placement algorithm <lb/>will never make an inductive assumption. <lb/>6.5 Variable Substitutions <lb/>Intuitively, a recurrence statement that uses an index variable should be placed in a loop over so <lb/>that the variable is in scope. For recurrences, however, this is actually too stringent a restriction that <lb/>at best will suppress loop fusion and at worst will prevent correct code generation. In many cases, <lb/>two index variables with different names (e.g. , ) may be logically interchangeable, meaning a <lb/>recurrence statement that uses can be rewritten in terms of , allowing it to be placed within a loop <lb/>over . For example, assume the user defines two recurrences, an identity recurrence = : &lt; <lb/>defined over and another identity recurrence = <lb/>: &lt; defined over . For a given loop <lb/>ordering of the single variable , a naive greedy placement algorithm would generate a loop over , <lb/>note the 2nd fragment is defined solely over , and naively deduce this fragment defined over <lb/>cannot be placed in the loop. However, the 2nd recurrence be safely rewritten as = : &lt; , as <lb/>&apos;s iteration space of [0, ) in the current RIN loop is equivalent to &apos;s iteration space in the original <lb/>recurrence. In this case the two variables are considered isomorphic, and the 2nd recurrence should <lb/>be rewritten so it can be placed within the the loop. <lb/>A recurrence lowering algorithm should thus determine if and when variables can be inter-<lb/>changed. Our algorithm can substitute index variables in fragments&apos; tensorAccesses if the new <lb/>variable is isomorphic to an existing variable . For a particular fragment, two variables and <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:16 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>f o r i : <lb/>r e a d y S ( : i ) <lb/>f o r a l l k&lt; i : <lb/>+= <lb/>r e a d y t S ( i ) <lb/>= <lb/>( ) <lb/>Fig. 10. RIN for ordering <lb/>f o r k : <lb/>r e a d y S ( : k ) <lb/>r e a d y t S ( : k+ 1 ) <lb/>= <lb/>( <lb/>) <lb/>r e a d y S ( k ) <lb/>f o r a l l i &gt;k : <lb/>+= <lb/>Fig. 11. RIN for ordering <lb/>are isomorphic at a program location if &apos;s lower and upper bound at location are the same as <lb/>&apos;s upper and lower bounds in the original fragment recurrence, as determined by the constraints. <lb/>To determine when variables should be interchanged, let be the th variable in the loop ordering, <lb/>be the th variable, and be the fragment (using ) we are attempting to place in RIN location , <lb/>where scope( ) is the list of all index variables in scope at . <lb/>= {iteration space of in fragment} = {iteration space of in scope( )} <lb/>⇐⇒ <lb/>( ) = <lb/>( ) ∧ <lb/>( ) = <lb/>( ) <lb/>( &lt; ) ∧ ( <lb/>) → replace with in <lb/>(3) <lb/>Therefore, variable only need be replaced with isomorphic variable if comes before in <lb/>the given loop ordering. This allows fragments using to be placed directly into a loop over if <lb/>it comes before a loop over or if a loop over does not yet exist. These substitutions normalize <lb/>a set of fragments with respect to a schedule and can be done during greedy placement itself. <lb/>Alternatively, this normalization can be executed once after fragment generation and before the <lb/>dependency graph is generated, eliminating the need to rewrite fragments while placing them. <lb/>In the <lb/>Cholesky decomposition, this can occur when placing the fragment recurrence for <lb/>= <lb/>. Because the fragment calculates a diagonal value where = , and because the outer <lb/>loop iterates over (the first loop in the ordering), it makes sense to redefine <lb/>in terms of . This <lb/>allows the equivalent fragment <lb/>= <lb/>√ <lb/>to be placed in the outer loop over . <lb/>This substitution is useful in when a summation variable comes before a non-summation variable <lb/>in the given loop ordering. Consider the recurrence = <lb/>( <lb/>), where is a summation <lb/>variable and &lt; . The code in Listings 10 and 11 show RIN for both and loop orderings. <lb/>In the left listing, an inductive assumption is made over . Once the loop over completes, <lb/>is considered fully computed, and the algorithm can then place the statement = <lb/>√ <lb/>. <lb/>In the right listing, an inductive assumption is made over . Because &gt; , for outer loop <lb/>iteration , the inner loop over writes to all <lb/>where &gt; . The inductive assumption over <lb/>implies that iteration will be the last iteration in which the inner loop writes to = +1 , implying <lb/>+1 is ready once the inner loop completes. This is equivalent to saying <lb/>is ready at the <lb/>beginning of the loop, and we can thus immediately place = <lb/>√ <lb/>there. To handle this, the <lb/>algorithm makes the substitution → to rewrite the fragment into = <lb/>√ <lb/>. <lb/>This above case happens in the <lb/>, , and <lb/>Cholesky decompositions. The transformation <lb/>shown in the above listings is also included as its own transformation in the Symbolic Fractal <lb/>Analysis [Menon and Pingali 2004] compiler framework, which has been used to change loop <lb/>orderings of the Cholesky decomposition and triangular solve. However, the transformation requires <lb/>an existing imperative Cholesky code as input. With variable substitutions and RIN, a hardcoded <lb/>transformation is not necessary, as the above transformation happens automatically. <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:17 <lb/></page>

    <body>7 C CODE GENERATION <lb/>Our RECUMA recurrence compiler is written in Python, and the toolchain includes a Python based <lb/>front-end for defining the recurrence equations, constraints, and a schedule. The aforementioned <lb/>fragment generation, DAG generation, and greedy lowering occur in this pipeline, ending in the <lb/>generation of the RIN intermediate representation. The RIN is then lowered to C code. This final <lb/>phase generates code that iterates over dense and sparse data structures. <lb/>Our RIN to C code generation phase uses the code generation ideas from the TACO compiler. <lb/>TACO is a tensor algebra compiler that compiles tensor algebra expressions, where tensors&apos; data <lb/>structures are separately specified, into loops that co-iterate over the different (dense and sparse) <lb/>data structures to compute the result. <lb/>We discussed the differences between recurrence equations, as supported by RECUMA, and tensor <lb/>index notation, as supported by TACO, in Section 4. These differences-recurrences, constraints, <lb/>multiple equations, and timestepping variables-primarily affect the algorithm that generates RIN <lb/>from recurrence equations, which we described in preceding sections. The primary exception <lb/>is the constraints, which result in triangular loop bounds in the RIN. The C code generation <lb/>phase in RECUMA can therefore use the same general approach as TACO, with a straight-forward <lb/>modification to support triangular loop bounds. We therefore refer the reader to the TACO body of <lb/>work for how to compile high-level loops over abstract tensors to loops over concrete sparse and <lb/>dense data structures [Kjølstad 2020]. <lb/>However, we discuss various optimizations that RECUMA performs that are particular to recur-<lb/>rences in Section 8. Some of these optimizations are performed during C code generation. Moreover, <lb/>in RECUMA, for any given recurrence and schedule, the choice of data structure does not affect <lb/>the correctness of the generated code, even if the the algorithm involves data access patterns for <lb/>which the data-structure is a poor fit. RECUMA guarantees that the generated C-code is always <lb/>correct, and will do so at the cost of the generated code&apos;s performance when necessary. If the loop <lb/>order requires a tensor to be accessed in a different way from the way it is stored in memory (e.g., <lb/>accessing a CSR matrix in column-major order), then RECUMA will insert a (log ) binary search <lb/>within the CSR data structure. <lb/>8 OPTIMIZATIONS <lb/>RECUMA supports a mix of general program optimizations and domain-specific optimizations <lb/>targeted towards families of similar recurrences. General techniques like loop fusion and loop <lb/>parallelization are automatic byproducts of the lowering process from recurrence equations to <lb/>recurrence index notation. Moreover, the marching pointers and mask optimizations can also be <lb/>specified by the user as part of their schedule. Other optimizations, such as tiling and wavefront <lb/>parallelism, are not currently supported by RECUMA. We believe, however, that they can be <lb/>incorporated in future work as loop transformations on the recurrence index notation. <lb/>8.1 Parallelization <lb/>The distinction between a for loop and forall loop in the recurrence index notation informs auto-<lb/>parallelization. If a loop is a forall and operates solely over dense data structures, the corresponding <lb/>C loop is parallelized with an OpenMP pragma. To illustrate, we use the Viterbi algorithm [Viterbi <lb/>1967], a dynamic program for determining the most likely path taken in a hidden Markov process. <lb/>It is often also treated as a graph problem, requiring sparse data structures, when the set of <lb/>states are sparsely connected with edges that represent transition probabilities. The algorithm <lb/>calculates the most likely sequence of hidden states encountered given a sequence of observations. <lb/>The associated recurrence utilizes three matrices. Given a transition probability matrix and an <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:18 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>emission probability matrix , the output matrix can be calculated with the equation below on <lb/>the right. In this case, using a <lb/>or <lb/>loop ordering will result in the outer loop over being a <lb/>for loop, and the inner loops being forall. This is shown in the following RIN, which implements a <lb/>loop ordering of the Viterbi recurrence: <lb/>f o r j : <lb/>f o r a l l i : <lb/>f o r a l l k : <lb/>= max( , -1 <lb/>) <lb/>= max , -1 <lb/>(4) <lb/>The index variable is expressible as a timestep variable, but is here used as a normal index <lb/>variable because knowing intermediate states is useful in Viterbi path problems. Performance <lb/>results of parallelization are shown in the evaluation. Enabling parallelization for the loop over <lb/>can be done with the following command: schedule[&quot;omp&quot;] =&quot;i&quot;. <lb/>8.2 Loop Fusion <lb/>As described in Section 6, the lowering algorithm greedily places each compute statement into <lb/>the first location of a RIN program where the statement&apos;s dependencies are satisfied. This implies <lb/>that the algorithm will always try to fuse the statement into an existing loop before attempting to <lb/>place it in a subsequent loop. In other words, loop fusion happens automatically in this framework. <lb/>Furthermore, the algorithm&apos;s ability to handle multiple recurrences implies that statements from <lb/>separately defined recurrences will, when possible, be placed within a single fused loop. This is <lb/>highly beneficial for performance, as a RECUMA program will never incur the cost of looping over <lb/>an array twice when one loop will suffice. This is especially important for memory-bound kernels <lb/>in which the costs of accessing large arrays dominates a kernel&apos;s execution time. <lb/>To illustrate, we show how two sparse triangular solves will be fused together automatically. <lb/>Given a lower-triangular matrix and an input vector , the triangular solve will attempt to solve <lb/>= with the following recurrence: = ( -<lb/>)/ <lb/>: &lt; . <lb/>It is common to have to solve the same matrix system for multiple right-hand side vectors. <lb/>In a situation that requires performing solves for two such vectors, the user can provide the <lb/>following recurrence equations that solve for and : <lb/>= 1 -<lb/>/ <lb/>: &lt; <lb/>= 2 -<lb/>/ <lb/>: &lt; <lb/>Usually, each triangular solve would require a separate doubly-nested loop over and . However, <lb/>if the user provides both equations and an loop-ordering, RECUMA will generate a single <lb/>doubly-nested loop that calculates both and . This performance benefits of using a fused kernel <lb/>in this example are demonstrated with experiments discussed in Section 9.4. <lb/>Two recurrences need not be identical (like in the case above) for fusion to occur; fusion can <lb/>occur between completely different recurrences. To illustrate, the Cholesky decomposition = <lb/>, <lb/>is commonly followed by two triangular solves to solve for in the equation <lb/>= <lb/>= . The <lb/>first triangular solve can be fused into the outer Cholesky loop. To generate such a fused kernel for <lb/>an <lb/>loop nest, the user need only provide the recurrence equations for both the triangular solve <lb/>and the Cholesky decomposition. The placement algorithm will then generate a loop nest in which <lb/>the triangular solve to calculate occurs in the outer-loop over from the <lb/>Cholesky loop nest. <lb/>8.3 Masks <lb/>Masks are used in sparse problems to represent a tensor&apos;s sparsity pattern. In a breadth-first search, <lb/>which is expressible as a recurrence, masks can be used to denote the subset of nodes in a graph <lb/>that are of interest to the user, such that the BFS now only needs to iterate over the relevant data. <lb/></body>

    <note place="headnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:19 <lb/></page>

    <body>In various matrix solvers, including the triangular solve and Cholesky, LU, and QR decompositions, <lb/>they are employed similarly, such that optimized kernels need only iterate over the entries of <lb/>the matrix that are known to be non-zero. The process of precalculating this nonzero pattern is <lb/>commonly referred to as symbolic factorization and is used as a preprocessing step to the numerical <lb/>factorization in which the nonzero values are calculated. <lb/>Generation of the mask is not within the scope of this work, as the underlying process is <lb/>not necessarily based on solving recurrence. The user hence provides a pre-calculated mask to <lb/>RECUMA, which is often justified due to many sparse problems consisting of repeatedly solving <lb/>different matrix problems with the same underlying sparsity pattern. Depending on the sparsity <lb/>pattern, generating and iterating over masks does not always improves performance. It is thus an <lb/>optimization parameter for sparse recurrences, and we briefly describe the significance of masks <lb/>for relevant problems. <lb/>In a triangular solve, symbolic analysis can be done with a data structure known as an elimination <lb/>graph [Gilbert 1994]. In this process, given a sparse b-vector, the nonzero elements of b are used as <lb/>roots to a depth-first search of the sparse matrix , in which a nonzero at location <lb/>represents an <lb/>edge between nodes and . The DFS emits a topologically sorted mask denoting the locations of <lb/>nonzeroes in , even though &apos;s values have not yet been calculated. For a Cholesky decomposition, <lb/>a data structure called the elimination tree [Liu 1986] can generate a mask of in linear time. <lb/>The user can optionally provide a mask, along with an indication of whether the mask is CSC <lb/>or CSR. When possible, the compiler will then lower any appropriate loops in the RIN to iterate <lb/>over the mask, thus avoiding needles computations whose output is zero. In cases, where the loops <lb/>need to iterate over both rows and columns, it may be useful to provide both CSC and CSR masks, <lb/>which is possible. The primitive for doing so is shown below: <lb/>s t o r a g e [ &quot; L &quot; ] = S t o r a g e ( [ Dense ( 0 ) , Compressed ( 1 ) ] ) # L s t o r e d i n CSR <lb/>s t o r a g e [ &quot; L &quot; ] . addMask ( S p a r s e M a s k ( [ Dense ( 0 ) , Compressed ( 1 ) ] ) #CSR Mask <lb/>s t o r a g e [ &quot; L &quot; ] . addMask ( S p a r s e M a s k ( [ Dense ( 1 ) , Compressed ( 0 ) ] ) #CSC Mask <lb/>8.4 Marching Pointers <lb/>When using a sparse data structure best suited to efficiently iterating over a row (e.g., CSR), it is <lb/>often useful to use an auxiliary data structure that also lets us efficiently iterate over a column (and <lb/>vice versa). This is useful in cases when the loop ordering and recurrence require efficient iteration <lb/>over both a row and a column. Figure 6 shows that for <lb/>Cholesky, the inner loop iterations over <lb/>(shown with triple head arrows) requires we iterate over a column when accessing <lb/>, so it is best <lb/>to use a CSC data structure. However, because the outer loop is over , we wish to build the output <lb/>row-by-row, in which the inner loop also iterates over the ℎ row. It thus is useful to be able to <lb/>efficiently iterate over the th row, while being able to efficiently iterate over columns. This can be <lb/>done with expensive binary-searches, but more efficiently implemented by keeping a 1D vector <lb/>of marching pointers, such that for outer iteration and inner loop iteration , ( ) contains an <lb/>offset to output element <lb/>in the CSC matrix , allowing us to easily iterate over row as if row <lb/>was row major. At the end of outer loop iteration , each offset is incremented -these increments <lb/>only happen in the outer loop, so they are infrequent and do not negatively affect performance. <lb/>Using marching pointers is easily accomplished with the primitive: <lb/>s t o r a g e [ &quot; L &quot; ] = S t o r a g e ( [ Dense ( 1 ) , Compressed ( 0 ) ] ) # L s t o r e d i n CSR <lb/>s t o r a g e [ &quot; L &quot; ] . a d d M a r c h e r s ( 0 , i ) # m a r c h e r s a l o n g d i m e n s i o n 0 ( rows ) , u p d a t e d on i t h i t e r a t i o n <lb/>9 EVALUATION <lb/>We evaluate the performance of recurrence implementations generated by RECUMA against hand-<lb/>optimized implementations in popular libraries (Section 9.1). Furthermore, to demonstrate the <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:20 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>Sparse Matrix <lb/>Speedup <lb/>0.0 <lb/>0.5 <lb/>1.0 <lb/>1.5 <lb/>2.0 <lb/>1138_bus nasa1824 bodyy6 <lb/>cbuckle <lb/>Cholesky Speedup Over CXSparse <lb/>Matrix Dimension Size <lb/>Speedup <lb/>0.0 <lb/>0.5 <lb/>1.0 <lb/>1.5 <lb/>2.0 <lb/>100 <lb/>200 <lb/>400 <lb/>800 <lb/>Floyd-Warshall Speedup Over Boost <lb/>Matrix Dimension Size <lb/>Speedup <lb/>0.0 <lb/>0.5 <lb/>1.0 <lb/>1.5 <lb/>2.0 <lb/>800 <lb/>1600 <lb/>3200 <lb/>6400 <lb/>Needleman-Wunsch Speedup Over Parasail <lb/>Sparse Matrix <lb/>Speedup <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>1.25 <lb/>1138_bus nasa1824 bodyy6 <lb/>cbuckle <lb/>Triangular Solve Speedup Over CXSparse <lb/>Matrix Dimension Size <lb/>Speedup <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>1.25 <lb/>100 <lb/>200 <lb/>400 <lb/>800 <lb/>1600 <lb/>Gauss-Seidel Speedup Over Polybench <lb/>Sparse Matrix <lb/>Speedup <lb/>0.00 <lb/>0.25 <lb/>0.50 <lb/>0.75 <lb/>1.00 <lb/>1.25 <lb/>nasa1824 1138_bus bodyy6 cbuckle orsreg_1 nemeth09 <lb/>SDDMM <lb/>SpMV <lb/>SDDMM and SpMV Speedup Over Taco <lb/>Fig. 12. Speedup of RECUMA-generated kernels over libraries. All times are averaged over 100 trials, except <lb/>SpMv (500 trials), and Gauss Seidel (1000 trials for N=[100,200,400]). Sparse matrices for tri solve and <lb/>Cholesky were chosen via stratified random sampling of SPD matrices from SuiteSparse from the following <lb/>intervals over #nonzeros [1-5K], [5K-50K], [50K-500K], and [500K-1M]. SDDMM and SpMV matrices include <lb/>the first four SPD systems used in tri. solve and Cholesky and two additional random non-SPD systems. <lb/>importance of a system that supports different data formats and optimizations, we also also perform <lb/>experiments that demonstrate the performance implications of data formats (Section 9.2), different <lb/>loop orderings (Section 9.3), fusion (Section 9.4), and parallelism (Section 9.5). <lb/>9.1 Comparison with Existing Libraries <lb/>We evaluate the performance of RECUMA-generated implementations of sparse Cholesky de-<lb/>composition, sparse triangular solve, sparse SDDMM, sparse SpMV, dense Floyd-Warshall, dense <lb/>Needleman-Wunsch, and dense Gauss-Seidel. SDDMM and SpMv are sparse tensor algebra routines <lb/>and are thus compared TACO-generated implementations. The remaining kernels are compared <lb/>against popular libraries. Unless noted, all performance metrics were aggregated over 100 trials <lb/>and conducted on a 40-core Intel Xeon CPU. Graphs showing speedup of RECUMA kernels over <lb/>existing library kernels are shown in Figure 12. <lb/>Triangular Solve. We evaluate a RECUMA kernel for performing a triangular solve with CSC data <lb/>structures against CXSparse, whose triangular solve uses the same schedule and data structures. <lb/>Figure 12 shows that their performance is comparable for the matrices tested. <lb/>Cholesky Decomposition. We generate a RECUMA kernel of an <lb/>Cholesky using CSC data struc-<lb/>tures with marching pointers, and compare it against a CXSparse kernel using the same ordering <lb/>and data structures (including marching pointers). Both implementations use a mask, with the main <lb/>difference being the mask is calculated in the CXSparse Cholesky loop nest, while it is calculated <lb/>before hand in RECUMA. However, mask calculation performance is almost inconsequential, as it <lb/>is an ( 2 ) algorithm while Cholesky is ( 3 ). Speedup plots are shown in Figure 12. <lb/>Floyd-Warshall. We generate a Floyd-Warshall RECUMA kernel and benchmark it against a 2D <lb/>implementation from the Boost graph library. The algorithm is inherently dense, so the connec-<lb/>tivity and sparse structure of the underlying matrix is irrelevant to the runtime; only the number <lb/>graph nodes matters. The RECUMA vs Boost speedup graph is shown in Figure 12, in which <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:21 <lb/></page>

    <body>RECUMA code outperforms Boost. Boost uses unnecessary C++ features and abstractions that <lb/>affect performance, while RECUMA&apos;s output is simple C code that operates on a C array. <lb/>Needleman-Wunsch. The Needleman-Wunsch algorithm is commonly used to align two strings <lb/>of genomics data. The algorithm operates on a dense 2D matrix, where each cell depends on its <lb/>left, upper, and upper-left neighbor. We evaluate code generated by RECUMA against the Parasail <lb/>library. As shown in Figure 12, for small sizes Parasail outperforms RECUMA. As we increase the <lb/>problem size, RECUMA consistently performs Parasail. During compile time, Parasail generates a <lb/>large suite of kernels utilizing different vectorization strategies. We show Parasail&apos;s best performing <lb/>kernel in Figure 12. We surmise that Parasail&apos;s vectorized code is not optimized for the target <lb/>processor, otherwise Parasail would likely demonstrate superior performance. <lb/>Gauss-Seidel. We generate a dense Gauss-Seidel iterative solver, representing an in-place 5-pt <lb/>stencil. We compare its performance to the Gauss-Seidel solver in the PolyBench benchmarking <lb/>suite, modified slightly to perform the same computation as our RECUMA kernel. Speedup plots <lb/>from Figure 12 show that performance of the RECUMA and PolyBench kernels are nearly identical. <lb/>Tensor Algebra. We evaluate two tensor algebra kernels: sparse matrix-vector multiply (SpMv) and <lb/>sampled dense times dense matrix multiply (SDDMM). We evaluate C code generated by RECUMA <lb/>against C code generated by TACO. The TACO kernels were generated without any additional <lb/>TACO scheduling primitives. As shown in the speedup plots in Figure 12 RECUMA expectedly <lb/>exhibits nearly equivalent performance to that of TACO for these two kernels. <lb/>9.2 Data Formats <lb/>The user specifies the format of the kernels&apos; data. Choosing formats amenable to a program&apos;s data <lb/>access patterns ensures good memory access locality and thus improves performance. <lb/>Many recurrences, like the Viterbi equation, are used in both dense and sparse settings. We <lb/>evaluate RECUMA&apos;s ability to generate both dense and sparse variants by changing user specification <lb/>of the data-structures. Figure 13 shows performance of RECUMA&apos;s generated Viterbi kernels when <lb/>tested with both sparse and dense matrix formats while varying the sparsity of the data. As expected, <lb/>a sparse format leads to significantly better performance as sparsity increases. <lb/>Standard libraries, on the other hand, are tied to one or only a few data formats. CXSparse, for <lb/>example, provides handwritten sparse matrix solver kernels, but requires input data be in a CSC <lb/>format. Users who have their input data in alternative format incur a performance penalty from <lb/>converting their data to the expected form. In a situation where input data is given to the users in <lb/>a CSR format, we show in Table 2 the cost of converting the matrix from CSR to CSC, the time <lb/>to calculate CXSparse&apos;s CSC triangular solve, as well as the time to calculate a RECUMA CSR <lb/>triangular solve, in which no format conversion is necessary. The conversion routines are optimized <lb/>Table 2. The ratio of the format conversion time against compute time can be significant. None of the kernels <lb/>shown here use elimination graphs, which are not always useful. Each matrix&apos;s Cholesky factor is used to <lb/>do the tri. solve, as the original matrices are SPD but not lower triangular. <lb/>Triangular Solve and Format Conversion Runtimes (microseconds) <lb/>Matrix <lb/>nonzeros <lb/>CXSparse-ji CSC <lb/>tri solve <lb/>Eigen CSR-&gt;CSC <lb/>conversion <lb/>RECUMA-ij CSR <lb/>tri solve <lb/>RECUMA-ji CSC <lb/>tri solve <lb/>1138_bus <lb/>4054 <lb/>47.371 <lb/>33.824 <lb/>51.25 <lb/>30.767 <lb/>nasa1824 <lb/>30280 <lb/>201.878 <lb/>114.519 <lb/>181 <lb/>141.614 <lb/>bodyy6 <lb/>134208 <lb/>21306 <lb/>418.2 <lb/>20277.2 <lb/>18753.9 <lb/>cbuckle <lb/>676515 <lb/>3291.39 <lb/>1969.58 <lb/>3635.19 <lb/>2946.36 <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:22 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>% Sparsity (% entries that are zero) <lb/>Runtime (μs) <lb/>0.00E+0 <lb/>1.00E+5 <lb/>2.00E+5 <lb/>3.00E+5 <lb/>4.00E+5 <lb/>5.00E+5 <lb/>0 <lb/>20 <lb/>40 <lb/>60 <lb/>80 <lb/>Dense <lb/>Sparse <lb/>Viterbi: Runtime of Using Sparse Format vs Dense Format (μs) <lb/>Fig. 13. N=1600 RECUMA Viterbi performance <lb/>w/ random sparsity pa ern <lb/>Number of States (N) <lb/>Normalized Runtime <lb/>0.0 <lb/>0.5 <lb/>1.0 <lb/>1.5 <lb/>2.0 <lb/>2.5 <lb/>100 <lb/>400 <lb/>800 <lb/>1600 <lb/>Serial <lb/>Parallel <lb/>Viterbi: Parallelization Normalized Runtime <lb/>Fig. 14. Viterbi RECUMA normalized runtime of <lb/>using parallel over serial loops on dense format <lb/>kernels from Eigen. For several problem sizes, these conversions consume a nontrivial portion of <lb/>runtime when compared to the time it takes to run the triangular solve. RECUMA&apos;s generality <lb/>avoids this problem entirely, as a custom kernel can be generated to fit the input data format. <lb/>9.3 Loop Ordering <lb/>The choice of loop ordering will influence a program&apos;s data access patterns and performance. <lb/>RECUMA&apos;s flexibily allows a user to select a loop ordering amenable to a given input data format. <lb/>The effect of loop schedules on sparse codes, in which data access patterns can be irregular, is <lb/>harder to predict than in dense codes. It is thus useful to generate and try different variants. Table 3 <lb/>compares <lb/>and <lb/>Cholesky kernels, which each perform better on different matrices. For the <lb/>triangular solve, Table 2 shows that the schedule always performs best on our examples. <lb/>9.4 Loop Fusion <lb/>Loop fusion occurs automatically in RECUMA due to the greedy nature of the RIN generation <lb/>algorithm. Fusing loops avoids the performance penalties of iterating over the same memory in <lb/>different loops. To demonstrate the benefits of fusion, we use RECUMA to generate a fused kernel <lb/>Table 3. Comparison of the <lb/>Cholesky of RECUMA and the CXSparse Cholesky which also uses an <lb/>strategy. Matrices were chosen with stratified random sampling from SuiteSparse Matrix collection of SPD <lb/>matrices to ensure proper spread of nonzero values <lb/>Cholesky Runtimes (microseconds) <lb/>Matrix <lb/>nonzeros <lb/>CXSparse ikj perf RECUMA ikj perf RECUMA ijk perf <lb/>1138_bus <lb/>4054 <lb/>1976.41 <lb/>1310.05 <lb/>3098.5 <lb/>nasa_10824 <lb/>39208 <lb/>12582.9 <lb/>10123.3 <lb/>21415.1 <lb/>bodyy6 <lb/>134208 <lb/>6415100 <lb/>5916360 <lb/>5765950 <lb/>cbuckle <lb/>676515 <lb/>441425 <lb/>447210 <lb/>448646 <lb/>Table 4. Performance comparison of a fused RECUMA kernel to perform an sparse triangular solve for two <lb/>right-handsides vs calling two separate RECUMA sparse triangular solve kernels. <lb/>Fused Triangular Solve vs Unfused Triangular Solves (microseconds) <lb/>Matrix <lb/>Fused runtime <lb/>Unfused runtime <lb/>Speedup <lb/>1138_bus <lb/>55.85 <lb/>107.4 <lb/>1.92x <lb/>nasa_10824 <lb/>302.83 <lb/>585.32 <lb/>1.93x <lb/>bodyy6 <lb/>20539.6 <lb/>40672.4 <lb/>1.98x <lb/>cbuckle <lb/>3478.72 <lb/>6675.27 <lb/>1.92x <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:23 <lb/></page>

    <body>that computes two sparse triangular solves for the same matrix but different right-hand side <lb/>vectors. We evaluate the fused performance against that of invoking a separate RECUMA kernel <lb/>for each triangular solve. The triangular solve is often dominated by the cost of iterating over the <lb/>matrix . The fused kernel iterates over once, so it performs nearly twice as fast as executing two <lb/>individual triangular solves, as shown in Table 4. <lb/>9.5 Parallelization <lb/>Figure 14 displays performance of the RECUMA code when the compiler performs parallelization <lb/>for the loop in a <lb/>Viterbi kernel. As expected, parallelization does not help for very small <lb/>problems where overheads of spawning threads hurt performance but does improve performance <lb/>as problem size increases. <lb/>10 RELATED WORK <lb/>Other domains have captured large program design spaces with compilers and DSLs for image <lb/>processing [Ragan-Kelley et al. 2013], tensor algebra [Kjolstad et al. 2017], and sparse array pro-<lb/>gramming [Henry et al. 2021]. The Symbolic Fractal Analysis [Menon and Pingali 2004] framework <lb/>has been used to change certain loop orderings of a given dense Cholesky program, but it does <lb/>not generate an imperative programs from declarative recurrences. The use of inductive proofs to <lb/>generate code for different <lb/>forms of a blocked LU decomposition was explored in the FLAME <lb/>project [Gunnels et al. 2001], though it is different in that it operates on recurrences over blocks to <lb/>generate a program that calculates the decomposition with BLAS calls. Ortega [1988a,b] discussed <lb/>the performance and design of different <lb/>forms of the Cholesky and LU decompositions. Sympiler <lb/>[Cheshmi et al. 2017] is a compilation framework that can explore the optimization space of sparse <lb/>Cholesky and triangular solves, generating native code specific to a sparse matrix&apos;s sparsity pattern. <lb/>For certain loop orderings of matrix decompositions, the multifrontal method [Liu 1992] can com-<lb/>pute the decomposition as a matrix assembly problem. The CHOLMOD library targets a supernodal <lb/>[Ng and Peyton 1993] form of the sparse Cholesky decomposition, in which subproblems are <lb/>computed with dense BLAS kernels. The supernodal algorithm uses a one-dimensional tiling of the <lb/>Cholesky decomposition. We leave loop tiling and supernodal optimizations as future work. <lb/>Dynamic programming was formalized by Bellman [2010] to address problems in optimization. <lb/>Gaussian elimination was presented as a type of dynamic program by Lehman [1960]. The process <lb/>of converting a loop nest that recalculates overlapping subproblems into an asymptotically better <lb/>dynamic program is addressed in the framework of simplifying reductions [Gautam and Rajopadhye <lb/>2006; Yang et al. 2021]. Huang [2008] showed that the Viterbi equation, CYK parsing, and certain <lb/>graph problems are all similar dynamic programming algorithms over different semirings. <lb/>Parasail [Daily 2016] can explore the optimization spaces of alignment algorithms and Halide <lb/>has support for optimizing in-place IIR filters [Chaurasia et al. 2015]. Parasail explores different <lb/>vectorization strategies, while other sequence alignment implementations utilize wavefront paral-<lb/>lelism. We leave vectorization of RECUMA kernels and wavefront loop skewing as future work. <lb/>GraphBLAS [Kepner et al. 2016] is a general and performant library that draws upon dualities <lb/>between linear algebra and graphs to implement graph algorithms based on recurrence equations. It <lb/>does not, however, handle dense dynamic programs or solvers. Bellman&apos;s GAP [Sauthoff et al. 2013] <lb/>is another DSL for certain dynamic programs, but like Parasail, is limited to alignment algorithms. <lb/>The polyhedral model [Feautrier 1991; Lamport 1974] applies to general programs and was <lb/>originally applied to finite difference method recurrences [Karp et al. 1967]. The sparse polyhedral <lb/>model extends the polyhedral model with some support for sparse data structures [Strout et al. <lb/>2018]. Our system provides a simple and effective alternative dependency management to that of <lb/>the polyhedral model while generalizing across sparse data structures. Dyna [Eisner et al. 2004] is <lb/></body>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:24 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <body>a DSL that accepts general recurrences and solves them bottom-up. However, it does not generate <lb/>a static loop nest. It computes outputs by placing every recursive subproblem (i.e., every output <lb/>element) into a queue, thus requiring expensive runtime analysis to determine which subproblems <lb/>to solve next. Dyna has similarities to logic programming languages [Kowalski and Clark 2003] like <lb/>PROLOG, which can solve such subproblems bottom-up but must handle dependencies at runtime. <lb/>Dyna can no longer be compiled (as verified by the author), so its exact performance is unknown. <lb/>11 CONCLUSION <lb/>We have described a compiler for a language of general recurrences that can express programs <lb/>across several domains, including dynamic programs, direct matrix solvers, graph algorithms, and <lb/>tensor algebra. The compiler lowers recurrences into imperative loop nests that iterate over dense <lb/>or sparse data structures. Our compiler controls loop ordering and uses induction to manage the <lb/>dependencies inherent to recurrences. We showed how fusion can be applied to recurrences over <lb/>sparse data structures, letting us generate code competitive with handwritten libraries. <lb/>Observing that these problems share a theoretical foundation, we envision a shared ecosystem <lb/>in which optimizations developed for one recurrence algorithm can be easily and readily applied <lb/>to similar algorithms in other fields, allowing for improvements in program performance and <lb/>developer productivity to be shared across the respective domains. <lb/></body>

    <div type="acknowledgement">12 ACKNOWLEDGMENTS <lb/>We would like to thank our anonymous reviewers for their valuable feedback, comments, and <lb/>insights on improving this manuscript. We wish to thank Scott Kovach for valuable discussions <lb/>on recursive computations and for extensive comments on an early draft of the paper. We also <lb/>thank Olivia Hsu, Rohan Yadav, Nathan Zhang, Matthew Sotoudeh, Manya Bansal, AJ Root, Rubens <lb/>Lacouture, Bobby Yan, James Dong, and Alexander Rucker for their comments on an early draft. <lb/>This work was supported in part by PRISM, one of seven centers in JUMP 2.0, a Semiconductor <lb/>Research Corporation (SRC) program sponsored by DARPA. This work was in part supported by <lb/>the National Science Foundation under Grant CCF-2216964. Shiv Sundram was supported by an <lb/>NSF Graduate Research Fellowship. <lb/></div>

    <div type="availability">13 DATA-AVAILABILITY STATEMENT <lb/>Performance results were generated with a publicly available artifact [Sundram et al. 2024] contain-<lb/>ing all benchmarking code and scripts. Instructions for reproducibility are available via an archived <lb/>version of the artifact on Zenodo. Benchmarking results may vary based on the hardware used. <lb/></div>

    <listBibl>REFERENCES <lb/>Richard E Bellman. 2010. Dynamic programming. Princeton university press. <lb/>Aart J.C. Bik, Bixia Zheng, Fredrik Kjolstad, Nicolas Vasilache, Penporn Koanantakool, and Tatiana Shpeisman. 2022. <lb/>Compiler Support for Sparse Tensor Computations in MLIR. ACM Transactions on Architecture and Code Optimization <lb/>(2022). <lb/>Boost. 2002. The Boost Graph Library: User Guide and Reference Manual. Addison-Wesley Longman Publishing Co., Inc., <lb/>USA. <lb/>Gaurav Chaurasia, Jonathan Ragan-Kelley, Sylvain Paris, George Drettakis, and Fredo Durand. 2015. Compiling high <lb/>performance recursive filters. In Proceedings of the 7th conference on high-performance graphics. 85-94. <lb/>Kazem Cheshmi, Shoaib Kamil, Michelle Mills Strout, and Maryam Mehri Dehnavi. 2017. Sympiler: transforming sparse <lb/>matrix codes by decoupling symbolic analysis. In Proceedings of the International Conference for High Performance <lb/>Computing, Networking, Storage and Analysis. 1-13. <lb/>Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2018. Format Abstraction for Sparse Tensor Algebra Compilers. <lb/>Proc. ACM Program. Lang. 2, OOPSLA, Article 123 (oct 2018), 30 pages. https://doi.org/10.1145/3276493 <lb/></listBibl>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <note place="headnote">Compiling Recurrences over Dense and Sparse Arrays <lb/></note>

    <page>103:25 <lb/></page>

    <listBibl>Jeffrey A. Daily. 2016. Parasail: SIMD C library for global, semi-global, and local pairwise sequence alignments. BMC <lb/>Bioinformatics 17 (2 2016). https://doi.org/10.1186/s12859-016-0930-z <lb/>T. A. Davis. 2006a. CXSparse: a Concise eXtended Sparse Matrix Package. https://github.com/DrTimothyAldenDavis/ <lb/>SuiteSparse/tree/dev/CXSparse <lb/>T. A. Davis. 2006b. Direct Methods for Sparse Linear Systems. SIAM, Philadelphia, PA. <lb/>Jason Eisner, Eric Goldlust, and Noah A. Smith. 2004. Dyna: a declarative language for implementing dynamic programs. In <lb/>Annual Meeting of the Association for Computational Linguistics. <lb/>Paul Feautrier. 1991. Dataflow Analysis of Array and Scalar References. International Journal of Parallel Programming 20, 1 <lb/>(1991), 23-53. <lb/>Gautam and S. Rajopadhye. 2006. Simplifying Reductions. SIGPLAN Not. 41, 1 (jan 2006), 30-41. <lb/>John R Gilbert. 1994. Predicting structure in sparse matrix computations. SIAM J. Matrix Anal. Appl. 15, 1 (1994), 62-79. <lb/>John A. Gunnels, Fred G. Gustavson, Greg M. Henry, and Robert A. van de Geijn. 2001. FLAME: Formal Linear Algebra <lb/>Methods Environment. ACM Trans. Math. Softw. 27, 4 (dec 2001), 422-455. https://doi.org/10.1145/504210.504213 <lb/>Rawn Henry, Olivia Hsu, Rohan Yadav, Stephen Chou, Kunle Olukotun, Saman Amarasinghe, and Fredrik Kjolstad. 2021. <lb/>Compilation of sparse array programming models. Proceedings of the ACM on Programming Languages 5, OOPSLA (2021), <lb/>1-29. <lb/>Liang Huang. 2008. Advanced dynamic programming in semiring and hypergraph frameworks. Coling 2008: Advanced <lb/>Dynamic Programming in Computational Linguistics: Theory, Algorithms and Applications-Tutorial notes (2008), 1-18. <lb/>Richard M Karp, Raymond E Miller, and Shmuel Winograd. 1967. The Organization of Computations for Uniform Recurrence <lb/>Equations. J. ACM 14, 3 (1967), 563-590. <lb/>Jeremy Kepner, Peter Aaltonen, David Bader, Aydin Buluç, Franz Franchetti, John Gilbert, Dylan Hutchison, Manoj Kumar, <lb/>Andrew Lumsdaine, Henning Meyerhenke, et al. 2016. Mathematical foundations of the GraphBLAS. In 2016 IEEE High <lb/>Performance Extreme Computing Conference (HPEC). IEEE, 1-9. <lb/>Fredrik Kjolstad, Peter Ahrens, Shoaib Kamil, and Saman Amarasinghe. 2019. Tensor Algebra Compilation with Workspaces. <lb/>In Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization (Washington, DC, <lb/>USA) (CGO 2019). IEEE Press, 180-192. <lb/>Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman P. Amarasinghe. 2017. The tensor algebra compiler. <lb/>Proc. ACM Program. Lang. 1, OOPSLA (2017), 77:1-77:29. <lb/>Fredrik Berg Kjølstad. 2020. Sparse tensor algebra compilation. Ph. D. Dissertation. Massachusetts Institute of Technology. <lb/>Robert Kowalski and Keith L Clark. 2003. Logic programming. In Encyclopedia of Computer Science. 1017-1031. <lb/>Leslie Lamport. 1974. The Parallel Execution of DO Loops. Commun. ACM 17, 2 (1974), 83-93. <lb/>R Sherman Lehman. 1960. DYNAMIC PROGRAMMING AND GAUSSIAN ELIMINATION. Technical Report. RAND CORP <lb/>SANTA MONICA CALIF. <lb/>Joseph W Liu. 1986. A compact row storage scheme for Cholesky factors using elimination trees. ACM Transactions on <lb/>Mathematical Software (TOMS) 12, 2 (1986), 127-148. <lb/>Joseph W. H. Liu. 1992. The Multifrontal Method for Sparse Matrix Solution: Theory and Practice. SIAM Rev. 34 (1992), <lb/>82-109. <lb/>Vijay Menon and Keshav Pingali. 2004. Look left, look right, look left again: An application of fractal symbolic analysis to <lb/>linear algebra code restructuring. International Journal of Parallel Programming 32 (2004), 501-523. <lb/>Esmond Ng and Barry W. Peyton. 1993. A Supernodal Cholesky Factorization Algorithm for Shared-Memory <lb/>Multiprocessors. SIAM Journal on Scientific Computing 14, 4 (1993), 761-769. https://doi.org/10.1137/0914048 <lb/>arXiv:https://doi.org/10.1137/0914048 <lb/>James M. Ortega. 1988a. The ijk forms of factorization methods I. Vector computers. Parallel Comput. 7 (1988), 135-147. <lb/>James M. Ortega. 1988b. The ijk forms of factorization methods II. Vector computers. Parallel Comput. 7 (1988), 149-162. <lb/>Louis-Noel Pouchet. 2016. PolyBench. http://web.cs.ucla.edu/~pouchet/software/polybench/ <lb/>Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman P. Amarasinghe. 2013. <lb/>Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. In <lb/>ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;13, Seattle, WA, USA, June 16-19, <lb/>2013, Hans-Juergen Boehm and Cormac Flanagan (Eds.). ACM, 519-530. <lb/>MMG Ricci and Tullio Levi-Civita. 1900. Méthodes de calcul différentiel absolu et leurs applications. Math. Ann. 54, 1-2 <lb/>(1900), 125-201. <lb/>Georg Sauthoff, Mathias Möhl, Stefan Janssen, and Robert Giegerich. 2013. <lb/>Bellman&apos;s GAP-a language <lb/>and compiler for dynamic programming in sequence analysis. <lb/>Bioinformatics 29, 5 (01 2013), 551-560. <lb/>arXiv:https://academic.oup.com/bioinformatics/article-pdf/29/5/551/16919063/btt022.pdf <lb/>Ryan Senanayake, Changwan Hong, Ziheng Wang, Amalee Wilson, Stephen Chou, Shoaib Kamil, Saman Amarasinghe, <lb/>and Fredrik Kjolstad. 2020. A Sparse Iteration Space Transformation Framework for Sparse Tensor Algebra. Proc. ACM <lb/>Program. Lang. 4, OOPSLA, Article 158 (nov 2020), 30 pages. https://doi.org/10.1145/3428226 <lb/></listBibl>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. <lb/></note>

    <page>103:26 <lb/></page>

    <note place="headnote">Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad <lb/></note>

    <listBibl>Michelle Mills Strout, Mary Hall, and Catherine Olschanowsky. 2018. The sparse polyhedral framework: Composing <lb/>compiler-generated inspector-executor code. Proc. IEEE 106, 11 (2018), 1921-1934. <lb/>Shiv Sundram, Muhammad Usman Tariq, and Fredrik Kjolstad. 2024. Artifact for OOPSLA 2024 Paper: Compiling Recurrences <lb/>over Dense and Sparse Arrays (version 1). https://doi.org/10.5281/zenodo.10774458 <lb/>Ruiqin Tian, Luanzheng Guo, Jiajia Li, Bin Ren, and Gokcen Kestor. 2021. A High Performance Sparse Tensor Algebra <lb/>Compiler in MLIR. (12 2021). <lb/>A. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions <lb/>on Information Theory 13, 2 (1967), 260-269. https://doi.org/10.1109/TIT.1967.1054010 <lb/>Cambridge Yang, Eric Atkinson, and Michael Carbin. 2021. Simplifying Dependent Reductions in the Polyhedral Model. <lb/>Proc. ACM Program. Lang. 5, POPL, Article 20 (jan 2021), 33 pages. <lb/>Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze. 2023. SparseTIR: Composable abstractions for sparse <lb/>compilation in deep learning. In Proceedings of the 28th ACM International Conference on Architectural Support for <lb/>Programming Languages and Operating Systems, Volume 3. 660-678. <lb/></listBibl>

    <front>Received 21-OCT-2023; accepted 2024-02-24 <lb/></front>

    <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 103. Publication date: April 2024. </note>


	</text>

</TEI>