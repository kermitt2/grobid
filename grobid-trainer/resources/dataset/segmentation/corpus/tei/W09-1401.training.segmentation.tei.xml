<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Proceedings of the Workshop on BioNLP: Shared Task, pages 1-9, <lb/>Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics <lb/>Overview of BioNLP&apos;09 Shared Task on Event Extraction <lb/>Jin-Dong Kim * Tomoko Ohta * Sampo Pyysalo * Yoshinobu Kano * Jun&apos;ichi Tsujii * † ‡ <lb/> * Department of Computer Science, University of Tokyo, Tokyo, Japan <lb/> † School of Computer Science, University of Manchester, Manchester, UK <lb/> ‡ National Centre for Text Mining, University of Manchester, Manchester, UK <lb/>{jdkim,okap,smp,kano,tsujii}@is.s.u-tokyo.ac.jp <lb/>Abstract <lb/>The paper presents the design and implemen-<lb/>tation of the BioNLP&apos;09 Shared Task, and <lb/>reports the final results with analysis. The <lb/>shared task consists of three sub-tasks, each of <lb/>which addresses bio-molecular event extrac-<lb/>tion at a different level of specificity. The data <lb/>was developed based on the GENIA event cor-<lb/>pus. The shared task was run over 12 weeks, <lb/>drawing initial interest from 42 teams. Of <lb/>these teams, 24 submitted final results. The <lb/>evaluation results are encouraging, indicating <lb/>that state-of-the-art performance is approach-<lb/>ing a practically applicable level and revealing <lb/>some remaining challenges. <lb/></front>

			<body>1 Introduction <lb/>The history of text mining (TM) shows that shared <lb/>tasks based on carefully curated resources, such <lb/>as those organized in the MUC (Chinchor, 1998), <lb/>TREC (Voorhees, 2007) and ACE (Strassel et al., <lb/>2008) events, have significantly contributed to the <lb/>progress of their respective fields. This has also been <lb/>the case in bio-TM. Examples include the TREC Ge-<lb/>nomics track (Hersh et al., 2007), JNLPBA (Kim et <lb/>al., 2004), LLL (Nédellec, 2005), and BioCreative <lb/>(Hirschman et al., 2007). While the first two ad-<lb/>dressed bio-IR (information retrieval) and bio-NER <lb/>(named entity recognition), respectively, the last two <lb/>focused on bio-IE (information extraction), seeking <lb/>relations between bio-molecules. With the emer-<lb/>gence of NER systems with performance capable of <lb/>supporting practical applications, the recent interest <lb/>of the bio-TM community is shifting toward IE. <lb/>Similarly to LLL and BioCreative, the <lb/>BioNLP&apos;09 Shared Task (the BioNLP task, here-<lb/>after) also addresses bio-IE, but takes a definitive <lb/>step further toward finer-grained IE. While LLL and <lb/>BioCreative focus on a rather simple representation <lb/>of relations of bio-molecules, i.e. protein-protein <lb/>interactions (PPI), the BioNLP task concerns the <lb/>detailed behavior of bio-molecules, characterized as <lb/>bio-molecular events (bio-events). The difference in <lb/>focus is motivated in part by different applications <lb/>envisioned as being supported by the IE methods. <lb/>For example, BioCreative aims to support curation <lb/>of PPI databases such as MINT (Chatr-aryamontri <lb/>et al., 2007), for a long time one of the primary tasks <lb/>of bioinformatics. The BioNLP task aims to support <lb/>the development of more detailed and structured <lb/>databases, e.g. pathway (Bader et al., 2006) or Gene <lb/>Ontology Annotation (GOA) (Camon et al., 2004) <lb/>databases, which are gaining increasing interest <lb/>in bioinformatics research in response to recent <lb/>advances in molecular biology. <lb/>As the first shared task of its type, the BioNLP <lb/>task aimed to define a bounded, well-defined bio-<lb/>event extraction task, considering both the actual <lb/>needs and the state of the art in bio-TM technology <lb/>and to pursue it as a community-wide effort. The <lb/>key challenge was in finding a good balance between <lb/>the utility and the feasibility of the task, which was <lb/>also limited by the resources available. Special con-<lb/>sideration was given to providing evaluation at di-<lb/>verse levels and aspects, so that the results can drive <lb/>continuous efforts in relevant directions. The pa-<lb/>per discusses the design and implementation of the <lb/>BioNLP task, and reports the results with analysis. <lb/></body>

			<page>1 <lb/></page>

			<body>Type <lb/>Primary Args. <lb/>Second. Args. <lb/>Gene expression <lb/>T(P) <lb/>Transcription <lb/>T(P) <lb/>Protein catabolism <lb/>T(P) <lb/>Phosphorylation <lb/>T(P) <lb/>Site <lb/>Localization <lb/>T(P) <lb/>AtLoc, ToLoc <lb/>Binding <lb/>T(P)+ <lb/>Site+ <lb/>Regulation <lb/>T(P/Ev), C(P/Ev) Site, CSite <lb/>Positive regulation <lb/>T(P/Ev), C(P/Ev) Site, CSite <lb/>Negative regulation T(P/Ev), C(P/Ev) Site, CSite <lb/>Table 1: Event types and their arguments. The type of the <lb/>filler entity is specified in parenthesis. The filler entity <lb/>of the secondary arguments are all of Entity type which <lb/>represents any entity but proteins: T=Theme, C=Cause, <lb/>P=Protein, Ev=Event. <lb/>2 Task setting <lb/>To focus efforts on the novel aspects of the event <lb/>extraction task, is was assumed that named entity <lb/>recognition has already been performed and the task <lb/>was begun with a given set of gold protein anno-<lb/>tation. This is the only feature of the task setting <lb/>that notably detracts from its realism. However, <lb/>given that state-of-the-art protein annotation meth-<lb/>ods show a practically applicable level of perfor-<lb/>mance, i.e. 88% F-score (Wilbur et al., 2007), we <lb/>believe the choice is reasonable and has several ad-<lb/>vantages, including focus on event extraction and ef-<lb/>fective evaluation and analysis. <lb/>2.1 Target event types <lb/>Table 1 shows the event types addressed in the <lb/>BioNLP task. The event types were selected from <lb/>the GENIA ontology, with consideration given to <lb/>their importance and the number of annotated in-<lb/>stances in the GENIA corpus. The selected event <lb/>types all concern protein biology, implying that they <lb/>take proteins as their theme. The first three types <lb/>concern protein metabolism, i.e. protein production <lb/>and breakdown. Phosphorylation is a representa-<lb/>tive protein modification event, and Localization and <lb/>Binding are representative fundamental molecular <lb/>events. Regulation (including its sub-types, Posi-<lb/>tive and Negative regulation) represents regulatory <lb/>events and causal relations. The last five are uni-<lb/>versal but frequently occur on proteins. For the bio-<lb/>logical interpretation of the event types, readers are <lb/>referred to Gene Ontology (GO) and the GENIA on-<lb/>tology. <lb/>The failure of p65 translocation to the nucleus . . . <lb/>T3 <lb/>(Protein, 40-46) <lb/>T2 <lb/>(Localization, 19-32) <lb/>E1 <lb/>(Type:T2, Theme:T3, ToLoc:T1) <lb/>T1 <lb/>(Entity, 15-18) <lb/>M1 (Negation E1) <lb/>Figure 1: Example event annotation. The protein an-<lb/>notation T3 is given as a starting point. The extraction <lb/>of annotation in bold is required for Task 1, T1 and the <lb/>ToLoc:T1 argument for Task 2, and M1 for Task 3. <lb/>As shown in Table 1, the theme or themes of all <lb/>events are considered primary arguments, that is, ar-<lb/>guments that are critical to identifying the event. For <lb/>regulation events, the entity or event stated as the <lb/>cause of the regulation is also regarded as a primary <lb/>argument. For some event types, other arguments <lb/>detailing of the events are also defined (Secondary <lb/>Args. in Table 1). <lb/>From a computational point of view, the event <lb/>types represent different levels of complexity. When <lb/>only primary arguments are considered, the first five <lb/>event types require only unary arguments, and the <lb/>task can be cast as relation extraction between a <lb/>predicate (event trigger) and an argument (Protein). <lb/>The Binding type is more complex in requiring the <lb/>detection of an arbitrary number of arguments. Reg-<lb/>ulation events always take a Theme argument and, <lb/>when expressed, also a Cause argument. Note that a <lb/>Regulation event may take another event as its theme <lb/>or cause, a unique feature of the BioNLP task com-<lb/>pared to other event extraction tasks, e.g. ACE. <lb/>2.2 Representation <lb/>In the BioNLP task, events are expressed using three <lb/>different types of entities. Text-bound entities (t-<lb/>entities hereafter) are represented as text spans with <lb/>associated class information. The t-entities include <lb/>event triggers (Localization, Binding, etc), protein <lb/>references (Protein) and references to other entities <lb/>(Entity). A t-entity is represented by a pair, (entity-<lb/>type, text-span), and assigned an id with the pre-<lb/>fix &quot;T&quot;, e.g. T1-T3 in Figure 1. An event is ex-<lb/>pressed as an n-tuple of typed t-entities, and has <lb/>a id with prefix &quot;E&quot;, e.g. E1. An event modifi-<lb/>cation is expressed by a pair, (predicate-negation-<lb/>or-speculation, event-id), and has an id with prefix <lb/>&quot;M&quot;, e.g. M1. <lb/></body>

			<page>2 <lb/></page>

			<body>Item <lb/>Training <lb/>Devel. <lb/>Test <lb/>Abstract <lb/>800 <lb/>150 <lb/>260 <lb/>Sentence <lb/>7,449 <lb/>1,450 <lb/>2,447 <lb/>Word <lb/>176,146 <lb/>33,937 <lb/>57,367 <lb/>Event <lb/>8,597 / 8,615 1,809 / 1,815 3,182 / 3,193 <lb/>Table 2: Statistics of the data sets. <lb/>For events, <lb/>Task1/Task2 shown separately as secondary arguments <lb/>may introduce additional differentiation of events. <lb/>2.3 Subtasks <lb/>The BioNLP task targets semantically rich event ex-<lb/>traction, involving the extraction of several different <lb/>classes of information. To facilitate evaluation on <lb/>different aspects of the overall task, the task is di-<lb/>vided to three sub-tasks addressing event extraction <lb/>at different levels of specificity. <lb/>Task 1. Core event detection detection of typed, <lb/>text-bound events and assignment of given pro-<lb/>teins as their primary arguments. <lb/>Task 2. Event enrichment recognition of sec-<lb/>ondary arguments that further specify the <lb/>events extracted in Task 1. <lb/>Task 3. Negation/Speculation detection detection <lb/>of negations and speculation statements <lb/>concerning extracted events. <lb/>Task 1 serves as the backbone of the shared task and <lb/>is mandatory for all participants. Task 2 involves the <lb/>recognition of Entity type t-entities and assignment <lb/>of those as secondary event arguments. Task 3 ad-<lb/>dresses the recognition of negated or speculatively <lb/>expressed events without specific binding to text. An <lb/>example is given in Fig. 1. <lb/>3 Data preparation <lb/>The BioNLP task data were prepared based on the <lb/>GENIA event corpus. The data for the training and <lb/>development sets were derived from the publicly <lb/>available event corpus (Kim et al., 2008), and the <lb/>data for the test set from an unpublished portion of <lb/>the corpus. Table 2 shows statistics of the data sets. <lb/>For data preparation, in addition to filtering out <lb/>irrelevant annotations from the original GENIA cor-<lb/>pus, some new types of annotation were added to <lb/>make the event annotation more appropriate for the <lb/>purposes of the shared task. The following sections <lb/>describe the key changes to the corpus. <lb/>3.1 Gene-or-gene-product annotation <lb/>The named entity (NE) annotation of the GENIA <lb/>corpus has been somewhat controversial due to dif-<lb/>ferences in annotation principles compared to other <lb/>biomedical NE corpora. For instance, the NE an-<lb/>notation in the widely applied GENETAG corpus <lb/>(Tanabe et al., 2005) does not differentiate proteins <lb/>from genes, while GENIA annotation does. Such <lb/>differences have caused significant inconsistency in <lb/>methods and resources following different annota-<lb/>tion schemes. To remove or reduce the inconsis-<lb/>tency, GENETAG-style NE annotation, which we <lb/>term gene-or-gene-product (GGP) annotation, has <lb/>been added to the GENIA corpus, with appropriate <lb/>revision of the original annotation. For details, we <lb/>refer to (Ohta et al., 2009). The NE annotation used <lb/>in the BioNLP task data is based on this annotation. <lb/>3.2 Argument revision <lb/>The GENIA event annotation was made based on <lb/>the GENIA event ontology, which uses a loose typ-<lb/>ing system for the arguments of each event class. <lb/>For example, in Figure 2(a), it is expressed that <lb/>the binding event involves two proteins, TRAF2 <lb/>and CD40, and that, in the case of CD40, its cy-<lb/>toplasmic domain takes part in the binding. With-<lb/>out constraints on the type of theme arguments, <lb/>the following two annotations are both legitimate: <lb/>(Type:Binding, Theme:TRAF2, Theme:CD40) <lb/>(Type:Binding, Theme:TRAF2, <lb/>Theme:CD40 cytoplasmic domain) <lb/>The two can be seen as specifying the same event <lb/>at different levels of specificity 1 . Although both al-<lb/>ternatives are reasonable, the need to have consis-<lb/>tent training and evaluation data requires a consis-<lb/>tent choice to be made for the shared task. <lb/>Thus, we fix the types of all non-event <lb/>primary arguments to be proteins (specifically <lb/>GGPs). For GENIA event annotations involving <lb/>themes other than proteins, additional argument <lb/>types were introduced, for example, as follows: <lb/></body>

			<note place="footnote">1 In the GENIA event annotation guidelines, annotators are <lb/>instructed to choose the more specific alternative, thus the sec-<lb/>ond alternative for the example case in Fig. 2(a). <lb/></note>

			<page>3 <lb/></page>

			<body>(a) <lb/>TRAF2 is a … which binds to the CD40 cytoplasmic domain <lb/>GGP <lb/>GGP <lb/>PDR <lb/>(b) <lb/>HMG-I binds to GATA motifs <lb/>GGP <lb/>DDR <lb/>(c) <lb/>alpha B2 bound the PEBP2 site within the GM-CSF promoter <lb/>GGP <lb/>GGP <lb/>DDR <lb/>DDR <lb/>Figure 2: Entity annotation to example sentences <lb/>from (a) PMID10080948, (b) PMID7575565, and (c) <lb/>PMID7605990 (simplified). <lb/>(a) <lb/>Ah receptor recognizes the B cell transcription factor, BSAP <lb/>(b) <lb/>Grf40 binds to linker for activation of T cells (LAT) <lb/>(c) <lb/>expression of p21(WAF1/CIP1) and p27(KIP1) <lb/>(d) <lb/>included both p50/p50 and p50/p65 dimers <lb/>(e) <lb/>IL-4 Stat, also known as Stat6 <lb/>Figure 3: Equivalent entities in example sentences from <lb/>(a) PMID7541987 (simplified), (b) PMID10224278, (c) <lb/>PMID10090931, (d) PMID9243743, (e) PMID7635985. <lb/>(Type:Binding, Theme1:TRAF2, Theme2:CD40, <lb/>Site2:cytoplasmic domain) <lb/>Note that the protein, CD40, and its domain, cyto-<lb/>plasmic domain, are associated by argument num-<lb/>bering. To resolve issues related to the mapping <lb/>between proteins and related entities systematically, <lb/>we introduced partial static relation annotation for <lb/>relations such as Part-Whole, drawing in part on <lb/>similar annotation of the BioInfer corpus (Pyysalo <lb/>et al., 2007). For details of this part of the revision <lb/>process, we refer to (Pyysalo et al., 2009). <lb/>Figure 2 shows some challenging cases. In (b), <lb/>the site GATA motifs is not identified as an argument <lb/>of the binding event, because the protein containing <lb/>it is not stated. In (c), among the two sites (PEBP2 <lb/>site and promoter) of the gene GM-CSF, only the <lb/>more specific one, PEBP2, is annotated. <lb/>3.3 Equivalent entity references <lb/>Alternative names for the same object are fre-<lb/>quently introduced in biomedical texts, typically <lb/>through apposition. This is illustrated in Figure 3(a), <lb/>where the two expressions B cell transcription fac-<lb/>tor and BSAP are in apposition and refer to the <lb/>same protein. Consequently, in this case the fol-<lb/>lowing two annotations represent the same event: <lb/>(Type:Binding, Theme:Ah receptor, <lb/>Theme:B cell transcription factor) <lb/>(Type:Binding, Theme:Ah receptor, Theme:BSAP) <lb/>In the GENIA event corpus only one of these is an-<lb/>notated, with preference given to shorter names over <lb/>longer descriptive ones. Thus of the above exam-<lb/>ple events, the latter would be annotated. How-<lb/>ever, as both express the same event, in the shared <lb/>task evaluation either alternative was accepted as <lb/>correct extraction of the event. In order to im-<lb/>plement this aspect of the evaluation, expressions <lb/>of equivalent entities were annotated as follows: <lb/>Eq (B cell transcription factor, BSAP) <lb/>The equivalent entity annotation in the revised GE-<lb/>NIA corpus covers also cases other than simple ap-<lb/>position, illustrated in Figure 3. A frequent case in <lb/>biomedical literature involves use of the slash sym-<lb/>bol (&quot;/&quot;) to state synonyms. The slash symbol is <lb/>ambiguous as it is used also to indicate dimerized <lb/>proteins. In the case of p50/p50, the two p50 are <lb/>annotated as equivalent because they represent the <lb/>same proteins at the same state. Note that although <lb/>rare, also explicitly introduced aliases are annotated, <lb/>as in Figure 3(e). <lb/>4 Evaluation <lb/>For the evaluation, the participants were given the <lb/>test data with gold annotation only for proteins. The <lb/>evaluation was then carried out by comparing the <lb/>annotation predicted by each participant to the gold <lb/>annotation. For the comparison, equality of anno-<lb/>tations is defined as described in Section 4.1. The <lb/>evaluation results are reported using the standard <lb/>recall/precision/f-score metrics, under different cri-<lb/>teria defined through the equalities. <lb/>4.1 Equalities and Strict matching <lb/>Equality of events is defined as follows: <lb/>Event Equality equality holds between any two <lb/>events when (1) the event types are the same, <lb/>(2) the event triggers are the same, and (3) the <lb/>arguments are fully matched. <lb/></body>

			<page>4 <lb/></page>

			<body>A full matching of arguments between two events <lb/>means there is a perfect 1-to-1 mapping between the <lb/>two sets of arguments. Equality of individual argu-<lb/>ments is defined as follows: <lb/>Argument Equality equality holds between any <lb/>two arguments when (1) the role types are the <lb/>same, and (2-1) both are t-entities and equality <lb/>holds between them, or (2-2) both are events <lb/>and equality holds between them. <lb/>Due to the condition (2-2), event equality is defined <lb/>recursively for events referring to events. Equality <lb/>of t-entities is defined as follows: <lb/>T-entity Equality equality holds between any two <lb/>t-entities when (1) the entity types are the same, <lb/>and (2) the spans are the same. <lb/>Any two text spans (beg1, end1) and (beg2, end2), <lb/>are the same iff beg1 = beg2 and end1 = end2. <lb/>Note that the event triggers are also t-entities thus <lb/>their equality is defined by the t-entity equality. <lb/>4.2 Evaluation modes <lb/>Various evaluation modes can be defined by varying <lb/>equivalence criteria. In the following, we describe <lb/>three fundamental variants applied in the evaluation. <lb/>Strict matching The strict matching mode requires <lb/>exact equality, as defined in section 4.1. As some <lb/>of its requirements may be viewed as unnecessarily <lb/>precise, practically motivated relaxed variants, de-<lb/>scribed in the following, are also applied. <lb/>Approximate span matching The approximate <lb/>span matching mode is defined by relaxing the <lb/>requirement for text span matching for t-entities. <lb/>Specifically, a given span is equivalent to a gold <lb/>span if it is entirely contained within an extension <lb/>of the gold span by one word both to the left and <lb/>to the right, that is, beg1 ≥ ebeg2 and end1 ≤ <lb/>eend2, where (beg1, end1) is the given span and <lb/>(ebeg2, eend2) is the extended gold span. <lb/>Approximate recursive matching In strict match-<lb/>ing, for a regulation event to be correct, the events it <lb/>refers to as theme or cause must also be be strictly <lb/>correct. The approximate recursive matching mode <lb/>is defined by relaxing the requirement for recursive <lb/>event matching, so that an event can match even <lb/>if the events it refers to are only partially correct. <lb/>Event <lb/>Release date <lb/>Announcement Dec 8 <lb/>Sample data <lb/>Dec 15 <lb/>Training data <lb/>Jan 19 → 21, Feb 2 (rev1), Feb 10 (rev2) <lb/>Devel. data <lb/>Feb 7 <lb/>Test data <lb/>Feb 22 → Mar 2 <lb/>Submission <lb/>Mar 2 → Mar 9 <lb/>3: Shared task schedule. The arrows indicate a <lb/>change of schedule. <lb/>Specifically, for partial matching, only Theme argu-<lb/>ments are considered: events can match even if re-<lb/>ferred events differ in non-Theme arguments. <lb/>5 Schedule <lb/>The BioNLP task was held for 12 weeks, from the <lb/>sample data release to the final submission. It in-<lb/>cluded 5 weeks of system design period with sam-<lb/>ple data, 6 weeks of system development period with <lb/>training and development data, and a 1 week test pe-<lb/>riod. The system development period was originally <lb/>planned for 5 weeks but extended by 1 week due to <lb/>the delay of the training data release and the revi-<lb/>sion. Table 3 shows key dates of the schedule. <lb/>6 Supporting Resources <lb/>To allow participants to focus development efforts <lb/>on novel aspects of event extraction, we prepared <lb/>publicly available BioNLP resources readily avail-<lb/>able for the shared task. Several fundamental <lb/>BioNLP tools were provided through U-Compare <lb/>(Kano et al., 2009) 2 , which included tools for to-<lb/>kenization, sentence segmentation, part-of-speech <lb/>tagging, chunking and syntactic parsing. <lb/>Participants were also provided with the syntactic <lb/>analyses created by a selection of parsers. We ap-<lb/>plied two mainstream Penn Treebank (PTB) phrase <lb/>structure parsers: the Bikel parser 3 , implementing <lb/>Collins&apos; parsing model (Bikel, 2004) and trained <lb/>on PTB, and the reranking parser of (Charniak <lb/>and Johnson, 2005) with the self-trained biomed-<lb/>ical parsing model of (McClosky and Charniak, <lb/>2008) 4 . We also applied the GDep 5 , native de-<lb/>pendency parser trained on the GENIA Treebank <lb/></body>

			<note place="footnote">2 http://u-compare.org/ <lb/>3 http://www.cis.upenn.edu/∼dbikel/software.html <lb/>4 http://www.cs.brown.edu/∼dmcc/biomedical.html <lb/>5 <lb/></note>

			<page>5 <lb/></page>

			<body>NLP <lb/>Task <lb/>Team <lb/>Task <lb/>Org <lb/>Word <lb/>Chunking <lb/>Parsing <lb/>Trigger <lb/>Argument <lb/>Ext. Resources <lb/>UTurku <lb/>1--<lb/>3C+2BI <lb/>Porter <lb/>MC <lb/>SVM <lb/>SVM (SVMlight) <lb/>JULIELab <lb/>1--1C+2L+2B <lb/>OpenNLP <lb/>OpenNLP <lb/>GDep <lb/>Dict+Stat <lb/>SVM(libSVM) <lb/>UniProt, Mesh, <lb/>Porter <lb/>ME(Mallet) <lb/>GOA, UMLS <lb/>ConcordU <lb/>1-3 <lb/>3C <lb/>Stanford <lb/>Stanford <lb/>Dict+Stat <lb/>Rules <lb/>WordNet, VerbNet, <lb/>UMLS <lb/>UT+DBCLS 12-<lb/>2C <lb/>Porter <lb/>MC <lb/>Dict <lb/>MLN(thebeast) <lb/>CCG <lb/>VIBGhent <lb/>1-3 <lb/>2C+1B <lb/>Porter, <lb/>Stanford <lb/>Dict <lb/>SVM(libSVM) <lb/>UTokyo <lb/>1--<lb/>3C <lb/>GTag <lb/>Dict <lb/>ME(liblinear) <lb/>UIMA <lb/>Enju <lb/>UNSW <lb/>1--<lb/>1C+1B <lb/>GDep <lb/>CRF <lb/>Rules <lb/>WordNet, MetaMap <lb/>UZurich <lb/>1--<lb/>3C <lb/>LingPipe, <lb/>LTChunk <lb/>Pro3Gres <lb/>Dict <lb/>Rules <lb/>Morpha <lb/>ASU+HU+BU 123 <lb/>6C+2BI <lb/>Porter <lb/>BioLG, <lb/>Dict <lb/>Rules <lb/>Lucene <lb/>Charniak <lb/>Rules <lb/>Cam <lb/>1--<lb/>3C <lb/>Porter <lb/>RASP <lb/>Dict <lb/>Rules <lb/>UAntwerp <lb/>12-<lb/>3C <lb/>GTag <lb/>GDep <lb/>MBL <lb/>MBL(TiMBL) <lb/>Rules <lb/>UNIMAN <lb/>1--<lb/>4C+2BI <lb/>Porter <lb/>GDep <lb/>Dict, CRF <lb/>SVM <lb/>MeSH, GO <lb/>GTag <lb/>Rules <lb/>SCAI <lb/>1--<lb/>1C <lb/>Rules <lb/>UAveiro <lb/>1--<lb/>1C+1L <lb/>NooJ <lb/>NooJ <lb/>Rules <lb/>BioLexicon <lb/>USzeged <lb/>1-3 <lb/>3C+1B <lb/>GTag <lb/>Dict, VSM <lb/>C4.5(WEKA) <lb/>BioScope <lb/>Rules <lb/>NICTA <lb/>1-3 <lb/>4C <lb/>GTag <lb/>ERG <lb/>CRF(CRF++) <lb/>Rules <lb/>JULIE <lb/>CNBMadrid <lb/>12-<lb/>2C+1B <lb/>Porter, <lb/>GTag <lb/>CBR <lb/>GTag <lb/>Rules <lb/>CCP-BTMG 123 <lb/>7C <lb/>LingPipe <lb/>LingPipe OpenDMAP LingPipe, CM <lb/>Rules <lb/>GO, SO, MIO, <lb/>UIMA <lb/>CIPS-ASU <lb/>1--<lb/>3C <lb/>MontyTagger Custom <lb/>Stanford <lb/>CRF(ABNER) <lb/>Rules, <lb/>NB(WEKA) <lb/>UMich <lb/>1--<lb/>2C <lb/>Stanford <lb/>MC <lb/>Dict <lb/>SVM(SVMlight) <lb/>PIKB <lb/>1--<lb/>5C+2B <lb/>MIRA <lb/>MIRA <lb/>KoreaU <lb/>1--<lb/>5C <lb/>GTag <lb/>GDep <lb/>Rules, ME <lb/>ME <lb/>WSJ <lb/>Table 4: Profiles of the participants: GTag=GENIAtagger, MLN=Markov Logic Network, UMLS=UMLS SPE-<lb/>CIALIST Lexicon/tools, MC=McClosky-Charniak, GDep=Genia Dependency Parser, Stanford=Stanford Parser, <lb/>CBR=Case-Based Reasoning, CM=ConceptMapper. <lb/>(Tateisi et al., 2005), and a version of the C&amp;C CCG <lb/>deep parser 6 adapted to biomedical text (Rimell and <lb/>Clark, 2008). <lb/>The text of all documents was segmented and to-<lb/>kenized using the GENIA Sentence Splitter and the <lb/>GENIA Tagger, provided by U-Compare. The same <lb/>segmentation was enforced for all parsers, which <lb/>were run using default settings. Both the native out-<lb/>put of each parser and a representation in the popular <lb/>Stanford Dependency (SD) format (de Marneffe et <lb/>al., 2006) were provided. The SD representation was <lb/>created using the Stanford tools 7 to convert from the <lb/>PTB scheme, the custom conversion introduced by <lb/>(Rimell and Clark, 2008) for the C&amp;C CCG parser, <lb/>and a simple format-only conversion for GDep. <lb/>7 Results and Discussion <lb/>7.1 Participation <lb/>In total, 42 teams showed interest in the shared task <lb/>and registered for participation, and 24 teams sub-<lb/></body>

			<note place="footnote">6 http://svn.ask.it.usyd.edu.au/trac/candc/wiki <lb/>7 http://nlp.stanford.edu/software/lex-parser.shtml <lb/></note>

			<body>mitted final results. All 24 teams participated in the <lb/>obligatory Task 1, six in each of Tasks 2 and 3, and <lb/>two teams completed all the three tasks. <lb/>Table 4 shows a profile of the 22 final teams, <lb/>excepting two who wished to remain anonymous. <lb/>A brief examination on the team organization (the <lb/>Org column) shows a computer science background <lb/>(C) to be most frequent among participants, with <lb/>less frequent participation from bioinformaticians <lb/>(BI), biologists (B) and liguists (L). This may be <lb/>attributed in part to the fact that the event extrac-<lb/>tion task required complex computational modeling. <lb/>The role of computer scientists may be emphasized <lb/>in part due to the fact that the task was novel to most <lb/>participants, requiring particular efforts in frame-<lb/>work design and implementation and computational <lb/>resources. This also suggests there is room for im-<lb/>provement from more input from biologists. <lb/>7.2 Evaluation results <lb/>The final evaluation results of Task 1 are shown in <lb/>Table 5. The results on the five event types involv-<lb/></body>

			<page>6 <lb/></page>

			<body>Team <lb/>Simple Event <lb/>Binding <lb/>Regulation <lb/>All <lb/>UTurku <lb/>64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95 <lb/>JULIELab <lb/>59.81 / 79.80 / 68.38 49.57 / 35.25 / 41.20 35.03 / 34.18 / 34.60 45.82 / 47.52 / 46.66 <lb/>ConcordU <lb/>49.75 / 81.44 / 61.76 20.46 / 40.57 / 27.20 27.47 / 49.89 / 35.43 34.98 / 61.59 / 44.62 <lb/>UT+DBCLS <lb/>55.75 / 72.74 / 63.12 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35 <lb/>VIBGhent <lb/>54.48 / 79.31 / 64.59 38.04 / 38.60 / 38.32 17.36 / 31.61 / 22.41 33.41 / 51.55 / 40.54 <lb/>UTokyo <lb/>45.69 / 72.19 / 55.96 34.58 / 50.63 / 41.10 14.22 / 34.26 / 20.09 28.13 / 53.56 / 36.88 <lb/>UNSW <lb/>45.85 / 69.94 / 55.39 23.63 / 37.27 / 28.92 16.58 / 28.27 / 20.90 28.22 / 45.78 / 34.92 <lb/>UZurich <lb/>44.92 / 66.62 / 53.66 30.84 / 37.28 / 33.75 14.82 / 30.21 / 19.89 27.75 / 46.60 / 34.78 <lb/>ASU+HU+BU 45.09 / 76.80 / 56.82 19.88 / 44.52 / 27.49 05.20 / 33.46 / 09.01 21.62 / 62.21 / 32.09 <lb/>Cam <lb/>39.17 / 76.40 / 51.79 12.68 / 31.88 / 18.14 09.98 / 37.76 / 15.79 21.12 / 56.90 / 30.80 <lb/>UAntwerp <lb/>41.29 / 65.68 / 50.70 12.97 / 31.03 / 18.29 11.07 / 29.85 / 16.15 22.50 / 47.70 / 30.58 <lb/>UNIMAN <lb/>50.00 / 63.21 / 55.83 12.68 / 40.37 / 19.30 04.05 / 16.75 / 06.53 22.06 / 48.61 / 30.35 <lb/>SCAI <lb/>43.74 / 70.73 / 54.05 28.82 / 35.21 / 31.70 12.64 / 16.55 / 14.33 25.96 / 36.26 / 30.26 <lb/>UAveiro <lb/>43.57 / 71.63 / 54.18 13.54 / 34.06 / 19.38 06.29 / 21.05 / 09.69 20.93 / 49.30 / 29.38 <lb/>Team 24 <lb/>41.29 / 64.72 / 50.41 22.77 / 35.43 / 27.72 09.38 / 19.23 / 12.61 22.69 / 40.55 / 29.10 <lb/>USzeged <lb/>47.63 / 44.44 / 45.98 15.27 / 25.73 / 19.17 04.17 / 18.21 / 06.79 21.53 / 36.99 / 27.21 <lb/>NICTA <lb/>31.13 / 77.31 / 44.39 16.71 / 29.00 / 21.21 07.80 / 18.12 / 10.91 17.44 / 39.99 / 24.29 <lb/>CNBMadrid <lb/>50.25 / 46.59 / 48.35 33.14 / 20.54 / 25.36 12.22 / 07.99 / 09.67 28.63 / 20.88 / 24.15 <lb/>CCP-BTMG <lb/>28.17 / 87.63 / 42.64 12.68 / 40.00 / 19.26 03.09 / 48.11 / 05.80 13.45 / 71.81 / 22.66 <lb/>CIPS-ASU <lb/>39.68 / 38.60 / 39.13 17.29 / 31.58 / 22.35 11.86 / 08.15 / 09.66 22.78 / 19.03 / 20.74 <lb/>UMich <lb/>52.71 / 25.89 / 34.73 31.70 / 12.61 / 18.05 14.22 / 06.56 / 08.98 30.42 / 14.11 / 19.28 <lb/>PIKB <lb/>26.65 / 75.72 / 39.42 07.20 / 39.68 / 12.20 01.09 / 30.51 / 02.10 11.25 / 66.54 / 19.25 <lb/>Team 09 <lb/>27.16 / 43.61 / 33.47 03.17 / 09.82 / 04.79 02.42 / 11.90 / 04.02 11.69 / 31.42 / 17.04 <lb/>KoreaU <lb/>20.56 / 66.39 / 31.40 12.97 / 50.00 / 20.59 00.67 / 37.93 / 01.31 09.40 / 61.65 / 16.31 <lb/>Table 5: Evaluation results of Task 1 (recall / precision / f-score). <lb/>Team <lb/>All <lb/>Site for Phospho.(56) AtLoc &amp; ToLoc (65) <lb/>All Second Args. <lb/>UT+DBCLS <lb/>35.86 / 54.08 / 43.12 71.43 / 71.43 / 71.43 <lb/>23.08 / 88.24 / 36.59 <lb/>32.14 / 72.41 / 44.52 <lb/>UAntwerp <lb/>21.52 / 45.77 / 29.27 00.00 / 00.00 / 00.00 01.54 /100.00 / 03.03 06.63 / 52.00 / 11.76 <lb/>ASU+HU+BU 19.70 / 56.87 / 29.26 00.00 / 00.00 / 00.00 <lb/>00.00 / 00.00 / 00.00 <lb/>00.00 / 00.00 / 00.00 <lb/>Team 24 <lb/>22.08 / 38.28 / 28.01 55.36 / 93.94 / 69.66 <lb/>21.54 / 66.67 / 32.56 <lb/>30.10 / 76.62 / 43.22 <lb/>CCP-BTMG <lb/>13.25 / 70.97 / 22.33 30.36 /100.00 / 46.58 00.00 / 00.00 / 00.00 08.67 /100.00 / 15.96 <lb/>CNBMadrid <lb/>25.02 / 18.32 / 21.15 85.71 / 57.14 / 68.57 <lb/>32.31 / 47.73 / 38.53 <lb/>50.00 / 09.71 / 16.27 <lb/>Table 6: Evaluation results for Task 2. <lb/>ing only a single primary theme argument are shown <lb/>in one merged class, &quot;Simple Event&quot;. The broad per-<lb/>formance range (31% -70%) indicates even the ex-<lb/>traction of simple events is not a trivial task. How-<lb/>ever, the top-ranked systems show encouraging per-<lb/>formance, achieving or approaching 70% f-score. <lb/>The performance ranges for Binding (5% -44%) <lb/>and Regulation (1% -40%) events show their ex-<lb/>traction to be clearly more challenging. It is in-<lb/>teresting that while most systems show better per-<lb/>formance for binding over regulation events, the <lb/>systems [ConcordU] and [UT+DBCLS] are better <lb/>for regulation, showing somewhat reduced perfor-<lb/>mance for Binding events. This is in particular con-<lb/>trast to the following two systems, [ViBGhent] and <lb/>[UTokyo], which show far better performance for <lb/>Binding than Regulation events. As one possible <lb/>explanation, we find that the latter two differentiate <lb/>binding events by their number of themes, while the <lb/>former two give no specific treatment to multi-theme <lb/>binding events. Such observations and comparisons <lb/>are a clear benefit of a community-wide shared task. <lb/>Table 6 shows the evaluation results for the teams <lb/>who participated in Task 2. The &quot;All&quot; column shows <lb/>the overall performance of the systems for Task 2, <lb/>while the &quot;All Second Args.&quot; column shows the <lb/>performance of finding only the secondary argu-<lb/>ments. The evaluation results show considerable <lb/>differences between the criteria. For example, the <lb/>system [Team 24] shows performance comparable <lb/>to the top ranked system in finding secondary argu-<lb/>ments, although its overall performance for Task 2 <lb/>is more limited. Table 6 also shows the three sys-<lb/>tems, [UT+DBCLS], [Team 24] and [CNBMadrid], <lb/></body>

			<page>7 <lb/></page>

			<body>Team <lb/>Negation <lb/>Speculation <lb/>ConcordU <lb/>14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27 <lb/>VIBGhent <lb/>10.57 / 45.10 / 17.13 08.65 / 15.79 / 11.18 <lb/>ASU+HU+BU 03.96 / 27.27 / 06.92 06.25 / 28.26 / 10.24 <lb/>NICTA <lb/>05.29 / 34.48 / 09.17 04.81 / 30.30 / 08.30 <lb/>USzeged <lb/>05.29 / 01.94 / 02.84 12.02 / 03.88 / 05.87 <lb/>CCP-BTMG 01.76 / 05.26 / 02.64 06.73 / 13.33 / 08.95 <lb/>Table 7: Evaluation results for Task 3. <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>02/18 02/21 02/24 02/27 03/02 03/05 03/08 <lb/>daily average <lb/>Figure 4: Scatterplot of the evaluation results on the de-<lb/>velopment data during the system development period. <lb/>show performance at a practical level in particular in <lb/>finding specific sites of phosphorylation. <lb/>As shown in Table 7, the performance range for <lb/>Task 3 is very low although the representation of the <lb/>task is as simple as the simple events. We attribute <lb/>the reason to the fact that Task 3 is the only task of <lb/>which the annotation is not bound to textual clue, <lb/>thus no text-bound annotation was provided. <lb/>Figure 4 shows a scatter plot of the performance <lb/>of the participating systems during the system devel-<lb/>opment period. The performance evaluation comes <lb/>from the log of the online evaluation system on the <lb/>development data. It shows the best performance <lb/>and the average performance of the participating <lb/>systems were trending upwards up until the dead-<lb/>line of final submission, which indicates there is still <lb/>much potential for improvement. <lb/>7.3 Ensemble <lb/>Table 8 shows experimental results of a system en-<lb/>semble using the final submissions. For the ex-<lb/>periments, the top 3-10 systems were chosen, and <lb/>the output of each system treated as a weighted <lb/>vote 8 . Three weighting schemes were used; &quot;Equal&quot; <lb/>weights each vote equally; &quot;Averaged&quot; weights each <lb/></body>

			<note place="footnote">8 We used the &apos;ensemble&apos; function of U-Compare. <lb/></note>

			<body>Ensemble Equal Averaged Event Type <lb/>Top 3 <lb/>53.19 <lb/>53.19 <lb/>54.08 <lb/>Top 4 <lb/>54.34 <lb/>54.34 <lb/>55.21 <lb/>Top 5 <lb/>54.77 <lb/>55.03 <lb/>55.10 <lb/>Top 6 <lb/>55.13 <lb/>55.77 <lb/>55.96 <lb/>Top 7 <lb/>54.33 <lb/>55.45 <lb/>55.73 <lb/>Top 10 <lb/>52.79 <lb/>54.63 <lb/>55.18 <lb/>Table 8: Experimental results of system ensemble. <lb/>vote by the overall f-score of the system; &quot;Event <lb/>Type&quot; weights each vote by the f-score of the sys-<lb/>tem for the specific event type. The best score, <lb/>55.96%, was obtained by the &quot;Event Type&quot; weight-<lb/>ing scheme, showing a 4% unit improvement over <lb/>the best individual system. While using the final <lb/>scores for weighting uses data that would not be <lb/>available in practice, similar weighting could likely <lb/>be obtained e.g. using performance on the devel-<lb/>opment data. The experiment demonstrates that an <lb/>f-score better than 55% can be achieved simply by <lb/>combining the strengths of the systems. <lb/>8 Conclusion <lb/>Meeting with the community-wide participation, the <lb/>BioNLP Shared Task was successful in introducing <lb/>fine-grained event extraction to the domain. The <lb/>evaluation results of the final submissions from the <lb/>participants are both promising and encouraging for <lb/>the future of this approach to IE. It has been revealed <lb/>that state-of-the-art performance in event extraction <lb/>is approaching a practically applicable level for sim-<lb/>ple events, and also that there are many remain-<lb/>ing challenges in the extraction of complex events. <lb/>A brief analysis suggests that the submitted data <lb/>together with the system descriptions are rich re-<lb/>sources for finding directions for improvements. Fi-<lb/>nally, the experience of the shared task participants <lb/>provides an invaluable basis for cooperation in fac-<lb/>ing further challenges. <lb/></body>

			<div type="acknowledgement">Acknowledgments <lb/>This work was partially supported by Grant-in-Aid <lb/>for Specially Promoted Research (MEXT, Japan) <lb/>and Genome Network Project (MEXT, Japan). <lb/></div>

			<page>8 <lb/></page>

			<listBibl>References <lb/>Gary D. Bader, Michael P. Cary, and Chris Sander. 2006. <lb/>Pathguide: a Pathway Resource List. Nucleic Acids <lb/>Research., 34(suppl 1):D504-506. <lb/>Daniel M. Bikel. 2004. Intricacies of Collins&apos; Parsing <lb/>Model. Computational Linguistics, 30(4):479-511. <lb/>Evelyn Camon, Michele Magrane, Daniel Barrell, Vi-<lb/>vian Lee, Emily Dimmer, John Maslen, David Binns, <lb/>Nicola Harte, Rodrigo Lopez, and Rolf Apweiler. <lb/>2004. <lb/>The Gene Ontology Annotation (GOA) <lb/>Database: sharing knowledge in Uniprot with Gene <lb/>Ontology. Nucl. Acids Res., 32(suppl 1):D262-266. <lb/>Eugene Charniak and Mark Johnson. 2005. Coarse-<lb/>to-Fine n-Best Parsing and MaxEnt Discriminative <lb/>Reranking. In Proceedings of the 43rd Annual Meet-<lb/>ing of the Association for Computational Linguistics <lb/>(ACL&apos;05), pages 173-180. <lb/>Andrew Chatr-aryamontri, Arnaud Ceol, Luisa Montec-<lb/>chi Palazzi, Giuliano Nardelli, Maria Victoria Schnei-<lb/>der, Luisa Castagnoli, and Gianni Cesareni. 2007. <lb/>MINT: the Molecular INTeraction database. Nucleic <lb/>Acids Research, 35(suppl 1):D572-574. <lb/>Nancy Chinchor. 1998. Overview of MUC-7/MET-2. <lb/>In Message Understanding Conference (MUC-7) Pro-<lb/>ceedings. <lb/>Marie-Catherine de Marneffe, Bill MacCartney, and <lb/>Christopher D. Manning. 2006. Generating Typed <lb/>Dependency Parses from Phrase Structure Parses. In <lb/>Proceedings of the Fifth International Conference <lb/>on Language Resources and Evaluation (LREC&apos;06), <lb/>pages 449-454. <lb/>William Hersh, Aaron Cohen, Ruslenm Lynn, , and <lb/>Phoebe Roberts. 2007. TREC 2007 Genomics track <lb/>overview. In Proceeding of the Sixteenth Text RE-<lb/>trieval Conference. <lb/>Lynette Hirschman, Martin Krallinger, and Alfonso Va-<lb/>lencia, editors. 2007. Proceedings of the Second <lb/>BioCreative Challenge Evaluation Workshop. CNIO <lb/>Centro Nacional de Investigaciones Oncológicas. <lb/>Yoshinobu Kano, William Baumgartner, Luke McCro-<lb/>hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter, <lb/>and Jun&apos;ichi Tsujii. 2009. U-Compare: share and <lb/>compare text mining tools with UIMA. Bioinformat-<lb/>ics. To appear. <lb/>Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, <lb/>Yuka Tateisi, and Nigel Collier. 2004. Introduction <lb/>to the bio-entity recognition task at JNLPBA. In Pro-<lb/>ceedings of the International Joint Workshop on Nat-<lb/>ural Language Processing in Biomedicine and its Ap-<lb/>plications (JNLPBA), pages 70-75. <lb/>Jin-Dong Kim, Tomoko Ohta, and Jun&apos;ichi Tsujii. 2008. <lb/>Corpus annotation for mining biomedical events from <lb/>lterature. BMC Bioinformatics, 9(1):10. <lb/>David McClosky and Eugene Charniak. 2008. Self-<lb/>Training for Biomedical Parsing. In Proceedings of <lb/>the 46th Annual Meeting of the Association for Com-<lb/>putational Linguistics -Human Language Technolo-<lb/>gies (ACL-HLT&apos;08), pages 101-104. <lb/>Claire Nédellec. 2005. Learning Language in Logic -<lb/>Genic Interaction Extraction Challenge. In J. Cussens <lb/>and C. Nédellec, editors, Proceedings of the 4th Learn-<lb/>ing Language in Logic Workshop (LLL05), pages 31-<lb/>37. <lb/>Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and <lb/>Jun&apos;ichi Tsujii. 2009. Incorporating GENETAG-style <lb/>annotation to GENIA corpus. In Proceedings of Nat-<lb/>ural Language Processing in Biomedicine (BioNLP) <lb/>NAACL 2009 Workshop. To appear. <lb/>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari <lb/>Björne, Jorma Boberg, Jouni Järvinen, and Tapio <lb/>Salakoski. 2007. BioInfer: A corpus for information <lb/>extraction in the biomedical domain. BMC Bioinfor-<lb/>matics, 8(50). <lb/>Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and <lb/>Jun&apos;ichi Tsujii. 2009. Static Relations: a Piece <lb/>in the Biomedical Information Extraction Puzzle. <lb/>In Proceedings of Natural Language Processing in <lb/>Biomedicine (BioNLP) NAACL 2009 Workshop. To <lb/>appear. <lb/>Laura Rimell and Stephen Clark. 2008. Porting a <lb/>lexicalized-grammar parser to the biomedical domain. <lb/>Journal of Biomedical Informatics, To Appear. <lb/>Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi <lb/>Song, and Kazuaki Maeda. 2008. Linguistic Re-<lb/>sources and Evaluation Techniques for Evaluation of <lb/>Cross-Document Automatic Content Extraction. In <lb/>Proceedings of the 6th International Conference on <lb/>Language Resources and Evaluation (LREC 2008). <lb/>Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-<lb/>ten, and John Wilbur. 2005. Genetag: a tagged cor-<lb/>pus for gene/protein named entity recognition. BMC <lb/>Bioinformatics, 6(Suppl 1):S3. <lb/>Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and <lb/>Jun&apos;ichi Tsujii. 2005. Syntax Annotation for the GE-<lb/>NIA corpus. In Proceedings of the IJCNLP 2005, <lb/>Companion volume, pages 222-227. <lb/>Ellen Voorhees. 2007. Overview of TREC 2007. In <lb/>The Sixteenth Text REtrieval Conference (TREC 2007) <lb/>Proceedings. <lb/>John Wilbur, Lawrence Smith, and Lorraine Tanabe. <lb/>2007. BioCreative 2. Gene Mention Task. In <lb/>L. Hirschman, M. Krallinger, and A. Valencia, editors, <lb/>Proceedings of Second BioCreative Challenge Evalu-<lb/>ation Workshop, pages 7-16. <lb/></listBibl>

			<page>9 </page>


	</text>
</tei>
