<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title level="a">Mechanizing Refinement Types</title>
        <author>
          <persName>
            <forename>Michael H.</forename>
            <surname>Borkowski</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Niki</forename>
            <surname>Vazou</surname>
          </persName>
        </author>
        <author>
          <persName>
            <forename>Ranjit</forename>
            <surname>Jhala</surname>
          </persName>
        </author>
      </titleStmt>
      <editionStmt>
        <edition>
          <date when="2025-10-29T15:43:58.089905Z">29.10.2025 15:43:58</date>
          <title>grobid.training.segmentation [default]</title>
          <idno type="fileref">10.1145$1$3632912</idno>
        </edition>
      </editionStmt>
      <publicationStmt>
        <publisher>Association for Computing Machinery (ACM)</publisher>
        <availability>
          <licence target="https://creativecommons.org/licenses/by/4.0/"/>
        </availability>
        <date type="publication">2024</date>
        <idno type="DOI">10.1145/3632912</idno>
      </publicationStmt>
      <sourceDesc>
        <bibl>Michael H. Borkowski, Niki Vazou, Ranjit Jhala. (2024). Mechanizing Refinement Types. Proceedings of the ACM on Programming Languages, 8(POPL), 2099-2128. DOI: 10.1145/3632912</bibl>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <appInfo>
        <application version="1.0" ident="pdf-tei-editor" type="editor">
          <ref target="https://github.com/mpilhlt/pdf-tei-editor"/>
        </application>
        <application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-10-29T15:43:58.089905Z" type="extractor">
          <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
          <label type="revision">eb7768b</label>
          <label type="flavor">default</label>
          <label type="variant-id">grobid.training.segmentation</label>
          <ref target="https://github.com/kermitt2/grobid"/>
        </application>
      </appInfo>
    </encodingDesc>
    <revisionDesc>
      <change when="2025-10-29T15:43:58.089905Z" status="draft">
        <desc>Generated with createTraining API</desc>
      </change>
    </revisionDesc>
  </teiHeader>
  <text xmlns="http://www.tei-c.org/ns/1.0" xml:lang="en">
        <front>UniSparse: An Intermediate Language for General Sparse <lb/>Format Customization <lb/>JIE LIU, Cornell University, USA <lb/>ZHONGYUAN ZHAO, Cornell University, USA <lb/>ZIJIAN DING, University of California, Los Angeles, USA <lb/>BENJAMIN BROCK, Intel, USA <lb/>HONGBO RONG, Intel, USA <lb/>ZHIRU ZHANG, Cornell University, USA <lb/>The ongoing trend of hardware specialization has led to a growing use of custom data formats when processing <lb/>sparse workloads, which are typically memory-bound. These formats facilitate optimized software/hardware <lb/>implementations by utilizing sparsity pattern-or target-aware data structures and layouts to enhance memory <lb/>access latency and bandwidth utilization. However, existing sparse tensor programming models and com-<lb/>pilers offer little or no support for productively customizing the sparse formats. Additionally, because these <lb/>frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to <lb/>accommodate numerous new variations of custom sparse data structures and layouts. <lb/>To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified <lb/>abstraction for representing and customizing sparse formats. Unlike the existing attribute-based frameworks, <lb/>UniSparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-<lb/>level memory layout, enabling the customization of both. As a result, a rich set of format customizations <lb/>can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. We also <lb/>develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, <lb/>and automatic code generation of format conversion and compute operations for heterogeneous architectures. <lb/>We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear <lb/>algebra operations with specialized formats on multiple different hardware targets, including an Intel CPU, an <lb/>NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory (PIM) device. <lb/>CCS Concepts: • Software and its engineering → Domain specific languages; Abstraction, modeling <lb/>and modularity; Source code generation. <lb/>Additional Key Words and Phrases: sparse data formats, compilers, programming languages, heterogeneous <lb/>systems <lb/>ACM Reference Format: <lb/>Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang. 2024. UniSparse: An <lb/>Intermediate Language for General Sparse Format Customization. Proc. ACM Program. Lang. 8, OOPSLA1, <lb/>Article 99 (April 2024), 29 pages. https://doi.org/10.1145/3649816 <lb/>Authors&apos; addresses: Jie Liu, Cornell University, Ithaca, USA, jl3952@cornell.edu; Zhongyuan Zhao, Cornell University, Ithaca, <lb/>USA, zhozh@qti.qualcomm.com; Zijian Ding, University of California, Los Angeles, Los Angeles, USA, bradyd@cs.ucla.edu; <lb/>Benjamin Brock, Intel, San Jose, USA, benjamin.brock@intel.com; Hongbo Rong, Intel, San Jose, USA, hongbo.rong@intel. <lb/>com; Zhiru Zhang, Cornell University, Ithaca, USA, zhiruz@cornell.edu. <lb/>© 2024 Copyright held by the owner/author(s). <lb/>ACM 2475-1421/2024/4-ART99 <lb/>https://doi.org/10.1145/3649816 <lb/>Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/>This work is licensed under a Creative Commons Attribution 4.0 International License. <lb/>99:2 <lb/>Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></front>

        <body>1 INTRODUCTION <lb/>As Dennard scaling ended in the mid-2000s and Moore&apos;s Law is approaching its limit, computer <lb/>engineers are increasingly turning to special-purpose hardware accelerators to meet the ever-<lb/>growing computational demands of emerging application domains such as graph analytics, machine <lb/>learning, and robotics. At the same time, there has been an explosion in the amount of data that <lb/>domain experts have to manage. Notably, much of this big data is sparse in nature. For example, <lb/>Amazon co-purchase graphs have 400K nodes and a density of 0.002%, and arXiv graph datasets <lb/>have 100M papers and a density of 0.00002% [Hu et al. 2020; Leskovec and Krevl 2014]. These <lb/>evident trends in technology and applications are driving computing systems towards heterogeneity <lb/>that can process sparse data in an efficient and high-performance manner. <lb/>Many important operations (i.e., kernels) of sparse processing are performed on sparse tensors, <lb/>a generalization of sparse matrices. Sparse tensors are typically represented using specialized data <lb/>structures that leverage the sparsity of the tensor to reduce storage size and/or memory footprint. <lb/>These data structures usually store only the non-zero elements (or non-zero blocks) of the tensor, <lb/>along with their associated coordinates that are encoded in a compressed form as metadata. Various <lb/>forms of data layouts, such as the structure of arrays (SoA) or array of structures (AoS), can be <lb/>employed to store the sparse data structure in memory. The data structure and data layout jointly <lb/>determine a sparse format. <lb/>The metadata can be viewed as a hierarchical tree that captures the multi-dimensional coordinates <lb/>of the non-zeros in a structured way. In this work, we refer to this tree as the metadata tree and <lb/>use it as a logical representation of the sparse format. To reconstruct the original coordinates of a <lb/>non-zero element, multiple indirect memory accesses are required to traverse the metadata tree. <lb/>Due to the input-dependent and irregular data access patterns that result from this process, sparse <lb/>workloads are typically memory-bound. <lb/>To efficiently utilize memory bandwidth, reduce memory accesses, and exploit data parallelism <lb/>to boost the performance of sparse tensor computation, researchers are increasingly using custom <lb/>sparse formats optimized for particular application domains and/or target hardware architectures. <lb/>Examples include hybrid formats for GPUs [Bell and Garland 2009; Choi et al. 2010; Guo et al. <lb/>2016] and banked formats for FPGAs [Hu et al. 2021; Fowers et al. 2014] and dedicated accelera-<lb/>tors [Srivastava et al. 2020]. While format customization can significantly improve performance, <lb/>we recognize two pressing issues: i) productivity -it takes substantial engineering effort to design <lb/>a custom sparse format and adapt the implementation of related compute operations that must <lb/>interact with the new format, and ii) permutability -there lacks a unified abstraction that can <lb/>systematically encode different variants (or permutations) of existing sparse formats to facilitate <lb/>the exploration of a complex design space, where the search of custom formats needs to account <lb/>for non-zero distribution patterns of inputs, inherent parallelism of the dominant compute kernels, <lb/>and the target hardware. <lb/>Prior research has attempted to address the productivity challenge by using either manually <lb/>optimized libraries or automatic compilers. Sparse linear/tensor algebra libraries (e.g., sparse BLAS, <lb/>Intel MKL, NVIDIA cuSPARSE) provide highly optimized target-specific sparse kernels. While <lb/>library functions achieve high performance, they only support a limited set of sparse formats. Recent <lb/>efforts on sparse tensor algebra compilers such as TACO [Kjolstad et al. 2017; Chou et al. 2018], <lb/>COMET [Tian et al. 2021], and SparseTIR [Ye et al. 2023] describe tensor dimensions in attributes <lb/>(e.g., dense or compressed), and generate sparse tensor algebra kernels assisted by predefined code <lb/>generation templates. This attribute-based format abstraction limits their extensibility to support <lb/>new custom formats, as the finite combinations of attributes restrict the range of possible data <lb/>structures, and the fixed code generation templates restrict the possible data layouts. As a result, <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:3 <lb/></page>

        <body>this abstraction offers programmers no further customization opportunities for the data structures <lb/>and layouts, and important details about the data structures and layouts necessary for identifying a <lb/>specific format uniquely may be omitted. <lb/>This work proposes UniSparse, which is the first intermediate language designed for general <lb/>sparse format customization. UniSparse aims to (1) provide a systematic way of expressing an <lb/>unlimited number of custom formats, (2) support format customization at both the logical data <lb/>structure and physical layout level, while taking into account the sparsity patterns of input tensors, <lb/>compute operations, and hardware targets, and (3) automate code generation for compute operations <lb/>and conversion with other formats for the newly defined formats. With UniSparse, the metadata <lb/>tree serves as a logical representation that can be expressed using an index map, a set of query <lb/>and structural mutation operations, which we call primitives ( §4.1). An additional set of layout <lb/>primitives specifies how to partition and traverse the metadata tree, which transforms the tree into <lb/>physical memory layouts ( §4.2). The index map and primitives are the essential components of a <lb/>succinct intermediate language for specifying custom sparse formats, including but not limited to <lb/>many previously proposed high-performance formats. <lb/>Compared to the previous attribute-based approach, the UniSparse language offers a holistic <lb/>representation of sparse formats using mapping functions and primitives, without presuming <lb/>dimension-wise composition of separate data structures. The language decouples logical repre-<lb/>sentations of sparse formats from their physical memory layouts, allowing both to be specialized. <lb/>Empowered by the language, the UniSparse compiler can reason formally about the correspondence <lb/>between various formats and their underlying layouts, automating both code generation and format <lb/>conversion. To this end, we develop a data structure and data layout inference algorithm ( §5.1) <lb/>that automatically determines the storage format of sparse tensors. Furthermore, we introduce a <lb/>format conversion algorithm ( §5.2) that enables the compiler to handle a broad range of source and <lb/>destination formats, including both conventional and specialized ones. We also provide a general <lb/>compute kernel generation algorithm ( §5.3) that supports custom sparse formats. <lb/>The UniSparse compiler is built on top of the MLIR infrastructure [Lattner et al. 2020] ( §6). We <lb/>include the compiler as an artifact for evaluation, and in Section 7, we demonstrate its efficacy <lb/>in customizing formats on multiple hardware platforms, including CPUs, GPUs, FPGAs, and a <lb/>simulated PIM device [Devic et al. 2022]. It can also automate the conversion among a variety of <lb/>sparse formats, resulting in significant productivity improvements. <lb/>2 BACKGROUND AND MOTIVATION <lb/>This section overviews common sparse formats (Figure 1), and prior research and their limitations <lb/>to motivate our work ( §2.2). Sparse matrices are used as illustrative examples for simplicity, while <lb/>the discussion generalizes to tensors. <lb/>2.1 Sparse Formats <lb/>To improve performance and adapt to various architectures and sparsity patterns, numerous tensor <lb/>formats have been developed. <lb/>The coordinate (COO) format [Bader and Kolda 2008] (Figure 1b) stores non-zero values along <lb/>with their complete coordinate information. COO is widely used as the default format for many <lb/>sparse data files, such as the Matrix Market (.mtx) [Boisvert et al. 1996] and FROSTT (.tns) [Smith <lb/>et al. 2017] file formats. While COO stores row and column indices as separate arrays, the dictionary <lb/>of keys (DOK) format [Johansson and Johansson 2015] (Figure 1f) adopts a different memory layout <lb/>by pairing up indices in a single array. Sparse tensors may have multiple non-zero values per row, and <lb/>many non-zero elements can share a common row index. The compressed sparse row (CSR) format <lb/>(Figure 1c) replaces these row coordinates with a compact pointer array that slices values belonging <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:4 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>2 <lb/>3 4 <lb/>1 <lb/>6 <lb/>5 <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>0 1 2 <lb/>Rows (d0) <lb/>Columns (d1) <lb/>4 <lb/>3 <lb/>(a) A 5x4 matrix <lb/>4 <lb/>2 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>d0 <lb/>idx <lb/>3 <lb/>1 <lb/>1 <lb/>2 <lb/>3 <lb/>0 <lb/>d1 <lb/>5 <lb/>3 <lb/>2 <lb/>4 <lb/>6 <lb/>1 <lb/>val <lb/>idx <lb/>(b) COO <lb/>5 <lb/>d0 <lb/>size <lb/>3 <lb/>1 <lb/>1 <lb/>2 <lb/>3 <lb/>0 <lb/>d1 <lb/>5 <lb/>3 <lb/>2 <lb/>4 <lb/>6 <lb/>1 <lb/>val <lb/>idx <lb/>6 <lb/>5 <lb/>5 <lb/>0 <lb/>2 <lb/>1 <lb/>ptr <lb/>(c) CSR <lb/>4 <lb/>2 <lb/>1 <lb/>0 <lb/>d0 <lb/>idx <lb/>3 <lb/>1 <lb/>1 <lb/>2 <lb/>3 <lb/>0 <lb/>d1 <lb/>5 <lb/>3 <lb/>2 <lb/>4 <lb/>6 <lb/>1 <lb/>val <lb/>idx <lb/>5 <lb/>2 <lb/>6 <lb/>1 <lb/>0 <lb/>ptr <lb/>(d) DCSR <lb/>5 <lb/>1 <lb/>d0 <lb/>0 <lb/>size <lb/>idx <lb/>3 <lb/>2 <lb/>1 <lb/>d1 <lb/>1 <lb/>val <lb/>3 <lb/>* <lb/>2 <lb/>5 <lb/>4 <lb/>3 <lb/>6 <lb/>* <lb/>(e) LIL <lb/>d0 <lb/>idx <lb/>d1 <lb/>1 <lb/>val <lb/>2 3 <lb/>6 <lb/>4 5 <lb/>0 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>1 <lb/>4 <lb/>3 <lb/>2 <lb/>2 <lb/>2 <lb/>3 <lb/>(f) DOK <lb/>0 <lb/>0 <lb/>6 <lb/>0 <lb/>3 <lb/>0 <lb/>0 <lb/>5 <lb/>0 <lb/>0 <lb/>1 2 <lb/>0 <lb/>0 <lb/>4 <lb/>val <lb/>1 <lb/>0 <lb/>-1 <lb/>idx <lb/>5 <lb/>size <lb/>d1 -d0 <lb/>d0 <lb/>(g) DIA <lb/>0 <lb/>0 <lb/>0 <lb/>4 <lb/>0 <lb/>0 <lb/>3 <lb/>6 <lb/>0 <lb/>5 <lb/>1 2 <lb/>val <lb/>1 <lb/>0 <lb/>-1 <lb/>idx <lb/>4 <lb/>size <lb/>d1 -d0 <lb/>d1 <lb/>(h) DIA-variant <lb/>3 <lb/>1 <lb/>0 0 1 <lb/>d1 / 2 <lb/>d0 / 2 <lb/>2 <lb/>size <lb/>1 3 4 <lb/>0 <lb/>size <lb/>ptr <lb/>idx <lb/>2 <lb/>size <lb/>d0 % <lb/>0 <lb/>0 <lb/>0 <lb/>6 <lb/>5 0 <lb/>4 <lb/>0 <lb/>0 <lb/>0 <lb/>3 0 <lb/>1 <lb/>2 <lb/>0 0 <lb/>val <lb/>d1 % <lb/>(i) BCSR <lb/>2 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>d0 % 2 <lb/>d1 / 2 <lb/>1 <lb/>1 <lb/>1 <lb/>0 <lb/>1 <lb/>0 <lb/>idx <lb/>6 <lb/>5 <lb/>5 <lb/>2 <lb/>0 <lb/>2 3 <lb/>size <lb/>ptr <lb/>idx <lb/>3 <lb/>d0 / 2 <lb/>size <lb/>5 <lb/>3 <lb/>2 <lb/>4 <lb/>6 <lb/>1 <lb/>val <lb/>d1 % 2 <lb/>(j) CSB <lb/>2 <lb/>1 <lb/>0 <lb/>d0 <lb/>5 <lb/>idx <lb/>size <lb/>0 <lb/>1 <lb/>2 <lb/>1 <lb/>3 <lb/>1 <lb/>0 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>0 <lb/>1 <lb/>3 2 <lb/>d1 <lb/>0 <lb/>0 <lb/>5 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>0 <lb/>3 <lb/>0 <lb/>6 <lb/>4 <lb/>val <lb/>idx <lb/>indirect (d1) <lb/>(k) ELL <lb/>0 <lb/>d0 / 2 <lb/>d0 % 2 <lb/>3 <lb/>idx <lb/>size <lb/>d1 <lb/>3 <lb/>6 <lb/>4 5 <lb/>1 <lb/>val <lb/>1 <lb/>3 3 <lb/>2 <lb/>0 <lb/>idx <lb/>5 <lb/>1 4 <lb/>0 <lb/>ptr <lb/>1 <lb/>3 <lb/>1 1 1 <lb/>0 <lb/>1 <lb/>2 <lb/>(l) C 2 SR (2 banks) <lb/>0 <lb/>d0 <lb/>idx <lb/>d1 <lb/>4 5 <lb/>1 3 <lb/>val <lb/>indirect (d0) <lb/>0 2 <lb/>idx <lb/>1 2 3 <lb/>0 <lb/>idx <lb/>0 1 4 <lb/>ptr <lb/>1 <lb/>2 6 <lb/>3 <lb/>1 <lb/>1 4 <lb/>0 1 <lb/>(m) CISR (2 banks) <lb/>2 <lb/>d1 -d0 <lb/>d0 / 3 <lb/>2 <lb/>1 <lb/>4 <lb/>val <lb/>size <lb/>5 <lb/>d1 <lb/>d0 <lb/>6 <lb/>5 <lb/>3 <lb/>val <lb/>3 <lb/>1 3 <lb/>size <lb/>ptr <lb/>0 <lb/>ptr <lb/>3 <lb/>size <lb/>d0 % 3 <lb/>1 <lb/>1 <lb/>0 <lb/>idx <lb/>3 <lb/>2 <lb/>2 <lb/>0 <lb/>0 <lb/>0 <lb/>idx <lb/>(n) Hybrid BDIA/CSR <lb/>3 <lb/>d0 / 2 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>4 5 <lb/>0 <lb/>1 <lb/>0 <lb/>val <lb/>0 <lb/>0 1 <lb/>size <lb/>idx <lb/>0 <lb/>idx <lb/>indirect (d1/2) <lb/>2 4 <lb/>3 <lb/>1 <lb/>d1 <lb/>d0 <lb/>6 <lb/>3 <lb/>val <lb/>idx <lb/>idx <lb/>2 <lb/>size <lb/>2 <lb/>size <lb/>d1 / 2 <lb/>d0 % 2 <lb/>d1 % 2 <lb/>(o) Hybrid BELL/COO <lb/>Fig. 1. Different formats of a sparse matrix ( ) -Yellow-shaded blocks refer to the data structure of the <lb/>same tensor element. The size, ptr, idx, and val labels on the le indicate the size of a dimension, a pointer <lb/>array, an index array, or a value array. The index expressions on the right denote the index map at each level. <lb/>Horizontal do ed lines separate different tensor dimensions, while vertical do ed lines divide the matrix <lb/>into sub-matrices. <lb/>to different rows, thus saving more space than COO. The linked list (LIL) format [Johansson and <lb/>Johansson 2015] (Figure 1e) employs an alternative physical layout of the CSR format, where <lb/>column indices and values are stored separately per row, eliminating the need for row pointers. The <lb/>doubly-compressed sparse row (DCSR) format [Buluc and Gilbert 2008] (Figure 1d) provides further <lb/>optimizations for matrices with many empty rows by storing the pointers array as a compressed <lb/>list and avoiding storing pointers for empty rows in the pointers array. <lb/>To efficiently store matrices with non-zero values clustered along the diagonals, the diagonal <lb/>(DIA) format [Saad 2003] (Figure 1g) stores only the diagonals containing non-zero values. The <lb/>traditional DIA format pads diagonals to the full size of the row dimension (d0 in Figure 1), while a <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:5 <lb/></page>

        <body>variant of the DIA format (Figure 1h) pads diagonals to the full size of the column dimension (d1), <lb/>which is more space-efficient for matrices with fewer columns. <lb/>Blocked formats partition a tensor into sub-tensor chunks. For instance, a blocked variant of CSR, <lb/>known as the block compressed sparse row (BCSR) format [Im and Yelick 1998] (Figure 1i), stores a <lb/>compressed collection of small dense matrix blocks. In contrast, the compressed sparse block (CSB) <lb/>format [Buluç et al. 2009] (Figure 1j) stores a dense collection of matrix blocks in a compressed <lb/>format. BCSR improves register reuse, while CSB exposes parallel execution opportunities in <lb/>sparse matrix-vector multiplication (SpMV). Another format that balances workloads for vectorized <lb/>processing is the ELLPACK (ELL) format [Kincaid et al. 1989] (Figure 1k). It packs non-zero values <lb/>in each row and reduces the column size to the maximum amount of non-zero values per row. <lb/>Hardware accelerators for sparse processing can benefit from formats customized per data <lb/>access patterns or memory systems. One such format is the cyclic channel sparse row (C 2 SR) <lb/>format [Srivastava et al. 2020] (Figure 1l). This format partitions a tensor&apos;s rows into sub-tensors, <lb/>one sub-tensor for each memory bank, increasing memory bandwidth utilization. Another example <lb/>is the compressed interleaved sparse row (CISR) format [Fowers et al. 2014] (Figure 1m), which <lb/>schedules rows of the CSR format to different compute units of an accelerator, balancing the <lb/>workload and eliminating memory access conflicts. <lb/>Formats can also be customized based on the sparsity patterns of input tensors. A hybrid format <lb/>represents sub-tensors with their best suitable formats independently. One example is the hybrid <lb/>blocked-DIA (BDIA) format [Fukaya et al. 2021] and CSR format (Figure 1n), which achieves <lb/>higher performance for datasets with non-zeros clustered along diagonals on multi-thread CPUs by <lb/>increasing data temporal locality in cache blocks. Another example is the hybrid blocked-ELLPACK <lb/>(BELL)/COO format (Figure 1o), which combines several format optimization techniques [Guo et al. <lb/>2016; Bell and Garland 2009; Choi et al. 2010]. <lb/>2.2 Prior Work on Sparse Format Abstraction <lb/>Early sparse tensor algebra compilers [Bik and Wijshoff 1993; Pugh and Shpeisman 1999] transform <lb/>dense linear algebra programs to runnable sparse code with sparsity predicates (guards), but only a <lb/>few formats are supported with hard-coded format descriptions. The idea of supporting different <lb/>data structures with a format abstraction was pioneered by the Bernoulli Compiler [Kotlyar et al. <lb/>1997], which introduces an index hierarchy and per-dimension accessing rules to describe various <lb/>sparse formats. Another work [Arnold et al. 2010] defines a functional language for specifying <lb/>sparse matrix formats as a sequence of constructs that facilitates code verification. <lb/>Recent years have seen a surge of research on sparse tensor compilers. TACO [Kjolstad et al. <lb/>2017; Chou et al. 2018] represents sparse formats as per-dimension attributes and generates code <lb/>for tensor algebra expressions based on the attributes. Automated format conversion is proposed in <lb/>[Chou et al. 2020a], which uses index maps and queries to specify formats in a more programmable <lb/>way. The MLIR SparseTensor dialect [Bik et al. 2022] is a recent effort that leverages TACO&apos;s <lb/>code generation theories to build a sparse linear algebra compiler within the MLIR infrastructure. <lb/>Other compiler frameworks, such as COMET [Tian et al. 2021], also use attribute-based format <lb/>abstractions similar to TACO and target heterogeneous architectures and computational chemistry <lb/>workloads. Additionally, SparseTIR [Ye et al. 2023] generates high-performance sparse kernels for <lb/>machine learning workloads on GPUs using hybrid formats. Table 1 summarizes recent work on <lb/>sparse tensor algebra compilers with format abstraction, categorized into two classes: <lb/>Attribute-Based Format Abstraction. Previous research, such as TACO [Kjolstad et al. 2017; <lb/>Chou et al. 2018], MLIR&apos;s SparseTensor dialect [Bik et al. 2022], COMET [Tian et al. 2021], and <lb/>SparseTIR [Ye et al. 2023], uses per-dimension attributes to describe formats. For example, in TACO, <lb/>the CSR format is encoded as {dense, compressed} in row and column dimensions, respectively. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:6 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>Table 1. State-of-the-art sparse tensor algebra compilers. <lb/>Abstraction <lb/>Prior Work <lb/>Custom <lb/>Index Maps <lb/>Sparsity Pa ern-Aware <lb/>Hybrid Formats <lb/>Backend-Aware <lb/>Memory Layouts <lb/>Automated <lb/>Conversion <lb/>A ribute-based <lb/>TACO <lb/>[Kjolstad et al. 2017], <lb/>[Chou et al. 2018] <lb/>MLIR SparseTensor <lb/>[Bik et al. 2022] <lb/>COMET <lb/>[Tian et al. 2021] <lb/>SparseTIR <lb/>[Ye et al. 2023] <lb/>Language-based <lb/>LL <lb/>[Arnold et al. 2010] <lb/>TACO-conversion <lb/>[Chou et al. 2020a] <lb/>UniSparse <lb/>(This Work) <lb/>Similarly, SparseTIR uses {I = dense_fixed(I_Size, dataType), J = sparse_variable(I, ..., <lb/>dataType)} to represent the same format. <lb/>However, limited by the finite combinations of attributes, the attribute-based approach is unable <lb/>to support a vast number of new formats. For instance, while TACO can express the traditional <lb/>DIA format (Figure 1g), it cannot accommodate the DIA-variant format (Figure 1h). Additionally, <lb/>TACO lacks support for custom index maps, which are necessary for the CISR format (Figure 1m). <lb/>MLIR&apos;s SparseTensor dialect and SparseTIR have incorporated index expressions, which enhance <lb/>expressiveness, but do not entirely overcome the limitation. Moreover, the prior approach generates <lb/>compute operations with fixed iteration and access templates, resulting in non-customizable <lb/>memory layouts. Additionally, different memory layouts such as SoA and AoS are not distinguished. <lb/>Lastly, since attributes do not directly reflect the memory layouts, implementing fully automated <lb/>format conversion becomes challenging. <lb/>Language-Based Format Abstraction. Earlier research [Arnold et al. 2010] defines a sparse <lb/>format by specifying the compression process using a functional language. However, this approach <lb/>strongly couples the specification of compute operations with the format description, leading to <lb/>significant variations in user programs for the same compute kernel with different formats. Another <lb/>work [Chou et al. 2020a] introduces a language to assist format conversion. It supports index <lb/>map functions and blocking formats, and thus can handle formats such as the DIA-variant (Figure <lb/>1h), BCSR (Figure 1i), and CSB (Figure 1j). However, it still does not support custom index maps <lb/>required by CISR (Figure 1m), or discernible layouts required by LIL (Figure 1e) and DOK (Figure <lb/>1f). Moreover, sparsity pattern-aware hybrid formats are not supported. <lb/>To the best of our knowledge, UniSparse is the first language for format abstraction that can <lb/>encode a wide range of custom formats including those with custom index maps, hybrid formats <lb/>aware of sparsity patterns, and target-specific memory layouts. <lb/>3 OVERVIEW OF UNISPARSE <lb/>UniSparse is an intermediate language designed to formally and succinctly represent a wide range <lb/>of sparse formats, along with a compiler that automates both format conversion and customiza-<lb/>tion. Figure 2 offers an overview of UniSparse&apos;s design, with the format abstraction forming the <lb/>foundation of the entire framework. We develop an intermediate language that enables users to <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:7 <lb/></page>

        <body>Format <lb/>Abstraction <lb/>Intermediate <lb/>Language <lb/>Code <lb/>Generation <lb/>Compute Kernel <lb/>Specification <lb/>Backend <lb/>Query Primitives <lb/>MLIR Lowering <lb/>Runtime <lb/>Library <lb/>Format Customization <lb/>Lowering <lb/>MLIR SparseTensor <lb/>Lowering <lb/>Index Maps <lb/>Mutation Primitives <lb/>link <lb/>Layout Inference <lb/>Data Structure <lb/>Sparse Format <lb/>Format Customization <lb/>Sparsity Pattern Analysis <lb/>Tensor Decomposition <lb/>Format Conversion <lb/>Layout <lb/>Primitives <lb/>Data Layout <lb/>CPUs <lb/>GPUs <lb/>PIM <lb/>FPGAs <lb/>Fig. 2. An overview of UniSparse. <lb/>specify both format customization and compute kernels independently. The intermediate language <lb/>program can run on various backends through automated code generation. <lb/>Our approach to sparse tensor format abstraction revolves around a succinct yet expressive <lb/>representation for both data structures and layouts. The data structures of a format are logically <lb/>represented as a metadata tree ( §4.1), encoded by an index map ( §4.1.1) and a set of query ( §4.1.2) <lb/>and mutation primitives ( §4.1.3). From the metadata tree, we obtain the data layouts of a format <lb/>using layout primitives ( §4.2). <lb/>During the code generation stage ( §5), the UniSparse compiler first infers the data structures <lb/>and layouts from format encodings ( §5.1). Once the formats are determined, the compiler proceeds <lb/>to lower format customization and compute operations. For format conversion, the compiler <lb/>automatically applies a sequence of rewrite rules to convert from the source format to the destination <lb/>format ( §5.2). To lower compute operations, UniSparse leverages the MLIR SparseTensor dialect for <lb/>its supported conventional formats. The compute kernels for custom formats are generated based on <lb/>the proposed two-step algorithm ( §5.3). The UniSparse intermediate language ( §6) is implemented <lb/>as a new dialect in MLIR. Our compiler can support multiple hardware targets, including CPUs, <lb/>GPUs, FPGAs, and a PIM simulator [Devic et al. 2022]. <lb/>As shown in Figure 2, UniSparse follows a design principle of decoupling format abstraction and <lb/>compiler implementation at three levels: <lb/>• Data structures and memory layouts are expressed in a decoupled way, enabling support for a <lb/>broader range of custom sparse formats. <lb/>• The format customization and compute operations are specified independently, freeing developers <lb/>from tedious format-specific implementation details when writing compute kernels. <lb/>• The format abstraction is independent of the input language, making it portable and comple-<lb/>menting prior attribute-based approaches. <lb/>An Illustrative Example. Figure 3 illustrates our approach using the SpMV kernel with an <lb/>input matrix (Figure 1a), originally in the COO format (Figure 3a) and then converted to the hybrid <lb/>BDIA/CSR format (Figure 3b). Although we will cover the formal syntax and technical details of <lb/>the UniSparse language in later sections, the purpose of this example is to provide an intuitive <lb/>understanding of how we address the following questions: (1) The sparsity patterns of the matrices <lb/>are input-dependent. How to decompose the matrix according to the non-zero distribution patterns? <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:8 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>2 <lb/>A <lb/>5 <lb/>3 <lb/>2 <lb/>d1 <lb/>d0 <lb/>val <lb/>6 <lb/>3 <lb/>4 <lb/>(a) Source data structure: COO <lb/>1 <lb/>4 <lb/>2 <lb/>0 <lb/>0 <lb/>val <lb/>d0 / 3 <lb/>d1 -d0 <lb/>3 5 <lb/>1 3 <lb/>2 <lb/>val <lb/>d0 <lb/>d1 <lb/>A 0 <lb/>0 <lb/>2 <lb/>1 <lb/>d0 % 3 <lb/>3 <lb/>0 1 <lb/>4 <lb/>A 1 <lb/>6 <lb/>3 <lb/>1 <lb/>(b) Target data structure: BDIA/CSR <lb/>// COO to BDIA <lb/>COO_structure <lb/>.Skew(0, 1, -1) <lb/>.TileSplit(0, 3).Sort() <lb/>.Fill(0).Merge(0) <lb/>// COO to CSR <lb/>COO_structure <lb/>.Fill(0).Merge(0) <lb/>(c) Convert COO to BDIA/CSR. <lb/>1 // Format abstraction <lb/>2 #COO = #unisparse.encoding&lt;{ <lb/>3 <lb/>idx_map = #unisparse.map&lt;(d0,d1)-&gt;(d0,d1)&gt;, mutation = #unisparse.prim&lt;trim(0,1)&gt; }&gt; <lb/>4 #CSR = #unisparse.encoding&lt;{ <lb/>5 <lb/>idx_map = #unisparse.map&lt;(d0,d1)-&gt;(d0,d1)&gt;, mutation = #unisparse.prim&lt;merge(0), trim(1,1)&gt; }&gt; <lb/>6 #BDIA = #unisparse.encoding&lt;{ <lb/>7 <lb/>idx_map = #unisparse.map&lt;(d0,d1)-&gt;(d0/3,d1-d0,d0%3 )&gt;, mutation = #unisparse.prim&lt;merge(0), <lb/>trim(1,1)&gt; }&gt; <lb/>8 #COO_COO = #unisparse.hybrid&lt;{ fmats = [#COO, #COO] }&gt; <lb/>9 #BDIA_CSR = #unisparse.hybrid&lt;{ fmats = [#BDIA, #CSR] }&gt; <lb/>10 // Format pre-processing <lb/>11 #sum = #unisparse.sum&lt; groupBy (d0,d1) -&gt; (d0/3,d1-d0), with val ne 0 -&gt; 1 | otherwise -&gt; 0 &gt; <lb/>12 %A1 = unisparse.decompose (%in_A, %thld) { query = #sum }: tensor&lt;?x?xf32, #COO_COO&gt; <lb/>13 %A2 = unisparse.convert (%A1): tensor&lt;?x?xf32, #BDIA_CSR&gt; <lb/>14 // Compute operation <lb/>15 #spmv = { indexing_maps = [ <lb/>16 <lb/>affine_map&lt;(d0,d1)-&gt;(d0,d1)&gt;, // for argument %A2 <lb/>17 <lb/>affine_map&lt;(d0,d1)-&gt;(d1)&gt;, <lb/>// for argument %in_X <lb/>18 <lb/>affine_map&lt;(d0,d1)-&gt;(d0)&gt;], // for argument %out_Y <lb/>19 <lb/>iterator_types = [&quot;parallel&quot;, &quot;reduction&quot;] } <lb/>20 %0 = linalg.generic #spmv ins(%A2, %in_X : tensor&lt;?x?xf32, #BDIA_CSR&gt;, tensor&lt;?xf32&gt;) <lb/>21 <lb/>outs(%out_Y: tensor&lt;?xf32&gt;) { <lb/>22 <lb/>^bb0(%A_val: f32, %X_val: f32, %Y_val: f32): <lb/>23 <lb/>%1 = arith.mulf %A_val, %X_val : f32 <lb/>24 <lb/>%o = arith.addf %Y_val, %1 : f32 <lb/>25 <lb/>linalg.yield %o : f32 <lb/>26 <lb/>} -&gt; tensor&lt;?xf32&gt; <lb/>(d) A UniSparse program of the SpMV kernel with the format of the input tensor converted from COO to <lb/>hybrid BDIA/CSR. The blocking size of the BDIA format is 3. File input operations are omi ed. The decompose <lb/>operation in Line 12 divides the tensor into two sub-tensors adaptively by embedding a sum primitive that <lb/>queries the sparsity pa ern of the tensor data. A convert operation in Line 13 translates the source into the <lb/>target format. The SpMV kernel is specified using a linalg.generic operation in Line 21 -26. <lb/>Fig. 3. An illustration of UniSparse. <lb/>(2) How to express the source and target format in a general way? Here we use COO and BDIA/CSR <lb/>for illustration purposes only, while in practice, other formats can be used. (3) For productivity, <lb/>conversion between the formats should be automated. How could our compiler figure out the <lb/>memory layouts of the formats, and automatically convert one to the other? (4) For generality, it is <lb/>desirable to write a compute kernel only once but the kernel works with a matrix in any format <lb/>after conversion. How to decouple the compute kernel from a specific sparse format? <lb/>In Figure 3d, Line 2-9 specify the source and target formats. The sparsity pattern of the input is <lb/>queried (Line 11), and the result is used to decompose the source format into two parts, both in the <lb/>original source format COO (Line 12). Then, the decomposed source format is converted to the <lb/>target format (Line 13). This target format is a parameter to the compute kernel (Line 15-26). In other <lb/>words, the compute kernel and the format of its input matrix are completely decoupled. According <lb/>to the above UniSparse program, the UniSparse compiler automatically infers the memory layout <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:9 <lb/></page>

        <body>of the source format (Figure 1b), and emits a sequence of internal operators (Figure 3c), which, <lb/>once executed, result in the memory layout of the target format (Figure 1n). <lb/>4 TENSOR FORMAT ABSTRACTION <lb/>This section describes the proposed format abstraction that supports custom data structures and <lb/>layouts for sparse tensors. We first describe the expression of sparse data structures in Section 4.1, <lb/>which is logically represented as a metadata tree with index maps, query, and mutation primitives. <lb/>The metadata tree representation and index maps are first introduced in TACO [Kjolstad et al. 2017; <lb/>Chou et al. 2020a]. This work extends index maps to express a diverse range of custom formats. We <lb/>further propose a new encoding method that holistically expresses data structures in primitives, <lb/>without presuming per-dimension data structure separation. Section 4.2 shows how memory layouts <lb/>are determined by applying layout primitives to the metadata tree, allowing formats to be expressed <lb/>and customized at the physical layout level, which is also our new contribution. <lb/>4.1 Metadata Tree <lb/>To retain all the information from the original tensor, sparse formats store the metadata of non-zero <lb/>elements explicitly. The structure of metadata determines the dimension order and intra-dimension <lb/>coordinate arrangement, which dictate the structure of the corresponding value elements. The <lb/>metadata structure is represented by a metadata tree that stacks tensor dimensions with the major <lb/>one at the top. Value elements are attached at the bottom of the metadata tree, each associated with <lb/>a path of the tree. In the following, we will introduce how the metadata tree is expressed by index <lb/>maps ( §4.1.1), query ( §4.1.2), and mutation ( §4.1.3) primitives. Figure 4 depicts the metadata trees <lb/>of selected formats presented in Figure 1. <lb/>4.1.1 Index Map. We define the index map, denoted by M, as a mapping from a tuple of logical <lb/>dimension iterators to a tuple of destination index expressions that describe physical dimension <lb/>iterators: <lb/>M ≔ ( 0 , 1 , ..., ) ↦ → ( 0 , 1 , ..., ) <lb/>where the logical indices ( 0 , 1 , ..., ) identify the dimensions of the original tensor, and the <lb/>physical index expressions ( 0 , 1 , ..., ) serve as new dimension identifiers of the sparse format. The <lb/>index map of a lossless sparse format has its reverse map M -1 ≔ ( 0 , 1 , ..., ) ↦ → ( 0 , 1 , ..., ), <lb/>which retrieves the dimension iterators of the original tensor. <lb/>Index maps affect the data structures of a sparse format by determining the major order of <lb/>dimensions and the values of metadata. A simple example of an index map is (d0, d1) -&gt; (d1, <lb/>d0), which represents a column-major matrix layout. In Figure 4, physical index expressions are <lb/>associated with each level of the metadata tree for different formats in Figure 4a -4i, where the <lb/>original matrix in Figure 1a is indexed by d0 and d1. For example, the CSB format in Figure 4g <lb/>uses an index map of (d0, d1) -&gt; (d0/2, d1/2, d0%2, d1%2), where block ids (d0/2, d1/2) are <lb/>at the major two levels, and (d0%2, d1%2) are inner block dimensions. Another example is the <lb/>DIA format. The traditional DIA format (Figure 4e) uses an index map of (d0, d1) -&gt; (d1-d0, d0), <lb/>which stores elements along the same diagonal with diagonal offsets expressed by (d1-d0) at the <lb/>major dimension. By changing the second dimension of the dst-index-list in the traditional DIA <lb/>format from d0 to d1, a variant of the DIA format is obtained, as shown in Figure 4d. In the given <lb/>example, the DIA-variant format is more space-efficient with d1 ranging from 0 to 3 than the <lb/>traditional DIA format with d0 ranging from 0 to 4. Compared to TACO [Chou et al. 2018], which <lb/>only supports the traditional DIA format by hardcoding level functions with types &quot;range&quot; and <lb/>&quot;offset&quot;, our abstraction is more flexible and expressive by covering a wider range of formats such <lb/>as DIA-variant via the index map. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:10 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>3 <lb/>5 <lb/>3 <lb/>4 <lb/>6 <lb/>3 <lb/>A <lb/>d1 <lb/>d0 <lb/>val <lb/>(a) CSR or LIL <lb/>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>5 <lb/>3 <lb/>4 <lb/>6 <lb/>3 <lb/>A <lb/>d1 <lb/>d0 <lb/>val <lb/>(b) DCSR <lb/>1 <lb/>4 <lb/>3 <lb/>5 <lb/>0 <lb/>2 <lb/>1 <lb/>3 <lb/>0 <lb/>1 <lb/>0 <lb/>2 <lb/>6 <lb/>3 <lb/>1 <lb/>val <lb/>d1 <lb/>d0 % 2 <lb/>2 <lb/>1 <lb/>A <lb/>0 <lb/>1 <lb/>d0 / 2 <lb/>2 <lb/>(c) C 2 SR <lb/>3 <lb/>0 <lb/>0 <lb/>A <lb/>6 <lb/>2 <lb/>1 <lb/>4 0 <lb/>1 <lb/>0 <lb/>2 3 <lb/>0 0 0 <lb/>0 1 2 <lb/>5 <lb/>3 <lb/>1 <lb/>-1 <lb/>0 <lb/>0 <lb/>2 <lb/>1 <lb/>3 <lb/>d1 <lb/>d1 -d0 <lb/>val <lb/>(d) DIA-variant <lb/>0 <lb/>3 <lb/>0 <lb/>0 <lb/>A <lb/>6 <lb/>2 <lb/>1 <lb/>4 0 <lb/>0 <lb/>2 <lb/>1 <lb/>3 4 <lb/>0 0 0 5 <lb/>0 1 2 <lb/>0 0 <lb/>3 4 <lb/>1 <lb/>-1 <lb/>0 <lb/>0 <lb/>2 <lb/>1 <lb/>3 4 <lb/>val <lb/>d1 -d0 <lb/>d0 <lb/>(e) DIA <lb/>1 <lb/>0 <lb/>0 <lb/>2 <lb/>0 <lb/>1 <lb/>0 <lb/>1 <lb/>0 <lb/>A <lb/>0 <lb/>0 <lb/>0 <lb/>3 <lb/>0 <lb/>1 <lb/>0 <lb/>1 <lb/>4 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>1 0 <lb/>0 <lb/>1 <lb/>1 0 <lb/>5 0 0 <lb/>0 1 1 <lb/>0 <lb/>0 <lb/>1 0 1 0 <lb/>6 <lb/>0 <lb/>1 <lb/>0 <lb/>1 <lb/>1 <lb/>0 <lb/>1 <lb/>2 <lb/>0 0 <lb/>1 1 <lb/>0 1 <lb/>d1 / 2 <lb/>d0 / 2 <lb/>d0 % 2 <lb/>d1 % 2 <lb/>val <lb/>(f) BCSR <lb/>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>A <lb/>5 <lb/>1 <lb/>0 <lb/>6 <lb/>1 <lb/>0 <lb/>0 <lb/>0 <lb/>0 <lb/>1 <lb/>0 <lb/>2 <lb/>1 <lb/>1 <lb/>d1 / 2 <lb/>d0 / 2 <lb/>d0 % 2 <lb/>d1 % 2 <lb/>val <lb/>1 <lb/>0 <lb/>1 <lb/>2 <lb/>(g) CSB <lb/>1 <lb/>3 <lb/>2 <lb/>0 <lb/>0 <lb/>2 <lb/>1 <lb/>3 <lb/>0 <lb/>A <lb/>6 <lb/>4 <lb/>0 <lb/>0 <lb/>4 <lb/>1 <lb/>0 <lb/>2 <lb/>0 <lb/>3 <lb/>0 <lb/>1 <lb/>1 <lb/>0 3 <lb/>0 <lb/>1 <lb/>2 1 <lb/>0 0 0 <lb/>4 0 1 <lb/>5 <lb/>2 <lb/>0 2 2 3 <lb/>0 0 <lb/>3 4 <lb/>2 1 <lb/>1 <lb/>2 <lb/>indirect(d1) <lb/>d1 <lb/>d0 <lb/>val <lb/>(h) ELLPACK <lb/>1 <lb/>2 <lb/>5 <lb/>6 <lb/>0 <lb/>1 <lb/>3 <lb/>3 <lb/>1 <lb/>2 <lb/>3 <lb/>1 <lb/>val <lb/>d1 <lb/>d0 <lb/>4 <lb/>2 <lb/>A <lb/>0 <lb/>1 <lb/>indirect(d0) <lb/>4 <lb/>0 <lb/>(i) CISR <lb/>Fig. 4. The metadata trees of the matrix in Figure 1a -Yellow-shaded blocks refer to the data structure of <lb/>the same tensor element. The root of a metadata tree is the symbol of the tensor. 0 and 1 are sub-matrices <lb/>of the matrix . The circular nodes are metadata nodes that contain indices, and the arrows represent pointers <lb/>from a higher major dimension to a lower one. Index expressions are on the le of each coordinate level. <lb/>Nodes with dashed lines are elements padded for memory alignment. <lb/>1 index-map := src-index-list `-&gt;`dst-index-list <lb/>2 src-index-list := `(`id {`,`id} `)3 <lb/>dst-index-list := `(`dst-expr {`,`dst-expr} `)4 <lb/>dst-expr := id_expr <lb/>5 <lb/>| `-`? integer-literal <lb/>6 <lb/>| `-`? integer-literal `*`dst-expr <lb/>7 <lb/>| dst-expr `+`dst-expr <lb/>8 <lb/>| dst-expr `-`dst-expr <lb/>9 <lb/>| id_expr `%`integer-literal <lb/>10 <lb/>| id_expr `/`integer-literal <lb/>11 id_expr := id <lb/>12 <lb/>| `indirect`dst-index-list <lb/>(a) The syntax of index maps. <lb/>1 query-primitive := query-func `(`var {`,`var} <lb/>`)`{group-by-clause} {traverse-by-clause} <lb/>{`with`val-map} <lb/>2 query-func := `sum`| `enum`| `reorder`| <lb/>`schedule3 <lb/>group-by-clause := `groupBy`index-map <lb/>4 traverse-by-clause := `traverseBy`index-map <lb/>5 val-map := {cond-val `-&gt;`integer-literal `|`} <lb/>`otherwise``-&gt;`integer-literal <lb/>6 cond-val := var `ne`| `eq`| `bt`| `be`| `lt| <lb/>`le`integer-literal <lb/>7 var := id | value <lb/>(b) The syntax for query primitives. <lb/>Fig. 5. The syntax for index maps and query primitives. id denotes dimension identifiers, value denotes <lb/>tensor element values, and integer-literal denotes constant numbers. Expressions enclosed in curly braces <lb/>can be repeated zero or more times. <lb/>The UniSparse syntax of index maps is shown in Figure 5a. The map takes a list of logical dimen-<lb/>sion indices as inputs and returns a list of arithmetic expressions that represent physical dimension <lb/>iterators. The physical dimension iterators are typically expressed as closed-form functions using <lb/>basic arithmetic operations (e.g., +, -, *, /, %) as shown in Line 4-10 of Figure 5a. We refer to index <lb/>maps with pure arithmetic operations as direct maps. For generality, we further allow user-defined <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:11 <lb/></page>

        <body>custom functions, namely indirect maps, for the index iterator, rather than being limited to closed-<lb/>form expressions. These dimension iterators are marked with the keyword indirect, followed by <lb/>their parameter index terms (expressed as dst-index-list). Examples of indirect maps can be seen <lb/>at the major dimensions of several formats, including ELLPACK (Figure 1k), CISR (Figure 1m), and <lb/>the BELL portion of the hybrid BELL/COO (Figure 1o) format. In the next section, we introduce <lb/>how indirect map functions are constructed in more detail. <lb/>4.1.2 <lb/>ery Primitives. UniSparse provides methods to obtain statistics of a sparse tensor through <lb/>query primitives. Each query primitive consists of a query function, a group-by clause, a traverse-by <lb/>clause, and a value map. The syntax of query primitives is shown in Figure 5b. <lb/>The group-by clause uses an index map with usually fewer dimensions in the destination index <lb/>expressions to divide tensor elements into groups. Commonly used operators include divide and <lb/>modulo. For instance, the group-by function with map (d0, d1) -&gt; (d0%2) assigns values in even <lb/>rows to one group and values in odd rows to another group. Another map (d0, d1) -&gt; (d1-d0) <lb/>groups elements on the same diagonals together. The traverse-by clause specifies the traversing <lb/>order of the indices with also an index map. For example, traverse-by (d0, d1) -&gt; (d0) and (d0, <lb/>d1) -&gt; (d1) indicate traversing elements in increasing order of the row and column dimension <lb/>identifier, respectively. The value map assigns a new value for each set of values satisfying a certain <lb/>condition. For example, value ne 0 -&gt; 1 | otherwise -&gt; 0 assigns non-zero elements to 1 and <lb/>others to 0. <lb/>We predefine several query functions in Line 2: sum, enumerate, reorder, and schedule. The sum <lb/>function computes the accumulation of values returned by the value map with a set of groups <lb/>defined by the group-by clause. The enumerate assigns counting numbers with the start number <lb/>defined by the value map in groups that follow the specified traversing order by traverse-by. The <lb/>reorder returns the indices of the sorted elements with a specific traversing order. The schedule <lb/>assigns tensor elements into a specified number of partitions in a balanced manner. <lb/>There are primarily two scenarios in UniSparse where query primitives are used. The first <lb/>scenario involves supporting hybrid formats, where the sum function is used to query the non-zero <lb/>distribution patterns before decomposing the input tensor. The other scenario involves supporting <lb/>custom index maps with indirect mapping functions. Figure 7 illustrates how query primitives <lb/>are used to construct indirect functions. Specifically, sum and enumerate are used to construct the <lb/>indirect level of the ELL format in Lines 10. These functions assign counting numbers to non-zero <lb/>and zero elements in increasing order. The sum and schedule functions constitute the indirect level <lb/>in the CISR format (Line 14), which assigns rows to two buckets, each with a balanced number of <lb/>non-zero elements. <lb/>4.1.3 Mutation Primitives. While index maps can be used to define new index values and change <lb/>the dimension order, they do not take advantage of the sparsity to compress the data structures. In <lb/>this section, we present primitives that enable the compression of tensor data structures. Specifically, <lb/>we propose two mutation primitives -trim, which indicates the removal of tensor components <lb/>associated with zero-values, and merge, which expresses the reduction of replicated components. <lb/>We also introduce conversion operators related to the primitives trim and merge to better illustrate <lb/>how the expression of one format can be changed to another format by making slight modifications <lb/>to its encoding. Note that these conversion operators are distinct from encoding primitives used <lb/>to describe formats. To differentiate between them, we name conversion operators in CamelCase <lb/>notation. <lb/>Trim. The trim primitive takes two numerical values representing dimension levels, where the <lb/>number 0 corresponds to the major dimension at the top of the metadata hierarchy, and subsequent <lb/>levels increase from top to bottom. The primitive trim(S,E) indicates the removal of zero values (at <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:12 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>2 <lb/>A <lb/>5 <lb/>3 <lb/>2 <lb/>d1 <lb/>d0 <lb/>val <lb/>6 <lb/>3 <lb/>4 <lb/>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>5 <lb/>3 <lb/>4 <lb/>6 <lb/>3 <lb/>A <lb/>d1 <lb/>d0 <lb/>val <lb/>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>3 <lb/>5 <lb/>3 <lb/>4 <lb/>6 <lb/>3 <lb/>A <lb/>d1 <lb/>d0 <lb/>val <lb/>1 <lb/>3 <lb/>2 <lb/>4 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>2 <lb/>A <lb/>5 <lb/>3 <lb/>2 <lb/>d1 <lb/>d0 <lb/>val <lb/>6 <lb/>3 <lb/>4 <lb/>3 <lb/>Format D (DCSR) <lb/>trim(0,1), merge(0) <lb/>Format C (CSR/LIL) <lb/>trim(1,1), merge(0) <lb/>Format B (COO/DOK) <lb/>trim(0,1) <lb/>Format A <lb/>trim(1,1) <lb/>Trim(0) <lb/>Fill(0) <lb/>Trim(0) <lb/>Fill(0) <lb/>Merge(0) <lb/>Split(0) <lb/>Merge(0) <lb/>Split(0) <lb/>DOK <lb/>d0 <lb/>idx <lb/>d1 <lb/>1 <lb/>val <lb/>2 3 <lb/>6 <lb/>4 5 <lb/>0 <lb/>0 <lb/>1 <lb/>1 <lb/>2 <lb/>1 <lb/>4 <lb/>3 <lb/>2 <lb/>2 <lb/>2 <lb/>3 <lb/>COO <lb/>4 <lb/>2 <lb/>2 <lb/>0 <lb/>2 <lb/>1 <lb/>d0 <lb/>idx <lb/>3 <lb/>1 <lb/>1 <lb/>2 <lb/>3 <lb/>0 <lb/>d1 <lb/>5 <lb/>3 <lb/>2 <lb/>4 <lb/>6 <lb/>1 <lb/>val <lb/>idx <lb/>LIL <lb/>5 <lb/>1 <lb/>d0 <lb/>0 <lb/>size <lb/>idx <lb/>3 <lb/>2 <lb/>1 <lb/>d1 <lb/>1 <lb/>val <lb/>3 <lb/>* <lb/>2 <lb/>5 <lb/>4 <lb/>3 <lb/>6 <lb/>* <lb/>CSR <lb/>5 <lb/>d0 <lb/>size <lb/>3 <lb/>1 <lb/>1 <lb/>2 <lb/>3 <lb/>0 <lb/>d1 <lb/>5 <lb/>3 <lb/>2 <lb/>4 <lb/>6 <lb/>1 <lb/>val <lb/>idx <lb/>6 <lb/>5 <lb/>5 <lb/>0 <lb/>2 <lb/>1 <lb/>ptr <lb/>Pack(0,1) <lb/>Pack(0,1) <lb/>Fig. 6. An illustration of mutation primitives and layout primitives. The formats A, B, C, and D of the matrix <lb/>in Figure 1a can be described using different combinations of the merge and trim primitives. Each format <lb/>can be transformed into its neighbor via a conversion operator indicated by the arrows. Green arrows denote <lb/>sparsification directions, and red arrows denote densification directions. Notice that the two conversion <lb/>operators Trim and Merge are orthogonal and commutative. Blue arrows point to different physical layouts <lb/>of the formats. <lb/>the bottom of the tree) and the associated metadata (on the path from the root to leaves) between a <lb/>starting level S and an ending level E, where S≤E (i.e., S is closer to the root than E). Specifically, <lb/>trim(S,E) first removes data nodes belonging to the sub-tensor identified by (d0, d1, ..., dE), if <lb/>all elements in that sub-tensor are zeros. Then it cleans up dangling nodes that have no children <lb/>nodes from level E up to level S. For example, in formats B and D of Figure 6, the primitive trim(0,1) <lb/>indicates that all zero values identified by dimensions (d0, d1) are removed, along with all their <lb/>metadata. In contrast, the primitive trim(1,1) in formats A and C only removes zero values and <lb/>their column indices at the least major dimension (d1), while leaving dangling nodes (row index 3) <lb/>at dimension (d0). <lb/>The conversion operator Trim(L) implements the removal of metadata at level L if the sub-tensor <lb/>identified by indices at level L contains only zeros. The reverse operator Fill(L) inserts nodes at <lb/>level L for all existing parents by creating nodes with missing index values in the range of [0,DL) <lb/>where DL is the dimension size at level L. Figure 6 shows how Fill(0) converts format B to A and <lb/>D to C by adding index nodes for the empty row. <lb/>Merge. The merge primitive reduces storage space by merging equivalent paths at specified <lb/>levels. It takes in a list of numerical values representing dimension levels, and for each level L that <lb/>has merge applied, all repeating paths from the top-level dimension down to level L will be fused <lb/>into one path. Children nodes at level L+1 sharing the same metadata are inherited by the fused <lb/>parent node. In Figure 6, formats C and D have the encoding merge(0), which indicates fusing <lb/>repetitive nodes at dimension (d0). <lb/>The conversion operator Merge(L) implements the fusion of repetitive metadata nodes at level L. <lb/>The reverse operator Split(L) restores multiple copies of parent paths from the root to level L for <lb/>nodes with more than one child at level L. In Figure 6, applying Split(0) converts format C to A <lb/>and D to B by replicating nodes at the major dimension, ensuring that each row index node has <lb/>only one child node. <lb/>4.2 Data Layout <lb/>A metadata tree can be realized using different physical memory layouts. By default, it is stored in <lb/>an SoA layout, where each dimension is stored using separate arrays. UniSparse further provides <lb/>two primitives to express different memory layout options. The partition primitive determines <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:13 <lb/></page>

        <body>1 #COO = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d0,d1)&gt;, mutation&lt;trim(0,1)&gt; }&gt; <lb/>2 #DOK = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d0,d1)&gt;, mutation&lt;trim(0,1)&gt;, layout&lt;pack(0,1)&gt; }&gt; <lb/>3 #CSR = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d0,d1)&gt;, mutation&lt;merge(0), trim(1,1)&gt; }&gt; <lb/>4 #DCSR = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d0,d1)&gt;, mutation&lt;merge(0), trim(0,1)&gt; }&gt; <lb/>5 #DIA = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d1-d0,d0)&gt;, mutation&lt;merge(0), trim(0,0)&gt; }&gt; <lb/>6 #DIA-variant = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d1-d0,d1)&gt;, mutation&lt;merge(0), trim(0,0)&gt; }&gt; <lb/>7 #BCSR = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d0/2,d1/2,d0%2,d1%2)&gt;, mutation&lt;merge(0,1), trim(1,1)&gt; }&gt; <lb/>8 #CSB = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d0/2,d1/2,d0%2,d1%2)&gt;, mutation&lt;merge(1), trim(2,3)&gt; }&gt; <lb/>9 #ELL = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(indirect(d1),d0,d1)&gt;, mutation&lt;merge(0), trim(0,0)&gt;, <lb/>10 <lb/>indirect&lt;{ sum(value) groupBy (d0,d1)-&gt;(d0) with value ne 0 -&gt; 1 | otherwise -&gt; 0 <lb/>11 <lb/>enum(value) groupBy (d0,d1)-&gt;(d0) traverseBy (d0,d1)-&gt;(d1) with value eq 0 -&gt; sumVal | <lb/>otherwise -&gt; 0 }&gt; }&gt; <lb/>12 #C2SR = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(d0%2,d0/2,d1)&gt;, mutation&lt;merge(0,1), trim(2,2)&gt;, layout&lt;partition(0)&gt;}&gt; <lb/>13 #CISR = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(indirect(d0),d0,d1)&gt;, mutation&lt;merge(0,1), trim(1,2)&gt;, <lb/>14 <lb/>indirect&lt;{ sum(value) groupBy (d0,d1)-&gt;(d0) with value ne 0 -&gt; 1 | otherwise -&gt; 0 <lb/>15 <lb/>schedule(d0) traverseBy (d0,d1)-&gt;(d0/2) }&gt;, <lb/>16 <lb/>layout&lt;partition(0)&gt; }&gt; <lb/>Fig. 7. Encodings of selected sparse formats in Figure 1 wri en in pseudo MLIR code. <lb/>whether to divide one tensor into several sub-tensors, which can be beneficial for storing the sparse <lb/>tensor in banked memories. The pack primitive switches from the SoA layout to an AoS layout for <lb/>a single tensor, providing a larger design space with more layout options for users to explore. <lb/>Partition. The partition primitive splits metadata with their descendant nodes at the specified <lb/>dimension level into subtrees, each stored separately as a sub-tensor. This is particularly useful for <lb/>multi-bank memory systems, where each sub-tensor can be stored in a different bank to reduce <lb/>memory access conflicts and improve performance. For example, the C 2 SR and CISR formats <lb/>are partitioned into sub-tensors with partition(0), and each sub-tensor is designed to occupy a <lb/>memory bank. The format encodings of the C 2 SR and CISR formats are shown in Figure 7. <lb/>Pack. The pack primitive enables the creation of an AoS layout for a tensor by specifying <lb/>two dimension levels, S and E, and enforcing a depth-first traversal of the metadata tree for <lb/>dimensions from S to E. By default, a breadth-first traversal of the metadata tree generates level-wise <lb/>arrays which correspond to an SoA format. For example, in Figure 6, a breadth-first traversal of <lb/>the metadata tree B results in {d0=[0,1,2,2,2,4], d1=[0,1,1,2,3,3], val=[1,2,3,4,5,6]}, which <lb/>corresponds to the COO format. Similarly, the default traversal order of the metadata tree C leads <lb/>to an SoA layout, which is the CSR format. On the other hand, adding the primitive pack(0,1) to <lb/>the metadata tree B pairs up index values at dimensions (d0, d1) and generates an AoS layout, <lb/>i.e., [{d0=0, d1=0, val=1}, {d0=1, d1=1, val=2}, ..., {d0=4, d1=3, val=6}], corresponding to <lb/>the DOK format. Similarly, adding the primitive pack(0,1) to format C generates the AoS layout <lb/>counterpart of the CSR format, which is the LIL format. <lb/>With the above primitives, UniSparse can express a wide range of custom formats, including <lb/>those with custom index maps (e.g., load-balanced formats in Figure 1m), sparsity pattern-aware <lb/>hybrid formats (e.g., Figure 1n/1o), and backend-aware memory layouts (e.g., banked formats in <lb/>Figure 1l/1m). Figure 7 lists the abstractions of all the formats in Figure 1. <lb/>5 COMPILATION <lb/>To generate code for format customization and compute operations, the UniSparse compiler first <lb/>decodes the data structures and layouts from format descriptions specified in the intermediate <lb/>language ( §5.1). Once the source and destination formats are determined, the compiler lowers the <lb/>convert operation by applying a sequence of rewrite rules and emitting conversion operators that <lb/>gradually convert from the source format to the destination format ( §5.2). For compute operation <lb/>lowering, UniSparse leverages the MLIR SparseTensor dialect to handle classic formats with the <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:14 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>index map (d0, d1)-&gt;(d0, d1). In the case of custom formats, UniSparse implements a general <lb/>compute kernel generation algorithm ( §5.3). <lb/>5.1 Inference of Data Structure and Layout <lb/>The UniSparse compiler infers the data structures and layouts, like those shown in Figure 1, from <lb/>the formats specified in the UniSparse language (Figure 7). The conservative approach for storing a <lb/>sparse format involves maintaining an array of indices (idx) and an array of pointers (ptr) at each <lb/>level of the metadata tree. An element in an idx array identifies a node at the current tree level. An <lb/>element in a ptr array indicates how many nodes are connected to a parent node, i.e., it encodes a <lb/>down arrow ↓ in Figure 4. However, the storage can be simplified in special cases. For example, <lb/>when the indices at the current level are contiguous numbers starting from 0, the idx array can <lb/>simply be replaced with a size. In another case where nodes have a one-to-one correspondence <lb/>with their parents, the ptr array can be skipped. <lb/>Based on these principles, the compiler infers the simplified data structures of a sparse format. A <lb/>dimension level with no trim or merge applied is stored in size. The trim(S,E) primitive requires <lb/>explicit idx arrays at dimensions from S to level E, as indices at these levels are no longer contiguous <lb/>after being trimmed. The merge primitive adds a ptr array to the descendant levels of the specified <lb/>levels, if the descendant levels also have trim applied, since the number of children nodes varies <lb/>from one parent node to another at the level being merged. For example, to store the CSR format, an <lb/>idx array is required at the column dimension because it is trimmed, and a ptr array is also needed <lb/>at the column dimension, as the parent dimension (i.e., the row dimension) is merged. The DCSR <lb/>format requires an idx array at both the row and column dimensions since they are both trimmed. <lb/>The DIA format requires an idx array at the major dimension, which stores the diagonal offsets. <lb/>Although the major dimension has been merged, it does not add a ptr array to its descendant level <lb/>since the second dimension is not trimmed. Indirect maps introduce less regular index patterns, <lb/>which also require an explicit idx array. <lb/>The aforementioned steps infer the data structures of a sparse format, but the actual physical <lb/>layout is not yet determined. In UniSparse, the physical memory layout of a sparse tensor can be <lb/>customized using the partition and pack primitives. The partition primitive divides indices and <lb/>all their descendant metadata at the specified level into sub-tensors. For example, the C 2 SR format <lb/>stores sub-tensors separately in memory banks, with the partition primitive applied at the major <lb/>level. On the other hand, the pack(S,E) primitive generates an AoS layout of the tensor from level <lb/>S to level E, as opposed to the default SoA layout. The value array at the bottom of the metadata <lb/>tree, which is treated as an additional level of data, can also be packed with metadata. If the packed <lb/>levels have the same number of elements, they are stored in an array of tuples, such as the DOK <lb/>format. Alternatively, the elements can be stored in an array of variable-length lists, such as the <lb/>LIL format. <lb/>5.2 Format Conversion <lb/>The UniSparse compiler incorporates a general algorithm that automates conversion between <lb/>any two custom formats specified with mutation primitives and index maps that contain purely <lb/>arithmetic operations {+, -, *, %, /}. Indirect functions and layout primitives are not reversible, <lb/>and therefore, UniSparse does not support automated conversion when the source format contains <lb/>indirect functions or layout primitives. However, they are allowed in the target format. <lb/>The format conversion algorithm within UniSparse involves a thorough analysis of both the <lb/>source and destination format encoding. It proceeds by individually conducting pattern matching <lb/>on each component of the source and target format encoding. This process applies a series of <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:15 <lb/></page>

        <body>rewrite rules to convert the source format step-by-step to the target format. These rewrite rules are <lb/>implemented as conversion operators that transform data structures and layouts in atomic steps. <lb/>Table 2 presents a collection of rewrite rules along with their corresponding conversion operators. <lb/>These rewrite rules can be categorized into four sets: the rules for rewriting index maps including <lb/>affine ( 1 , 2 , and 3 ) and non-affine ( 4 and 5 ) transformations, for matching mutation primitives <lb/>( 6 -11), for querying (12-15) and for layout transformation (16-17). <lb/>Affine transformations are equivalently represented as matrices in the table. For instance, consider <lb/>rule 1 , which matches the case when two dimensions in the source format switch their positions <lb/>in the target format. The effect of rule 1 is expressed through an elementary transformation matrix <lb/>as shown in the &quot;Matrix&quot; column of the table, and the operator Swap updates the data structures in <lb/>the target metadata tree. Non-affine transformation rules are applicable when there are `/`and <lb/>`%`in the source or target index map. These non-affine transformations are implemented using the <lb/>TileUnion and TileSplit operators. <lb/>Rule 6 -11 handle the situations when either the source or the target format contains mutation <lb/>primitives. For example, if dimension to dimension are trimmed in the source format, but <lb/>the preceding dimension -1 is additionally trimmed in the target format, the corresponding <lb/>conversion operator should perform this additional trim (Rule 6 ). Conversely, if the target format <lb/>has one less dimension trimmed than the source format, the corresponding conversion operator <lb/>should fill that dimension (Rule 7 ). Similarly, the formats could differ in the last trimmed dimension, <lb/>and these cases are handled by Rule 8 and 9 . These two rules also cover the special situation <lb/>when == , in which case one format contains trim and the other does not. Furthermore, Rule <lb/>10 merges one more dimension based on the source format, and Rule 11 removes one merged <lb/>dimension. <lb/>Finally, Rule 12-17 directly work on the target format when the target format contains query or <lb/>layout primitives. Note that these two sets of rewrite rules, as well as the encoding primitives, are <lb/>open-ended and can be extended when necessary to support new user-defined custom formats. <lb/>The rewrite rules presented in Table 2 ensure that UniSparse is fully capable of converting formats <lb/>encoded only with direct index maps and mutation primitives (referred to as set ) into arbitrary <lb/>formats expressed in UniSparse notation (referred to as set ). To demonstrate the completeness of <lb/>format conversion in UniSparse, we first establish its ability to convert between any two formats <lb/>within set using the rewrite rules provided in Table 2. Then, we discuss the conversion from a <lb/>format in set to a format in set -that involves indirect functions and layout primitives. <lb/>We discuss the conversion between arbitrary formats in set in two steps. <lb/>(i) Reversibility. The conversion process between formats within set is reversible, implying that <lb/>for any two formats, A and B, encoded solely using direct index maps and mutation primitives, <lb/>if there exists a sequence of conversion operators transforming format A to format B, there <lb/>must also exist a conversion path that can transform format B back to format A. This reversibility <lb/>property is guaranteed by the conversion operators outlined in Table 2. Specifically, Table 2 contains <lb/>arithmetic conversion operators, each having its reverse pair: Swap(i, j) and Swap(j, i), Scale(i, <lb/>f) and Scale(i, 1/f), Skew(i, j, f) and Skew(i, j, -f), TileUnion(i, f) and TileSplit(i, f). <lb/>Furthermore, the conversion operators for rule 6 and 7 , those for rule 8 and 9 , and rule 10 <lb/>and rule 11 also form reverse pairs. <lb/>(ii) Reachability. We further show that there exists a reference format, denoted as , from which <lb/>all formats within set can be reached, meaning they can be converted from . Without loss of <lb/>generality, we select the COO format as . Given the reversibility property of format conversion <lb/>within set , we only need to show that any arbitrary format within set can be converted into <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:16 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>Table 2. Rewrite rules and conversion operators that support the UniSparse format conversion algorithm. <lb/>Parameters of query primitives are omi ed as {*} due to the limited length of the table. The syntax of the <lb/>query primitives is shown in Figure 5b, and examples can be found in Figure 7. <lb/>Type Rule <lb/>Source <lb/>Target <lb/>Conversion <lb/>Matrix <lb/>Operator <lb/>Index Map <lb/>1 <lb/>( 0 , ..., , ..., , ..., <lb/>) <lb/>( 0 , ..., , ..., , ..., <lb/>) <lb/>1 . . . <lb/>0 ... 1 <lb/>. . . <lb/>. . . <lb/>. . . <lb/>1 ... 0 . . . <lb/>1 <lb/>Swap(i,j) <lb/>2 <lb/>( 0 , ..., , ..., <lb/>) <lb/>( 0 , ..., * , ..., <lb/>) <lb/>1 . . . <lb/>. . . <lb/>1 <lb/>Scale(i,f) <lb/>3 <lb/>( 0 , ..., , ..., , ..., <lb/>) ( 0 , ..., , ..., * + , ..., <lb/>) <lb/>1 . . . <lb/>1 ... 0 <lb/>. . . <lb/>. . . <lb/>. . . <lb/>... 1 <lb/>. . . <lb/>1 <lb/>Skew(i,j,f) <lb/>4 ( 0 , ..., / , % , ..., <lb/>) <lb/>( 0 , ..., , ..., <lb/>) <lb/>TileUnion(i,f) <lb/>5 <lb/>( 0 , ..., , ..., <lb/>) <lb/>( 0 , ..., / , % , ..., <lb/>) <lb/>TileSplit(i,f) <lb/>Type Rule <lb/>Source <lb/>Target <lb/>Conversion <lb/>Type Rule Target Conversion <lb/>Mutation <lb/>6 <lb/>( , ), ≤ <lb/>( -1 , ), ≤ <lb/>Trim(S-1) <lb/>Query <lb/>12 <lb/>{ * } <lb/>Sum() <lb/>7 <lb/>( -1 , ), ≤ <lb/>( , ), ≤ <lb/>Fill(S-1) <lb/>13 <lb/>{ * } <lb/>Enumerate() <lb/>8 <lb/>( , -1 ), ≤ <lb/>( , ), ≤ <lb/>Devectorize(E), <lb/>14 <lb/>{ * } <lb/>Reorder() <lb/>Trim(E), Vectorize(E+1) <lb/>15 <lb/>ℎ <lb/>{ * } <lb/>Schedule() <lb/>9 <lb/>( , ), ≤ <lb/>( , -1 ), ≤ <lb/>Devectorize(E+1), <lb/>Layout <lb/>16 <lb/>( , ) Pack(S, E) <lb/>Fill(E), Vectorize(E) <lb/>10 <lb/>( 0, ..., -1 ) <lb/>( 0, ..., -1, ) <lb/>Merge(i) <lb/>17 <lb/>( ) Partition(i) <lb/>11 <lb/>( 0, ..., -1, ) <lb/>( 0, ..., -1 ) <lb/>Split(i) <lb/>the COO format. This can be achieved through the following observations: Rule 5 eliminates <lb/>pairs of divide and modulo operations, ensuring that any formats within set can be converted <lb/>into those without divide and modulo operations in the index maps; rules 1 through 3 cover <lb/>the entire affine transformation space, allowing for the conversion of affine index expressions into <lb/>the original dimension identifiers in the COO format; and rule 11 removes merged levels, while <lb/>rules 6 and 8 expand trimmed levels, enabling the conversion of any mutation primitives into <lb/>the encoding form of COO format. <lb/>Combining the two key points (i) and (ii), we can conclude that UniSparse can convert between <lb/>any format within set using the rewrite rules and conversion operators in Table 2. <lb/>For the conversion from a format within set to a format within set -, which includes <lb/>indirect functions and layout primitives, one can directly apply rules 12 through 17 that align a <lb/>source format with the target format&apos;s encoding. Building upon the earlier discussion on format <lb/>conversion within set , we know that if there exists one format within set capable of converting <lb/>to any format within -, then any arbitrary format from set will have the ability to convert to <lb/>any format in -as well. As a result, UniSparse can perform conversions from any format in set <lb/>to any format in set . <lb/>Figure 8 shows the pseudocode of our format conversion algorithm, which includes three steps. <lb/>Initially, the compiler aligns the source index map with the target index map (Step 1), followed by <lb/>the mutation of data structures (Step 2). Finally, the memory layout is generated (Step 3). <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:17 <lb/></page>

        <body>1 Algorithm: Format Conversion <lb/>2 Input: source format encoding S, target format encoding T, source format storage S_format <lb/>3 Output: target format storage T_format <lb/>4 FormatConversion { <lb/>5 <lb/>T_format = S_format // directly manipulate the source format storage <lb/>6 <lb/>if index_map of S != index_map of T: // step 1: Index Map Alignment <lb/>7 <lb/>if index_map of S contains (di/f, di%f): <lb/>8 <lb/>apply Rule 4 to T_format; <lb/>9 <lb/>transMatrix = indexMapMatrix(S) -1 * indexMapMatrix(T); <lb/>10 <lb/>for each non-identity sub-matrix E of transMatrix: <lb/>11 <lb/>apply E to T_format; // Rule 1 , 2 , 3 in Table 2 <lb/>12 <lb/>if index_map of T contains (di/f, di%f): <lb/>13 <lb/>apply Rule 5 to T_format; <lb/>14 <lb/>for each indirect_function f of {sum, enum, reorder, schedule} in T: <lb/>15 <lb/>apply f to T_format; // Rule 12-15 in Table 2 <lb/>16 <lb/>apply Sort to T_format; // sort indices <lb/>17 <lb/>while trim of S != trim of T: // step 2: Structure Mutation <lb/>18 <lb/>apply Rule 6 / 7 / 8 / 9 to T_format; <lb/>19 <lb/>while merge of S != merge of T: <lb/>20 <lb/>apply Rule 10/11 to T_format; <lb/>21 <lb/>for each layout l of {pack, partition} in T: // step 3: Layout Generation <lb/>22 <lb/>apply l to T_format; // Rule 16,17 in Table 2 <lb/>23 <lb/>return T_format; <lb/>24 } <lb/>Fig. 8. The pseudo-code of the UniSparse format conversion algorithm. <lb/>In the first step, new indices are calculated for the target format (line 6 -16). If the source format <lb/>encoding S contains `/`and `%`operations, the compiler employs TileUnion to eliminate these <lb/>non-affine operations and generate an affine index map of S (line 7 -8). Next, the compiler computes <lb/>the transformation matrix transMatrix by multiplying the inverse matrix of the source affine <lb/>index map with the matrix of the target affine index map (line 9). The function indexMapMatrix <lb/>outputs the matrix representation of an affine index map, e.g., (1, 1; -1, 1) for (d0+d1, d1-d0). <lb/>By breaking down the transformation matrix into elementary transformation matrices (line 10), <lb/>the corresponding conversion operators are sequentially applied to the source format (line 11). <lb/>Following this, the compiler applies TileSplit if the target index map contains tiling (line 12 -13). <lb/>The handling of indirect functions (line 14 -15) adheres to the encoding sequence of the target <lb/>format. Now all the target indices are ready, and a Sort step orders the indices as specified in the <lb/>target format. In the second step, the compiler addresses the disparities between trim and merge <lb/>primitives in the source and target formats, thereby mutating data structures (line 17 -20) using <lb/>the conversion rules 6 -11 outlined in Table 2. In the final step, the compiler handles the pack <lb/>and partition in the target format and generates custom layouts (line 21 -22). <lb/>Examples of conversions from COO to BDIA and from COO to CSR are shown in Figure 3c. <lb/>When converting from COO to BDIA format, the process begins with the application of index <lb/>map rules to calculate a new set of index iterators in the target format: Skew(0, 1, -1) transforms <lb/>the COO index map (d0, d1) into (d0, d1-d0), and TileSplit(0, 3) subsequently divides d0 into <lb/>tiling dimensions d0/3 and d0%3. To ensure the proper metadata order (d0/3, d1-d0, d0%3), the <lb/>Sort operator is employed. Following index map alignment, the two-dimensional format COO turns <lb/>into a three-dimensional intermediate format with a mutation primitive trim(0,1). To convert <lb/>the intermediate format into the target BDIA format that would contain {merge(0), trim(1,1)}, <lb/>Fill(0) is applied to obtain trim(1,1), and Merge(0) is used to derive merge(0). In the case of COO to <lb/>CSR format conversion, the distinction lies only in their mutation primitives. Therefore, converting <lb/>from COO to CSR involves just two operators: Fill(0), which matches the pattern converting from <lb/>trim(0,1) in COO to trim(1,1) in CSR, and Merge(0), which transforms the absence of merge in <lb/>COO to merge(0) in CSR. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:18 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>5.3 Compute Kernel Generation <lb/>UniSparse extends the MLIR SparseTensor dialect, which leverages TACO&apos;s code generation al-<lb/>gorithms, to generate compute kernels for conventional formats with the index map (d0, d1) <lb/>-&gt; (d0, d1). When dealing with formats with custom index maps that are unsupported by MLIR <lb/>SparseTensor, UniSparse adopts a two-step code generation algorithm, where the compiler gen-<lb/>erates a functionally correct code in the first step, and optimizes the generated code for better <lb/>performance in the second step. Figure 9 illustrates the UniSparse kernel generation algorithm <lb/>using the SpMV kernel in BDIA format as an example. <lb/>In the first step, the UniSparse compiler generates a compute kernel that conducts exhaustive <lb/>iterations across all dimensions of all operands (i.e., tensors of both inputs and output) and performs <lb/>computations on values with matching indices. The rationale behind generating an exhaustive <lb/>iteration across all metadata dimensions is rooted in the fact that different physical dimension <lb/>iterators of tensor operands can not be co-iterated. For instance, consider the SpMV case with the <lb/>BDIA format, as illustrated in Figure 9a. Here, the matrix A features dimension iterators expressed <lb/>as (e0=d0/3, e1=d1-d0, e2=d0%3), while the input vector has iterator (d0) and the output vector <lb/>has iterator (d1). There exist five distinct physical dimension iterators {e0, e1, e2, d0, d1}, but <lb/>no pairs can be iterated together. Therefore, as illustrated in Figure 9b, the UniSparse compiler <lb/>generates a loop nest that iterates through all the dimensions of the input tensor (line 1 -4), the <lb/>input vector (line 5), as well as the output vector (line 6). The iteration through the dimensions of <lb/>the tensor corresponds to traversing the metadata tree. Progressing from one tree level (i.e., a tensor <lb/>dimension) to the next level requires traversing a single tree edge, which may involve memory <lb/>accesses. In this example, there are 3 tree levels (see the BDIA example in Figure 1n): the first and <lb/>last levels are encoded by two sizes, each visited by a single loop (lines 1 and 4, respectively), and <lb/>the indices of the tensor for these two levels are simply the loop variables (line 7 and 9); however, <lb/>the middle level is encoded by an index array and a pointer array, requiring it to be visited in <lb/>two loops (line 2-3), and the tensor&apos;s index for this level is retrieved from the pointer array via a <lb/>memory access (line 8). <lb/>Now that the indices of the tensor in BDIA format have been retrieved, the logical indices can be <lb/>restored (lines 10 -11). In general, the UniSparse compiler employs the reverse transformation M -1 <lb/>to restore the logical indices, as illustrated in Figure 9a. In this specific example, M -1 comprises <lb/>the transformations d0=(d0/3)*3+(d0%3) and d1=(d1-d0)+d0. Once the logical indices are recovered, <lb/>the compiler generates code to verify whether these logical indices fall within valid boundaries <lb/>(line 14), and ensures the index values of the contraction dimensions match (line 15). In this case, <lb/>the contraction dimension is d1, so the generated code checks for the equality of index d1 shared <lb/>by the matrix and the input vector . Inside the loop body, tensor element values are accessed <lb/>(lines 16 -18), followed by computation (line 19) which is lowered from the program specification <lb/>in Figure 9a, and the results are written into the output tensor (line 20). <lb/>In the second step, the UniSparse compiler removes redundant iterations from the generated <lb/>kernel for better performance. To merge iterations, the UniSparse compiler checks for several cases: <lb/>a. Co-iteration within a single tensor -consecutively trimmed dimensions contain metadata of the <lb/>same length, enabling them to be co-iterated. For instance, in the COO format with trim(0,1), <lb/>both the row and column dimensions contain index arrays of identical lengths, allowing them <lb/>to be iterated together using a single loop. <lb/>b. Co-iteration across multiple tensors -<lb/>Case I. The contraction dimension of multiple tensors can be co-iterated if they share the <lb/>same physical dimension iterator. For example, when multiplying DIA (index map (d0, d1) <lb/>-&gt; (d1-d0, d0)) with DCSR (index map (d0, d1) -&gt; (d0, d1)), the second dimension of DIA <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/>99:19 <lb/></note>

        <body>e0 = <lb/>d0 / 3 <lb/>e1 = <lb/>d1 -d0 <lb/>e2 = <lb/>d0 % 3 <lb/>A_BDIA <lb/>M -1 <lb/>A_val <lb/>d0 <lb/>d1 <lb/>Y_dense <lb/>X_dense <lb/>Y_val <lb/>X_val <lb/>^bb0(%A_val:f32,%X_val:f32,%Y_val:f32): <lb/>%1 = arith.mulf %A_val,%X_val:f32 <lb/>%o = arith.addf %Y_val,%1:f32 <lb/>linalg.yield %o:f32 <lb/>Y_dense = A_BDIA ✕ X_dense <lb/>(a) Logical index matching among <lb/>tensor operands. <lb/>1 for id0 in range(0,bdia_size_0): <lb/>2 <lb/>for id_1_0 in range(0,bdia_ptr_1.len()-1): <lb/>3 <lb/>for id_1_1 in range(bdia_ptr_1[id_1_0],bdia_ptr_1[id_1_0+1]): <lb/>4 <lb/>for id2 in range(0,bdia_size_2): <lb/>5 <lb/>for x_id in range(0,X_dense.len()): // step 2: remove <lb/>6 <lb/>for y_id in range(0,Y_dense.len()): // step 2: remove <lb/>7 <lb/>bdia_e0 = id0 <lb/>8 <lb/>bdia_e1 = bdia_idx_1[id_1_1] <lb/>9 <lb/>bdia_e2 = id2 <lb/>10 <lb/>bdia_d0 = bdia_e0*3 + bdia_e2 <lb/>11 <lb/>bdia_d1 = bdia_e1 + bdia_d0 <lb/>12 <lb/>X_dense_d1 = x_id // step 2: X_dense_d1 = bdia_d1 <lb/>13 <lb/>Y_dense_d0 = y_id // step 2: Y_dense_d0 = bdia_d0 <lb/>14 <lb/>if (bdia_d0 in range(0,D0) and bdia_d1 in range(0,D1) <lb/>15 <lb/>and bdia_d1 == X_dense_d1): // step 2: remove <lb/>16 <lb/>A_val = bdia_val[id_1_1 * bdia_size_2 + id2] <lb/>17 <lb/>X_val = X_dense[X_dense_d1] <lb/>18 <lb/>Y_val = Y_dense[Y_dense_d0] <lb/>19 <lb/>res = Y_val + A_val*X_val <lb/>20 <lb/>Y_dense[Y_dense_d0] = res <lb/>(b) The pseudo-code of the generated SpMV kernel. <lb/>Fig. 9. Exemplifying the UniSparse kernel generation algorithm with the SpMV kernel using BDIA format. <lb/>and the row dimension of DCSR share the physical iterator d0, allowing these two dimensions <lb/>to be co-iterated as while (dia_id1 &lt; dia_id1.len() and dcsr_id0 &lt; dcsr_idx_0.len()). <lb/>Case II. Dimensions that support random access can reuse the shared iterator of other tensors. <lb/>For example, in Figure 9, the input and output vectors are dense arrays that can be randomly <lb/>accessed, enabling them to directly borrow the logical index iterators of the matrix A. Conse-<lb/>quently, the compiler eliminates iterations in lines 5-6 and the index equality check in line 15. <lb/>6 THE UNISPARSE MLIR DIALECT <lb/>The UniSparse intermediate language is implemented as a standalone MLIR dialect. Figure 3d <lb/>shows a UniSparse program of the SpMV kernel with the hybrid BDIA/CSR format. This section <lb/>introduces the key ingredients of the UniSparse language: <lb/>Types and Attributes. Format descriptions are specified as MLIR attributes. In the example <lb/>program shown in Figure 3d, all formats are specified beforehand using index maps (idx_map) <lb/>and mutation primitives (mutation) in Line 2-9. Sparse tensors are declared with tensor types and <lb/>sparse format encoding attributes. Annotations specify the desired sparse format, eliminating the <lb/>need for explicit handling of sparsity in code. <lb/>Tensor Preprocessing Operations. The UniSparse MLIR dialect defines two key operations <lb/>for sparse format customization: decompose and convert. The decompose operation splits the input <lb/>tensor into sub-tensors with different sparsity ranges, based on the non-zero distribution patterns <lb/>(identified by the sum primitive).The convert operation specifies format conversion between source <lb/>and destination formats based on their respective encodings. <lb/>Compute Kernels. The compute kernel is specified using the linalg.generic operation within <lb/>MLIR. Figure 3d (lines 15-24) provides an example of the linalg.generic operation that specifies <lb/>an SpMV kernel. Within this operation, the #spmv attribute (lines 15-19) indicates the logical <lb/>dimension iterators and compute patterns of tensor operands. The inputs and outputs (lines 20-21) <lb/>of the linalg.generic operation define the iteration space, while the loop body (lines 23-24) details <lb/>the computational logic. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:20 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>Table 3. A summary of the matrices used for evaluation with the abbreviated names in parentheses. <lb/>Matrix <lb/>Shape <lb/>Density <lb/>Nonzero <lb/>Diagonals <lb/>Matrix <lb/>Shape <lb/>Density <lb/>Nonzero <lb/>Diagonals <lb/>email-Eu-core (ee) <lb/>1.01K × 1.01K <lb/>2.5e-2 <lb/>1.84K <lb/>ML_Geer (ge) <lb/>1.50M × 1.50M <lb/>4.9e-5 <lb/>9.35K <lb/>ss (ss) <lb/>1.65M × 1.65M <lb/>1.3e-5 <lb/>1.68M <lb/>ML_Laplace (lp) <lb/>377K × 377K <lb/>1.9e-4 <lb/>4.70K <lb/>Transport (tp) <lb/>1.60M × 1.60M <lb/>9.2e-6 <lb/>15 <lb/>rajat31 (ra) <lb/>4.69M × 4.69M <lb/>9.2e-7 <lb/>5.05K <lb/>TSOPF_RS_b2383 (ts) <lb/>38.1K × 38.1K <lb/>1.1e-2 <lb/>23.4K <lb/>memchip (mc) <lb/>2.71M × 2.71M <lb/>2.0e-6 <lb/>1.74M <lb/>vas_stokes_1M (vs) <lb/>1.09M × 1.09M <lb/>2.9e-5 <lb/>1.67M <lb/>crystm02 (cm) <lb/>14.0K × 14.0K <lb/>1.7e-3 <lb/>27 <lb/>cant (ct) <lb/>62.5K × 62.5K <lb/>1.0e-3 <lb/>99 <lb/>c8_mat11 (c8) <lb/>4.56K × 5.76K <lb/>9.4e-2 <lb/>10.3K <lb/>nemeth21 (nm) <lb/>9.51K × 9.51K <lb/>1.3e-2 <lb/>169 <lb/>heart1 (h1) <lb/>3.56K × 3.56K <lb/>1.1e-1 <lb/>6.53K <lb/>bibd_18_9 (b9) <lb/>153 × 48.6K <lb/>2.4e-1 <lb/>48.6K <lb/>cari (cr) <lb/>400 × 1.20K <lb/>3.2e-1 <lb/>1.16K <lb/>transformer-0.5 (tf-0.5) <lb/>512 × 33.3K <lb/>5.0e-1 <lb/>33.8K <lb/>transformer-0.6 (tf-0.6) <lb/>512 × 33.3K <lb/>4.0e-1 <lb/>33.8K <lb/>transformer-0.7 (tf-0.7) <lb/>512 × 33.3K <lb/>3.0e-1 <lb/>33.8K <lb/>transformer-0.8 (tf-0.8) <lb/>512 × 33.3K <lb/>2.0e-1 <lb/>33.8K <lb/>transformer-0.9 (tf-0.9) <lb/>512 × 33.3K <lb/>1.0e-1 <lb/>33.8K <lb/>transformer-0.95 (tf-0.95) <lb/>512 × 33.3K <lb/>5.0e-2 <lb/>33.7K <lb/>roadNet-PA (rp) <lb/>1.09M × 1.09M <lb/>1.3e-6 <lb/>66.4K <lb/>mouse_gene (mg) <lb/>45.1K × 45.1K <lb/>7.1e-3 <lb/>77.8K <lb/>google-plus (gp) <lb/>108K × 108K <lb/>1.2e-3 <lb/>203K <lb/>pokec (pk) <lb/>1.63M × 1.63M <lb/>1.2e-5 <lb/>2.28M <lb/>hollywood (hw) <lb/>1.07M × 1.07M <lb/>4.9e-5 <lb/>2.08M <lb/>ogbl-ppa (op) <lb/>576K × 576K <lb/>1.3e-4 <lb/>1.14M <lb/>LiveJournal (lj) <lb/>4.85M × 4.85M <lb/>2.9e-6 <lb/>6.55M <lb/>wikipedia-20051105 (wp) <lb/>1.63M × 1.63M <lb/>7.4e-6 <lb/>2.86M <lb/>chem_master1 (ch) <lb/>40.4K × 40.4K <lb/>1.2e-5 <lb/>5 <lb/>majorbasis (mj) <lb/>160K × 160K <lb/>6.8e-5 <lb/>22 <lb/>shyy161 (sh) <lb/>76.5K × 76.5K <lb/>5.6e-5 <lb/>7 <lb/>Baumann (bm) <lb/>112K × 112K <lb/>5.9e-5 <lb/>7 <lb/>wiki-Vote (wv) <lb/>8.30K × 8.30K <lb/>1.5e-3 <lb/>11.4K <lb/>mario002 (m2) <lb/>390K × 390K <lb/>1.4e-5 <lb/>507K <lb/>scircuit (sc) <lb/>171K × 171K <lb/>3.3e-5 <lb/>159K <lb/>p2pGnutella31 (pg) <lb/>62.6K × 62.6K <lb/>3.8e-5 <lb/>53.2K <lb/>cage12 (ca) <lb/>130K × 130K <lb/>1.2e-4 <lb/>75.5K <lb/>filter3D (f3) <lb/>106K × 106K <lb/>2.4e-4 <lb/>13.4K <lb/>ca-CondMat (cc) <lb/>23.1K × 23.1K <lb/>3.5e-4 <lb/>40.2K <lb/>poisson3Da (p3) <lb/>13.5K × 13.5K <lb/>1.9e-3 <lb/>26.1K <lb/>bwm2000 (bw) <lb/>2.00K × 2.00K <lb/>2.0e-3 <lb/>5 <lb/>af23560 (af) <lb/>23.6K × 23.6K <lb/>8.7e-4 <lb/>33 <lb/>cryg10000 (cg) <lb/>10.0K × 10.0K <lb/>5.0e-4 <lb/>8 <lb/>ex19 (ex) <lb/>12.0K × 12.0K <lb/>1.8e-3 <lb/>185 <lb/>mycielskian12 (my) <lb/>3.07K × 3.07K <lb/>4.3e-2 <lb/>6.12K <lb/>ogbl-ddi (od) <lb/>4.27K × 4.27K <lb/>5.8e-2 <lb/>8.50K <lb/>7 EVALUATION <lb/>This section demonstrates the efficacy of UniSparse through a series of case studies ( §7.2). These <lb/>case studies illustrate how the adoption of custom formats enabled by UniSparse leads to improved <lb/>performance for common sparse linear algebra kernels across a variety of hardware platforms, <lb/>including an Intel multi-core CPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated PIM <lb/>device. Furthermore, we assess the performance of automatic format conversion ( §7.3) and compute <lb/>kernel generation ( §7.4) using UniSparse. Our evaluation demonstrates that the programs generated <lb/>by UniSparse achieve performance that matches state-of-the-art approaches [Bik et al. 2022; Chou <lb/>et al. 2020a], while UniSparse offers broader coverage for handling a wider range of custom formats. <lb/>7.1 Experiment Setup <lb/>We obtain sparse matrices from various popular datasets, including SuiteSparse [Davis and Hu 2011], <lb/>SNAP [Leskovec and Krevl 2014], and OGB [Hu et al. 2020], covering a rich mix of application <lb/>domains. Additionally, we also collect a set of sparse weight tensors/matrices from a pruned <lb/>Transformer model [Gale et al. 2019]. Table 3 summarizes all the sparse matrices used in our <lb/>experiments. For improved readability, we use abbreviated names for these matrices in all the <lb/>illustrations presented below. <lb/>All format conversion experiments are performed on a dual-socket 24C/24T Intel(R) Xeon(R) Gold <lb/>6248R CPU @ 3.00 GHz. The compute kernels generated by UniSparse are executed on multiple <lb/>backends, including the same CPU, an NVIDIA GPU A6000, an AMD Xilinx FPGA, and a PIM-core <lb/>simulator [Devic et al. 2022]. We provide more details of the configurations for each experiment in <lb/>the corresponding subsections. <lb/>7.2 Format Customization <lb/>We present four case studies evaluating the format customization feature of UniSparse. Our results <lb/>demonstrate performance improvements on the CPU and the GPU using hybrid BDIA/CSR ( §7.2.1) <lb/>and hybrid BELL/COO ( §7.2.2) formats that are tailored to the non-zero distribution patterns of <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:21 <lb/></page>

        <body>matrices. We showcase the versatility of UniSparse by supporting the recently proposed Serpens <lb/>format [Song et al. 2022a], which enables high-performance sparse processing on FPGAs ( §7.2.3). <lb/>Furthermore, we demonstrate that UniSparse can handle partitioned formats for PIM systems using <lb/>C 2 SR and CISR. Finally, we explore a more load-balanced version of CISR, referred to as CISR-plus, <lb/>by leveraging the custom map functions and the Reorder primitive in UniSparse ( §7.2.4). <lb/>7.2.1 Case Study: The Hybrid BDIA/CSR Format on CPUs. We generate the hybrid BDIA/CSR <lb/>format using UniSparse. Initially, the input matrix is decomposed into two sub-matrices, both <lb/>represented in COO format. One sub-matrix is converted to the BDIA format, while the other <lb/>is converted to the CSR format. The decomposition pattern is adjusted by tuning the blocking <lb/>factor and the non-zero threshold of each diagonal, which can affect the performance of the sparse <lb/>kernel. In this work, we manually select the blocking size and thresholds with the best performance. <lb/>Automatically searching for the optimal decomposition factors will be left for future work. The <lb/>block sizes and thresholds used for each dataset are presented above the result bars on the right <lb/>side of Figure 10. <lb/>We profile the execution time breakdown of converting COO to BDIA on multiple datasets and <lb/>summarize the results in the left side of Figure 10. The Sort operator is critical and accounts for <lb/>65% of the total conversion time since it involves intensive memory reads and writes. <lb/>As shown in the right side of Figure 10, we evaluate the SpMV kernel with the hybrid BDIA/CSR <lb/>format (program in Figure 3d) in single precision on four hardware configurations, using a 48-<lb/>core Intel Xeon Gold 6248R CPU at 3.00 GHz and an NVIDIA RTX A6000 GPU. The SpMV kernel <lb/>on the CPU is parallelized using OpenMP, and the kernel on the GPU leverages APIs provided <lb/>by the cuSPARSE library. The heterogeneous configuration runs the SpMV kernel with one sub-<lb/>matrix in BDIA on the CPU, and the other sub-matrix in CSR on the GPU simultaneously. The <lb/>homogeneous configuration runs SpMV with both sub-matrices in BDIA and CSR formats on the <lb/>CPU. Another two configurations run the SpMV kernel with only the CSR format implemented in <lb/>cuSPARSE on the GPU, and in the Intel MKL library on the CPU. For the MKL implementation of <lb/>SpMV, we store the matrix in the CSR format (mkl_sparse_s_create_csr), called the optimization <lb/>function (mkl_sparse_optimize) and the Inspector-Executor (IE) routine (mkl_sparse_s_mv). The <lb/>heterogeneous configuration yields 5.63×, 1.28×, and 10.73× speedup in Geomean over the SpMV <lb/>kernel with the hybrid format only on the CPU (homogeneous configuration), using cuSPARSE on <lb/>the GPU, and using MKL library on the CPU, respectively. <lb/>g e <lb/>s s <lb/>lp <lb/>t p <lb/>r a <lb/>t s <lb/>m <lb/>c <lb/>v s <lb/>G M <lb/>0.5 <lb/>1.0 <lb/>Execution Time Ratio <lb/>8.923 s <lb/>0.967 s <lb/>1.405 s <lb/>0.272 s <lb/>2.094 s <lb/>0.559 s <lb/>1.524 s <lb/>0.616 s <lb/>Swap <lb/>Skew <lb/>TileSplit <lb/>Sort <lb/>Merge <lb/>Fill <lb/>Vectorize <lb/>ge <lb/>ss <lb/>lp <lb/>tp <lb/>ra <lb/>ts <lb/>mc <lb/>vs <lb/>GM <lb/>0 <lb/>10 <lb/>20 <lb/>Normalized Execution Time <lb/>26.75 <lb/>21.68 <lb/>23.58 <lb/>100/0.6 <lb/>100/0.5 <lb/>100/0.5 <lb/>100/0.5 <lb/>1000/0.5 <lb/>100/0.5 <lb/>500/0.5 <lb/>500/0.5 <lb/>Heterogeneous BDIA/CSR <lb/>Homogeneous BDIA/CSR <lb/>CSR in cuSPARSE <lb/>CSR in MKL <lb/>Fig. 10. The case study of the hybrid BDIA/CSR format. The le figure shows the breakdown of the execution <lb/>time for converting from COO to BDIA format. The right figure shows the performance comparison of <lb/>the SpMV kernel across various configurations. The execution times are normalized to the heterogeneous <lb/>configuration, and execution times over 20 are marked with red numbers inside bars. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:22 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>7.2.2 Case Study: The Hybrid BELL/COO Format on GPUs. The hybrid BELL/COO format is <lb/>generated by decomposing the input matrix into two sub-matrices, one converting to the BELL <lb/>format while the other remaining in the COO format. The time breakdown of the conversion from <lb/>COO to BELL is profiled across multiple datasets, and the results are summarized on the left side of <lb/>Figure 11. We manually tune the decomposition parameters by adjusting the block size and the <lb/>non-zero threshold within each block, selecting the configuration that yields the best performance. <lb/>These decomposition parameters are indicated above each result bar on the right side of Figure 11. <lb/>We evaluate sparse matrix-matrix multiplication (SpMM) in single precision using the hybrid <lb/>BELL/COO format and compare it with the one using only the CSR format. The compute kernel is <lb/>deployed on an NVIDIA RTX A6000 GPU through APIs provided by the cuSPARSE library. Figure 11 <lb/>shows the normalized run time of the SpMM kernel using the CSR format vs. the BELL/COO format. <lb/>The hybrid BELL/COO format on the selected sparse matrices leads to a 2.7× Geomean speedup. <lb/>e e <lb/>c 8 <lb/>n m <lb/>h 1 <lb/>b 9 <lb/>c r <lb/>t f-0 .6 <lb/>t f-0 .7 <lb/>G M <lb/>0.5 <lb/>1.0 <lb/>Execution Time Ratio <lb/>0.001 s <lb/>0.096 s <lb/>0.048 s <lb/>0.062 s <lb/>0.073 s <lb/>0.006 s <lb/>0.289 s <lb/>0.226 s <lb/>TileSplit <lb/>Sort <lb/>Merge <lb/>Fill <lb/>Vectorize <lb/>Enumerate <lb/>ee <lb/>c8 <lb/>nm <lb/>h1 <lb/>b9 <lb/>cr <lb/>tf-0.5 tf-0.6 tf-0.7 tf-0.8 tf-0.9 tf-0.95 GM <lb/>0.0 <lb/>1.0 <lb/>2.0 <lb/>3.0 <lb/>4.0 <lb/>5.0 <lb/>5.5 <lb/>Normalized Execution Time <lb/>32/0.1 <lb/>32/0.2 <lb/>64/0.2 <lb/>64/0.2 <lb/>16/0.3 <lb/>16/0.3 <lb/>64/0.1 <lb/>64/0.1 <lb/>64/0.1 <lb/>32/0.1 <lb/>64/0.1 <lb/>64/0.1 <lb/>BELL/COO <lb/>CSR <lb/>Fig. 11. The case study of the hybrid BELL/COO format. The le figure shows the breakdown of execution <lb/>time for converting from COO to the BELL format. The right figure shows the normalized run time of <lb/>cuSPARSE SpMM using the BELL/COO decomposed by UniSparse vs. CSR on NVIDIA A6000. Datasets <lb/>transformer-50 to 95 are pruned weight matrices of Transformer [Gale et al. 2019] with sparsity ranging from <lb/>50% to 95%. <lb/>7.2.3 Case Study: The Serpens Format on FPGAs. We evaluate the sparse format proposed in the <lb/>Serpens accelerator [Song et al. 2022a,b] and demonstrate how UniSparse can express and generate <lb/>the Serpens format using indirect functions and query primitives that change the order of elements. <lb/>As shown in Figures 12a and 12b, the Serpens format traverses elements in column order, preserving <lb/>the dependency length between adjacent row elements with a few padding zeros. This allows the <lb/>Serpens accelerator to achieve significant throughput improvement for the SpMV kernel. <lb/>UniSparse automatically converts from COO to the Serpens format. Despite the Sort operator <lb/>consuming a significant portion (84% in Geomean) of the total time (Figure 12c), UniSparse is able <lb/>to generate this high-performance format in seconds, significantly improving the productivity of <lb/>hardware developers and providing easier access to sparse acceleration for software developers. <lb/>According to [Song et al. 2022a], utilizing this custom format in Serpens results in 1.91× better <lb/>throughput and 1.71× better energy efficiency on an AMD Xilinx Alveo U280 device compared to <lb/>the prior state-of-the-art FPGA accelerator [Hu et al. 2021]. <lb/>7.2.4 Case Study: The C 2 SR, CISR, and CISR-plus Formats on PIMs. We evaluate SpMV using the <lb/>C 2 SR and CISR formats on a simulated PIM device [Devic et al. 2022] with 1024 and 2048 cores. <lb/>Each PIM core has a copy of the input dense vector and computes a subset of the output vector in a <lb/>lock-free execution pattern. Figure 14a shows the maximum vs. the average number of non-zeros <lb/>processed per core. As the number of cores increases, the load imbalance introduced by the C 2 SR <lb/>format gradually becomes a bottleneck, whereas the CISR format mitigates this issue. Figure 14b <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:23 <lb/></page>

        <body>3 <lb/>2 <lb/>1 <lb/>6 <lb/>0 <lb/>0 <lb/>1 <lb/>1 <lb/>0 <lb/>4 <lb/>0 <lb/>2 <lb/>1 <lb/>0 <lb/>4 2 <lb/>val <lb/>indirect (d0) / 2 <lb/>d0 <lb/>0 <lb/>1 <lb/>1 <lb/>A <lb/>d1 <lb/>0 <lb/>1 <lb/>0 <lb/>indirect (d1) <lb/>0 <lb/>5 <lb/>0 <lb/>2 <lb/>0 <lb/>1 <lb/>0 <lb/>0 <lb/>2 <lb/>1 <lb/>1 <lb/>0 <lb/>3 2 1 3 2 <lb/>indirect (d0) % 2 <lb/>(a) The data structure of Serpens <lb/>0 1 <lb/>0 <lb/>0 <lb/>5 <lb/>3 <lb/>2 <lb/>0 <lb/>6 <lb/>0 <lb/>1 <lb/>4 <lb/>val <lb/>idx <lb/>2 <lb/>1 <lb/>0 <lb/>idx <lb/>2 <lb/>1 <lb/>3 <lb/>2 <lb/>0 <lb/>1 <lb/>3 <lb/>1 <lb/>0 <lb/>1 <lb/>2 <lb/>2 <lb/>0 <lb/>0 <lb/>4 <lb/>2 <lb/>idx <lb/>idx <lb/>d0 <lb/>d1 <lb/>4 <lb/>3 <lb/>2 <lb/>0 <lb/>ptr <lb/>indirect (d1) <lb/>indirect (d0) / 2 <lb/>1 <lb/>0 <lb/>1 <lb/>0 <lb/>0 1 <lb/>1 <lb/>0 <lb/>idx <lb/>indirect (d0) % 2 <lb/>8 <lb/>6 <lb/>4 <lb/>2 <lb/>0 <lb/>ptr <lb/>(b) The data layout of Serpens <lb/>0.5 <lb/>1.0 <lb/>Execution Time Ratio <lb/>mg <lb/>gp <lb/>pk <lb/>hw <lb/>op <lb/>ts <lb/>lp <lb/>GM <lb/>4.300 s <lb/>3.851 s <lb/>9.675 s <lb/>39.081 s <lb/>13.705 s <lb/>4.414 s <lb/>6.499 s <lb/>TileSplit <lb/>Sort <lb/>Vectorize <lb/>Enumerate <lb/>Sum <lb/>Reorder <lb/>Pack <lb/>(c) Breakdown of COO→Serpens <lb/>Fig. 12. The Serpens format of the matrix in Figure 1a. <lb/>3 <lb/>2 <lb/>1 <lb/>6 <lb/>1 <lb/>1 <lb/>0 <lb/>3 <lb/>1 <lb/>2 <lb/>4 <lb/>2 <lb/>val <lb/>d1 <lb/>d0 <lb/>5 <lb/>3 <lb/>A <lb/>0 <lb/>1 <lb/>indirect(d0) <lb/>0 <lb/>4 <lb/>(a) The data structure of CISR-plus <lb/>0 <lb/>d0 <lb/>idx <lb/>d1 <lb/>5 <lb/>3 4 <lb/>val <lb/>indirect (d0) <lb/>2 <lb/>idx <lb/>3 <lb/>1 2 <lb/>idx <lb/>0 3 <lb/>ptr <lb/>1 <lb/>1 2 6 <lb/>0 <lb/>3 <lb/>1 <lb/>4 <lb/>0 1 <lb/>3 <lb/>2 <lb/>0 1 <lb/>(b) The data layout of CISR-plus <lb/>0.5 <lb/>1.0 <lb/>Execution Time Ratio <lb/>rp <lb/>mg <lb/>gp <lb/>pk <lb/>hw <lb/>op <lb/>lj <lb/>wp <lb/>GM <lb/>0.246 s <lb/>1.003 s <lb/>0.969 s <lb/>3.073 s <lb/>8.219 s <lb/>3.607 s <lb/>7.198 s <lb/>1.996 s <lb/>Sort <lb/>Merge <lb/>Sum <lb/>Reorder <lb/>Schedule <lb/>Partition <lb/>(c) Breakdown of COO→CISR-plus <lb/>1 #CISR-plus = #encoding&lt;{ idx_map&lt;(d0,d1)-&gt;(indirect(d0),d0,d1)&gt;, mutation&lt;merge(0,1), trim(1,2)&gt;, <lb/>2 <lb/>indirect&lt;{ sum(value) groupBy (d0,d1)-&gt;(d0) with value ne 0 -&gt; 1 | otherwise -&gt; 0 <lb/>3 <lb/>reorder(d0) traverseBy (d0,d1)-&gt;(d0) <lb/>4 <lb/>schedule(d0) traverseBy (d0,d1)-&gt;(d0/2) }&gt;, <lb/>5 <lb/>layout&lt;partition(0)&gt; }&gt; <lb/>(d) The format encoding of CISR-plus. <lb/>Fig. 13. The CISR-plus format of the matrix in Figure 1a. <lb/>shows the normalized execution time of SpMV on 1024 PIM cores using the C 2 SR vs. the CISR <lb/>format. Compared with the C 2 SR format, using the CISR format improves the performance by <lb/>1.35× in Geomean. <lb/>Using UniSparse, we can further explore an optimized version of CISR, called CISR-plus. CISR-<lb/>plus schedules rows with the maximum number of non-zeros first, which leads to better load <lb/>balance among PIM cores. The data structure and layout of CISR-plus are shown in Figures 13a and <lb/>13b. The reorder primitive along with custom map functions enable us to express (Figure 13d) and <lb/>generate the CISR-plus format using UniSparse. The conversion time breakdown is shown in Figure <lb/>13c. We can see that the Sort operator still dominates the conversion time, while the Reorder and <lb/>Schedule operators are relatively faster. The improved load balance by using CISR-plus format is <lb/>demonstrated in Figure 14a. We further evaluate the performance of CISR-plus by comparing it to <lb/>CISR and C 2 SR when running SpMV on 1024 PIM cores, as shown in Figure 14b. Using CISR-plus <lb/>leads to a 1.14× and 1.54× speedup in Geomean over CISR and C 2 SR, respectively. <lb/>Figure 14c presents a roofline analysis of employing C 2 SR, CISR, and CISR-plus formats for <lb/>3 selected matrices using 1024 PIM cores. While all kernels are memory-bound, adopting the <lb/>CISR-plus format brings the design nearer to the roofline by addressing the load imbalance issue <lb/>among PIM cores, thereby increasing throughput. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:24 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>rp <lb/>mg <lb/>gp <lb/>pk <lb/>hw <lb/>op <lb/>lj <lb/>wp <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>max / avg nnz <lb/>CISR-plus-1024-core <lb/>CISR-1024-core <lb/>C2SR-1024-core <lb/>CISR-plus-2048-core <lb/>CISR-2048-core <lb/>C2SR-2048-core <lb/>(a) Load imbalance on different numbers of PIM cores. <lb/>rp <lb/>mg <lb/>gp <lb/>pk <lb/>hw <lb/>op <lb/>lj <lb/>wp <lb/>GM <lb/>0.5 <lb/>1.0 <lb/>1.5 <lb/>2.0 <lb/>2.5 <lb/>3.0 <lb/>Normalized Execution Time <lb/>CISR-plus <lb/>CISR <lb/>C2SR <lb/>(b) Normalized run time on 1024 PIM cores. <lb/>(c) Roofline analysis for 3 selected matrices on 1024 <lb/>PIM cores. <lb/>Fig. 14. The SpMV profiling results using CISR-plus, CISR, and C 2 SR formats on the simulated PIM device. <lb/>7.2.5 Case Study: Label Propagation on GPUs. We evaluate the label propagation application <lb/>using custom formats generated by UniSparse and compare it against the traditional CSR format. <lb/>Label propagation is a semi-supervised machine learning algorithm that iteratively propagates <lb/>label information from previously labeled data points to unlabeled ones [Iscen et al. 2019]. This <lb/>algorithm operates through iterative matrix multiplications, where the right matrix, referred to as <lb/>the label matrix, contains the label vectors for each data point (or node), and the left one is the <lb/>adjacency matrix that represents the node connectivity. Initially, the label matrix is sparse, with only <lb/>a few nodes having non-zero values in each label category. While employing sparse-sparse matrix <lb/>multiplication (SpGEMM) for the initial stage of the algorithm may lead to better performance, <lb/>as the density of the label matrix increases, it requires ad hoc experiments for each dataset to <lb/>determine the density threshold to switch using SpMM for the rest of the computation. In our <lb/>experiments, for simplicity, we use SpMM for the entire process of label propagation. <lb/>Table 4 presents the evaluation results of the label propagation algorithm. The label propagation <lb/>is implemented through 20 iterations of SpMM. Before computation, we normalize the sparse <lb/>matrices to ensure each row sums up to 1, and we generate label matrices with a fixed column size <lb/>of 1000. The hybrid BELL/COO format is generated by UniSparse using 24 threads on an Intel Xeon <lb/>Gold 6242 CPU at 2.80 GHz. The compute kernels are executed on an NVIDIA RTX A6000 GPU <lb/>through APIs provided by the cuSPARSE library. <lb/>As presented in Table 4, the total execution time of the label propagation algorithm using the <lb/>hybrid BELL/COO format includes a format preprocessing time and a kernel execution time. The <lb/>preprocessing of the hybrid BELL/COO format involves intensive memory accesses to perform <lb/>both matrix decomposition and format conversion, thus introducing a non-negligible overhead to <lb/>overall performance. However, in applications involving multiple iterations of sparse linear algebra <lb/>kernels, such as label propagation, the overhead from format preprocessing is amortized across <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:25 <lb/></page>

        <body>iterations. This allows for an overall speedup by utilizing custom sparse formats. Our evaluation <lb/>shows that using the hybrid BELL/COO format for the label propagation application leads to a <lb/>1.61× speedup in Geomean compared to the traditional CSR format. <lb/>Table 4. The case study of label propagation application. The numbers for each execution time (in milliseconds) <lb/>± standard deviation are computed based on 20 independent runs. <lb/>Data <lb/>CSR Format <lb/>Hybrid BELL/COO Format <lb/>Speedup <lb/>over CSR <lb/>Format Preprocessing Kernel Execution <lb/>Total <lb/>ee <lb/>11.969 ± 0.106 <lb/>5.490 ± 0.644 <lb/>4.460 ± 0.136 <lb/>9.950 ± 0.658 <lb/>1.20 <lb/>c8 <lb/>190.272 ± 1.149 <lb/>63.022 ± 2.257 <lb/>55.960 ± 0.246 <lb/>118.983 ± 2.270 <lb/>1.60 <lb/>nm <lb/>96.842 ± 0.666 <lb/>21.376 ± 2.665 <lb/>49.809 ± 0.119 <lb/>71.185 ± 2.667 <lb/>1.36 <lb/>h1 <lb/>111.431 ± 0.589 <lb/>27.172 ± 1.509 <lb/>16.163 ± 0.125 <lb/>43.335 ± 1.514 <lb/>2.57 <lb/>cr <lb/>11.764 ± 0.101 <lb/>3.884 ± 0.146 <lb/>3.661 ± 0.006 <lb/>7.545 ± 0.146 <lb/>1.56 <lb/>my <lb/>32.629 ± 0.015 <lb/>15.209 ± 0.555 <lb/>11.162 ± 0.528 <lb/>26.371 ± 0.766 <lb/>1.24 <lb/>od <lb/>246.337 ± 0.280 <lb/>69.728 ± 3.687 <lb/>55.450 ± 0.204 <lb/>125.177 ± 3.693 <lb/>1.97 <lb/>wv <lb/>1464.949 ± 7.565 <lb/>380.897 ± 10.194 <lb/>458.716 ± 0.415 839.613 ± 10.202 <lb/>1.74 <lb/>Geomean <lb/>1.61 <lb/>7.3 Format Conversion <lb/>Compared to state-of-the-art compilers, UniSparse offers a wider coverage of supported formats in <lb/>its automatic format conversion routines. Table 5 lists 7 representative format conversion cases <lb/>supported by UniSparse. In the listed format conversion cases, both MLIR SparseTensor dialect [Bik <lb/>et al. 2022] and TACO [Chou et al. 2020a] lack stable support for DCSC → BCSR, CSB → DIA-<lb/>variant, COO → C 2 SR, and COO → CISR, while MLIR SparseTensor does not implement COO → <lb/>DIA nor COO → ELL. Additionally, to the best of our knowledge, none of the prior sparse linear <lb/>algebra compilers support specialized formats like CISR and Serpens. <lb/>Furthermore, UniSparse demonstrates comparable performance in the given format conversion <lb/>cases compared to two baseline compilers. Format conversion is memory-intensive, and both <lb/>UniSparse and TACO optimize their code to minimize memory accesses. However, differences in <lb/>the implementations of code generation lead to performance variations. The format decoding in <lb/>UniSparse introduces performance overhead, which leads to inferior performance for CSR → CSC <lb/>and COO → ELL conversions compared with TACO on small matrices (ch, mj, sh, bm) with high <lb/>sparsity (∼1.0e-5). We also notice that the transpose operation in MLIR SparseTensor utilizes an <lb/>auxiliary class to enumerate values in any permutation order of tensor dimensions. While this <lb/>approach offers functional versatility, it leads to increased time complexity and more memory <lb/>accesses, resulting in slower performance when converting CSR to CSC. <lb/>7.4 Compute Kernel Generation <lb/>Table 6 provides a performance comparison of SpMM and SpGEMM kernels across various formats <lb/>generated by UniSparse, MLIR SparseTensor, and TACO. The SpMM kernel performs matrix multi-<lb/>plication between a sparse matrix in the dataset and a synthetic dense matrix with a fixed column <lb/>size of 1000. The SpGEMM kernel conducts matrix multiplication between a sparse matrix in the <lb/>dataset and itself. These kernels operate with double precision and execute in a single-threaded <lb/>environment on an Intel Xeon Gold 6248R CPU at 3.00 GHz. <lb/>UniSparse leverages the kernel generation passes from the MLIR SparseTensor dialect for compute <lb/>kernels including CSR SpMM, DCSC SpMM, CSR×CSR→CSR SpGEMM, and CSC×CSC→CSC <lb/>SpGEMM. However, UniSparse and MLIR SparseTensor differ in the implementation of the tensor <lb/>storage. The tensor initialization function of MLIR SparseTensor is slightly more complicated. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:26 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <body>Table 5. Comparison of the actual execution times for format conversion programs generated by UniSparse, the <lb/>MLIR SparseTensor dialect under the LLVM 15.0.0 release, and the format conversion artifact of TACO [Chou <lb/>et al. 2020b]. The numbers for each execution time (in seconds) ± standard deviation are computed based on <lb/>20 independent runs. <lb/>Data <lb/>CSR → CSC <lb/>DCSC → BCSR CSB → DIA-variant <lb/>TACO <lb/>SparseTensor <lb/>UniSparse <lb/>UniSparse <lb/>UniSparse <lb/>wv <lb/>0.001 ± 0.000 0.003 ± 0.000 0.001 ± 0.000 <lb/>0.012 ± 0.001 <lb/>0.067 ± 0.001 <lb/>ee <lb/>0.000 ± 0.000 0.001 ± 0.000 0.000 ± 0.000 <lb/>0.003 ± 0.000 <lb/>0.004 ± 0.000 <lb/>nm <lb/>0.005 ± 0.000 0.031 ± 0.001 0.002 ± 0.000 <lb/>0.039 ± 0.000 <lb/>0.032 ± 0.001 <lb/>cm <lb/>0.001 ± 0.000 0.006 ± 0.000 0.001 ± 0.000 <lb/>0.012 ± 0.001 <lb/>0.007 ± 0.000 <lb/>ct <lb/>0.018 ± 0.000 0.078 ± 0.004 0.009 ± 0.000 <lb/>0.148 ± 0.001 <lb/>0.184 ± 0.002 <lb/>lp <lb/>0.191 ± 0.004 0.659 ± 0.005 0.193 ± 0.003 <lb/>2.554 ± 0.015 <lb/>4.509 ± 0.031 <lb/>tp <lb/>0.168 ± 0.004 0.562 ± 0.004 0.139 ± 0.002 <lb/>2.249 ± 0.010 <lb/>6.157 ± 0.038 <lb/>ts <lb/>0.162 ± 0.002 0.462 ± 0.006 0.168 ± 0.001 <lb/>1.733 ± 0.009 <lb/>2.509 ± 0.017 <lb/>ch <lb/>0.001 ± 0.000 0.006 ± 0.000 0.001 ± 0.000 <lb/>0.015 ± 0.001 <lb/>0.009 ± 0.000 <lb/>mj <lb/>0.006 ± 0.000 0.043 ± 0.001 0.008 ± 0.000 <lb/>0.134 ± 0.001 <lb/>0.178 ± 0.002 <lb/>sh <lb/>0.001 ± 0.000 0.009 ± 0.001 0.001 ± 0.000 <lb/>0.023 ± 0.000 <lb/>0.018 ± 0.000 <lb/>bm <lb/>0.002 ± 0.000 0.018 ± 0.000 0.003 ± 0.000 <lb/>0.058 ± 0.001 <lb/>0.063 ± 0.001 <lb/>Geomean <lb/>Speedup <lb/>1 <lb/>0.20 <lb/>1.13 <lb/>1 <lb/>1 <lb/>Data <lb/>COO → ELL <lb/>COO → DIA <lb/>COO → C 2 SR COO → CISR <lb/>TACO <lb/>UniSparse <lb/>TACO <lb/>UniSparse <lb/>UniSparse <lb/>UniSparse <lb/>wv <lb/>0.018 ± 0.000 0.014 ± 0.001 0.092 ± 0.001 0.065 ± 0.004 <lb/>0.003 ± 0.000 <lb/>0.003 ± 0.000 <lb/>ee <lb/>0.002 ± 0.000 0.001 ± 0.000 0.005 ± 0.000 0.003 ± 0.000 <lb/>0.001 ± 0.000 <lb/>0.001 ± 0.000 <lb/>nm <lb/>0.011 ± 0.000 0.006 ± 0.000 0.008 ± 0.000 0.003 ± 0.000 <lb/>0.017 ± 0.000 <lb/>0.017 ± 0.000 <lb/>cm <lb/>0.002 ± 0.000 0.002 ± 0.000 0.001 ± 0.000 0.001 ± 0.000 <lb/>0.005 ± 0.000 <lb/>0.005 ± 0.000 <lb/>ct <lb/>0.048 ± 0.000 0.023 ± 0.000 0.031 ± 0.000 0.013 ± 0.000 <lb/>0.071 ± 0.001 <lb/>0.077 ± 0.001 <lb/>lp <lb/>0.289 ± 0.006 0.298 ± 0.006 0.433 ± 0.004 0.352 ± 0.008 <lb/>1.069 ± 0.004 <lb/>1.080 ± 0.021 <lb/>tp <lb/>0.208 ± 0.005 0.220 ± 0.002 0.133 ± 0.003 0.127 ± 0.002 <lb/>0.929 ± 0.009 <lb/>1.303 ± 0.062 <lb/>ts <lb/>0.274 ± 0.003 0.249 ± 0.004 0.192 ± 0.002 0.163 ± 0.002 <lb/>0.541 ± 0.005 <lb/>0.508 ± 0.045 <lb/>ch <lb/>0.001 ± 0.000 0.002 ± 0.000 0.001 ± 0.000 0.001 ± 0.000 <lb/>0.006 ± 0.001 <lb/>0.007 ± 0.001 <lb/>mj <lb/>0.010 ± 0.000 0.017 ± 0.000 0.010 ± 0.000 0.008 ± 0.000 <lb/>0.060 ± 0.001 <lb/>0.072 ± 0.001 <lb/>sh <lb/>0.002 ± 0.000 0.003 ± 0.000 0.002 ± 0.000 0.001 ± 0.000 <lb/>0.011 ± 0.000 <lb/>0.013 ± 0.000 <lb/>bm <lb/>0.004 ± 0.000 0.007 ± 0.000 0.003 ± 0.000 0.004 ± 0.000 <lb/>0.026 ± 0.000 <lb/>0.031 ± 0.000 <lb/>Geomean <lb/>Speedup <lb/>1 <lb/>1.01 <lb/>1 <lb/>1.37 <lb/>1 <lb/>1 <lb/>Therefore, the compute kernels generated by MLIR SparseTensor are slightly slower in some cases <lb/>but mostly on par with UniSparse. <lb/>The compute kernels generated by TACO show slower performance due to its different kernel <lb/>generation strategies. For instance, in the case of CSR×CSR→CSR SpGEMM, all three compilers <lb/>generate the row-wise product implementation, while the TACO-generated kernel involves an <lb/>additional sorting of a partial sum index buffer, which makes it slower. We exclude the TACO <lb/>column for the CSC×CSC→CSC SpGEMM kernel because the kernel generated by TACO produces <lb/>incorrect outputs according to our experiments. <lb/>Furthermore, UniSparse generates the SpMM kernel with the DIA-variant format using the kernel <lb/>generation method described in Section 5.3, whereas MLIR SparseTensor and TACO lack support <lb/>for this format. From Table 6, we observe that the performance of SpMM using the DIA-variant <lb/>format is inferior compared to the CSR or DCSC format on the common datasets ee, ch, sh, mj, and <lb/>bm. This performance gap is primarily due to the extra computation required for zero paddings <lb/>along the diagonals in the DIA-variant format during single-threaded execution. As presented in <lb/>Section 7.2.1 and 7.2.2, formats with zero paddings exhibit better performance in a multi-threaded <lb/>setting where computation among the padded dimensions can be parallelized. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:27 <lb/></page>

        <body>Table 6. Comparison of the actual execution times for compute kernels generated by UniSparse, the MLIR <lb/>SparseTensor dialect under the LLVM 15.0.0 release, and the main branch of TACO [Kjolstad et al. 2019]. The <lb/>numbers for each execution time (in seconds) ± standard deviation are computed based on 20 independent <lb/>runs. <lb/>Data <lb/>CSR SpMM <lb/>DCSC SpMM <lb/>Data <lb/>DIA-variant <lb/>SpMM <lb/>TACO <lb/>SparseTensor <lb/>UniSparse <lb/>TACO <lb/>SparseTensor <lb/>UniSparse <lb/>UniSparse <lb/>m2 <lb/>9.682 ± 0.109 1.937 ± 0.309 1.847 ± 0.038 <lb/>9.608 ± 0.059 1.889 ± 0.086 1.857 ± 0.018 <lb/>bw <lb/>0.078 ± 0.001 <lb/>sc <lb/>4.344 ± 0.046 0.727 ± 0.005 0.734 ± 0.042 <lb/>4.410 ± 0.314 0.739 ± 0.034 0.730 ± 0.007 <lb/>af <lb/>8.136 ± 0.135 <lb/>pg <lb/>0.877 ± 0.009 0.154 ± 0.002 0.153 ± 0.006 <lb/>0.875 ± 0.011 0.151 ± 0.006 0.150 ± 0.002 <lb/>cg <lb/>0.528 ± 0.012 <lb/>ca <lb/>8.162 ± 0.495 1.534 ± 0.020 1.498 ± 0.030 <lb/>7.983 ± 0.080 1.567 ± 0.062 1.528 ± 0.027 <lb/>ex <lb/>22.642 ± 0.323 <lb/>f3 <lb/>10.327 ± 0.137 1.749 ± 0.139 1.702 ± 0.059 10.247 ± 0.079 1.765 ± 0.055 1.729 ± 0.038 <lb/>cm <lb/>2.026 ± 0.027 <lb/>cc <lb/>0.818 ± 0.011 0.185 ± 0.004 0.187 ± 0.009 <lb/>0.813 ± 0.008 0.188 ± 0.006 0.187 ± 0.003 <lb/>ct <lb/>32.896 ± 0.460 <lb/>p3 <lb/>1.364 ± 0.020 0.298 ± 0.007 0.302 ± 0.021 <lb/>1.353 ± 0.013 0.302 ± 0.006 0.302 ± 0.006 <lb/>nm <lb/>7.841 ± 0.133 <lb/>ee <lb/>0.098 ± 0.002 0.014 ± 0.001 0.014 ± 0.001 <lb/>0.098 ± 0.002 0.014 ± 0.001 0.014 ± 0.001 <lb/>ee <lb/>3.806 ± 0.044 <lb/>ch <lb/>0.918 ± 0.007 0.132 ± 0.007 0.136 ± 0.009 <lb/>0.917 ± 0.007 0.136 ± 0.008 0.135 ± 0.008 <lb/>ch <lb/>2.179 ± 0.084 <lb/>sh <lb/>1.561 ± 0.008 0.223 ± 0.015 0.223 ± 0.016 <lb/>1.566 ± 0.012 0.230 ± 0.015 0.222 ± 0.002 <lb/>sh <lb/>5.821 ± 0.202 <lb/>mj <lb/>6.997 ± 0.049 1.165 ± 0.196 1.133 ± 0.124 <lb/>6.993 ± 0.051 1.101 ± 0.026 1.125 ± 0.024 <lb/>mj <lb/>30.623 ± 0.472 <lb/>bm <lb/>3.266 ± 0.011 0.533 ± 0.031 0.542 ± 0.049 <lb/>3.274 ± 0.024 0.546 ± 0.043 0.529 ± 0.005 <lb/>bm <lb/>8.672 ± 0.235 <lb/>Geomean <lb/>Speedup <lb/>1 <lb/>5.77 <lb/>5.81 <lb/>1 <lb/>5.72 <lb/>5.81 <lb/>1 <lb/>Data <lb/>CSR×CSR→CSR <lb/>SpGEMM <lb/>CSC×CSC→CSC <lb/>SpGEMM <lb/>TACO <lb/>SparseTensor <lb/>UniSparse <lb/>SparseTensor <lb/>UniSparse <lb/>m2 <lb/>0.610 ± 0.010 0.390 ± 0.004 0.305 ± 0.006 0.388 ± 0.005 0.309 ± 0.006 <lb/>sc <lb/>0.434 ± 0.003 0.245 ± 0.002 0.242 ± 0.003 0.244 ± 0.003 0.244 ± 0.008 <lb/>pg <lb/>0.050 ± 0.002 0.032 ± 0.001 0.028 ± 0.001 0.031 ± 0.001 0.025 ± 0.001 <lb/>ca <lb/>1.384 ± 0.010 0.725 ± 0.007 0.657 ± 0.014 0.720 ± 0.012 0.654 ± 0.006 <lb/>f3 <lb/>2.041 ± 0.017 1.178 ± 0.017 1.058 ± 0.008 1.166 ± 0.010 1.062 ± 0.008 <lb/>cc <lb/>0.268 ± 0.004 0.146 ± 0.002 0.137 ± 0.001 0.143 ± 0.002 0.137 ± 0.002 <lb/>p3 <lb/>0.385 ± 0.005 0.191 ± 0.003 0.179 ± 0.003 0.189 ± 0.002 0.178 ± 0.002 <lb/>ee <lb/>0.055 ± 0.005 0.021 ± 0.002 0.020 ± 0.001 0.021 ± 0.001 0.019 ± 0.001 <lb/>ch <lb/>0.034 ± 0.001 0.012 ± 0.001 0.012 ± 0.001 0.012 ± 0.000 0.013 ± 0.000 <lb/>sh <lb/>0.047 ± 0.001 0.020 ± 0.002 0.020 ± 0.001 0.019 ± 0.000 0.021 ± 0.004 <lb/>mj <lb/>0.504 ± 0.006 0.251 ± 0.022 0.219 ± 0.015 0.242 ± 0.013 0.217 ± 0.001 <lb/>bm <lb/>0.154 ± 0.001 0.079 ± 0.006 0.079 ± 0.007 0.077 ± 0.004 0.077 ± 0.005 <lb/>Geomean <lb/>Speedup <lb/>1 <lb/>1.98 <lb/>2.15 <lb/>1 <lb/>1.06 <lb/>8 CONCLUSION <lb/>We present UniSparse, an intermediate language that describes sparse tensor formats using de-<lb/>coupled data structures and data layouts. Our approach formulates the data structures of a sparse <lb/>format virtually as a metadata tree, described using an index map and a set of structure mutation <lb/>primitives. To enhance the expressibility of sparse data structures, we introduce query primitives <lb/>that allow custom index map functions. Data layouts are determined using layout primitives that <lb/>enable switching from SoA to AoS layout and partitioning a tensor. With the well-formulated format <lb/>description, the UniSparse compiler automates format customization, as well as code generation <lb/>for format conversion and compute kernels. Our work facilitates research into efficient formats <lb/>for various types of tensors, expediting the development of fast sparse tensor algebra in diverse <lb/>domains, including machine learning, scientific computing, and data analytics. <lb/></body>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <page>99:28 <lb/></page>

        <note place="headnote">Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang <lb/></note>

        <div type="availability">DATA-AVAILABILITY STATEMENT <lb/>The software supporting this paper is maintained publicly on GitHub [Liu et al. 2024a]. The version <lb/>submitted to the OOPSLA&apos; 24 Artifact Evaluation Committee (AEC) is permanently archived on <lb/>Zenodo [Liu et al. 2024b]. <lb/></div>

        <div type="acknowledgement">ACKNOWLEDGMENTS <lb/>This work was supported in part by ACE, one of the seven centers in JUMP 2.0, a Semiconductor <lb/>Research Corporation (SRC) program sponsored by DARPA, NSF Awards #1909661, #2019306, <lb/>#2118709 and #2212371, and by AFRL and DARPA under agreement FA8650-18-2-7863. <lb/></div>

        <listBibl>REFERENCES <lb/>Gilad Arnold, Johannes Hölzl, Ali Sinan Köksal, Rastislav Bodík, and Mooly Sagiv. 2010. Specifying and verifying sparse <lb/>matrix codes. ACM Sigplan Notices 45, 9 (2010), 249-260. https://doi.org/10.1145/1863543.1863581 <lb/>Brett W Bader and Tamara G Kolda. 2008. Efficient MATLAB computations with sparse and factored tensors. SIAM Journal <lb/>on Scientific Computing 30, 1 (2008), 205-231. https://doi.org/10.1137/060676489 <lb/>Nathan Bell and Michael Garland. 2009. Implementing sparse matrix-vector multiplication on throughput-oriented processors. <lb/>Int&apos;l Conf. on High Performance Computing Networking, Storage and Analysis (2009), 1-11. https://doi.org/10.1145/1654059. <lb/>1654078 <lb/>Aart Bik, Penporn Koanantakool, Tatiana Shpeisman, Nicolas Vasilache, Bixia Zheng, and Fredrik Kjolstad. 2022. Compiler <lb/>support for sparse tensor computations in MLIR. ACM Trans. on Architecture and Code Optimization (TACO) 19, 4 (2022), <lb/>1-25. https://doi.org/10.1145/3544559 <lb/>Aart Bik and Harry Wijshoff. 1993. Compilation techniques for sparse matrix computations. Int&apos;l Symp. on Supercomputing <lb/>(ICS) (1993). https://doi.org/10.1145/165939.166023 <lb/>Ronald F Boisvert, Ronald F Boisvert, and Karin A Remington. 1996. The matrix market exchange formats: Initial design. <lb/>Vol. 5935. US Department of Commerce, National Institute of Standards and Technology. <lb/>Aydin Buluc and John R Gilbert. 2008. On the representation and multiplication of hypersparse matrices. Int&apos;l Parallel and <lb/>Distributed Processing Symp. (IPDPS) (2008). https://doi.org/10.1109/IPDPS.2008.4536313 <lb/>Aydın Buluç, Jeremy T. Fineman, Matteo Frigo, John R. Gilbert, and Charles E. Leiserson. 2009. Parallel sparse matrix-<lb/>vector and matrix-transpose-vector multiplication using compressed sparse blocks. Symp. on Parallel Algorithms and <lb/>Architectures (SPAA) (2009), 233-244. https://doi.org/10.1145/1583991.1584053 <lb/>Jee W Choi, Amik Singh, and Richard W Vuduc. 2010. Model-driven autotuning of sparse matrix-vector multiply on GPUs. <lb/>ACM SIGPLAN Notices 45, 5 (2010), 115-126. https://doi.org/10.1145/1837853.1693471 <lb/>Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2018. Format abstraction for sparse tensor algebra compilers. <lb/>Proceedings of the ACM on Programming Languages 2, OOPSLA (2018), 1-30. https://doi.org/10.1145/3276493 <lb/>Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2020a. Automatic generation of efficient sparse tensor format <lb/>conversion routines. (2020), 823-838. https://doi.org/10.1145/3385412.3385963 <lb/>Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2020b. Automatic generation of efficient sparse tensor format <lb/>conversion routines. https://github.com/stephenchouca/taco/tree/pldi20ae <lb/>Timothy A. Davis and Yifan Hu. 2011. The University of Florida Sparse Matrix Collection. ACM Trans. on Mathematical <lb/>Software (TOMS) 38, 1 (2011). https://doi.org/10.1145/2049662.2049663 <lb/>Alexandar Devic, Siddhartha Balakrishna Rai, Anand Sivasubramaniam, Ameen Akel, Sean Eilert, and Justin Eno. 2022. To <lb/>PIM or not for emerging general purpose processing in DDR memory systems. Int&apos;l Symp. on Computer Architecture <lb/>(ISCA) (2022). https://doi.org/10.1145/3470496.3527431 <lb/>Jeremy Fowers, Kalin Ovtcharov, Karin Strauss, Eric S Chung, and Greg Stitt. 2014. A high memory bandwidth FPGA <lb/>accelerator for sparse matrix-vector multiplication. IEEE Symp. on Field Programmable Custom Computing Machines <lb/>(FCCM) (2014). https://doi.org/10.1109/FCCM.2014.23 <lb/>Takeshi Fukaya, Koki Ishida, Akie Miura, Takeshi Iwashita, and Hiroshi Nakashima. 2021. Accelerating the SpMV kernel on <lb/>standard CPUs by exploiting the partially diagonal structures. arXiv preprint arXiv:2105.04937 (2021). <lb/>Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574 <lb/>(2019). <lb/>Dahai Guo, William Gropp, and Luke N Olson. 2016. A Hybrid Format for Better Performance of Sparse Matrix-Vector <lb/>Multiplication on a GPU. The International Journal of High Performance Computing Applications 30, 1 (2016), 103-120. <lb/>https://doi.org/10.1177/1094342015593156 <lb/>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. <lb/>Open Graph Benchmark: Datasets for Machine Learning on Graphs. Advances in Neural Information Processing Systems <lb/></listBibl>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. <lb/></note>

        <note place="headnote">UniSparse: An Intermediate Language for General Sparse Format Customization <lb/></note>

        <page>99:29 <lb/></page>

        <listBibl>(2020). https://doi.org/10.5555/3495724.3497579 <lb/>Yuwei Hu, Yixiao Du, Ecenur Ustun, and Zhiru Zhang. 2021. GraphLily: Accelerating graph linear algebra on HBM-equipped <lb/>FPGAs. Int&apos;l Conf. on Computer-Aided Design (ICCAD) (2021). https://doi.org/10.1109/ICCAD51958.2021.9643582 <lb/>Eun-Jin Im and Katherine Yelick. 1998. Model-based memory hierarchy optimizations for sparse matrices. Workshop on <lb/>Profile and Feedback-Directed Compilation 139 (1998). <lb/>Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. 2019. Label propagation for deep semi-supervised <lb/>learning. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (2019), 5070-5079. https: <lb/>//doi.org/10.1109/CVPR.2019.00521 <lb/>Robert Johansson and Robert Johansson. 2015. Sparse Matrices and Graphs. Numerical Python: A Practical Techniques <lb/>Approach for Industry (2015), 235-254. https://doi.org/10.1007/978-1-4842-0553-2_10 <lb/>David R Kincaid, Thomas C Oppe, and David M Young. 1989. ITPACKV 2D user&apos;s guide. Technical Report. Texas Univ., <lb/>Austin, TX (USA). Center for Numerical Analysis. <lb/>Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. 2017. The tensor algebra compiler. <lb/>Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1-29. https://doi.org/10.1145/3133901 <lb/>Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. 2019. The Tensor Algebra Compiler. <lb/>https://github.com/tensor-compiler/taco <lb/>Vladimir Kotlyar, Keshav Pingali, and Paul Stodghill. 1997. Compiling parallel sparse code for user-defined data structures. <lb/>Technical Report. Cornell University. <lb/>Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, <lb/>Nicolas Vasilache, and Oleksandr Zinenko. 2020. MLIR: A compiler infrastructure for the end of Moore&apos;s law. arXiv <lb/>preprint arXiv:2002.11054 (2020). <lb/>Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network Dataset Collection. http://snap.stanford. <lb/>edu/data. (2014). https://doi.org/10.1145/2898361 <lb/>Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang. 2024a. UniSparse: An Intermediate <lb/>Language for General Sparse Format Customization. Cornell University. https://github.com/cornell-zhang/UniSparse <lb/>Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang. 2024b. UniSparse: An Intermediate <lb/>Language for General Sparse Format Customization. Cornell University. https://doi.org/10.5281/zenodo.10464500 <lb/>William Pugh and Tatiana Shpeisman. 1999. SIPR: A new framework for generating efficient code for sparse matrix <lb/>computations. Languages and Compilers for Parallel Computing (1999), 213-229. <lb/>Yousef Saad. 2003. Iterative Methods for Sparse Linear Systems (second ed.). Society for Industrial and Applied Mathematics. <lb/>https://doi.org/10.1137/1.9780898718003 arXiv:https://epubs.siam.org/doi/pdf/10.1137/1.9780898718003 <lb/>Shaden Smith, Jee W. Choi, Jiajia Li, Richard Vuduc, Jongsoo Park, Xing Liu, and George Karypis. 2017. FROSTT: The <lb/>Formidable Repository of Open Sparse Tensors and Tools. http://frostt.io/ <lb/>Linghao Song, Yuze Chi, Licheng Guo, and Jason Cong. 2022a. Serpens: A High Bandwidth Memory Based Accelerator <lb/>for General-Purpose Sparse Matrix-Vector Multiplication. Design Automation Conf. (DAC) (2022), 211-216. https: <lb/>//doi.org/10.1145/3489517.3530420 <lb/>Linghao Song, Yuze Chi, Atefeh Sohrabizadeh, Young-kyu Choi, Jason Lau, and Jason Cong. 2022b. Sextans: A streaming <lb/>accelerator for general-purpose sparse-matrix dense-matrix multiplication. (2022), 65-77. https://doi.org/10.1145/ <lb/>3490422.3502357 <lb/>Nitish Srivastava, Hanchen Jin, Jie Liu, David Albonesi, and Zhiru Zhang. 2020. MatRaptor: A sparse-sparse matrix <lb/>multiplication accelerator based on row-wise product. Int&apos;l Symp. on Microarchitecture (MICRO) (2020). https://doi.org/ <lb/>10.1109/MICRO50266.2020.00068 <lb/>Ruiqin Tian, Luanzheng Guo, Jiajia Li, Bin Ren, and Gokcen Kestor. 2021. A High Performance Sparse Tensor Algebra <lb/>Compiler in MLIR. (12 2021). https://doi.org/10.1109/LLVMHPC54804.2021.00009 <lb/>Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze. 2023. SparseTIR: Composable Abstractions for Sparse <lb/>Compilation in Deep Learning. Int&apos;l Conf. on Architectural Support for Programming Languages and Operating Systems <lb/>(ASPLOS) 3 (2023), 660-678. https://doi.org/10.1145/3582016.3582047 <lb/>Received 21-OCT-2023; accepted 2024-02-24 <lb/></listBibl>

        <note place="footnote">Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 99. Publication date: April 2024. </note>


	</text>

</TEI>