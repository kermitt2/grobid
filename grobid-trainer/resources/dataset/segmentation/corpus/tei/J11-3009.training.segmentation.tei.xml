<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_J11-3009"/>
	</teiHeader>
	<text xml:lang="en">
			<front> Book Reviews <lb/> Computational Modeling of Human Language Acquisition <lb/> Afra Alishahi <lb/> (University of the Saarland) <lb/>Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by <lb/>Graeme Hirst, volume 11), 2010, xiv+93 pp; paperbound, ISBN 978-1-60845-339-9, <lb/>$40.00; ebook, ISBN 978-1-60845-340-5, $30.00 or by subscription <lb/> Reviewed by <lb/>Sharon Goldwater <lb/>University of Edinburgh <lb/></front>

			<body> For much of the last 25 years or more, researchers in natural language processing <lb/>(NLP) and those interested in human language acquisition have had little to say to <lb/>one another. NLP researchers were increasingly focusing on data-intensive supervised <lb/>learning methods, mostly using structured representations, while models of language <lb/>acquisition were typically based either on symbolic nativist accounts (Dresher and <lb/>Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations <lb/>of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996). <lb/>Moreover, language acquisition researchers have understandably been more interested <lb/>in unsupervised than supervised learning, and (perhaps due to the much more difficult <lb/>nature of this problem) have often focused on learning from toy data sets rather than <lb/>large naturalistic corpora. <lb/>Recent developments in both fields have led to a narrowing of the research gap, <lb/>however. NLP researchers have become increasingly interested in unsupervised and <lb/>minimally supervised methods, and the rise of probabilistic models of cognition (Chater <lb/>and Oaksford 1998; Griffiths, Kemp, and Tenenbaum 2008) means there is now a grow-<lb/>ing number of cognitive scientists who are well-versed in many of the same statistical <lb/>methods that are used in NLP. Thus, a short introductory text on computational mod-<lb/>eling of human language acquisition seems particularly apt at this time. Alishahi&apos;s slim <lb/>volume is not intended to be comprehensive, but rather to provide a brief overview <lb/>of the goals and methods of the field for researchers in related areas—either language <lb/>acquisition researchers with little computational experience or NLP researchers with-<lb/>out much knowledge of cognitive science. It aims for intuitive explanations rather <lb/>than highly technical ones, and includes a number of figures and diagrams, but no <lb/>equations. <lb/></body>
			
			<front>© 2011 Association for Computational Linguistics <lb/></front>
			
			<body>The book can be divided logically into two parts followed by a brief concluding <lb/>chapter. The first part (Chapters 1 and 2) provides an overview of the major research <lb/>questions and methodologies in the field. Chapter 1 begins by introducing some of the <lb/>main theoretical debates in the field of language acquisition—questions of modularity <lb/>and learnability. NLP researchers with some background in linguistics will probably <lb/>already be familiar with these debates at the level of detail presented here; these sections <lb/>will be more useful to those with a straight computer science background. Also included <lb/>in Chapter 1 is a section motivating the use of computational models as an alternative <lb/>to behavioral studies for studying language acquisition. This section will be useful to <lb/>anyone who is new to the idea of computational modeling of cognition, as will Chapter 2 <lb/>of the book. Chapter 2 discusses Marr&apos;s (1982) influential analysis of the different kinds <lb/> </body>
			
			<front>Computational Linguistics <lb/>Volume 37, Number 3 <lb/></front> 
			
			<body>of explanations that models can provide, as well as the criteria for cognitive plausibility <lb/>against which models are judged, and the main frameworks for model development <lb/>(symbolic, connectionist, probabilistic). Finally, it describes the various ways models <lb/>can be evaluated, along with a list of corpus resources. <lb/>In the second part of the book (Chapters 3–5), Alishahi focuses on three areas of <lb/>language acquisition in particular: learning the meanings of words (Chapter 3), learning <lb/>morphology and syntax (Chapter 4), and learning relationships between syntax and <lb/>semantics, such as verb–argument structure and semantic roles (Chapter 5). Each chap-<lb/>ter is divided into sections focusing on more specific topics—for example, Chapter 4 <lb/>includes sections on morphology, syntactic categories, and syntactic structure. Each <lb/>section begins by reviewing the most salient empirical facts about children&apos;s acquisition <lb/>in that domain, along with relevant linguistic concepts and theories that have influenced <lb/>the modeling community (e.g., Mutual Exclusivity in word learning; Construction <lb/>Grammar and the Principles and Parameters theory in syntactic acquisition; theories <lb/>of selectional restrictions in the acquisition of verb–argument structure). The empirical <lb/>and theoretical background is followed by an overview of many of the models that have <lb/>been proposed in the area, and finally one or more &quot; case studies &quot; (more on these in a <lb/>moment). <lb/>In general the structure and content of this book is appropriate for researchers from <lb/>neighboring fields (including NLP) who want to get a quick taste of what computational <lb/>modeling of human language acquisition is all about. They can read the first couple of <lb/>chapters to get a general idea of the questions and methodologies, and then pick and <lb/>choose any topics from the remaining chapters that might be of interest. There are very <lb/>few dependencies between sections in the second part of the book, because most of the <lb/>relevant background for each section is provided in that section itself. Someone who is <lb/>interested in pursuing the field further (either a beginning graduate student or a more <lb/>advanced researcher moving into the field) will also find many useful references, at least <lb/>in the three areas that Alishahi focuses on. Many other active areas of modeling are not <lb/>covered at all (for example phonetic and phonological acquisition), although this choice <lb/>is understandable given the length of the book. <lb/>The main weakness of the book is in the execution of the case studies. Each case <lb/>study (with some exceptions, as noted subsequently) details a single model, with a half-<lb/>page to two-page description of the model&apos;s primary methods and assumptions, as well <lb/>as the input and results. It is an excellent idea to provide concrete examples showing <lb/>how models can be used to address important questions in language acquisition. But <lb/>in practice, most of the case studies fall short of this goal, as they are too light on <lb/>motivation and analysis. Alishahi is not always clear about why certain models, rather <lb/>than others, were chosen for case studies (are they ground-breaking in some way, or <lb/>merely a simple example of a particular theoretical idea put into practice?), nor how <lb/>each model&apos;s assumptions and results relate to the broader goals of modelling set out <lb/>in the first two chapters. In addition, three out of the four &quot; case studies &quot; in Chapter 4 <lb/>are really just additional review sections, covering models of the English past tense, <lb/>learning algorithms based on Principles and Parameters, and distributional models of <lb/>syntactic structure (all worthy topics, but not case studies). This leaves only one true <lb/>case study in Chapter 4, on the MOSAIC model of grammar induction (Jones, Gobet, <lb/>and Pine 2000). Finally, although there are case studies of symbolic, connectionist, and <lb/>probabilistic models, no Bayesian models are given a detailed look (though they are <lb/>included in the review sections). Bayesian models are, of course, a subset of probabilistic <lb/>models, but take a very different philosophical approach to most other models, includ-<lb/>ing the type of incremental probabilistic model that Alishahi discusses in more detail. <lb/>

			<page>628 <lb/></page>

			<note place="headnote">Book Reviews <lb/></note> 
			
			Bayesian modeling is now an important force in cognitive science generally and has <lb/>begun to make an impact in language acquisition specifically (Xu and Tenenbaum 2007; <lb/>Foraker et al. 2009; Goldwater, Griffiths, and Johnson 2009); as such it is worth taking a <lb/>bit more space to explain the ideas behind at least one of these models. <lb/>Despite these weaknesses in the case studies, there is enough useful material in <lb/>this book to make reading it worthwhile to any researcher who wants to get a quick <lb/>overview of the main goals and approaches in the field, along with some of the many <lb/>models that have been developed over the years. It will also be helpful to those who <lb/>are looking for a starting point for a more in-depth study of models in one of the three <lb/>areas of acquisition that Alishahi focuses on. Overall, it is a very accessible, if necessarily <lb/>selective, brief introduction to the field. <lb/></body>

			<listBibl> References <lb/> Chater, Nicholas and Mike Oaksford, editors. <lb/>1998. Rational Models of Cognition. Oxford <lb/>University Press, Oxford. <lb/>Dresher, B. Elan and Jonathan Kaye. 1990. <lb/>A computational learning model for <lb/>metrical phonology. Cognition, <lb/> 34(2):137–195. <lb/>Elman, Jeffrey, Elizabeth Bates, Mark H. <lb/>Johnson, Anette Karmiloff-Smith, <lb/>Domenico Parisi, and Kim Plunkett. 1996. <lb/> Rethinking Innateness: A Connectionist <lb/>Perspective on Development. MIT <lb/>Press/Bradford Books, Cambridge, MA. <lb/>Foraker, Stephanie, Terry Regier, Naveen <lb/>Khetarpal, Amy Perfors, and Joshua B. <lb/>Tenenbaum. 2009. Indirect evidence and <lb/>the poverty of the stimulus: The case of <lb/>anaphoric one. Cognitive Science, <lb/> 33(2):287–300. <lb/>Gibson, Edward and Kenneth Wexler. <lb/>1994. Triggers. Linguistic Inquiry, <lb/> 25(3):407–454. <lb/>Goldwater, Sharon, Thomas L. Griffiths, <lb/>and Mark Johnson. 2009. A Bayesian <lb/>framework for word segmentation: <lb/>Exploring the effects of context. Cognition, <lb/> 112(1):21–54. <lb/>Griffiths, Thomas L., Charles Kemp, and <lb/>Joshua B. Tenenbaum. 2008. Bayesian <lb/>models of cognition. In Ron Sun, editor, <lb/> Cambridge Handbook of Computational <lb/>Cognitive Modeling. Cambridge University <lb/>Press, Cambridge, chapter 3. <lb/>Jones, Gary, Fernand Gobet, and Julian M. <lb/>Pine. 2000. A process model of children&apos;s <lb/>early verb use. In Proceedings of the 22nd <lb/>Meeting of the Cognitive Science Society, <lb/> pages 723–728, Philadelphia, PA. <lb/>Marr, David. 1982. Vision: A Computational <lb/>Approach. Freeman &amp; Co., San <lb/>Francisco, CA. <lb/>Rumelhart, David and James McClelland. <lb/>1986. On learning the past tenses of <lb/>English verbs. In David Rumelhart and <lb/>James McClelland, editors, Parallel <lb/>Distributed Processing: Explorations in the <lb/>Microstructure of Cognition. MIT Press, <lb/>Cambridge, MA, chapter 18. <lb/>Xu, Fei and Joshua B. Tenenbaum. 2007. <lb/>Word learning as Bayesian inference. <lb/> Psychological Review, 114(2):245–272. <lb/></listBibl> 
			
			<body>This book review was edited by Pierre Isabelle. <lb/>Sharon Goldwater is a Lecturer in the Institute for Language, Cognition and Computation, School <lb/>of Informatics, University of Edinburgh. Her research interests include unsupervised learning of <lb/>linguistic structure (including word segmentation, phonology, morphology, and syntax) and the <lb/>application of machine learning techniques, especially Bayesian methods, to the computational <lb/>study of human language acquisition. Goldwater&apos;s address is Informatics Forum, 10 Crichton <lb/>Street, Edinburgh EH8 9AB, United Kingdom; e-mail: sgwater@inf.ed.ac.uk. <lb/></body>

			<page> 629 </page>


	</text>
</tei>
