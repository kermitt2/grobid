<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="-1"/>
	</teiHeader>
	<text xml:lang="en">
			<front>This work is licensed under a Creative Commons Attribution 4.0 International License, <lb/>which permits unrestricted use, distribution, and reproduction in any medium, provided the <lb/>original work is properly cited. <lb/>ech <lb/>T <lb/>Press <lb/>Science <lb/>DOI: 10.32604/cmc.2024.050913 <lb/>ARTICLE <lb/>Contemporary Study for Detection of COVID-19 Using Machine Learning <lb/>with Explainable AI <lb/>Saad Akbar 1,2 , Humera Azam 1 , Sulaiman Sulmi Almutairi 3,* , Omar Alqahtani 4 , Habib Shah 4 and <lb/>Aliya Aleryani 4 <lb/>1 Department of Computer Science, University of Karachi, Karachi, 75270, Pakistan <lb/>2 Department of Computing, Faculty of Engineering Science and Technology, Hamdard University, Karachi, 75540, Pakistan <lb/>3 Department of Health Informatics, College of Applied Medical Sciences, Qassim University, Qassim, 51452, Saudi Arabia <lb/>4 Department of Computer Science, College of Computer Science, King Khalid University, Abha, 61421, Saudi Arabia <lb/>*Corresponding Author: Sulaiman Sulmi Almutairi. Email: ssmtiery@qu.edu.sa <lb/>Received: 22 February 2024 Accepted: 23 May 2024 Published: 18 July 2024 <lb/>ABSTRACT <lb/>The prompt spread of COVID-19 has emphasized the necessity for effective and precise diagnostic tools. In this <lb/>article, a hybrid approach in terms of datasets as well as the methodology by utilizing a previously unexplored <lb/>dataset obtained from a private hospital for detecting COVID-19, pneumonia, and normal conditions in chest <lb/>X-ray images (CXIs) is proposed coupled with Explainable Artificial Intelligence (XAI). Our study leverages <lb/>less preprocessing with pre-trained cutting-edge models like InceptionV3, VGG16, and VGG19 that excel in the <lb/>task of feature extraction. The methodology is further enhanced by the inclusion of the t-SNE (t-Distributed <lb/>Stochastic Neighbor Embedding) technique for visualizing the extracted image features and Contrast Limited <lb/>Adaptive Histogram Equalization (CLAHE) to improve images before extraction of features. Additionally, an <lb/>Attention Mechanism is utilized, which helps clarify how the model makes decisions, which builds trust in artificial <lb/>intelligence (AI) systems. To evaluate the effectiveness of the proposed approach, both benchmark datasets and a <lb/>private dataset obtained with permissions from Jinnah Postgraduate Medical Center (JPMC) in Karachi, Pakistan, <lb/>are utilized. In 12 experiments, VGG19 showcased remarkable performance in the hybrid dataset approach, <lb/>achieving 100% accuracy in COVID-19 vs. pneumonia classification and 97% in distinguishing normal cases. <lb/>Overall, across all classes, the approach achieved 98% accuracy, demonstrating its efficiency in detecting COVID-<lb/>19 and differentiating it from other chest disorders (Pneumonia and healthy) while also providing insights into the <lb/>decision-making process of the models. <lb/>KEYWORDS <lb/>COVID-19 detection; deep neural networks; support vector machine; CXIs; InceptionV3; VGG16; VGG19; <lb/>t-SNE embedding; CLAHE; attention mechanism; XAI <lb/></front>

			<body>1 Introduction <lb/>Coronavirus disease also recognized as COVID-19 is a very transmissible respiratory disease/in-<lb/>fection caused by the SARS-CoV-2 virus. The World Health Organization (WHO) has received reports <lb/></body>

			<page>1076 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>of 774,593,066 confirmed COVID-19 cases, with 7,028,881 resulting in death, by 04 February 2024, the <lb/>cumulative number of administered vaccine doses reached 13,195,832,385 [1]. The worldwide COVID-<lb/>19 problem has affected many people and economies everywhere [2]. The virus has caused lots of deaths <lb/>and made millions of people sick. Also, many businesses and industries are having big problems and <lb/>losing money, and the situation is getting worse. People with COVID-19 usually have common signs <lb/>like fever, feeling tired, and a dry cough [3]. They might also feel like they cannot breathe well and <lb/>have body aches, a sore throat, and sometimes even diarrhea, nausea, or a runny nose [4]. The social <lb/>and psychological impact of the pandemic has also been significant, with many people experiencing <lb/>stress, anxiety, and loneliness due to social distancing measures and other restrictions. The COVID-<lb/>19 pandemic had a widespread and unprecedented impact on the world, and efforts are ongoing to <lb/>address the various challenges it has created [5]. In general, COVID-19 diagnosis is possible through <lb/>PCR tests, however, these types of tests can be time-consuming and require specialized laboratory <lb/>equipment. There has been growing interest in using deep learning (DL) to examine X-ray images <lb/>especially CXIs to find various diseases related to the lungs, including COVID-19, Tuberculosis, or <lb/>pneumonia [6-10]. One of the primary drawbacks of CT imaging is that it generally requires more time <lb/>compared to X-ray imaging. Additionally, the availability of CT scanners is limited in many regions, <lb/>which can hinder timely screening for COVID-19. In contrast, X-rays are the most commonly used <lb/>and widely available diagnostic imaging technique [10] and are portable. CXIs are often thought to be <lb/>better than CT scans for finding COVID-19 because they are easy to get and the pictures are made <lb/>quickly. COVID-19 can make the lungs look different, and X-rays can show if someone has it by <lb/>looking at their lungs. <lb/>In recent years, AI has made remarkable strides, but its inner workings often remain opaque, <lb/>hindering trust and understanding. XAI is a new growing field focused on making AI systems <lb/>transparent and interpretable. This article explores the significance of XAI in the detection of COVID-<lb/>19, fostering accountability, and enhancing user confidence in AI technologies, especially in the field <lb/>of medical systems. Reference [11] proposed an AI model, enhanced with XAI, to detect and interpret <lb/>COVID-19-positive CXIs, aiming to aid clinical decisions. Another technique proposed [12] is a novel <lb/>methodology integrating copy number alteration, DNA methylation, and gene expression data to <lb/>predict Gleason&apos;s score in prostate cancer. By providing a visual representation of the integrated data, <lb/>researchers can gain insights into how different omics features contribute to the predictive model&apos;s <lb/>outcomes, thereby enhancing interpretability and transparency in the predictive process. <lb/>2 Objectives and Contributions <lb/>In this section, the proposed methodology for the experiment is shown in Fig. 1 below. In <lb/>this study, CNN is used for feature extraction by the removal of the fully connected layers from <lb/>the CNN, a t-SNE embedding is also utilized to visualize the extracted features of the diseases. <lb/>In the end, the attention mechanism, which is associated with XAI techniques, is employed to improve <lb/>the transparency and interpretability of the model&apos;s predictions. However, this approach is very helpful <lb/>and novel for viewing the features, and SVM is used for classification tasks. <lb/>3 Related Work <lb/>In recent times, there has been a notable surge in interest surrounding the application of DL <lb/>techniques to analyze medical images. Different state-of-the-art studies use cutting-edge technologies <lb/>to utilize the images for diagnosis purposes [13-15]. Limited studies have been identified that leverage <lb/>XAI techniques in the domain of medical imaging analysis. Gamage et al. [16] utilized recently <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1077 <lb/></page>

			<body>the XAI technique to identify skin cancer. This surge stems from their capability to enhance the <lb/>precision and proficiency of disease identification. Sarkar et al. [17] proposed a novel Multi-Scale <lb/>CNN architecture coupled with SHAP and Grad-CAM explainable AI techniques enhances accuracy <lb/>in multi-class lung disease classification from CXIs. In [18] reseracher with his team introduces <lb/>a framework for classifying XAI methods in this field, categorizes existing papers based on this <lb/>framework and anatomical location, and discuss future prospects for XAI in medical image analysis. <lb/>Wang et al. [19] proposed a Particle Swarm Optimization guided Self-Tuning CNN (PSTCNN) for <lb/>COVID-19 diagnosis, combining automated hyperparameter optimization with XAI for enhanced <lb/>accuracy, efficiency, and interpretability. Overall, it offers valuable insights into the current state and <lb/>future directions of XAI in healthcare. <lb/>Figure 1: Comprehensive experimental process overview <lb/>Specifically in relation to COVID-19, CXIs have emerged as a prevalent diagnostic tool, valued <lb/>for their ease of access and non-intrusive characteristics. This section dedicated to a review of existing <lb/>literature aims to investigate into the current forefront of implementation. Also, delve into diverse <lb/>methodologies and architectures employed, the datasets coupled for training and assessment, as well <lb/>as the varied performance outcomes documented across multiple studies as mentioned in Table 1. <lb/>CXIs are classic approach and broadly used for the detection of lung illnesses, including COVID-<lb/>19. However, understanding CXIs can be difficult, especially when there are a lot of images to look <lb/>at during a pandemic, and radiologists have many pictures to review. Lately, deep neural networks <lb/>(DNNs) have been used a lot to understand pictures better. They have been helpful in telling what&apos;s <lb/>in the pictures, especially in medical pictures. However, these studies have provided a comprehensive <lb/>summary of their findings, including relevant details, also not utlizaing the XAI approach in relation <lb/>to COVID-19 detection from CXIs as discussed in Table 1. <lb/></body>

			<page>1078 <lb/></page>

			<front>CMC, 2024, vol.80, no.1 <lb/>Table 1: Approaches and performance summary for COVID-19 detection using CXIs <lb/>Ref. Preprocess XAI t-SNE <lb/>Contrast <lb/>algorithm <lb/>Classification <lb/>Findings <lb/>FCL <lb/>SVM <lb/>[20] <lb/>Yes <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>Yes <lb/>The study achieved an <lb/>accuracy of 99.38%. <lb/>[21] <lb/>Yes <lb/>No <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>The method attained an <lb/>impressive overall <lb/>classification rate of <lb/>99.27%. <lb/>[22] <lb/>No <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>This model showed very <lb/>good results in finding <lb/>COVID-19, with <lb/>99.53% accuracy overall <lb/>and 95.35% accuracy <lb/>for identifying two <lb/>different groups in the <lb/>classification. <lb/>[7] <lb/>Yes <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>Accuracy rates exceeded <lb/>90% in all scenarios <lb/>except one, and it also <lb/>achieved a remarkable <lb/>99% accuracy. <lb/>[23] <lb/>Yes <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>The research shows <lb/>good scores for a <lb/>measure called <lb/>F1-score. The normal <lb/>case result is 0.89 and <lb/>the result of COVID-19 <lb/>is 0.91, 0.00 and 0.67 <lb/>are the worst scores. <lb/>[24] <lb/>Yes <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>Average levels of <lb/>specificity, sensitivity, <lb/>accuracy, and ROC are <lb/>achieved at 99.18%, <lb/>95.7%, 98.50%, and <lb/>96.51%, respectively. <lb/>[25] <lb/>No <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>In the proposed <lb/>methodology, a <lb/>sensitivity of 90%, <lb/>accuracy of 95%, and <lb/>AUC of 0.97 were <lb/>achieved. <lb/>(Continued) <lb/></front>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1079 <lb/></page>

			<body>Table 1 (continued) <lb/></body>

			<front>Ref. Preprocess XAI t-SNE <lb/>Contrast <lb/>algorithm <lb/>Classification <lb/>Findings <lb/>FCL <lb/>SVM <lb/>[26] <lb/>No <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>The model attained a <lb/>classification accuracy <lb/>of 96.78% with 98.66% <lb/>sensitivity and <lb/>specificity rates of <lb/>96.46%. <lb/>[27] <lb/>No <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>Accuracy is 98.08%, and <lb/>87.02% for binary and <lb/>multi-class, respectively. <lb/>[28] <lb/>No <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>The average accuracy is <lb/>0.97. <lb/>[29] <lb/>Yes <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>No <lb/>Overall accuracy of <lb/>94.5% achieved using <lb/>CNN-based CAD <lb/>scheme. <lb/>[30] <lb/>No <lb/>No <lb/>No <lb/>Yes <lb/>Yes <lb/>Yes <lb/>Model exhibited <lb/>impressive classification <lb/>metrics, including, <lb/>94.81% recall, 94.94% <lb/>precision, 97.41% <lb/>accuracy, 98.27% <lb/>specificity, and a <lb/>94.86% F1-score. <lb/></front>

			<body>Table 1 summarizes several implementations on COVID-19 classification using different models <lb/>of DL, detailing their preprocessing steps, baseline models, contrast amplification, classification <lb/>methods, and findings in terms of accuracy and other metrics, where FCL stands for Fully Connected <lb/>Layer and SVM for support vector machine. The model proposed in [31] presents an automated <lb/>approach for designing to classify COVID-19 from CXIs, approach uses DCNN architecture with <lb/>a minimum number of convolutional layers, also approach involves an iterative process that optimizes <lb/>the hyper parameters until no further improvement in accuracy is achieved. This approach starts <lb/>with a simple CNN and gradually enhances the model by adding up to two additional layers. A <lb/>practical solution utilizes cutting-edge Machine Learning (ML) techniques, particularly EfficientNet <lb/>and MixNet, for detecting COVID-19 from CXIs and lung computed tomography images [32]. <lb/>Through rigorous validation across multiple datasets, the approach consistently achieves high accuracy <lb/>rates exceeding 95%, demonstrating significant performance gains compared to existing studies. Also <lb/>in [33] survey assesses the role of DL in combating the COVID-19 pandemic across various domains, <lb/>highlighting its applications in Natural Language Processing (NLP), Computer Vision, Life Sciences, <lb/>and Epidemiology, while addressing key challenges and potential directions for future research. <lb/></body>

			<page>1080 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>Wu et al. [22] proposed a novel approach for detection of COVID-19 using CXIs. They combined a <lb/>CNN with UL-Net, a modified version of U-Net, to create their proposed structure. The modified <lb/>structure included a new down sampling side and omitted the connections among the FCL. This <lb/>adaptation was inspired by the structure of UL. By leveraging this architecture, the main purpose of the <lb/>researchers is to escalate the detection accuracy of COVID-19. Sheykhivand with his team [7] utilized <lb/>CXIs to classify two to four classes, which included healthy, viral, bacterial, and COVID-19 classes, <lb/>based on seven distinct and meaningful scenarios. To improve the classification performance within <lb/>the CXIs dataset, the authors proposed an architecture that incorporated LSTM and transfer learning <lb/>with Generative Adversarial Networks (GANs). This combination of techniques aimed to enhance <lb/>the accuracy of classifying the given classes. Canayaz [20] investigated the diagnosis of COVID-19 <lb/>utilizing CXIs. In the research, they made a dataset with different types of images: normal ones, ones <lb/>with COVID-19, and ones with pneumonia. They wanted to see how well certain methods could tell <lb/>these three types apart in the CXIs dataset. Ying et al. [34] created a special way to make pictures <lb/>look clearer. Their method was really useful for making pictures better, and it made datasets used <lb/>for different things much better too. Hemdan et al. [23] used primarily concentrated on COVIDX-<lb/>Net, a novel model that integrates seven different methods of DCNN. Among these methods are <lb/>advanced models such as VGG19, as well as an additional variant of Google MobileNet. Each DCNN <lb/>model within COVIDX-Net is specifically designed to analyze the normalized intensities of CXIs and <lb/>diagnose whether a patient&apos;s condition is negative or positive for COVID-19. The research aimed to <lb/>assess the efficacy of the different DCNN models in accurately classifying COVID-19 cases through <lb/>the analysis of CXIs. Togaçar et al. [21] worked with datasets that included three classification classes: <lb/>COVID-19, healthy and pneumonia cases. To enhance the image quality in the preprocessing stage, <lb/>the datasets the Fuzzy Color method was applied, and the images were stacked with the original <lb/>images for organization after preprocessing feature extraction was performed by applying SqueezeNet <lb/>and MobileNetV2 models, aided by the Social Mimic Optimization (SMO) technique and, a SVM <lb/>was used as the classifier in the final step of the detection process. This integrated approach aimed <lb/>to achieve accurate classification of COVID-19 cases based on the CXIs dataset by incorporating <lb/>preprocessing, feature extraction, and classification techniques. In [24], the researchers pro a CNN-<lb/>based approach for identifying COVID-19 diseases with the help of CXIs. They utilized of previous <lb/>training models, which are considered state-of-the-art, for transfer learning. The design they suggested, <lb/>named COVID-CAPS, used a special kind of network called Capsule Network. This network had <lb/>four layers that understand shapes in images and three layers that understand features. They aimed <lb/>to be very good at finding COVID-19, while using as few settings as possible. Their main goal was <lb/>to help doctors and patients quickly know if someone had COVID-19. They tried their design and <lb/>it worked-it could find COVID-19 in the medical images. The combination of a Capsule Network <lb/>architecture proved to be effective in achieving accurate and efficient COVID-19 diagnosis from CXIs. <lb/>Apostolopoulos et al. [26] employed transfer learning for detection of COVID-19 from CXIs in which <lb/>system utilized well known CNN architectures that have been proposed in recent years. By including <lb/>transfer learning, the goal of the study was to leverage the knowledge acquired from available previous <lb/>training models on huge datasets. References [35,36] also used DL technique to detect COVID-19 <lb/>from CXIs. Ozturk et al. [27] also utilized CXIs and introduced a model for the detection of COVID-<lb/>19, the model they proposed employed an end-to-end framework, eliminating the need for separate <lb/>feature extraction. For both multi-class and binary-class classification tasks, the DarkNet model was <lb/>utilized. Through experimental testing conducted on recovered patients between 5 to 13 days after <lb/>recovery, the model demonstrated its capability to identify positive symptoms [37]. Brunese et al. [28] <lb/>proposed a model designed to discern between CXIs depicting COVID-19, pneumonia, and healthy <lb/>cases. The approach proposed by the researchers consisted of three distinct stages, each fulfilling a <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1081 <lb/></page>

			<body>specific objective. Heidari et al. [29] utilized CAD for the detection of COVID-19, while Azad et al. [30] <lb/>employed different techniques for classifying CXIs. On the other hand, Azad et al. directed their efforts <lb/>towards CXIs classification by utilizing pre-trained CNN models alongside Local Binary Pattern <lb/>(LBP) for extracting features. These captured characteristics were subsequently utilized as input for <lb/>various classifiers, such as the PIN, SVM, CT, RT, and KCN. Although the specific classes are not <lb/>mentioned, their aim was to classify CXIs into four classes. By leveraging pre-trained CNN models <lb/>and utilizing LBP for feature extraction, they aimed to capture relevant patterns and information from <lb/>the images and improve the classification task. In a recent study [38], a hybridized approach combining <lb/>manual and deep spatial features, along with traditional transfer learning techniques using CNN-based <lb/>deep learning models, achieved a diagnostic accuracy of 95.57% for COVID-19 detection from CXIs. <lb/>Zhang et al. [39] used the FECNet model, utilizing a varying-distance GLCM-based feature extraction <lb/>and ELMclassifier, and achieved an accuracy of 92.70% for COVID-19 recognition on one dataset and <lb/>92.53% on another, outperforming five other state-of-the-art models. <lb/>4 Research Gap <lb/>In the field of diagnosing COVID-19 through medical imaging, there is a notable research gap <lb/>that needs attention. Currently, many studies have focused on using CXIs together with advanced deep <lb/>neural networks or state-of-the-art CNN architectures to detect COVID-19 features. However, XAI <lb/>remains underutilized in the detection and visualization of COVID-19, and other medical imaging <lb/>tasks. Also, CXIs have been extensively studied, CT scans offer a different perspective and can <lb/>potentially provide valuable insights for a more accurate and comprehensive diagnosis of COVID-<lb/>19 with XAI techniques. Investigating the application of these state-of-the-art DL learning models, <lb/>alongside other cutting-edge pre-trained models, on CT scans may reveal new possibilities and improve <lb/>the accuracy of COVID-19 detection with XAI to find the unexplored areas in the image. In this regard, <lb/>the survey of additional preprocessing techniques may improve the performance of these models, <lb/>making them more effective in addressing this research gap in the field of medical diagnosis using <lb/>images. <lb/>5 Datasets <lb/>This research study involved the use of chest X-ray images from real-time patient data collected <lb/>between November 2020 and November 2021. Data for the experiments were sourced from Jinnah <lb/>Postgraduate Medical Center, Karachi. The data collection received ethical approval from the Ethics <lb/>Committee of Jinnah Postgraduate Medical Center, Karachi (MXR(G)-39/21/JPMC). <lb/>The experiments conducted in this study utilized four distinct datasets for COVID-19 detection <lb/>from CXIs. One dataset is collected from JPMC hospital, which is the private dataset. For ensuring <lb/>worth and suitability of dataset for our experiments, data preprocessing steps were performed. Firstly, <lb/>a data cleaning process was conducted to remove any corrupt or unusable images from the datasets. <lb/>Additionally, resizing operations were applied to standardize the images to a uniform size of 128 × 128 <lb/>pixels, which is a commonly used resolution for CXIs. Fig. 2 below shows the process before training <lb/>the model and cleaning the datasets. <lb/>The first dataset, obtained from Kermany et al. [40], consists of normal and pneumonia cases. It <lb/>encompasses CXIs of individuals without respiratory abnormalities as well as those diagnosed with <lb/>pneumonia, including both bacterial and viral cases. The second dataset is a private dataset obtained <lb/>from JPMC, for which proper permission was obtained which contains COVID-19 cases. This dataset <lb/></body>

			<page>1082 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>contains CXIs of patients diagnosed with COVID-19, confirmed through RT-PCR tests. In Fig. 3, <lb/>JPMC dataset example is given. <lb/>Figure 2: Data collection and preprocessing flow before performing the experiment <lb/>Figure 3: Example of COVID-19 dataset obtained from JPMC <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1083 <lb/></page>

			<body>Lastly, the JP Cohen dataset was utilized, which specifically focuses on the CXIs of patients <lb/>diagnosed with COVID-19 [41]. The inclusion of these datasets, representing normal, pneumonia, <lb/>and COVID-19 cases, allows for a comprehensive evaluation of our DL approach effectiveness in <lb/>differentiating between these conditions accurately. <lb/>In this experiment, a balanced number of images were used to ensure equal representation and <lb/>minimize potential biases. The use of a balanced dataset is important for obtaining accurate and <lb/>reliable results, as it helps to mitigate any potential confusing factors that may arise due to an unequal <lb/>distribution of images with dimensions of 128 × 128 pixels after resizing the images from higher <lb/>dimensions. Fig. 4 shows the summary and distribution of the CXIs used to perform the experiments. <lb/>Figure 4: Ratio of images used to perform the experiment <lb/>This study was conducted with an additional 500 (after cleaning the data) COVID-19 patients who <lb/>had been confirmed positive and discharged from the hospital between November 2020 and November <lb/>2021. Most of those patients are not younger than 50 years old. Images are in the DICOM format and <lb/>before training cleaning, preprocessing, and image enhancements were applied. <lb/>6 Methods <lb/>6.1 CLAHE <lb/>Several studies [42-45] have shown how important it is to make DL models work better for this <lb/>task they do this by using the right techniques or methods to process the pictures that are put into the <lb/>models. The CLAHE algorithm is a commonly used technique in image processing that improves the <lb/>contrast and details in an image by redistributing the pixel intensities. Fig. 5 shows the steps which are <lb/>in Algorithm 1 applied to enhance the visibility of an image. <lb/>In this regard, the CLAHE algorithm is implemented to increase visibility of images before train-<lb/>ing. Fig. 6 illustrates an instance demonstrating the unaltered image alongside the image improved <lb/>through the implementation of the CLAHE method. <lb/>Histogram Equalization is applied to the grayscale image denoted as x, where the variable i <lb/>represents a grayscale intensity ranging up to a maximum value of 256. To determine the probabilities <lb/>associated with a pixel in the image at intensity level j, the calculation is performed mathematically as <lb/>follows in Eq. (1): <lb/>P x (j) = P (x = l) = <lb/>n l <lb/>n <lb/>, 0 ≤ j &lt; L <lb/>(1) <lb/></body>

			<page>1084 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>where L denotes the total count of gray levels present in the image, n corresponds to the total number <lb/>of pixels and P x (j) signifies a portrayal of the image histogram, associating the pixel intensity with an <lb/>index j. <lb/>Algorithm 1: Histograms Equalization Function <lb/>Input: Grayscale image I Input <lb/>Output: Histogram equalized image I Output <lb/>1. Read the input grayscale image: I Input <lb/>2. Compute the histogram equalization transformation: <lb/>Let n be the pixels in I Input image. <lb/>Let p i be the normalized histogram of pixel intensity i in I Input , where i ranges from 0 to 255. <lb/>Compute the cumulative distribution function (CDF) Ci for each pi: <lb/>C i = <lb/>i <lb/>j=0 P j <lb/>Compute the transformation function Ti for each pi: <lb/>T i = round(255.C i ) <lb/>Apply the transformation function to each pixel in I Input to obtain the histogram equalized image <lb/>I Output : <lb/>I Output (x, y) = I Input (x, y), where (x, y) are the pixel coordinates. <lb/>3. Display images for comparison: <lb/>Display I Input and I Output side by side for visual comparison. <lb/>4. End. <lb/>Figure 5: Process of CLAHE algorithm to perform the enhancement <lb/>6.2 XAI-Attention Mechanism <lb/>Attention mechanism help models pay attention to the right things when they are working on <lb/>tasks of recognizing objects in pictures or patterns and also on NLP [46]. The technique works like a <lb/>spotlight that shines on the important parts of the data, making it easier for the model to do its job <lb/>well. They are inspired by how humans pay attention to things, and they help the models perform <lb/>better and make their decisions more understandable. Attention mechanisms have not been widely <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1085 <lb/></page>

			<body>utilized as an approach for explainability in medical imaging, particularly in the context of COVID-<lb/>19 diagnosis so far. In [16], authors employed XAI techniques for medical imaging in their study on <lb/>classifying melanoma skin cancer images. <lb/>Figure 6: Example of CLAHE, before and after applied in an image <lb/>The attention mechanism is like a spotlight for a transformer model. It helps the model focus on <lb/>important parts of the input while ignoring less relevant ones. Here is how it works: First, the input is <lb/>broken down into pieces, like words in a sentence or patches in an image. Then, the model learns three <lb/>things for each piece: what to look for (query), what to compare it to (key), and what information <lb/>to gather (value). The model calculates how similar each piece is to the query. This similarity score <lb/>determines how much attention the model gives to each piece. After that, the model uses these scores <lb/>to decide how much importance each piece gets. Finally, it combines all the information from the <lb/>pieces, giving more weight to the ones that are most relevant to the query. So, the attention mechanism <lb/>helps the model focus on what matters most for the task it is doing [47,48]. <lb/>6.3 t-SNE <lb/>To gain insights into the extracted features and explore the clustering patterns within the extracted <lb/>features, we employed t-SNE, a widespread dimensionality reduction technique. t-SNE is suitable for <lb/>visualizing high-dimensional data by mapping them into a lower-dimensional space while preserving <lb/>their local relationships [49]. In this experiment, t-SNE was utilized to the extracted features obtained <lb/>from the models used in the experiment to visualize the features. This visualization allowed us to assess <lb/>the separability of different classes and identify any inherent clustering patterns within the dataset. The <lb/></body>

			<page>1086 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>t-SNE embedding served as a valuable tool for understanding the relationships and distributions of <lb/>the extracted features. <lb/>6.4 SVM <lb/>A linear kernel and SVM are used for the classification problem, and the utilization of hyper-<lb/>plane performs well for linearly separable data [50,51]. SVM employs a technique known as the kernel <lb/>trick to handle the classification of linearly inseparable data as mentioned in Table 2 below. The <lb/>linear kernel computes the dot product between the feature vectors, which measures the similarity or <lb/>dissimilarity between them. It establishes a linear decision boundary within the feature space, rendering <lb/>it appropriate for datasets that are linearly separable mathematically in Eq. (2). <lb/>w T x + b = b + <lb/>m <lb/>i=1 <lb/>α i x T x (i) <lb/>(2) <lb/>Table 2: Hyperparameter settings utilized in SVM for classification throughout the experiments <lb/>Hyperparameter <lb/>Value <lb/>Description <lb/>Kernel <lb/>Linear <lb/>Type of kernel function used in SVM <lb/>C <lb/>1 <lb/>Regularization parameter (trade-off <lb/>between margin and error) <lb/>Random state <lb/>42 <lb/>Seed for random number generation <lb/>6.5 CNNs <lb/>6.5.1 InceptionV3 <lb/>In 2015, Szegedy and his team introduced InceptionV3 [52]. The Inception Module is the key <lb/>component of this network, characterized by its distinct features. It incorporates convolutions of <lb/>different sizes, such as 1 × 1, 3 × 3, and 5 × 5. <lb/>6.5.2 VGG16 <lb/>To extract informative features from the preprocessed CXIs, the VGG16 model was applied, a <lb/>widely used CNN architecture known for its excellent performance in image classification tasks. The <lb/>VGG16 model, pre-trained on large-scale image datasets, has shown remarkable capability in learning <lb/>hierarchical representations of images [53]. By utilizing transfer learning, the pre-trained weights of <lb/>the VGG16 model removed the fully connected layers, transforming them into a feature extractor. <lb/>6.5.3 VGG19 <lb/>The VGG19 model is a DCNN that builds upon the architecture of VGG16 with 16 and 3, <lb/>convolutional layers and FCL, respectively. The pre-trained VGG19 model is utilized for extracting the <lb/>features from the preprocessed images of the hybrid dataset. During training, the pre-trained weights of <lb/>the VGG19 model were frozen, and only the newly added layer was trained to extract features from the <lb/>CXIs. The extracted high-level features from the model were then used as inputs to train and evaluate <lb/>the subsequent DL model also used to train the COVID-19 [54]. This utilization of the VGG19 model <lb/>allowed us to explore its potential for feature extraction and its impact on the performance of our <lb/>classification task using SVM. <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1087 <lb/></page>

			<body>7 Experiment and Performance Evaluation <lb/>7.1 Experiment Setup <lb/>Colab (Google Colaboratory) is a free feature, it uses with the help of Jupyter for ML imple-<lb/>mentation in research [35,36]. Python 3 is embedded in Jupiter Notebook, along with important ML <lb/>libraries. This eliminates the requirement for local installation ensures a hassle-free environment, and <lb/>supports full flash implementation of DL, including GPU acceleration. <lb/>7.2 Performance Evaluation Method <lb/>This section presents a comprehensive analysis of the evolving performance metrics, encompassing <lb/>accuracy (Acc), recall (Rec), F1-score (F1), and precision (Prec), to assess the efficacy of the proposed <lb/>methodology which is given in Eqs. (3)-(6), respectively. These metrics serve as essential benchmarks <lb/>in evaluating the model&apos;s ability to accurately classify and characterize the underlying data patterns. <lb/>Acc = <lb/>TN + TP <lb/>TP + FP + TN + FN <lb/>(3) <lb/>Rec = <lb/>TP <lb/>TP + FN <lb/>(4) <lb/>F1 = <lb/>TP <lb/>TP + <lb/>1 <lb/>2 <lb/>(FP + FN) <lb/>(5) <lb/>Prec = <lb/>TP <lb/>TP + FP <lb/>(6) <lb/>7.3 Confusion Matrix <lb/>To further assess the effectiveness of our DL models, a confusion matrix was employed. A <lb/>confusion matrix is a tabular representation that summarizes the predicted and actual class labels of a <lb/>classification model which provides valuable insights into the model&apos;s performance by quantifying the <lb/>number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). This <lb/>analysis provided a more comprehensive understanding of the model&apos;s strengths and weaknesses in <lb/>correctly classifying Normal, Pneumonia, and COVID-19 cases. By visualizing the confusion matrices, <lb/>valuable insights into the models&apos; performance and identified any specific patterns or challenges <lb/>in their predictions, enabling us to make informed decisions for further model improvement and <lb/>optimization. <lb/>7.4 Results <lb/>Four different target approaches were used to evaluate the three DL pre-trained models as shown <lb/>in Table 3 below. <lb/>In all experiments mentioned in Table 3 image data are divided into two parts: 80% was used to <lb/>teach the model how to recognize patterns, and the remaining 20% was saved to check how well it <lb/>learned. However, this same split every time to run the experiments. You can find the code we used to <lb/>do this by clicking the provided link in the end of the article. <lb/>In first step features was extracted using the state-of-art previously trained models and extracted <lb/>features was visualized using the t-SNE embedding techniques as discussed earlier in the for better <lb/></body>

			<page>1088 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>understanding of the extracted features. Fig. 7 shows the t-SNE embedding using extracted features <lb/>from InceptionV3, Fig. 8 shows the results of VGG16 and Fig. 9 shows the results of VGG19. <lb/>Table 3: Classification approaches and classes <lb/>Approach <lb/>Classification classes <lb/>1 <lb/>Normal v/s COVID-19 <lb/>2 <lb/>Normal v/s Pneumonia <lb/>3 <lb/>COVID-19 v/s Pneumonia <lb/>4 <lb/>Normal v/s Pneumonia v/s COVID-19 <lb/>Figure 7: t-SNE visualizing the high-dimensional features extracted by the InceptionV3 <lb/>Figure 8: t-SNE embedding visualizing the high-dimensional features extracted by the VGG16 <lb/>Figure 9: t-SNE embedding visualizing the high-dimensional features extracted by the VGG19 <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1089 <lb/></page>

			<body>7.5 Model Performance Evaluation with State-of-the-Art Pre-Trained Models <lb/>InceptionV3, VGG16, and VGG19 were evaluated on four classification tasks on different <lb/>approaches mentioned in Table 3. The highlighted results in the table show the tasks where the models <lb/>achieved the highest accuracy. Tables 4-6 show the performance evaluation results of the InceptionV3, <lb/>VGG16, and VGG19 architecture with a linear SVM kernel, respectively. The model&apos;s effectiveness is <lb/>evaluated across four distinct classification tasks. <lb/>Table 4: Performance metrics of the InceptionV3-SVM for various classification tasks <lb/>Model: InceptionV3-SVM kernel type = Linear <lb/>Approach <lb/>Prec <lb/>Rec <lb/>F1 <lb/>Acc <lb/>Normal v/s COVID-19 <lb/>0.96 <lb/>0.96 <lb/>0.96 <lb/>0.96 <lb/>Normal v/s Pneumonia <lb/>0.95 <lb/>0.95 <lb/>0.95 <lb/>0.95 <lb/>COVID-19 v/s Pneumonia <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>Pneumonia v/s normal v/s COVID-19 <lb/>0.93 <lb/>0.93 <lb/>0.93 <lb/>0.93 <lb/>Table 5: Performance metrics of the VGG16-SVM for various classification tasks <lb/>Model: Inception VGG16-SVM kernel type = Linear <lb/>Approach <lb/>Prec <lb/>Rec <lb/>F1 <lb/>Acc <lb/>Normal v/s COVID-19 <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>Normal v/s pneumonia <lb/>0.97 <lb/>0.97 <lb/>0.97 <lb/>0.97 <lb/>COVID-19 v/s pneumonia <lb/>1.00 <lb/>1.00 <lb/>1.00 <lb/>1.00 <lb/>Pneumonia v/s normal v/s COVID-19 <lb/>0.98 <lb/>0.98 <lb/>0.98 <lb/>0.98 <lb/>Table 6: Performance metrics of the VGG19-SVM for various classification tasks <lb/>Model: Inception VGG19-SVM kernel type = Linear <lb/>Approach <lb/>Prec <lb/>Rec <lb/>F1 <lb/>Acc <lb/>Normal v/s COVID-19 <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>0.99 <lb/>Normal v/s pneumonia <lb/>0.97 <lb/>0.97 <lb/>0.97 <lb/>0.97 <lb/>COVID-19 v/s pneumonia <lb/>1.00 <lb/>1.00 <lb/>1.00 <lb/>1.00 <lb/>Pneumonia v/s normal v/s COVID-19 <lb/>0.98 <lb/>0.98 <lb/>0.98 <lb/>0.98 <lb/>Figs. 10-12 illustrate the performance of the InceptionV3, VGG16, and VGG19 models, respec-<lb/>tively, in classifying various medical conditions. Curves that closely approach the top-left corner <lb/>indicate the superior ability of the model to distinguish between positive and negative cases. <lb/></body>

			<page>1090 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>Figure 10: InceptionV3 model&apos;s ROCAUC of all medical condition experiments <lb/>Figure 11: (Continued) <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1091 <lb/></page>

			<body>Figure 11: VGG16 model&apos;s ROCAUC of all medical condition experiments <lb/>Figure 12: VGG19 model&apos;s ROCAUC of all medical condition experiments <lb/></body>

			<page>1092 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>In the first approach, all three models achieved high accuracies, with InceptionV3 and VGG16 at <lb/>96% and VGG19 at 99%, demonstrating excellent performance in identifying Normal and COVID-<lb/>19 cases. In the second approach, accuracies ranged from 95% (InceptionV3) to 97% (VGG16 and <lb/>VGG19), effectively distinguishing Normal and Pneumonia cases. For classifying COVID-19 vs. Pneu-<lb/>monia, all models achieved perfect accuracy (100%). In the last approach, accuracies ranged from 93% <lb/>(InceptionV3) to 98% (VGG16 and VGG19), showcasing effectiveness in multi-class classification. <lb/>The precision, recall, and F1-scores were consistently high across all approaches. Figs. 13-15 show the <lb/>confusion matrices for the various runs, detailing the models&apos; classification performance. <lb/>Figure 13: Confusion matrix obtained using InceptionV3 and SVM using linear kernel <lb/>The findings in the experiments demonstrate that these models exhibit robust performance <lb/>without showing signs of overfitting. This can be attributed to several technical factors. <lb/>1. The CNN architectures provide effective feature representation, capturing relevant patterns in <lb/>the input data while avoiding noise. <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1093 <lb/></page>

			<body>2. The use of a linear kernel promotes simplicity in decision boundaries, preventing excessive <lb/>complexity that could lead to overfitting. <lb/>3. The incorporation of regularization mechanisms within the SVM framework further aids in <lb/>preventing overfitting by penalizing overly complex models. <lb/>4. Also, the balanced representation of classes ensures that the models learn effectively from all <lb/>categories, contributing to their generalization ability. <lb/>5. The models underwent thorough cross-validation and hyperparameter tuning of SVM, ensur-<lb/>ing their performance consistency across different subsets of the data and mitigating the risk of <lb/>overfitting. These technical factors collectively contribute to the robustness of the SVM models <lb/>and their ability to generalize well to unseen data. <lb/>Figure 14: Confusion matrix obtained using VGG16 and SVM using linear kernel <lb/></body>

			<page>1094 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>Figure 15: Confusion matrix obtained using VGG19 and SVM using linear kernel <lb/>7.6 XAI-Visualization <lb/>In this section, we present our methodology for enhancing the interpretability and trustworthiness <lb/>of the AI system utilized in this study. Initially as mentioned in earlier section, a state-of-the-art <lb/>pre-trained DL model, leveraging its powerful feature extraction capabilities. To gain insight into <lb/>the underlying structure of these features. To sustain confidence in the AI model&apos;s decision-making <lb/>process, we integrated an Attention Mechanism typically utilized in NLP tasks, but this study&apos;s <lb/>innovation lies in the application of Attention Mechanism to CXIs, shedding light on how the AI <lb/>system identifies various chest disorders. <lb/>Experiment was performed by selecting four CXIs and presenting their corresponding attention <lb/>weight distribution graphs. These graphs represent the relationship between pixel indices along the <lb/>x-axis and attention weights along the y-axis, offering insights into which regions of the images the <lb/>AI model focuses on during its analysis. In the attention mechanism in figures below from Figs. 16 to <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

            <page>1095 <lb/></page>

			<body>27 red color indicates regions of the CXIs that the model considers to be most important or in other <lb/>words consider relevant for making predictions according to the mentioned classes. <lb/>Figure 16: Attention mechanism to visualize the InceptionV3 (COVID-19 vs. pneumonia) <lb/>Figure 17: Attention mechanism to visualize the InceptionV3 (Normal vs. COVID-19) <lb/></body>

			<page>1096 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>Figure 18: Attention mechanism to visualize the InceptionV3 (Normal vs. pneumonia) <lb/>Figure 19: Attention mechanism to visualize the InceptionV3 (Multi class) <lb/>Figure 20: Attention mechanism to visualize the VGG16 (COVID-19 vs. pneumonia) <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1097 <lb/></page>

			<body>Figure 21: Attention mechanism to visualize the VGG16 (Normal vs. COVID-19) <lb/>Figure 22: Attention mechanism to visualize the VGG16 (Normal vs. pneumonia) <lb/>Figure 23: Attention mechanism to visualize the VGG16 (Multi class) <lb/></body>

			<page>1098 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>Figure 24: Attention mechanism to visualize the VGG19 (COVID-19 vs. pneumonia) <lb/>Figure 25: Attention mechanism to visualize the VGG19 (Normal vs. COVID-19) <lb/>Figure 26: Attention mechanism to visualize the VGG19 (Normal vs. pneumonia) <lb/></body>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1099 <lb/></page>

			<body>Figure 27: Attention mechanism to visualize the VGG19 (Multi class) <lb/>The high attention values suggest that the model focuses its processing resources on specific <lb/>image parts. These regions are likely to strongly influence the model&apos;s predictions. On the other hand, <lb/>blue areas in the attention heat map suggest that the model is paying less attention to those regions. <lb/>These regions are likely to have a weaker influence on the model&apos;s predictions compared to regions <lb/>highlighted in other colors (e.g., red, yellow). <lb/>8 Comparison of Dataset Utilized for the Experiment <lb/>In this section, comparisons of performance studies of both classifications (multi-class and binary) <lb/>using CXIs. Table 7 shows some recent studies that did not report accuracy due to their datasets being <lb/>heavily imbalanced. While many of the related studies reported high accuracy values, a prevailing trend <lb/>among them is the scarcity of a substantial number of COVID-19 cases. <lb/>Table 7: Number of CXIs used by the different studies <lb/>Ref. <lb/>COVID-19 Healthy Other disease Total Benchmark dataset Unseen new dataset <lb/>[20] <lb/>420 <lb/>2000 <lb/>N/A <lb/>2420 Yes <lb/>No <lb/>[21] <lb/>219 <lb/>1314 <lb/>1345 <lb/>2878 Yes <lb/>No <lb/>[31] <lb/>371 <lb/>2923 <lb/>2778 <lb/>6072 Yes <lb/>No <lb/>[22] <lb/>364 <lb/>364 <lb/>364 <lb/>1092 Yes <lb/>No <lb/>[7] <lb/>25 <lb/>25 <lb/>N/A <lb/>50 <lb/>Yes <lb/>No <lb/>[23] <lb/>295 <lb/>68 <lb/>98 <lb/>461 <lb/>Yes <lb/>No <lb/>[24] <lb/>225 <lb/>1583 <lb/>4292 <lb/>6100 Yes <lb/>No <lb/>[25] <lb/>-<lb/>-<lb/>94323 <lb/>-<lb/>Yes <lb/>No <lb/>[26] <lb/>224 <lb/>504 <lb/>700 <lb/>1428 Yes <lb/>No <lb/>[27] <lb/>250 <lb/>1000 <lb/>500 <lb/>1750 Yes <lb/>No <lb/>[28] <lb/>250 <lb/>3520 <lb/>2753 <lb/>6523 Yes <lb/>No <lb/>[29] <lb/>445 <lb/>2880 <lb/>5179 <lb/>8504 Yes <lb/>No <lb/>This study 1136 <lb/>1136 <lb/>1136 <lb/>3408 Yes <lb/>Yes <lb/></body>

			<page>1100 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<body>The field of DL for medical diagnosis, especially in COVID-19 classification, has seen significant <lb/>growth with an extensive body of literature. However, to ensure the reliability and generalizability of <lb/>models, it is crucial to have access the large and unexplored datasets. <lb/>9 Future Scope <lb/>With more and more people getting affected by COVID-19 in some regions now, there is a need <lb/>to test a lot of cases quickly. In this study, we tried to use different state-of-the-art models to figure <lb/>out if someone has COVID-19 by looking at their CXIs with the XAI technique to model dynamically <lb/>weigh the importance of different input features or parts of the input data when making predictions <lb/>or generating outputs. However, XAI is still underutilized in the medical and healthcare sector due <lb/>to regulatory, technical, privacy, integration, and awareness challenges. It is also important to talk to <lb/>doctors and experts if we want to use this in real life. Also, more experiments will be beneficial to <lb/>perform with more state-of-the-art architectures. <lb/>10 Conclusions <lb/>This study presented a novel approach by utilizing the XAI technique, specifically the attention <lb/>mechanism, to visualize the decision-making areas crucial for CNNs in feature extraction, as well as to <lb/>assign importance scores to each input feature, providing explanations for predictions. While applied <lb/>this technique in image analysis, it has also been extensively employed in NLP. Also, the attention <lb/>mechanism was able to evaluate how well our suggested solution performed in a real-world clinical <lb/>setting, thereby enhancing the generalizability and practicality of findings. By incorporating data <lb/>from a private hospital, our study contributes to the growing body of research on DL applications <lb/>in healthcare and demonstrates the adaptability of our system in diverse medical environments. This <lb/>system has the potential to significantly improve the efficiency of diagnosis, isolation, and treatment of <lb/>COVID-19 patients, relieve the workload of radiologists, and help control the spread of the epidemic. <lb/>The best results were achieved in the classification of COVID-19 and pneumonia, where an accuracy <lb/>of 100% was obtained. <lb/></body>

			<div type="acknowledgement">Acknowledgement: The authors extend their appreciation to the Deanship of Scientific Research at <lb/>King Khalid University for funding this work through the Small Groups Project under Grant Number <lb/>RGP.1/369/44. <lb/></div>

			<div type="funding">Funding Statement: The researchers would like to thank the Deanship of Graduate Studies and <lb/>Scientific Research at Qassim University for financial support (QU-APC-2024-9/1). <lb/></div>

			<div type="contribution">Author Contributions: The authors confirm their contribution to the paper as follows: Saad Akbar; <lb/>study conception and design, methodology development, draft preparation, experiment and inves-<lb/>tigation, data curation and visualization. Humera Azam; supervision, draft editing, review, project <lb/>administration. Sulaiman Sulmi Almutairi; validation and analysis. Omar Alqahtani and Habib Shah; <lb/>paper finalization. Aliya Aleryani; final editing. All authors reviewed the results and approved the final <lb/>version of the manuscript. <lb/></div>

            <div type="availability">Availability of Data and Materials: The code utilized in this research project can be accessed at https:// <lb/>sites.google.com/view/saadakbar/covid-19-project (accessed on 20/04/2024). However, per privacy <lb/>protocols and the sensitive nature of the data employed, the dataset utilized in this study will not <lb/>be made publicly accessible. <lb/></div>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1101 <lb/></page>

			<div type="annex">Ethics Approval: This research study involved the use of chest X-ray images from real-time patient data <lb/>collected between November 2020 and November 2021. The following information pertains to the <lb/>ethical approval for this study: 1. Inclusion of Human Subjects: This study involved the use of medical <lb/>imaging data obtained from human subjects. 2. Ethical Approval Committee: The research project <lb/>received ethical approval from the Ethics Committee of Jinnah Postgraduate Medical Center, Karachi <lb/>3. Persistent Identifier: The ethical approval for this study is associated with a reference or approval <lb/>number issued by Jinnah Postgraduate Medical Center. The reference number for this approval is <lb/>MXR(G)-39/21/JPMC. All participating patients provided informed consent. <lb/></div>

            <div type="conflict">Conflicts of Interest: The authors declare that they have no conflicts of interest to report regarding the <lb/>present study. <lb/></div>

            <listBibl>References <lb/>[1] &quot;WHO coronavirus (COVID-19) dashboard,&quot; Accessed: Oct. 08, 2022. [Online]. Available: https://covid19. <lb/>who.int <lb/>[2] &quot;Coronavirus (COVID-19) events as they happen,&quot; Accessed: Feb. 18, 2023. [Online]. Available: https:// <lb/>www.who.int/emergencies/diseases/novel-coronavirus-2019/events-as-they-happen <lb/>[3] &quot;Coronavirus disease (COVID-19) situation reports,&quot; Accessed: Feb. 18, 2023. [Online]. Available: https:// <lb/>www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports <lb/>[4] &quot;Coronavirus,&quot; Accessed: May 09, 2023. [Online]. Available: https://www.who.int/health-topics/ <lb/>coronavirus <lb/>[5] &quot;Laboratory testing for 2019 novel coronavirus (2019-nCoV) in suspected human cases,&quot; Accessed: Feb. <lb/>18, 2023. [Online]. Available: https://www.who.int/publications-detail-redirect/10665-331501 <lb/>[6] S. Akbar, N. G. Haider, and H. Tariq, &quot;Tuberculosis diagnosis using X-ray images,&quot; Int. J. Adv. Res., vol. <lb/>7, no. 4, pp. 689-696, 2019. doi: 10.21474/ijar01/8872. <lb/>[7] S. Sheykhivand et al., &quot;Developing an efficient deep neural network for automatic detection of <lb/>COVID-19 using chest X-ray images,&quot; Alex. Eng. J., vol. 60, no. 3, pp. 2885-2903, Jun. 2021. doi: <lb/>10.1016/j.aej.2021.01.011. <lb/>[8] N. Subramanian, O. Elharrouss, S. Al-Maadeed, and M. Chowdhury, &quot;A review of deep learning-<lb/>based detection methods for COVID-19,&quot; Comput. Biol. Med., vol. 143, pp. 105233, Apr. 2022. doi: <lb/>10.1016/j.compbiomed.2022.105233. <lb/>[9] P. Silva et al., &quot;COVID-19 detection in CT images with deep learning: A voting-based scheme and cross-<lb/>datasets analysis,&quot; Inform. Med. Unlocked, vol. 20, pp. 100427, 2020. doi: 10.1016/j.imu.2020.100427. <lb/>[10] S. Akbar, H. Tariq, M. Fahad, G. Ahmed, and H. J. Syed, &quot;Contemporary study on deep neural networks <lb/>to diagnose COVID-19 using digital posteroanterior X-ray images,&quot; Electronics, vol. 11, no. 19, pp. 3113, <lb/>Jan. 2022. doi: 10.3390/electronics11193113. <lb/>[11] S. S. et al., &quot;An XAI approach for COVID-19 detection using transfer learning with X-ray images,&quot; Heliyon, <lb/>vol. 9, no. 4, pp. e15137, Apr. 2023. doi: 10.1016/j.heliyon.2023.e15137. <lb/>[12] H. Qattous, M. Azzeh, R. Ibrahim, I. A. Al-Ghafer, M. Al Sorkhy and A. Alkhateeb, &quot;PaCMAP-embedded <lb/>convolutional neural network for multi-omics data integration,&quot; Heliyon, vol. 10, no. 1, Jan. 2024. doi: <lb/>10.1016/j.heliyon.2023.e23195. <lb/>[13] H. Azam, H. Tariq, D. Shehzad, S. Akbar, H. Shah and Z. A. Khan, &quot;Fully automated skull stripping from <lb/>brain magnetic resonance images using mask RCNN-based deep learning neural networks,&quot; Brain Sci., vol. <lb/>13, no. 9, pp. 1255, Sep. 2023. doi: 10.3390/brainsci13091255. <lb/>[14] M. F. Mushtaq et al., &quot;BHCNet: Neural network-based brain hemorrhage classification using head CT <lb/>Scan,&quot; IEEE Access, vol. 9, pp. 113901-113916, 2021. doi: 10.1109/ACCESS.2021.3102740. <lb/>[15] L. T. Duong, N. H. Le, T. B. Tran, V. M. Ngo, and P. T. Nguyen, &quot;Detection of tuberculosis from chest X-<lb/>ray images: Boosting the performance with vision transformer and transfer learning,&quot; Expert. Syst. Appl., <lb/>vol. 184, pp. 115519, Dec. 2021. doi: 10.1016/j.eswa.2021.115519. <lb/></listBibl>

			<page>1102 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<listBibl>[16] L. Gamage, U. Isuranga, D. Meedeniya, S. de Silva, and P. Yogarajah, &quot;Melanoma skin cancer identification <lb/>with explainability utilizing mask guided technique,&quot; Electronics, vol. 13, no. 4, pp. 680, Jan. 2024. doi: <lb/>10.3390/electronics13040680. <lb/>[17] O. Sarkar et al., &quot;Multi-scale CNN: An explainable AI-integrated unique deep learning framework for <lb/>lung-affected disease classification,&quot; Technologies, vol. 11, no. 5, pp. 134, Oct. 2023. doi: 10.3390/technolo-<lb/>gies11050134. <lb/>[18] B. H. M. van der Velden, H. J. Kuijf, K. G. A. Gilhuijs, and M. A. Viergever, &quot;Explainable artificial <lb/>intelligence (XAI) in deep learning-based medical image analysis,&quot; Med. Image Anal., vol. 79, pp. 102470, <lb/>Jul. 2022. doi: 10.1016/j.media.2022.102470. <lb/>[19] W. Wang, Y. Pei, S. H. Wang, J. Gorrz, and Y. D. Zhang, &quot;PSTCNN: Explainable COVID-19 diagnosis <lb/>using PSO-guided self-tuning CNN,&quot; BIOCELL, vol. 47, no. 2, pp. 373-384, 2022. doi: 10.32604/bio-<lb/>cell.2023.025905. <lb/>[20] M. Canayaz, &quot;MH-COVIDNet: Diagnosis of COVID-19 using deep neural networks and meta-heuristic-<lb/>based feature selection on X-ray images,&quot; Biomed. Signal Process. Control, vol. 64, pp. 102257, Feb. 2021. <lb/>doi: 10.1016/j.bspc.2020.102257. <lb/>[21] M. Togaçar, B. Ergen, and Z. Cömert, &quot;COVID-19 detection using deep learning models to exploit social <lb/>mimic optimization and structured chest X-ray images using fuzzy color and stacking approaches,&quot; Comput. <lb/>Biol. Med., vol. 121, pp. 103805, Jun. 2020. doi: 10.1016/j.compbiomed.2020.103805. <lb/>[22] T. Wu, C. Tang, M. Xu, N. Hong, and Z. Lei, &quot;ULNet for the detection of coronavirus <lb/>(COVID-19) from chest X-ray images,&quot; Comput. Biol. Med., vol. 137, pp. 104834, Oct. 2021. doi: <lb/>10.1016/j.compbiomed.2021.104834. <lb/>[23] E. E. D. Hemdan, M. A. Shouman, and M. E. Karar, &quot;COVIDX-Net: A framework of deep learning <lb/>classifiers to diagnose COVID-19 in X-ray images,&quot; 2020. doi: 10.48550/arXiv.2003.11055. <lb/>[24] B. Sekeroglu and I. Ozsahin, &quot;Detection of COVID-19 from chest X-ray images using convolutional neural <lb/>networks,&quot; SLAS Technol., vol. 25, no. 6, pp. 553-565, Dec. 2020. doi: 10.1177/2472630320958376. <lb/>[25] P. Afshar, S. Heidarian, F. Naderkhani, A. Oikonomou, K. N. Plataniotis and A. Mohammadi, &quot;COVID-<lb/>CAPS: A capsule network-based framework for identification of COVID-19 cases from X-ray images,&quot; <lb/>Pattern Recognit. Lett., vol. 138, pp. 638-643, Oct. 2020. doi: 10.1016/j.patrec.2020.09.010. <lb/>[26] I. D. Apostolopoulos and T. A. Mpesiana, &quot;COVID-19: Automatic detection from X-ray images utilizing <lb/>transfer learning with convolutional neural networks,&quot; Phys. Eng. Sci. Med., vol. 43, no. 2, pp. 635-640, <lb/>Jun. 2020. doi: 10.1007/s13246-020-00865-4. <lb/>[27] T. Ozturk, M. Talo, E. A. Yildirim, U. B. Baloglu, O. Yildirim and U. Rajendra Acharya, &quot;Automated <lb/>detection of COVID-19 cases using deep neural networks with X-ray images,&quot; Comput. Biol. Med., vol. <lb/>121, pp. 103792, Jun. 2020. doi: 10.1016/j.compbiomed.2020.103792. <lb/>[28] L. Brunese, F. Mercaldo, A. Reginelli, and A. Santone, &quot;Explainable deep learning for pulmonary disease <lb/>and coronavirus COVID-19 detection from X-rays,&quot; Comput. Methods Programs Biomed., vol. 196, pp. <lb/>105608, Nov. 2020. doi: 10.1016/j.cmpb.2020.105608. <lb/>[29] M. Heidari, S. Mirniaharikandehei, A. Z. Khuzani, G. Danala, Y. Qiu and B. Zheng, &quot;Improving the <lb/>performance of CNN to predict the likelihood of COVID-19 using chest X-ray images with preprocessing <lb/>algorithms,&quot; Int. J. Med. Inform., vol. 144, pp. 104284, Dec. 2020. doi: 10.1016/j.ijmedinf.2020.104284. <lb/>[30] A. K. Azad, M. A. Alahi, and M. U. Ahmed, &quot;In search of an efficient and reliable deep learning model for <lb/>identification of COVID-19 infection from chest X-ray images,&quot; Diagnostics, vol. 13, no. 3, pp. 574, Jan. <lb/>2023. doi: 10.3390/diagnostics13030574. <lb/>[31] M. Khishe, F. Caraffini, and S. Kuhn, &quot;Evolving deep learning convolutional neural networks for early <lb/>COVID-19 detection in chest X-ray images,&quot; Mathematics, vol. 9, no. 9, pp. 1002, Apr. 2021. doi: <lb/>10.3390/math9091002. <lb/>[32] L. T. Duong, P. T. Nguyen, L. Iovino, and M. Flammini, &quot;Automatic detection of COVID-19 from chest <lb/>X-ray and lung computed tomography images using deep neural networks and transfer learning,&quot; Appl. <lb/>Soft Comput., vol. 132, pp. 109851, Jan. 2023. doi: 10.1016/j.asoc.2022.109851. <lb/></listBibl>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<page>1103 <lb/></page>

			<listBibl>[33] C. Shorten, T. M. Khoshgoftaar, and B. Furht, &quot;Deep learning applications for COVID-19,&quot; J. Big Data, <lb/>vol. 8, no. 1, pp. 484, 2021. doi: 10.1186/s40537-020-00392-9. <lb/>[34] Z. Ying, G. Li, Y. Ren, R. Wang, and W. Wang, &quot;A new image contrast enhancement algorithm using expo-<lb/>sure fusion framework,&quot; in M. Felsberg, A. Heyden, N. Krüger (Eds.), Computer Analysis of Images and Pat-<lb/>terns. Cham: Springer International Publishing, 2017, vol. 10425, pp. 36-46. 10.1007/978-3-319-64698-5_4. <lb/>[35] S. Mahajan, A. Raina, M. Abouhawwash, X. Z. Gao, and A. Kant Pandit, &quot;COVID-19 detection from <lb/>chest X-ray images using advanced deep learning techniques,&quot; Comput. Mater. Contin., vol. 70, no. 1, pp. <lb/>1541-1556, 2022. doi: 10.32604/cmc.2022.019496. <lb/>[36] A. Siddique, S. Talha, M. Aamir, A. Algarni, N. Soliman and W. El-Shafai, &quot;COVID-19 classification <lb/>from X-ray images: An approach to implement federated learning on decentralized dataset,&quot; Comput. Mat. <lb/>Contin., vol. 75, pp. 3883-3901, Mar. 2023. doi: 10.32604/cmc.2023.037413. <lb/>[37] M. E. H. Chowdhury et al., &quot;Can AI help in screening viral and COVID-19 pneumonia?,&quot; IEEE Access, <lb/>vol. 8, pp. 132665-132676, 2020. doi: 10.1109/ACCESS.2020.3010287. <lb/>[38] I. A. Choudhry, A. N. Qureshi, K. Aurangzeb, S. Iqbal, and M. Alhussein, &quot;Hybrid diagnostic model for <lb/>improved COVID-19 detection in lung radiographs using deep and traditional features,&quot; Biomimetics, vol. <lb/>8, no. 5, pp. 406, Sep. 2023. doi: 10.3390/biomimetics8050406. <lb/>[39] Y. D. Zhang, V. Govindaraj, and Z. Zhu, &quot;FECNet: A neural network and a mobile app for COVID-19 <lb/>recognition,&quot; Mobile Netw. Appl., vol. 9, Jul. 2023. doi: 10.1007/s11036-023-02140-8. <lb/>[40] D. Kermany, K. Zhang, and M. Goldbaum, &quot;Large dataset of labeled optical coherence tomography (OCT) <lb/>and chest X-ray images,&quot; vol. 3, Jun. 2018. doi: 10.17632/rscbjbr9sj.3. <lb/>[41] J. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Q. Duong and M. Ghassemi, &quot;ieee8023/covid-chestxray-<lb/>dataset,&quot; Sep. 26, 2022. Accessed: Sep. 27, 2022. [Online]. Available: https://github.com/ieee8023/covid-<lb/>chestxray-dataset <lb/>[42] C. Shorten and T. M. Khoshgoftaar, &quot;A survey on image data augmentation for deep learning,&quot; J. Big <lb/>Data, vol. 6, no. 1, pp. 60, Jul. 2019. doi: 10.1186/s40537-019-0197-0. <lb/>[43] K. K. Pal and K. S. Sudeep, &quot;Preprocessing for image classification by convolutional neural networks,&quot; in <lb/>2016 IEEE Int. Conf. Recent Trends Electron., Inform. Commun. Technol. (RTEICT), Bangalore, India, <lb/>May 2016, pp. 1778-1781. doi: 10.1109/RTEICT.2016.7808140. <lb/>[44] X. Chen, &quot;Image enhancement effect on the performance of convolutional neural networks,&quot; 2019. <lb/>Accessed: May 30, 2023. [Online]. Available: https://urn.kb.se/resolve?urn=urn:nbn:se:bth-18523 <lb/>[45] A. Schwartzman, M. Kagan, L. Mackey, B. Nachman, and L. D. Oliveira, &quot;Image processing, computer <lb/>vision, and deep learning: New approaches to the analysis and physics interpretation of LHC events,&quot; J. <lb/>Phys.: Conf. Ser., vol. 762, no. 1, pp. 12035, Oct. 2016. doi: 10.1088/1742-6596/762/1/012035. <lb/>[46] X. Yang, &quot;An overview of the attention mechanisms in computer vision,&quot; J. Phys.: Conf. Ser., vol. 1693, <lb/>no. 1, pp. 12173, Dec. 2020. doi: 10.1088/1742-6596/1693/1/012173. <lb/>[47] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, &quot;Squeeze-and-excitation networks,&quot; May 16, 2019. doi: <lb/>10.48550/arXiv.1709.01507. <lb/>[48] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jégou, &quot;Going deeper with image transform-<lb/>ers,&quot; Apr. 07, 2021. doi: 10.48550/arXiv.2103.17239. <lb/>[49] L. van der Maaten and G. Hinton, &quot;Visualizing data using t-SNE,&quot; J. Mach. Learn. Res., vol. 9, no. 86, <lb/>pp. 2579-2605, 2008. <lb/>[50] S. Guhathakurata, S. Kundu, A. Chakraborty, and J. S. Banerjee, &quot;A novel approach to predict COVID-<lb/>19 using support vector machine,&quot; in Data Science for COVID-19, 2021, vol. 395, pp. 351-364. doi: <lb/>10.1016/B978-0-12-824536-1.00014-9. <lb/>[51] N. A. Almansour et al., &quot;Neural network and support vector machine for the prediction of chronic <lb/>kidney disease: A comparative study,&quot; Comput Biol. Med., vol. 109, pp. 101-111, Jun. 2019. doi: <lb/>10.1016/j.compbiomed.2019.04.017. <lb/>[52] X. Guo et al., &quot;An ensemble learning method based on ordinal regression for COVID-19 diagnosis from <lb/>chest CT,&quot; Phys Med. Biol., vol. 66, no. 24, pp. 244001, Dec. 2021. doi: 10.1088/1361-6560/ac34b2. <lb/></listBibl>

			<page>1104 <lb/></page>

			<note place="headnote">CMC, 2024, vol.80, no.1 <lb/></note>

			<listBibl>[53] L. S. Chow, G. S. Tang, M. I. Solihin, N. M. Gowdh, N. Ramli and K. Rahmat, &quot;Quantitative and <lb/>qualitative analysis of 18 deep convolutional neural network (CNN) models with transfer learning to <lb/>diagnose COVID-19 on chest X-ray (CXR) images,&quot; SN Comput. Sci., vol. 4, no. 2, pp. 141, Jan. 2023. <lb/>doi: 10.1007/s42979-022-01545-8. <lb/>[54] S. Namani, L. Akkapeddi, and S. Bantu, &quot;Performance analysis of VGG-19 deep learning model for <lb/>COVID-19 detection,&quot; in 2022 9th Int. Conf. Computing Sustain Global Dev (INDIACom), New Delhi, <lb/>India, vol. 4, pp. 781-787, May 2022. doi: 10.23919/INDIACom54597.2022.9763177. </listBibl>


	</text>
</tei>
