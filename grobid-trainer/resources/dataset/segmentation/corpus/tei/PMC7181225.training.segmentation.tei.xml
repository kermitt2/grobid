<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>sensors <lb/>Article <lb/>OmniSCV: An Omnidirectional Synthetic Image <lb/>Generator for Computer Vision <lb/>Bruno Berenguel-Baeta *, Jesus Bermudez-Cameo and Jose J. Guerrero <lb/>Instituto de Investigación en Ingeniería de Aragón, Universidad de Zaragoza, 50018 Zaragoza, Spain; <lb/>bermudez@unizar.es (J.B.-C.); jguerrer@unizar.es (J.J.G.) <lb/>* Correspondence: berenguel@unizar.es <lb/>Received: 3 March 2020; Accepted: 3 April 2020; Published: 7 April 2020 <lb/>Abstract: Omnidirectional and 360 o images are becoming widespread in industry and in consumer <lb/>society, causing omnidirectional computer vision to gain attention. Their wide field of view allows <lb/>the gathering of a great amount of information about the environment from only an image. However, <lb/>the distortion of these images requires the development of specific algorithms for their treatment and <lb/>interpretation. Moreover, a high number of images is essential for the correct training of computer <lb/>vision algorithms based on learning. In this paper, we present a tool for generating datasets of <lb/>omnidirectional images with semantic and depth information. These images are synthesized from <lb/>a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through <lb/>an interface plugin. We gather a variety of well-known projection models such as equirectangular <lb/>and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models. <lb/>Furthermore, we include in our tool photorealistic non-central-projection systems as non-central <lb/>panoramas and non-central catadioptric systems. As far as we know, this is the first reported tool for <lb/>generating photorealistic non-central images in the literature. Moreover, since the omnidirectional <lb/>images are made virtually, we provide pixel-wise information about semantics and depth as well <lb/>as perfect knowledge of the calibration parameters of the cameras. This allows the creation of <lb/>ground-truth information with pixel precision for training learning algorithms and testing 3D vision <lb/>approaches. To validate the proposed tool, different computer vision algorithms are tested as line <lb/>extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using <lb/>equirectangular panoramas, and 3D reconstruction from non-central panoramas. <lb/>Keywords: computer vision; omnidirectional cameras; virtual environment; deep learning; <lb/>non-central systems; image generator; semantic label <lb/></front>

			<body>1. Introduction <lb/>The great amount of information that can be obtained from omnidirectional and 360 o images <lb/>makes them very useful. Being able to obtain information from an environment using only one shot <lb/>makes these kinds of images a good asset for computer vision algorithms. However, due to the <lb/>distortions they present, it is necessary to adapt or create special algorithms to work with them. New <lb/>computer vision and deep-learning-based algorithms have appeared to take advantage of the unique <lb/>properties of omnidirectional images. Nevertheless, for a proper training of deep-learning algorithms, <lb/>big datasets are needed. Existing datasets are quite limited in size due to the manual acquisition, <lb/>labeling and post-processing of the images. To make faster and bigger datasets, previous works such <lb/>as [1-5] use special equipment to obtain images, camera pose, and depth maps simultaneously from <lb/>indoor scenes. These kinds of datasets are built from real environments, but need post-processing <lb/>of the images to obtain semantic information or depth information. Tools like LabelMe [6] and new <lb/>neural networks such as SegNet [7] can be used to obtain automatic semantic segmentation from the <lb/></body>

			<front>Sensors 2020, 20, 2066; doi:10.3390/s20072066 <lb/>www.mdpi.com/journal/sensors <lb/></front>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>2 of 25 <lb/></page>

			<body>real images obtained in the previously mentioned datasets, yet without pixel precision. Datasets such <lb/>as [8,9] use video sequences from outdoor scenes to obtain depth information for autonomous driving <lb/>algorithms. In addition, for these outdoor datasets, neural networks are used to obtain semantic <lb/>information from video sequences [10,11], in order to speed up and enlarge the few datasets available. <lb/>Due to the fast development of graphic engines such as Unreal Engine [12], virtual environments <lb/>with realistic quality have appeared. To take advantage of this interesting property, simulators such as <lb/>CARLA [13] and SYNTHIA [14] recreate outdoor scenarios in different weather conditions to create <lb/>synthetic datasets with labeled information. If we can define all the objects in the virtual environment, <lb/>it is easier to create a semantic segmentation and object labeling, setting the camera pose through <lb/>time and computing the depth for each pixel. These virtual realistic environments have helped to <lb/>create large datasets of images and videos, mainly from outdoor scenarios, dedicated to autonomous <lb/>driving. Other approaches use photorealistic video games to generate the datasets. Since these games <lb/>already have realistic environments designed by professionals, many different scenarios are recreated, <lb/>with pseudo-realistic behaviors of vehicles and people in the scene. Works such as [15] use the video <lb/>game Grand Theft Auto V (GTA V) to obtain images from different weather conditions with total <lb/>knowledge of the camera pose, while [16,17] also obtaining semantic information and object detection <lb/>for tracking applications. In the same vein, [18,19] obtain video sequences with semantic and depth <lb/>information for the generation of autonomous driving datasets in different weather conditions and <lb/>through different scenarios, from rural roads to city streets. New approaches such as the OmniScape <lb/>dataset [20] uses virtual environments such as CARLA or GTA V to obtain omnidirectional images <lb/>with semantic and depth information in order to create datasets for autonomous driving. <lb/>However, most of the existing datasets have only outdoors images. There are very few synthetic <lb/>indoor datasets [21] and most of them only have perspective images or equirectangular panoramas. <lb/>Fast development of computer vision algorithms demands ever more omnidirectional images and that <lb/>is the gap between the resources that we want to fill in this work. In this work we present a tool to <lb/>generate image datasets from a huge diversity of omnidirectional projection models. <lb/>We focus not only on panoramas, but also on other central projections, such as fish-eye lenses [22,23], <lb/>catadioptric systems [24] and empiric models such as Scaramuzza&apos;s [25] or Kannala-Brandt&apos;s [26]. Our <lb/>novelty resides in the implementation of different non-central-projection models, such as non-central <lb/>panoramas [27] or spherical [28] and conical [29] catadioptric systems in the same tool. <lb/>The composition of the images is made in a virtual environment from Unreal Engine, <lb/>making camera calibration and image labeling easier. Moreover, we implement several tools to <lb/>obtain ground-truth information for deep-learning applications, for example layout recovery or <lb/>object detection. <lb/>The main contributions of this work can be summarized as follows: <lb/>• <lb/>Integrating in a single framework several central-projection models from different omnidirectional <lb/>cameras as panoramas, fish-eyes, catadioptric systems, and empiric models. <lb/>• <lb/>Creating the first photorealistic non-central-projection image generator, including non-central <lb/>panoramas and non-central catadioptric systems. <lb/>• <lb/>Devise a tool to create datasets with automatic labeled images from photorealistic virtual <lb/>environments. <lb/>• <lb/>Develop automatic ground-truth generation for 3D layout recovery algorithms and object <lb/>detection. <lb/>The next section of this work is divided in 4 main parts. In the first one, Section 2.1, we <lb/>introduce the virtual environment in which we have worked. Section 2.2 presents the mathematical <lb/>background of the projection models implemented and in Sections 2.3 and 2.4 we explain how the <lb/>models are implemented. <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>3 of 25 <lb/></page>

			<body>2. Materials and Methods <lb/>The objective of this work is to develop a tool to create omnidirectional images enlarging existing <lb/>datasets or making new ones to be exploited by computer vision algorithms under development. <lb/>For this purpose, we use virtual environments, such as Unreal Engine 4 [12], from where we can get <lb/>perspective images to compose 360 o and omnidirectional projections. In these environments, we can <lb/>define the camera (pose, orientation and calibration), the layout, and the objects arranged in the scene, <lb/>making it easier to obtain ground-truth information. <lb/>The proposed tool includes the acquisition of images from a virtual environment created with <lb/>Unreal Engine 4 and the composition of omnidirectional and 360 images from a set of central and <lb/>non-central camera systems. Moreover, we can acquire photorealistic images, semantic segmentation <lb/>on the objects of the scene or depth information from each camera proposed. Furthermore, given <lb/>that we can select the pose and orientation of the camera, we have enough information for <lb/>3D-reconstruction methods. <lb/>2.1. Virtual Environments <lb/>Virtual environments present a new asset in the computer vision field. These environments <lb/>allow the generation of customized scenes for specific purposes. Moreover, great development of <lb/>computer graphics has increased the quality and quantity of graphic software, obtaining even realistic <lb/>renderings. A complex modeling of the light transport and its interaction with objects is essential to <lb/>obtain realistic images. Virtual environments such as POV-Ray [30] and Unity [31] allow the generation <lb/>of customized virtual environments and obtain images from them. However, they do not have the <lb/>realism or flexibility in the acquisition of images we are looking for. A comparative of images obtained <lb/>from acquisitions in POV-Ray and acquisitions in Unreal Engine 4 is presented in Figure 1. <lb/>(a) <lb/>(b) <lb/>Figure 1. (a): Kannala-Brandt projection obtained from Unreal Engine 4. (b): Kannala-Brandt <lb/>projection obtained from POV-Ray. <lb/>The virtual environment we have chosen is Unreal Engine 4, (UE4) [12], which is a graphic engine <lb/>developed by EpicGames (https://www.epicgames.com). Being an open-source code has allowed <lb/>the development of a great variety of plugins for specific purposes. Furthermore, realistic graphics <lb/>in real time allows the creation of simulations and generation of synthetic image datasets that can be <lb/>used in computer vision algorithms. Working on virtual environments makes easier and faster the <lb/>data acquisition than working on the field. <lb/>In this work, we use UE4 with UnrealCV [32], which is a plugin designed for computer vision <lb/>purposes. This plugin allows client-server communication with UE4 from external Python scripts (see <lb/>Figure 2) which is used to automatically obtain many images. The set of available functions includes <lb/>commands for defining and operating virtual cameras; i.e., fixing the position and orientation of the <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>4 of 25 <lb/></page>

			<body>cameras and acquiring images. As can be seen in Figure 3, the acquisition can obtain different kinds of <lb/>information from the environment (RGB, semantic, depth or normals). <lb/>However, the combination of UE4+UnrealCV only allows perspective images, so it is necessary to <lb/>find a way to obtain enough information about the environment to obtain omnidirectional images and <lb/>in particular to build non-central images. For central omnidirectional images, the classical adopted <lb/>solution is the creation of a cube map [33]. This proposal consists of taking 6 perspective images from <lb/>one position so we can capture the whole environment around that point. We show that this solution <lb/>only works for central projections, where we have a single optical center that matches with the point <lb/>where the cube map has been taken. Due to the characteristics of non-central-projection systems, <lb/>we make acquisitions in different locations, which depend on the projection model, to compose the <lb/>final image. <lb/>Figure 2. Client-server communication between Unreal Engine 4 and an external program <lb/>via UnrealCV. <lb/>(a) <lb/>(b) <lb/>(c) <lb/>(d) <lb/>Figure 3. Capture options available in UnrealCV. (a): Lit RGB image; (b): Object mask; (c): Depth; <lb/>(d): Surface normal. <lb/>2.2. Projection Models <lb/>In this section, we introduce the projection models for the different cameras that are implemented <lb/>in the proposed tool. We are going to explain the relationship between image-plane coordinates and <lb/>the coordinates of the projection ray in the camera reference. We distinguish two types of camera <lb/>models: central-projection camera models and non-central-projection camera models. Among the <lb/>central projection cameras, we consider: <lb/>• <lb/>Panoramic images: Equirectangular and Cylindrical <lb/>• <lb/>Fish-eye cameras, where we distinguish diverse lenses: Equi-angular, Stereographic, Equi-solid angle, <lb/>Orthogonal <lb/>• <lb/>Catadioptric systems, where we distinguish different mirrors: Parabolic and Hyperbolic <lb/>• <lb/>Scaramuzza model for revolution symmetry systems <lb/>• <lb/>Kannala-Brandt model for fish-eye lenses <lb/>Among the non-central projection cameras, we consider: <lb/>• <lb/>Non-central panoramas <lb/>• <lb/>Catadioptric systems, where we distinguish different mirrors: Spherical and Conical <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>5 of 25 <lb/></page>

			<body>2.2.1. Central-Projection Cameras <lb/>Central-projection cameras are characterized by having a unique optical center. That means <lb/>that every ray coming from the environment goes through the optical center to the image. <lb/>Among omnidirectional systems, panoramas are the most used in computer vision. Equirectangular <lb/>panoramas are 360 o -field-of-view images that show the whole environment around the camera. This <lb/>kind of image is useful to obtain a complete projection of the environment from only one shot. However, <lb/>this representation presents heavy distortions in the upper and lower part of the image. That is because <lb/>the equirectangular panorama is based on spherical coordinates. If we take the center of the sphere <lb/>as the optical center, we can define the ray that comes from the environment in spherical coordinates <lb/>(θ, φ). Moreover, since the image plane is an unfolded sphere, each pixel can be represented in the <lb/>same spherical coordinates, giving a direct relationship between the image plane and the ray that <lb/>comes from the environment. This relationship is described by: <lb/>(θ, ϕ) = ((2x/x max − 1)π, (1/2 − y/y max )π) , <lb/>(1) <lb/>where (x, y) are pixel coordinates and (x max , y max ) the maximum value, i.e., the image resolution. <lb/>In the case of cylindrical panoramas, the environment is projected into the lateral surface of a <lb/>cylinder. This panorama does not have a 360 o practical field of view, since the perpendicular projection <lb/>to the lateral surface of the environment cannot be projected. However, we can achieve up to 360 o on <lb/>the horizontal field of view, FOV h , and theoretical 180 o on the vertical field of view, FOV v , that usually <lb/>is reduced from 90 o to 150 o for real applications. We can describe the relationship between the ray that <lb/>comes from the environment and the image plane as: <lb/>(θ, ϕ) = ((2x/x max − 1)FOV h /2, (1 − 2y/y max )FOV v /2) <lb/>(2) <lb/>Next, we introduce the fish-eye cameras [22]. The main characteristic of this kind of camera is the <lb/>wide field of view. The projection model for this camera system has been obtained in [34], where a <lb/>unified model for revolution symmetry cameras is defined. This method consists of the projection of <lb/>the environment rays into a unit radius sphere. The intersection between the sphere and the ray is <lb/>projected into the image plane through a non-linear functionr = h(φ), which depends on the angle <lb/>of the ray and the modeled fish-eye lens. In Table 1 h(φ) for the lenses implemented in this work <lb/>is defined. <lb/>Table 1. Definition of h(φ) =r for different fish-eye lenses. <lb/>Equi-Angular Stereographic Orthogonal Equi-Solid Angle <lb/>f φ <lb/>2 f tan(φ/2) <lb/>f sin φ <lb/>f sin(φ/2) <lb/>For catadioptric systems, we use the sphere model, presented in [24]. As in fish-eye cameras, <lb/>we have the intersection of the environment&apos;s ray with the unit radius sphere. Then, through a <lb/>non-linear function, we project the intersected point into a normalized plane. The non-linear function, <lb/>h(x), depends on the mirror we are modeling. The final step of this model projects the point in the <lb/>normalized plane into the image plane with the calibration matrix H c , defined as H c = K c R c M c , where <lb/>K c is the calibration matrix of the perspective camera, R c is the rotation matrix of the catadioptric <lb/>system and M c defines the behavior of the mirror (see Equation (3)). <lb/>K c = <lb/> <lb/> <lb/> <lb/>f x 0 u 0 <lb/>0 f y v 0 <lb/>0 0 1 <lb/> <lb/> <lb/> ; M c = <lb/> <lb/> <lb/> <lb/>Ψ − ξ <lb/>0 <lb/>0 <lb/>0 <lb/>ξ − Ψ 0 <lb/>0 <lb/>0 <lb/>1 <lb/> <lb/> <lb/> , <lb/>(3) <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>6 of 25 <lb/></page>

			<body>where ( f x , f y ) are the focal length of the camera, and (u 0 , v 0 ) the coordinates of the optical center in <lb/>the image, the parameters Ψ and ξ represent the geometry of the mirror and are defined in Table 2, d is <lb/>the distance between the camera and the mirror and 2p is the semi-latus rectum of the mirror. <lb/>Table 2. Definition of Ψ and ξ for different mirrors. <lb/>Catadioptric System <lb/>ξ <lb/>Ψ <lb/>Hyper-catadioptric <lb/>0 &lt; ξ &lt; 1 <lb/>d+2p <lb/>√ <lb/>d 2 +4p 2 <lb/>Para-catadioptric <lb/>1 <lb/>1+2p <lb/>The last central-projection models presented in this work are the Scaramuzza and Kannala-Brandt <lb/>models. Summarizing [25,26], these empiric models represent the projection of a 3D point into the <lb/>image plane through non-lineal functions. <lb/>In the Scaramuzza model, the projection is represented by λp = λg(u ) = PX. The non-lineal <lb/>function, g(u ) = (u , v , f (u , v )), is defined by the image coordinates and a n-grade polynomial <lb/>function f (u , v ) = a 0 + a 1 ρ + a 2 ρ 2 + • • • + a N ρ N , where ρ is defined as the distance of the pixel to the <lb/>optical center in the image plane, and [a 0 , a 1 , . . . , a N ] are calibration parameters of the modeled camera. <lb/>In the Kannala-Brandt camera model, the forward projection model is represented as: <lb/>π(x, i) = <lb/>f x d(θ) x <lb/>r ) <lb/>f y d(θ) <lb/>y <lb/>r ) <lb/>+ <lb/>c x <lb/>c y <lb/>, <lb/>(4) <lb/>where r = x 2 + y 2 , (c x , c y ) are the coordinates of the optical center and d(θ) is the non-linear function <lb/>which is defined as d(θ) = θ + k 1 θ 3 + k 2 θ 5 + k 3 θ 7 + k 4 θ 9 , where θ = arctan(r/z) and [k 1 , k 2 , k 3 , k 4 ] are <lb/>the parameters that characterize the modeled camera. <lb/>2.2.2. Non-Central-Projection Cameras <lb/>Central-projection cameras are characterized by the unique optical center. By contrast, <lb/>in non-central-projection models, we do not have a single optical center for each image. For the <lb/>definition of non-central-projection models, we use Plücker coordinates. In this work, we summarize <lb/>the models; however, a full explanation of the models and Plücker coordinates is described in [35]. <lb/>Non-central panoramas have similarities with equirectangular panoramas. The main difference <lb/>with central panoramas is that each column of the non-central panorama shares a different optical center. <lb/>Moreover, since central panoramas are the projection of the environment into a sphere, non-central <lb/>panoramas are the projection into a semi-toroid, as can be seen in Figure 4a. The optical center of <lb/>the image is distributed in the trajectory of centers of the semi-toroid. This trajectory is defined as a <lb/>circle, dashed line in Figure 4a, whose center is the revolution axis. The definition of the rays that go to <lb/>the environment from the non-central system is obtained in [27], given as the result of Equation (5). <lb/>The parameter R c is the radius of the circle of optical centers and (θ, ϕ) are spherical coordinates in the <lb/>coordinate system. <lb/>Ξ = ξξ <lb/>T <lb/>= sin ϕ cos θ, sin ϕ sin θ, cos ϕ, R c sin ϕ sin θ, −R c sin ϕ cos θ, 0 <lb/>T <lb/>(5) <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>7 of 25 <lb/></page>

			<body>(a) Non-central panorama <lb/>(b) Non-central Catadioptric <lb/>Figure 4. Non-central systems schemes. <lb/>Finally, spherical and conical catadioptric systems are also described by non-central-projection <lb/>models. Just as with non-central panoramas, a full explanation of the model can be found on [28,29]. <lb/>Even though the basis of non-central and central catadioptric systems is the same, we take a <lb/>picture of a mirror from a perspective camera, the mathematical model is quite different. As in <lb/>non-central panoramas, for spherical and conical mirror we also use the Plücker coordinates to define <lb/>projection rays; see Figure 4b. For conical catadioptric systems, we define Z r = Z c + R c cot φ, where Z c <lb/>and R c are geometrical parameters and cot φ = (z + r tan 2τ)/(z tan 2τ − r), where τ is the aperture <lb/>angle of the cone. When these parameters are known, the 3D ray in Plücker coordinates is defined by: <lb/>Ξ = ξξ <lb/>T <lb/>= sin ϕ cos θ, sin ϕ sin θ, cos ϕ, −Z r sin ϕ sin θ, Z r sin ϕ cos θ, 0 <lb/>T <lb/>(6) <lb/>For the spherical catadioptric system, we define the geometric parameters as Z s = Z m + R s , <lb/>Z rel = Z s /R s , r 2 = x 2 + y 2 and ρ 2 = x 2 + y 2 + z 2 . Given the coordinates at a point of the image plane, <lb/>the Equation (7) defines the ray that reflects on the mirror: <lb/>Ξ = ξξ <lb/>T <lb/>= −xδ, yδ, −ζ, yZ s , xZ s , 0 <lb/>T <lb/>, <lb/>(7) <lb/>where δ = 2r 2 Z 4 <lb/>rel − 2z <lb/>√ <lb/>γZ 2 <lb/>rel − 3ρ 2 Z 2 <lb/>rel + ρ 2 ; = (−r 2 + z 2 )Z 2 <lb/>rel + 2 <lb/>√ <lb/>γz + ρ 2 ; ζ = 2r 2 zZ 4 <lb/>rel − zρ 2 Z 2 <lb/>rel − <lb/>2 <lb/>√ <lb/>γ(−r 2 Z 2 <lb/>rel + ρ 2 ) − zρ 2 and γ = (−r 2 Z 2 <lb/>rel + ρ 2 )Z 2 <lb/>rel . <lb/>2.3. Central Cameras Simulator <lb/>In this section, we describe the simulator, the interaction with UnrealCV and how are the projection <lb/>models are implemented. The method to obtain omnidirectional images can be summarized in <lb/>two steps: <lb/>• Image acquisition: the first step is the interaction with UE4 through UnrealCV to obtain the <lb/>cube map from the virtual environment. <lb/>• Image composition: the second step is creating the final image. In this step we apply the <lb/>projection models to select the information of the environment that has been acquainted in the <lb/>first step. <lb/>For central-projection images, the two steps are independent from each other. Once we <lb/>have a cube map, we can build any central-projection image from that cube map. However, <lb/>for non-central-projection images, the two steps are mixed. We need to compute where the optical <lb/>center is for each pixel and make the acquisition for that pixel. Examples of the images obtained for <lb/>each projection model can be found in the Appendix A. <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>8 of 25 <lb/></page>

			<body>2.3.1. Image Acquisition <lb/>The image acquisition is the first step to build omnidirectional images. In this step we must <lb/>interact with UE4 through UnrealCV using Python scripts. Camera pose and orientation, acquisition <lb/>field of view and mode of acquisition are the main parameters that we must define in the scripts to <lb/>give the commands to UnrealCV. <lb/>In this work, we call cube map to the set of six perspective images that models the full 360 o <lb/>projection of the environment around a point; concept introduced in [33]. Notice that 360 o information <lb/>around a point can be projected into a sphere centered in this point. Composing a sphere from <lb/>perspective images requires a lot of time and memory. Simplifying the sphere into a cube, as seen in <lb/>Figure 5a, we have a good approximation of the environment without losing information; see Figure 5b. <lb/>We can make this affirmation since the defined cube is a smooth atlas of the spherical manifold S 2 <lb/>embedded in R 3 . <lb/>To create central-projection systems, the acquisition of each cube map must be done from a single <lb/>location. Each cube map is the representation of the environment from one position-the optical center <lb/>of the omnidirectional camera. That is why we use UE4 with UnrealCV, where we can define the camera <lb/>pose easily. Moreover, the real-time renderings of the realistic environments allows fast acquisitions of <lb/>the perspective images to build the cube maps. Nevertheless, other virtual environments can be used <lb/>to create central-projection systems whenever the cube map can be built with these specifications. <lb/>(a) <lb/>(b) <lb/>Figure 5. (a): Simplification of the sphere into the cube map; (b): Unfolded cube map from a scene. <lb/>Going back to UnrealCV, the plugin gives us different kinds of capture modes. For our simulator, <lb/>we have taken 3 of these modes: lit, object mask and depth. <lb/>In the lit mode, UnrealCV gives a photorealistic RGB image of the virtual environment. The degree <lb/>of realism must be created by the designer of the scenario. The second is the object mask mode. This <lb/>mode gives us semantic information of the environment. The images obtained have a colored code to <lb/>identify the different objects into the scene. The main advantage of this mode is the pixel precision for <lb/>the semantic information, avoiding the human error in manual labeling. Moreover, from this capture <lb/>mode, we can obtain ground-truth information of the scene and create specific functions to obtain <lb/>ground-truth data for computer vision algorithms, as layout recovery or object detection. The third <lb/>mode is depth. This mode gives a data file where we have depth information for each pixel of the <lb/>image. For the implementation of this mode in the simulator, we keep the exact data information and <lb/>compose a depth image in grayscale. <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>9 of 25 <lb/></page>

			<body>2.3.2. Image Composition <lb/>Images from central-projection cameras are composed from a cube map acquired in the scene. <lb/>The composition of each image depends on the projection model of the camera, but they follow the <lb/>same pattern. <lb/>The Algorithm 1 shows the steps in the composition of central-projection images. Initially, we <lb/>get the pixel coordinates from the destination image that we want to build. Then, we compute the <lb/>spherical coordinates for each pixel through the projection model of the camera we are modeling. <lb/>With the spherical coordinates we build the vector that goes from the optical center of the camera <lb/>to the environment. Then, we rotate this vector to match the orientation of the camera into the <lb/>environment. The intersection between this rotated vector and the cube map gives the information <lb/>of the pixel (color, label, or distance). In contrast to the coordinate system of UE4, which presents a <lb/>left-handed coordinate system and different rotations for each axis (see Figure 6a), our simulator uses a <lb/>right-handed coordinate system and spherical coordinates, as shown in Figure 6b. Changing between <lb/>the coordinate systems is computed internally in our tool to keep mathematical consistency between <lb/>the projection models and the virtual environment. <lb/>Algorithm 1: Central-projection composition <lb/>Input: Set-up of final image (Camera, Resolution, type, ...) <lb/>Load cube map <lb/>while Go through final image do <lb/>get pixel coordinates <lb/>compute spherical coordinates -&gt; Apply projection model of omnidirectional camera <lb/>compute vector/light ray -&gt; Apply orientation of the camera in the environment <lb/>get pixel information -&gt; Intersection between light ray and cube map <lb/>end <lb/>Y <lb/>φ <lb/>X <lb/>ψ <lb/>(a) UE4 coordinate system <lb/>θ <lb/>Φ <lb/>X <lb/>Z <lb/>Y <lb/>(b) OmniSCV coordinate system <lb/>Figure 6. (a): Coordinate system used in graphic engines focused on first-person video games; <lb/>(b): Coordinate system of our image simulator. <lb/>Equirectangular panoramas have an easy implementation in this simulator since equirectangular <lb/>images are the projection of the environment into a sphere. Then, the destination image can be defined <lb/>in spherical coordinates directly from the pixel coordinates as: <lb/>θ = <lb/>2u <lb/>u max <lb/>− 1 π; ϕ = 1/2 − <lb/>v <lb/>v max <lb/>π, <lb/>(8) <lb/>where these equations define the range of the spherical coordinates as −π &lt; θ &lt; π and −π/2 &lt; ϕ &lt; <lb/>π/2. <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>10 of 25 <lb/></page>

			<body>In the cylindrical model we also use spherical coordinates, but defined with the restrictions of <lb/>the cylindrical projection. In the definition of the Equation (9) we can see that two new parameters <lb/>appear. The FOV h parameter represents the horizontal field of view of the destination image, which <lb/>can go up to 360 o , and can be changed by the simulator user. The FOV v parameter models the height <lb/>of the lateral surface of the cylinder from the optical center point of view. <lb/>θ = <lb/>2u <lb/>u max <lb/>− 1 <lb/>FOV h <lb/>; ϕ = 1 − <lb/>2v <lb/>v max <lb/>FOV v <lb/>, <lb/>(9) <lb/>where the range of the spherical coordinates are −FOV h /2 &lt; θ &lt; FOV h /2 and −FOV v /2 &lt; ϕ &lt; <lb/>FOV v /2. <lb/>In fish-eye cameras we have a revolution symmetry, so we use polar coordinates. We transform <lb/>the pixel coordinates into polar coordinates with the next equation: <lb/>(r, θ) = <lb/>(u − u 0 ) 2 + (v − v 0 ) 2 , arctan((u 0 − u)/(v − v 0 )) , <lb/>(10) <lb/>where we define (u 0 , v 0 ) = (u max /2, v max /2). <lb/>Given this definition, the range of the polar coordinates is 0 &lt;r &lt; (u 2 <lb/>max + v 2 <lb/>max )/4 and <lb/>−π &lt; θ &lt; π. However, we cropr max = min(u max /2, v max /2) in order to constrain the rendered pixels <lb/>in the camera field of view. After obtaining the polar coordinates, we get the spherical coordinates <lb/>for each pixel through the projection model. In Table 3 we have the relationship between the polar <lb/>coordinater and the spherical coordinate φ given by the projection model of fish-eye cameras. <lb/>Table 3. Relationship betweenr and φ from the fish-eye projection model. <lb/>Equi-angular <lb/>Stereographic <lb/>Orthogonal <lb/>Equi-Solid Angle <lb/>φ =r/ f <lb/>φ = 2 arctan(r/2 f ) φ = arcsin(r/ f ) φ = 2 arcsin(r/ f ) <lb/>where f is the focal length of the fish-eye camera. <lb/>On catadioptric systems, we define the parameters ξ and η that model the mirror as shown in <lb/>Table 4. We use polar coordinates to select what pixels of the image are rendered, but we apply the <lb/>projection model on the pixel coordinates directly. <lb/>Table 4. Definition of ξ and η for central mirrors. <lb/>Catadioptric System <lb/>ξ <lb/>η <lb/>Para-catadioptric <lb/>1 <lb/>−2p <lb/>Hyper-catadioptric <lb/>d <lb/>√ <lb/>d 2 +4p 2 <lb/>−2p <lb/>√ <lb/>d 2 +4p 2 <lb/>For computing the set of rays corresponding to the image pixels, we use the back-projection <lb/>model described in [36]. The first step is projecting the pixel coordinates into a 2D projective space: <lb/>p = (u, v) T − →v = (x,ŷ,ẑ) T . Then, we re-project the point into the normalized plane with the inverse <lb/>calibration matrix of the catadioptric system asv = H −1 <lb/>cv , (see Section 2.2). Finally, through the inverse <lb/>of the non-linear function h(x), shown in the Equation (11), we can obtain the coordinates of the ray <lb/>that goes from the catadioptric system to the environment. <lb/>v = h −1 (v) = <lb/> <lb/> <lb/> <lb/>x <lb/>y <lb/>z <lb/> <lb/> <lb/> = <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/>xz <lb/>ξ + z 2 + (1 − ξ 2 )(x 2 +ȳ 2 ) <lb/>x 2 +ȳ 2 +z 2 <lb/>yz <lb/>ξ + z 2 + (1 − ξ 2 )(x 2 +ȳ 2 ) <lb/>x 2 +ȳ 2 +z 2 <lb/>zz <lb/>ξ + z 2 + (1 − ξ 2 )(x 2 +ȳ 2 ) <lb/>x 2 +ȳ 2 +z 2 <lb/>− ξ <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/>(11) <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>11 of 25 <lb/></page>

			<body>These projection models give us an oriented ray that comes out of the camera system. This ray <lb/>is expressed in the camera reference. Since the user of the simulator can change the orientation of <lb/>the camera in the environment in any direction, we need to change the reference system of the ray to <lb/>the world reference. First, we rotate the ray from the camera reference to the world reference. Then <lb/>we rotate again the ray in the direction of the camera inside the world reference. After these two <lb/>rotations, we have changed the reference system of the ray from the camera to the world reference, <lb/>taking into account the orientation of the camera in the environment. Once the rays are defined, we <lb/>get the information from the environment computing the intersection of each ray with the cube map. <lb/>The point of intersection has the pixel information of the corresponding ray. <lb/>2.4. Non-central Camera Simulator <lb/>The simulator for non-central cameras is quite different from the central camera one. In this case, <lb/>we can neither use only a cube map to build the final images nor save all the acquisitions needed. <lb/>The main structure proposed to obtain non-central-projection images is shown in Algorithm 2. Since <lb/>we have different optical centers for each pixel in the final image, we group the pixels sharing the <lb/>same optical center, reducing the number of acquisitions needed to build it. The non-central systems <lb/>considered group the pixel in different ways, so the implementations are different. <lb/>Algorithm 2: Non-central-projection composition <lb/>Input: Set-up of final image (Camera, Resolution, type, ...) <lb/>while Go through final image do <lb/>get pixel coordinates <lb/>compute optical center <lb/>make image acquisition -&gt; Captures from UnrealCV <lb/>compute spherical coordinates -&gt; Apply projection model <lb/>get pixel information -&gt; Intersection with acquired images <lb/>end <lb/>From the projection model of non-central panoramas, we get that pixels with the same u coordinate <lb/>share the same optical center. For each coordinate u of the image, the position for the image acquisition <lb/>is computed. For a given center (X c , Y c , Z c ) T , and radius R c , of the non-central system, we have to <lb/>compute the optical center of each u coordinate. <lb/>(x oc , y oc , z oc ) T = (X c , Y c , Z c ) T + R c (cos θ cos φ, sin θ, cos θ sin φ) T <lb/>(12) <lb/>θ = <lb/>2u <lb/>u max <lb/>− 1 π <lb/>(13) <lb/>To obtain each optical center, we use Equation (12), where θ is computed according to Equation (13) <lb/>and φ is the pitch angle of the non-central system. Once we have obtained the optical center, we make <lb/>the acquisition in that location, obtaining a cube map. Notice that this cube map only allows the <lb/>obtaining of information for the location it has acquired. This means that for every optical center of the <lb/>non-central system, we must make a new acquisition. <lb/>ϕ = 1/2 − <lb/>v <lb/>v max <lb/>π <lb/>(14) <lb/>Once the acquisition is obtained from the correct optical center, we compute the spherical <lb/>coordinates to cast the ray into the acquired images. From the Equation (13) we already have one of <lb/>the coordinates; the second is obtained from the Equation (14). <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>12 of 25 <lb/></page>

			<body>Table 5. Definition of cot φ and Z r for different mirrors. <lb/>Mirror <lb/>Z r <lb/>cot φ <lb/>Conical <lb/>Z c + R c cot φ (1 + r tan(2τ))/(tan(2τ) − r) <lb/>Spherical <lb/>−ξ/δ <lb/>Z s (δ + )/δ <lb/>In non-central catadioptric systems, pixels sharing the same optical center are grouped in <lb/>concentric circles. That means we go through the final image from the center to the edge, increasing <lb/>the radius pixel by pixel. For each radius, we compute the parameters Z r and cot φ as in Table 5, which <lb/>depend on the mirror we want to model (see Section 2.2). The parameter Z r allows us to compute the <lb/>optical center from where we acquire a cube map of the environment. <lb/>(u, v) = (u max /2 + r cos θ, v max /2 − r sin θ) <lb/>(15) <lb/>Once the cube map for each optical center is obtained, we go through the image using polar <lb/>coordinates. For a fixed radius, we change θ and compute the pixel to color, obtained from Equation (15). <lb/>Knowing θ and φ, from Table 5, we can cast a ray that goes from the catadioptric system to the cube <lb/>map acquired. The intersection gives the information for the pixel. <lb/>In these non-central systems, the number of acquisitions required depends on the resolution of <lb/>the final non-central image. That means, the more resolution the image has, the more acquisitions <lb/>are needed. For an efficient composition of images, we need to define as fast as possible the pose <lb/>of the camera in the virtual environment for each optical center. That is one of the reasons we use <lb/>Unreal Engine 4 as a virtual environment, where we can easily change the camera pose in the virtual <lb/>environment making fast acquisitions, since the graphics work in real time. <lb/>3. Results <lb/>Working on an environment of Unreal Engine 4 [12] and the simulator presented in this paper, <lb/>we have obtained a variety of photorealistic omnidirectional images from different systems. In the <lb/>Appendix A we have several examples of these images. To evaluate if our synthetic images can be <lb/>used in computer vision algorithms, we compare the evaluation of four algorithms with our synthetic <lb/>images and real ones. The algorithms selected are: <lb/>• <lb/>Corners For Layouts: CFL [37] is a neural network that recovers the 3D layout of a room from an <lb/>equirectangular panorama. We have used a pre-trained network to evaluate our images. <lb/>• <lb/>Uncalibtoolbox: the algorithm presented in [34] is a MatLab toolbox for line extraction and <lb/>camera calibration for different fish-eye and central catadioptric systems. We compare the <lb/>calibration results from different images. <lb/>• <lb/>OpenVSLAM: a virtual Simultaneous Location and Mapping framework, presented in [38], <lb/>which allows use of omnidirectional central image sequences. <lb/>• <lb/>3D line Reconstruction from single non-central image which was presented in [39,40] using <lb/>non-central panoramas. <lb/>3.1. 3D Layout Recovery, CFL <lb/>Corners For layouts (CFL) is a neural network that recovers the layout of a room from one <lb/>equirectangular panorama. This neural network provides two outputs: one is the intersection of walls <lb/>or edges in the room and the second is the corners of the room. With those representations, we can <lb/>build a 3D reconstruction of the layout of the room using Manhattan world assumptions. <lb/>For our evaluation, we have used a pre-trained CFL (https://github.com/cfernandezlab/CFL) <lb/>network with Equirectangular Convolutions (Equiconv). The training dataset was composed by <lb/>equirectangular panoramas built from real images and the ground truth was made manually, which <lb/>increases the probability of mistakes due to human error. The rooms that compose this dataset are <lb/>4-wall rooms (96%) and 6-wall rooms (6%). <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>13 of 25 <lb/></page>

			<body>To compare the performance of CFL, the test images are divided into real images and synthetic <lb/>images. In the set of real images, we have used the test images from the datasets STD2D3D [4], <lb/>composed of 113 equirectangular panoramas of 4-wall rooms, and the SUN360 [3], composed of <lb/>69 equirectangular panoramas of 4-wall rooms and 3 of 6-wall rooms. The set of synthetic images <lb/>are divided in panoramas from 4-wall rooms and from 6-walls rooms. Both sets are composed by <lb/>10 images taken on 5 different places in the environment in 2 orientations. Moreover, for the synthetic <lb/>sets, the ground-truth information of the layout has been obtained automatically with pixel precision. <lb/>The goal of these experiments is testing the performance of the neural network in the different <lb/>situations and evaluate the results using our synthetic images and comparing with those obtained <lb/>with the real ones. In the figures above the ground-truth generation can be seen, Figures 7a,c and 8a,c, <lb/>and the output of CFL, Figures 7b,d and 8b,d, for a equirectangular panorama in the virtual <lb/>environments recreated. On the 4-wall layout environment we can observe that the output of CFL <lb/>is similar to the ground truth. This seems logical since most of the images from the training dataset <lb/>have the same layout. On the other hand, the 6-wall layout environment presents poorer results. <lb/>The output from CFL in this environment only fits four walls of the layout, probably due to the gaps in <lb/>the training data. <lb/>(a) <lb/>(b) <lb/>(c) <lb/>(d) <lb/>Figure 7. Results of CFL using a synthetic image from a 4-wall environment. (a): Edges ground truth; <lb/>(b): Edges output from CFL; (c): Corners ground truth; (d): Corners output from CFL. <lb/>(a) <lb/>(b) <lb/>(c) <lb/>(d) <lb/>Figure 8. Results of CFL using a synthetic image from a 6-wall environment. (a): Edges ground truth; <lb/>(b): Edges output from CFL; (c): Corners ground truth; (d): Corners output from CFL. <lb/>To quantitatively compare the results of CFL, in Table 6 we present the results using real images <lb/>from existing datasets with the results using our synthetic images. We compare five standard metrics: <lb/>Intersection over union of predicted corner/edge pixels (IoU), accuracy Acc, precision P, Recall R and <lb/>F1 Score F 1 . <lb/>Table 6. Comparative of results of images from different datasets. OmniSCV contains the images <lb/>created with our tool on a 6-wall room and a 4-wall room. The real images have been obtained from <lb/>the test dataset of [2,4]. <lb/>Test Images <lb/>Edges <lb/>Corners <lb/>IoU <lb/>Acc <lb/>P <lb/>R <lb/>F 1 <lb/>IoU <lb/>Acc <lb/>P <lb/>R <lb/>F 1 <lb/>6-wall OmniSCV 0.424 0.881 0.694 0.519 0.592 <lb/>0.370 0.974 0.547 0.526 0.534 <lb/>4-wall OmniSCV 0.474 0.901 0.766 0.554 0.642 <lb/>0.506 0.985 0.643 0.698 0.669 <lb/>Real images <lb/>0.452 0.894 0.633 0.588 0.607 <lb/>0.382 0.967 0.758 0.425 0.544 <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>14 of 25 <lb/></page>

			<body>3.2. Uncalibtoolbox <lb/>The uncalibtoolbox is a MatLab toolbox where we can compute a line extraction and calibration <lb/>on fish-eye lenses and catadioptric systems. This toolbox makes the line extraction from the image <lb/>and computes the calibration parameters from the distortion on these lines. The more distortion the <lb/>lines present in the image, the more accurate the calibration parameters are computed. Presented <lb/>in [34], this toolbox considers the projection models to obtain the main calibration parameterr vl of the <lb/>projection system. Thisr vl parameter encapsulates the distortion of each projection and is related with <lb/>the field of view of the camera. <lb/>On this evaluation we want to know if our synthetic images can be processed as real images <lb/>on computer vision algorithms. We take several dioptric and catadioptric images generated by our <lb/>simulator and perform the line extraction on them. To compare the results of the line extraction, we <lb/>compare with real images from [34]. <lb/>In the previous Figures, we presented four equi-angular fish-eye, Figure 9, and two catadioptric <lb/>systems, Figure 10. The behavior of the line extraction algorithm of uncalibtoolbox for the synthetic <lb/>images of our simulator is similar to the real images presented in [34]. That encourages our <lb/>work, because we have made photorealistic images that can be used as real ones for computer <lb/>vision algorithms. <lb/>(a) <lb/>(b) <lb/>(c) <lb/>(d) <lb/>Figure 9. Line extraction on fish-eye camera. (a,b): Real fisheye images from examples of [34]; <lb/>(c,d): Synthetic images from our simulator. <lb/>(a) <lb/>Hyper-catadioptric <lb/>(b) <lb/>Para-catadioptric <lb/>(c) <lb/>Hyper-catadioptric <lb/>(d) <lb/>Para-catadioptric <lb/>Figure 10. Line extraction on catadioptric systems. (a,b): Real catadioptric images from examples <lb/>of [34]; (c,d): Synthetic images from our simulator. <lb/>On the other hand, we compare the accuracy of the calibration process between the results <lb/>presented in [34] and the obtained with our synthetic images. The calibration parameter has been <lb/>obtained testing 5 images for each r vl and taking the mean value. Since we impose the calibration <lb/>parameters in our simulator, we have selected 10 values of the parameter r vl , in the range 0.5 &lt; r vl &lt; <lb/>1.5, in order to compare our images with the results of [34]. The calibration results are presented in the <lb/>Figure 11. <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>15 of 25 <lb/></page>

			<body>0.4 <lb/>0.6 <lb/>0.8 <lb/>1 <lb/>1.2 <lb/>1.4 <lb/>1.6 <lb/>1.8 <lb/>2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1 <lb/>1.2 <lb/>1.4 <lb/>1.6 <lb/>1.8 <lb/>2 <lb/>r <lb/>vl <lb/>r vl <lb/>(a) <lb/>r <lb/>vl <lb/>r vl <lb/>Equisolid <lb/>Stereographic <lb/>Para-catadioptric <lb/>Ground Truth <lb/>Equiangular <lb/>Hyper-catadioptric <lb/>Orthogonal <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1 <lb/>1.2 <lb/>1.4 <lb/>1.6 <lb/>1.8 <lb/>2 <lb/>0.4 <lb/>0.6 <lb/>0.8 <lb/>1 <lb/>1.2 <lb/>1.4 <lb/>1.6 <lb/>1.8 <lb/>2 <lb/>(b) <lb/>Figure 11. Normalized result for the calibration parameters using different omnidirectional cameras. <lb/>(a): Calibration results from [34]; (b): Calibration results using images from our simulator. <lb/>3.3. OpenVSLAM <lb/>The algorithm presented in [38] allows the obtaining of a SLAM for different cameras, <lb/>from perspective to omnidirectional central systems. This open-source algorithm is based on an <lb/>indirect SLAM algorithm, such as ORB-SLAM [41] and ProSLAM [42]. The main difference with other <lb/>SLAM approaches is that the proposed framework allows definition of various types of central cameras <lb/>other than perspective, such as fish-eye or equirectangular cameras. <lb/>To evaluate the synthetic images generated with OmniSCV, we create a sequence in a virtual <lb/>environment simulating the flight of a drone. Once the trajectory is defined, we generate the images <lb/>where we have the ground truth of the pose of the drone camera. <lb/>The evaluation has been made with equirectangular panoramas of 1920 × 960 pixels through a <lb/>sequence of 28 seconds and 30 frames per second. The ground-truth trajectory as well as the scaled <lb/>SLAM trajectory can be seen in Figure 12. In the Appendix B we include several captures from the <lb/>SLAM results as well as the corresponding frame obtained with OmniSCV. We evaluate quantitatively <lb/>the precision of the SLAM algorithm computing the position and orientation error of each frame respect <lb/>to the ground-truth trajectory. We consider error in rotation, ε θ , and in translation, ε t , as follows: <lb/>ε t = arccos <lb/>t T <lb/>gt • t est <lb/>|t gt ||t est | <lb/>; ε θ = arccos <lb/> <lb/> <lb/>Tr R gt R est <lb/>T − 1 <lb/> <lb/> , <lb/>(16) <lb/>where t gt , R gt are the position and rotation matrix of a frame in the ground-truth trajectory and t est , R est <lb/>are the estimated position up to scale and rotation matrix of the SLAM algorithm in the same frame. <lb/>The results of these errors are shown in Figure 13. <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>16 of 25 <lb/></page>

			<body>4 <lb/>2 <lb/>0 <lb/>Trajectory <lb/>-2 <lb/>X(m) <lb/>-4 <lb/>-6 <lb/>-8 <lb/>-4 <lb/>Y(m) <lb/>0 <lb/>4 <lb/>3 <lb/>2 <lb/>4 <lb/>Z(m) <lb/>Figure 12. Visual odometry from SLAM algorithm. The red line is the ground-truth trajectory while <lb/>the blue line is the scaled trajectory of the SLAM algorithm. <lb/>Frames <lb/>0 <lb/>100 <lb/>200 <lb/>300 <lb/>400 <lb/>500 <lb/>600 <lb/>700 <lb/>800 <lb/>900 <lb/>Error(Deg) <lb/>0 <lb/>5 <lb/>10 <lb/>Position Error <lb/>(a) <lb/>Frames <lb/>100 <lb/>200 <lb/>300 <lb/>400 <lb/>500 <lb/>600 <lb/>700 <lb/>800 <lb/>900 <lb/>Error(Deg) <lb/>0 <lb/>5 <lb/>10 <lb/>Orientation Error <lb/>(b) <lb/>Figure 13. (a): Position error of the SLAM reconstruction. (b): Orientation error of the SLAM <lb/>reconstruction. Both errors are measured in degrees. <lb/>3.4. 3D Line Reconstruction from Single Non-Central Image <lb/>One of the particularities of non-central images is that line projections contain more geometric <lb/>information. In particular, the entire 3D information of the line is mapped on the line projection [43,44]. <lb/>For evaluating if synthetic non-central images generated by our tool conserve this property, we <lb/>have tested the proposal presented in [40]. This approach assumes that the direction of the gravity <lb/>is known (this information could be recovered from an inertial measurement unit (IMU)) and lines <lb/>are arranged in vertical and horizontal lines. Horizontal lines can follow any direction contained <lb/>in any horizontal plane (soft-Manhattan constraint). The non-central camera captures a non-central <lb/>panorama of 2048 × 1024 pixels with a radius of R c = 1m and an inclination of 10 degrees from the <lb/>vertical direction. <lb/>A non-central-depth synthetic image has been used as ground truth of the reconstructed points <lb/>(see Figure 14b). In Figure 14a we show the extracted line projections and segments on the non-central <lb/>panoramic image; meanwhile, Figure 15 presents the reconstructed 3D line segments. We have omitted <lb/>the segments with low effective baseline in the 3D representation for visualization purposes. <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>17 of 25 <lb/></page>

			<body>(a) <lb/>(b) <lb/>Figure 14. (a) Extracted line projections and segments on the non-central panorama. (b) Ground-truth <lb/>point-cloud obtained from depth-map. <lb/>-2 <lb/>3 <lb/>2 <lb/>2 <lb/>-1 <lb/>X(m) <lb/>1 <lb/>1 <lb/>0 <lb/>Y(m) <lb/>Z(m) <lb/>0 <lb/>0 <lb/>-1 <lb/>1 <lb/>-1 <lb/>-2 <lb/>-2 <lb/>-3 <lb/>(a) <lb/>-3 <lb/>-2 <lb/>-1 <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>X(m) <lb/>-2 <lb/>-1 <lb/>0 <lb/>1 <lb/>2 <lb/>Y(m) <lb/>(b) <lb/>-3 <lb/>-2 <lb/>-1 <lb/>0 <lb/>1 <lb/>2 <lb/>3 <lb/>4 <lb/>X(m) <lb/>-2 <lb/>-1 <lb/>0 <lb/>1 <lb/>Z(m) <lb/>(c) <lb/>Figure 15. 3D line segments reconstructed from line extraction in non-central panorama. In red the <lb/>reconstructed 3D line segments. In black the ground truth. In blue the circular location of the optical <lb/>center and the Z axis. In green the axis of the vertical direction. (a) Orthographic view. (b) Top view. (c) <lb/>Lateral view. <lb/>4. Discussion <lb/>From our tool we have obtained RGB, depth and semantic images from a great amount of <lb/>omnidirectional projection systems. These images have been obtained from a photorealistic virtual <lb/>world where we can define every parameter. To validate the images obtained from our tool, we have <lb/>made evaluations with computer vision algorithms that use real images. <lb/>In the evaluation with CFL we have interesting results. On one hand, we have obtained results <lb/>comparable to datasets with real images. This behavior shows that the synthetic images generated <lb/>with our tool are as good as real images from the existing datasets. On the other hand, we have <lb/>made different tests changing the layout of our scene, something that cannot be done in real scenarios. <lb/>On these changes we have realized that CFL does not work properly with some layouts. This happens <lb/>because existing datasets have mainly 4-wall rooms to use as training data and the panoramas have <lb/>been taken in the middle of the room [2,4]. This makes it hard for the neural network to generalize for <lb/>rooms with more than 4 walls or panoramas that have been taken in different places inside the room. <lb/>Our tool can aid in solving this training problem. Since we can obtain images from every place in the <lb/>room and we can change the layout, we can fill the gaps of the training dataset. With bigger and richer <lb/>datasets for training, neural networks can improve their performance and make better generalizations. <lb/>In the evaluation with uncalibtoolbox, we have tested catadioptric systems and fish-eye lenses. <lb/>We have compared the precision of the toolbox for real and synthetic images. In the line extraction, <lb/>the toolbox has no problems nor makes any distinction from one kind of images or the other. That <lb/>encourages our assumptions that our synthetic images are photorealistic enough to be used as <lb/>real images. When we compare the calibration results, we can see that the results of the images <lb/></body>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>18 of 25 <lb/></page>

			<body>obtained from [34] and the results from our synthetic images are comparable. There are no big <lb/>differences in precision. The only different values observed are in hyper-catadioptric systems. For the <lb/>hyper-catadioptric systems presented in [34], the computed calibration parameters differ from the real <lb/>ones while in the synthetic hyper-catadioptric systems, we have more accurate parameters. A possible <lb/>conclusion of this effect is the absence of the reflection of the camera in the synthetic images. For those, <lb/>we have more information of the environment in the synthetic images than in real ones, helping the <lb/>toolbox to obtain better results for the calibration parameters. From the results shown, we can conclude <lb/>that our tool can help to develop and test future calibration tools. Since we are the ones that set the <lb/>calibration of the system in the tool, we have perfect knowledge of the calibration parameters of the <lb/>image. However, real systems need to be calibrated a priori or we must trust the calibration parameters <lb/>that the supplier of the system gives us. <lb/>In the evaluation of the SLAM algorithm, we test if the synthetic images generated with our <lb/>tool can be used in computer vision algorithms for tracking and mapping. If we compare the results <lb/>obtained from the OpenVSLAM algorithm [38], with the ground-truth information that provides our <lb/>tool, we can conclude that the synthetic images generated with OmniSCV can be used for SLAM <lb/>applications. The position error is computed in degrees due to the lack of scale in the SLAM algorithm. <lb/>Moreover, we observe the little position and orientation error of the camera along the sequence (see <lb/>Figure 13), keeping the estimated trajectory close to the real one. Both errors are less than 8 degrees <lb/>and decrease along the trajectory. This correction of the position is the effect of the loop closure of <lb/>the SLAM algorithm. On the other hand, we obtain ground-truth information of the camera pose <lb/>for every frame. This behavior encourages the assumptions we have been referring to in this section: <lb/>that synthetic images generated from our tool can be used as real ones in computer vision algorithms, <lb/>obtaining more accurate ground-truth information too. <lb/>Finally, in the evaluation of the non-central 3D line fitting from single view we can see how the <lb/>non-central images generated with our tool conserve the full projection of the 3D lines of the scene. It <lb/>is possible to recover the metric 3D reconstruction of the points composing these lines. As presented <lb/>in [39] this is only possible when the set of projecting skew rays composing the projection surface of <lb/>the segment have enough effective baseline. <lb/>5. Conclusions <lb/>In this work, we present a tool to create omnidirectional synthetic photorealistic images to be used <lb/>in computer vision algorithms. We devise a tool to create a great variety of omnidirectional images, <lb/>outnumbering the state of the art. We include in our tool different panoramas such as equirectangular, <lb/>cylindrical and non-central; dioptric models based on fish-eye lenses (equi-angular, stereographic, <lb/>orthogonal and equi-solid angle); catadioptric systems with different kinds of mirrors as spherical, <lb/>conical, parabolic and hyperbolic; and two empiric models, Scaramuzza&apos; and Kannala-Brandt&apos;s. <lb/>Moreover, we get not only the photorealistic images but also labeled information. We obtain semantic <lb/>and depth information for each of the omnidirectional systems proposed with pixel precision and <lb/>can build specific functions to obtain ground truth for computer vision algorithms. Furthermore, <lb/>the evaluations of our images show that we can use synthetic and real images equally. The synthetic <lb/>images created by our tool are good enough to be used as real images in computer vision algorithms <lb/>and deep-learning-based algorithms. <lb/></body>

			<div type="annex">Author Contributions: Investigation, B.B.-B.; software, B.B.-B.; validation, B.B.-B. and J.B.-C.; data curation, <lb/>B.B.-B.; visualization, B.B.-B.; writing-original draft preparation, B.B.-B.; writing-review and editing, J.B.-C. and <lb/>J.J.G.; supervision, J.J.G. and J.B.-C.; resources, J.J.G. and J.B.-C.; funding acquisition, J.J.G. and J.B.-C. All authors <lb/>have read and agreed to the published version of the manuscript. <lb/></div>

			<div type="funding">Funding: This research was funded by the Project RTI2018-096903-B-I00 of the AEI/FEDER UE and the Project <lb/>JIUZ-2019-TEC-03 of Unizar and Fundacion Ibercaja. <lb/></div>

			<div type="annex">Conflicts of Interest: The authors declare no conflict of interest. <lb/></div>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>19 of 25 <lb/></page>

			<div type="annex">Appendix A. <lb/>In this appendix we present an example of illustrations obtained with our system. We present <lb/>the 39 different types of omnidirectional images that can be obtained with the tool presented in this <lb/>work. We offer 13 projection models with 3 different image options to get diverse information. These <lb/>models are divided in 3 panoramic models, 4 kinds of fish eye lenses, catadioptric systems with 4 <lb/>different mirrors and 2 empiric models for central projection models with revolution symmetry. <lb/>The code for creating these images, as well as these images, will be available in https://github. <lb/>com/Sbrunoberenguel/OmniSCV. <lb/>Appendix A.1. Panoramas <lb/>(a) Equirectangular <lb/>(b) Cylindrical <lb/>(c) Non-Central <lb/>Figure A1. Lit mode. <lb/>(a) Equirectangular <lb/>(b) Cylindrical <lb/>(c) Non-Central <lb/>Figure A2. Semantic mode. <lb/>(a) Equirectangular <lb/>(b) Cylindrical <lb/>(c) Non-Central <lb/>Figure A3. Depth mode. <lb/></div>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>20 of 25 <lb/></page>

			<div type="annex">Appendix A.2. Fish Eye Lenses <lb/>(a) Equiangular <lb/>lens <lb/>(b) <lb/>Stereographic <lb/>lens <lb/>(c) Orthogonal <lb/>lens <lb/>(d) Equi-solid <lb/>angle lens <lb/>Figure A4. Lit mode. <lb/>(a) Equiangular <lb/>lens <lb/>(b) <lb/>Stereographic <lb/>lens <lb/>(c) Orthogonal <lb/>lens <lb/>(d) Equi-solid <lb/>angle lens <lb/>Figure A5. Semantic mode. <lb/>(a) Equiangular <lb/>lens <lb/>(b) <lb/>Stereographic <lb/>lens <lb/>(c) Orthogonal <lb/>lens <lb/>(d) Equi-solid <lb/>angle lens <lb/>Figure A6. Depth mode. <lb/></div>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>21 of 25 <lb/></page>

			<div type="annex">Appendix A.3. Catadioptric System <lb/>(a) <lb/>Parabolic <lb/>mirror <lb/>(b) Hyperbolic <lb/>mirror <lb/>(c) <lb/>Spherical <lb/>mirror <lb/>(d) <lb/>Conical <lb/>mirror <lb/>Figure A7. Lit mode. <lb/>(a) <lb/>Parabolic <lb/>mirror <lb/>(b) Hyperbolic <lb/>mirror <lb/>(c) <lb/>Spherical <lb/>mirror <lb/>(d) <lb/>Conical <lb/>mirror <lb/>Figure A8. Semantic mode. <lb/>(a) <lb/>Parabolic <lb/>mirror <lb/>(b) Hyperbolic <lb/>mirror <lb/>(c) <lb/>Spherical <lb/>mirror <lb/>(d) <lb/>Conical <lb/>mirror <lb/>Figure A9. Depth mode. <lb/></div>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>22 of 25 <lb/></page>

			<div type="annex">Appendix A.4. Empiric Models <lb/>(b) <lb/>Scaramuzza&apos;s <lb/>model <lb/>(c) <lb/>Kannala-Brandt <lb/>model <lb/>(c) Lit mode <lb/>(e) Scaramuzza&apos;s <lb/>model <lb/>(f) <lb/>Kannala-Brandt <lb/>model <lb/>(f) Semantic mode <lb/>(h) <lb/>Scaramuzza&apos;s <lb/>model <lb/>(i) <lb/>Kannala-Brandt <lb/>model <lb/>(i) Depth mode <lb/>Figure A10. Scaramuzza and Kannala-Brandt empiric models. <lb/></div>

			<div type="annex">Appendix B. <lb/>In this appendix we show the initial frame, frame 105, frame 575 and final frame obtained from the <lb/>OpenVSLAM algorithm [38]. The full sequence can be found in https://github.com/Sbrunoberenguel/ <lb/>OmniSCV. <lb/></div>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>23 of 25 <lb/></page>

			<listBibl>References <lb/>1. <lb/>Dai, A.; Chang, A.X.; Savva, M.; Halber, M.; Funkhouser, T.; Nießner, M. ScanNet: Richly-annotated 3D <lb/>Reconstructions of Indoor Scenes. In Proceedings of the Conference on Computer Vision and Pattern <lb/>Recognition, Honolulu, HI, USA, 21-26 July 2017. <lb/>2. <lb/>Song, S.; Lichtenberg, S.P.; Xiao, J. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings <lb/>of the Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7-12 June 2015; <lb/>pp. 567-576. <lb/>3. <lb/>Xiao, J.; Ehinger, K.A.; Oliva, A.; Torralba, A. Recognizing scene viewpoint using panoramic place <lb/>representation. In Proceedings of the Conference on Computer Vision and Pattern Recognition, Providence, <lb/>RI, USA, 16-21 June 2012; pp. 2695-2702. <lb/>4. <lb/>Armeni, I.; Sax, S.; Zamir, A.R.; Savarese, S. Joint 2d-3d-semantic data for indoor scene understanding. arXiv <lb/>2017, arXiv:1702.01105 <lb/>5. <lb/>Straub, J.; Whelan, T.; Ma, L.; Chen, Y.; Wijmans, E.; Green, S.; Engel, J.J.; Mur-Artal, R.; Ren, C.; Verma, S.; <lb/>others. The Replica Dataset: A Digital Replica of Indoor Spaces. arXiv 2019 arXiv:1906.05797. <lb/>6. <lb/>Russell, B.C.; Torralba, A.; Murphy, K.P.; Freeman, W.T. LabelMe: A database and web-based tool for image <lb/>annotation. Int. J. Comput. Vis. 2008, 77, 157-173. [CrossRef] <lb/>7. <lb/>Badrinarayanan, V.; Kendall, A.; Cipolla, R. Segnet: A deep convolutional encoder-decoder architecture for <lb/>image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2017, 39, 2481-2495. [CrossRef] <lb/>8. <lb/>Geiger, A.; Lenz, P.; Stiller, C.; Urtasun, R. Vision meets robotics: The KITTI dataset. Int. J. Rob. Res. 2013, <lb/>32, 1231-1237. [CrossRef] <lb/>9. <lb/>Zhang, C.; Wang, L.; Yang, R. Semantic segmentation of urban scenes using dense depth maps. <lb/>In Proceedings of the European Conference on Computer Vision, Crete, Greece, 5-11 September 2010; <lb/>pp. 708-721. <lb/>10. Cordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler, M.; Benenson, R.; Franke, U.; Roth, S.; Schiele, <lb/>B. The Cityscapes Dataset for Semantic Urban Scene Understanding. In Proceedings of the Conference on <lb/>Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June-1 July 2016. <lb/>11. Cordts, M.; Omran, M.; Ramos, S.; Scharwächter, T.; Enzweiler, M.; Benenson, R.; Franke, U.; Roth, S.; <lb/>Schiele, B. The cityscapes dataset. CVPR Workshop on the Future of Datasets in Vision, Boston, MA, USA, <lb/>11 June 2015; Volume 2. <lb/>12. EpicGames. Unreal Engine 4 Documentation. Available online: https://docs.unrealengine.com/en-US/ <lb/>index.html (accessed on 1 February 2020). <lb/>13. Dosovitskiy, A.; Ros, G.; Codevilla, F.; Lopez, A.; Koltun, V. CARLA: An open urban driving simulator. <lb/>arXiv 2017 arXiv:1711.03938. <lb/></listBibl>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>24 of 25 <lb/></page>

			<listBibl>14. Ros, G.; Sellart, L.; Materzynska, J.; Vazquez, D.; Lopez, A.M. The SYNTHIA Dataset: A Large Collection <lb/>of Synthetic Images for Semantic Segmentation of Urban Scenes. In Proceedings of the Conference on <lb/>Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June-1 July 2016. <lb/>15. Doan, A.D.; Jawaid, A.M.; Do, T.T.; Chin, T.J. G2D: From GTA to Data. arXiv 2018, arXiv:1806.07381. <lb/>16. Richter, S.R.; Hayder, Z.; Koltun, V. Playing for benchmarks. In Proceedings of the International Conference <lb/>on Computer Vision, Venice, Italy, 22-29 October 2017; pp. 2213-2222. <lb/>17. Richter, S.R.; Vineet, V.; Roth, S.; Koltun, V. Playing for data: Ground truth from computer games. <lb/>In proceedings of the European conference on computer vision, Amsterdam, The Netherland, 8-16 October <lb/>2016; pp.102-118. <lb/>18. Johnson-Roberson, M.; Barto, C.; Mehta, R.; Sridhar, S.N.; Rosaen, K.; Vasudevan, R. Driving in the matrix: <lb/>Can virtual worlds replace human-generated annotations for real world tasks? arXiv 2016 arXiv:1610.01983 . <lb/>19. Angus, M.; ElBalkini, M.; Khan, S.; Harakeh, A.; Andrienko, O.; Reading, C.; Waslander, S.; Czarnecki, K. <lb/>Unlimited road-scene synthetic annotation (URSA) dataset. In Proceedings of the International Conference <lb/>on Intelligent Transportation Systems, Lisbon, Portugal, 14-17 October 2018; pp. 985-992. <lb/>20. ARSekkat. OmniScape. Available online: https://github.com/ARSekkat/OmniScape (accessed on <lb/>9 February 2020). <lb/>21. McCormac, J.; Handa, A.; Leutenegger, S.; Davison, A.J. Scenenet rgb-d: Can 5m synthetic images beat <lb/>generic imagenet pre-training on indoor segmentation? In Proceedings of the International Conference on <lb/>Computer Vision, Venice, Italy, 22-29 October 2017; pp. 2678-2687. <lb/>22. Schneider, D.; Schwalbe, E.; Maas, H.G. Validation of geometric models for fisheye lenses. ISPRS J. <lb/>Photogramm. Remote Sens. 2009, 64, 259-266. [CrossRef] <lb/>23. Kingslake, R. A History of the Photographic Lens; Elsevier: Amsterdam, The Netherlands, 1989. <lb/>24. Baker, S.; Nayar, S.K. A theory of single-viewpoint catadioptric image formation. Int. J. Comput. Vis. 1999, <lb/>35, 175-196. [CrossRef] <lb/>25. Scaramuzza, D.; Martinelli, A.; Siegwart, R. A flexible technique for accurate omnidirectional camera <lb/>calibration and structure from motion. In Proceedings of the International Conference on Computer Vision <lb/>Systems, New York City, NY, USA, 5-7 January 2006; pp. 45-45. <lb/>26. Kannala, J.; Brandt, S.S. A generic camera model and calibration method for conventional, wide-angle, and <lb/>fish-eye lenses. IEEE Trans. Pattern Anal. Mach. Intell. 2006, 28, 1335-1340. [CrossRef] <lb/>27. Menem, M.; Pajdla, T. Constraints on perspective images and circular panoramas. In Proceedinsg of the <lb/>British Machine Vision Conference (BMVC), London, UK, 7-9 September 2004; pp. 1-10. <lb/>28. Agrawal, A.; Ramalingam, S. Single image calibration of multi-axial imaging systems. In Proceedings of <lb/>the IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, 23-28 June 2013; <lb/>pp. 1399-1406. <lb/>29. Lin, S.S.; Bajcsy, R. Single-view-point omnidirectional catadioptric cone mirror imager. IEEE Trans. Pattern <lb/>Anal. Mach. Intell. 2006, 28, 840-845. <lb/>30. POV-Ray. The Persistence of Vision Raytracer. Available online: http://www.povray.org/ (accessed on <lb/>16 March 2020). <lb/>31. Unity. Unity Engine. Available online: https://unity.com/ (accessed on 16 March 2020). <lb/>32. Qiu, W.; Zhong, F.; Zhang, Y.; Qiao, S.; Xiao, Z.; Kim, T.S.; Wang, Y. Unrealcv: Virtual worlds for computer <lb/>vision. In Proceedings of the ACM International Conference on Multimedia, Mountain View, CA, USA, <lb/>23-27 October 2017; pp. 1221-1224. <lb/>33. Greene, N. Environment mapping and other applications of world projections. IEEE Comput. Graph. Appl. <lb/>1986, 6, 21-29. [CrossRef] <lb/>34. Bermudez-Cameo, J.; Lopez-Nicolas, G.; Guerrero, J.J. Automatic line extraction in uncalibrated <lb/>omnidirectional cameras with revolution symmetry. Int. J. Comput. Vis. 2015, 114, 16-37. [CrossRef] <lb/>35. Bermudez-Cameo, J.; Lopez-Nicolas, G.; Guerrero, J.J. Fitting line projections in non-central catadioptric <lb/>cameras with revolution symmetry. Comput. Vis. Image Underst. 2018, 167, 134-152. [CrossRef] <lb/>36. Puig, L.; Bermúdez, J.; Sturm, P.; Guerrero, J.J. Calibration of omnidirectional cameras in practice: <lb/>A comparison of methods. Comput. Vis. Image Underst. 2012, 116, 120-137. [CrossRef] <lb/>37. Fernandez-Labrador, C.; Facil, J.M.; Perez-Yus, A.; Demonceaux, C.; Civera, J.; Guerrero, J. Corners for <lb/>layout: End-to-end layout recovery from 360 images. Rob. Autom. Lett. 2020, 5, 1255-1262. [CrossRef] <lb/></listBibl>

			<note place="headnote">Sensors 2020, 20, 2066 <lb/></note>

			<page>25 of 25 <lb/></page>

			<listBibl>38. Sumikura, S.; Shibuya, M.; Sakurada, K. OpenVSLAM: A Versatile Visual SLAM Framework. In Proceedings <lb/>of the ACM International Conference on Multimedia, Amherst, MA, USA, 18-21 June 2019; pp. 2292-2295. <lb/>39. Bermudez-Cameo, J.; Saurer, O.; Lopez-Nicolas, G.; Guerrero, J.J.; Pollefeys, M. Exploiting line metric <lb/>reconstruction from non-central circular panoramas. Pattern Recognit. Lett. 2017, 94, 30-37. [CrossRef] <lb/>40. Bermudez-Cameo, J.; Demonceaux, C.; Lopez-Nicolas, G.; Guerrero, J. Line Reconstruction Using <lb/>Prior Knowledge in Single Non-Central View. 2016. Available online: https://www.researchgate.net/ <lb/>publication/308656900_Line_reconstruction_using_prior_knowledge_in_single_non-central_view (accessed <lb/>on 16 March 2020). <lb/>41. Mur-Artal, R.; Montiel, J.M.M.; Tardos, J.D. ORB-SLAM: A versatile and accurate monocular SLAM system. <lb/>IEEE Trans. Rob. 2015, 31, 1147-1163. [CrossRef] <lb/>42. Schlegel, D.; Colosi, M.; Grisetti, G. Proslam: Graph SLAM from a programmer&apos;s perspective. In Proceedings <lb/>of the International Conference on Robotics and Automation (ICRA), Brisbane, Australia, 31 May-4 June <lb/>2018; pp. 1-9. <lb/>43. Teller, S.; Hohmeyer, M. Determining the lines through four lines. J. Graph. Tools 1999, 4, 11-22. [CrossRef] <lb/>44. Gasparini, S.; Caglioti, V. Line localization from single catadioptric images. Int. J. Comput. Vis. 2011, <lb/>94, 361-374. <lb/></listBibl>

			<front>c 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access <lb/>article distributed under the terms and conditions of the Creative Commons Attribution <lb/>(CC BY) license (http://creativecommons.org/licenses/by/4.0/). </front>


	</text>
</tei>
