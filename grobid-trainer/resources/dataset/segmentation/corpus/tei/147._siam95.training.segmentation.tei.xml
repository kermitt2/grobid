<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>A Portable Parallel N-body Solver <lb/>3 <lb/>E Christopher Lewis y Calvin Lin y Lawrence Snyder y George Turkiyyah z <lb/>Abstract <lb/>We present parallel solutions for direct and fast n-body solvers written in the ZPL <lb/>language. We describe the direct solver, compare its performance against a sequential <lb/>C program, and show performance results for two very dierent parallel machines: the <lb/>KSR-2 and the Paragon. We also discuss the implementation of the fast solver in ZPL, <lb/>including factors pertinent to data movement. <lb/></front>

			<body>1 Introduction <lb/>Parallelism is an important means of obtaining high performance, but parallel programs <lb/>are notoriously dicult to write. To reduce these programming costs, many high level <lb/>languages have been proposed by the computer science community. However, the portability <lb/>and performance of these languages have typically been established only for toy programs, <lb/>and these languages have not been embraced by engineers and scientists interested in high <lb/>performance computing. This paper demonstrates the feasibility of writing portable parallel <lb/>programs in a high level language, ZPL [5], to solve a realistic problem: the N-body solution <lb/>kernel of a high Reynolds number wind engineering simulation. Using ZPL, the parallel <lb/>application has a clean and concise solution that achieves good performance on two widely <lb/>dierent parallel architectures: the Kendall Square KSR-2 and the Intel Paragon. <lb/>The context of the problem is a wind engineering simulation for studying wind eects <lb/>on buildings [7]. The objective is to understand the temporal and spatial distributions <lb/>of the velocity and pressure elds around buildings and building complexes, and to assess <lb/>the signicance of geometric eects (building shape, nearby buildings, etc.) on the wind <lb/>response so that improved design recommendations can be developed. The simulation <lb/>uses a Lagrangian particle-based numerical scheme (vortex method) appropriate for uid <lb/>ows characterized by high Reynolds numbers and complex geometries. When coupled <lb/>with a fast solver for computing vortex interactions, vortex methods appear to have <lb/>several computational advantages over grid-based methods because they do not suer from <lb/>numerical diusion; they are simpler to implement, particularly for complex geometries; <lb/>and they are likely to exploit the architectures of distributed-memory computers more <lb/>eectively. <lb/>The simulation uses a random vortex method and is coupled with two N-body solvers <lb/>for computing the vortex interactions at each time step: One solver computes vortex <lb/>interactions in a thin region around the ground and building boundaries (a \numerical&quot; <lb/>boundary layer), while another handles the interactions in the exterior region of the ow. <lb/>This latter solver is by far the most computationally expensive part of the solution. Parallel <lb/></body>

			<front>3 This research was supported in part by ONR Contract N00014-92-J-1824, NASA grant NAG 2-831, and <lb/>NSF Contracts CDA-9211095 and DDM-929622. <lb/>y Dept. of Computer Science and Engineering, FR-35, University of Washington, Seattle, WA 98195 <lb/>z Dept. of Civil Engineering, FX-10, University of Washington, Seattle, WA 98195 <lb/></front>

			<page>1 <lb/>2 <lb/></page>

			<note place="headnote">Lewis et al. <lb/></note>

			<body>solutions to this component of the simulation are particularly important because meaningful <lb/>3D problems typically require hundreds of thousands of vortices. <lb/>This paper describes our preliminary implementation of portable, parallel solutions for <lb/>this N-body solver. After a brief introduction to ZPL, we describe the implementation <lb/>of a simple direct (O(n 2 )) version and show its performance characteristics. We then <lb/>describe the development of a fast version (O(n)) that exhibits a multigrid-like structure. <lb/>We describe the data structures we use and show how the evaluation of vortex interactions <lb/>can be eectively expressed in ZPL. We conclude by examining adaptive techniques that <lb/>would make the fast solver applicable to problems with more irregular point distributions. <lb/>2 The ZPL Language <lb/>ZPL is an array sublanguage that provides support for data parallel computations [5]. <lb/>As a sublanguage of Orca C|a lower level, more general language that supports MIMD <lb/>parallelism|ZPL is free to be extremely clean and concise, avoiding any complicating <lb/>features that do not pertain directly to data parallelism. In addition to most of the standard <lb/>control ow constructs and data types that are found in languages such as Pascal, ZPL <lb/>has the notion of ensemble arrays, which are given special support: Ensemble arrays are <lb/>distributed across processors and can be manipulated as whole entities using new language <lb/>constructs|regions, directions, and the At operator (@)|that eliminate tedious and error-<lb/>prone array indexing and clearly expose communication to the compiler. ZPL also provides <lb/>reduction and scan operators, as well as support for the clean specication of boundary <lb/>conditions. (ZPL also has standard arrays, which we refer to simply as \arrays.&quot;) <lb/>A region is an index set that is applied to an entire statement or block of statements. <lb/>The following code fragment shows the declaration of a region, R, and illustrates its use: <lb/>The elements of ensemble array B that are in the index set f1..Ng 2 f1..Mg are assigned <lb/>to the corresponding elements of the ensemble array A. <lb/>region R = [1..N, 1..M]; <lb/>[R] A := B; <lb/>A direction is a vector that is used with the At operator to shift an ensemble array reference <lb/>by some user-dened distance. For example, the following code fragment shifts the elements <lb/>of A to the right by one column. <lb/>direction west = [0,-1]; <lb/>[R] A := A@west; <lb/>Directions are also used to dene neighboring regions. For example, [west of R] refers to <lb/>the region that borders [R] to the left. <lb/>Scalar data types can be promoted to ensemble arrays. Similarly sequential functions <lb/>can be promoted, i.e., applied to ensemble array arguments, which encourages code re-use. <lb/>ZPL programs have sequential semantics, which allow them to be developed and <lb/>debugged on workstations. The portability and good performance of ZPL programs stem <lb/>from the parallel programming model|the Phase Abstractions [1, 3, 6]|upon which it is <lb/>built. This programming model encourages locality of reference and parameterized control <lb/>over granularity and communication. Previous studies have presented evidence that this <lb/>model supports portability across diverse parallel machines [4]. Finally, an important <lb/>feature of ZPL is the exibility to specify at runtime key parameters that can aect <lb/>communication granularity and data mapping. <lb/></body>

			<note place="headnote">A Portable Parallel N-body Solver <lb/></note>

			<page>3 <lb/></page>

			<body>3 Implementation of the Direct Solver <lb/>The simplest but most expensive strategy for computing vortex interactions computes all <lb/>pairwise interactions individually. Therefore, each vortex must accumulate the eect of <lb/>every other vortex. A general data structure for such an algorithm is a 1D ensemble array <lb/>of cells, where each cell is a list of particles. This can be expressed in ZPL as: <lb/>var <lb/>vortices: [R] array [1..M] of particle; <lb/>potential: [R] array [1..M] of double; <lb/>region R = [1..P]; <lb/>The size, P, of the ensemble array and the number of particles to place in each cell of <lb/>the array can be specied at run-time. This scheme provides exibility in choosing an <lb/>appropriate granularity of parallelism. One would select P to suit the target architecture, <lb/>for example to match the number of processors or the machine&apos;s ideal granularity of <lb/>communication. By default, the ZPL compiler maps 2D ensemble arrays to processors <lb/>in a 2D blocked fashion. <lb/>The algorithm consists of P iterations (shown below). Each iteration shifts the contents <lb/>of a cell to its neighbor using a torus topology. In ZPL the At operator shifts an entire cell&apos;s <lb/>contents, in this case M particles, and the cyclic shift is completed using the wrap operator, <lb/>which connects ends of an array as in a torus. <lb/>The computation portion of an iteration involves computing the pairwise interactions <lb/>of just two cells. The procedure add carrier effects() is a scalar procedure that is dened <lb/>on arrays. The parallelism comes implicitly from the ensemble array data structure. The <lb/>code itself is identical to what would be written for a scalar computation. It is a nested <lb/>loop to compute the eects of the particles in a visitor cell on the particles in the cell. <lb/>ZPL allows this function to be promoted to operate on ensemble arrays. This promotion <lb/>of scalar functions to array functions greatly simplies the programming process. <lb/>[R] begin <lb/>read_input(&quot;data&quot;, vortices); <lb/>potential := 0.0; <lb/>--initialize all particles of all cells <lb/>carrier := vortices <lb/>for i := 1 to P do <lb/>[left of R] wrap carrier; <lb/>--send right cell to left <lb/>carrier := carrier@left; <lb/>--shift cells left <lb/>add_carrier_effects(vortices, carrier, potential); <lb/>end; <lb/>write(phi); <lb/>end; <lb/>The ZPL compiler produces ANSI C code that can execute on any number of processors. <lb/>This output code is identical for all target machines but is linked with a small machine-<lb/>dependent library that denes operations such as message sends and receives. For shared <lb/>memory machines such as the KSR-2, message passing is implemented as shared queues. <lb/>The compiler currently performs no machine-specic optimizations. <lb/>Figure 1 shows that the direct solver achieves good speedup relative to the hand-coded <lb/>C version on the Intel Paragon. The Paragon is a mesh-connected distributed memory <lb/>computer with Intel i860 processors and 16MB of memory per node. The ZPL program <lb/>running on one processor is 7.9% slower than the C version. With 16 processors the speedup <lb/>is 13.76, and the relative speedup (based on the ZPL program running on one processor) <lb/>is 14.85. <lb/></body>

			<page>4 <lb/></page>

			<note place="headnote">Lewis et al. <lb/></note>

			<body>The KSR-2 is a shared memory multiprocessor with a ring of rings interconnection <lb/>structure. Each processor runs at 40Mhz and has 32MB of local memory. On the KSR-<lb/>2 the ZPL program is 15% slower than C. For 16 processors the speedup relative to C is <lb/>11.47, and the relative speedup is 13.23. There are two reasons for the lower speedup on the <lb/>KSR. The processor is faster, making the cost of communication relatively more expensive, <lb/>and our current message passing implementation copies data more times than is necessary. <lb/>With larger problem sizes the speedup will naturally improve. <lb/>2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>12 <lb/>14 <lb/>16 <lb/>2 <lb/>4 <lb/>6 <lb/>8 10 12 14 16 <lb/>speedup <lb/>processors <lb/>linear <lb/>Paragon <lb/>KSR-2 <lb/>Fig. 1. Speedup for the Direct Code (6000 particles) <lb/>4 Implementation of a Fast Solver <lb/>A direct method that evaluates all interactions is prohibitively expensive for realistic <lb/>simulations. Fast solvers exploit the fact that the eect of a neighboring particle decreases <lb/>as its distance increases. This section describes the implementation of a fast vortex method <lb/>that requires in principle a linear amount of work. Computational savings in the fast solver <lb/>are obtained by combining large numbers of particles into a small set of discrete values (a <lb/>ring) whose eect approximates the eect of the cluster of particles. There are two kinds <lb/>of cluster approximations: \outer-rings&quot; that represent the eect of a cluster in the far <lb/>eld and \inner rings&quot; that represent the eect of far-away particles in the near eld. Fast <lb/>solvers construct and evaluate these cluster approximations in a hierarchical fashion, much <lb/>like multigrid solvers. The approximations we use are based on Poisson&apos;s formula [2]. <lb/>The data structure consists of a hierarchy of distributed grids that store these ring <lb/>approximations and transfer information between adjacent levels. The particles are stored <lb/>at the nest level as an ensemble array of lists of particles, as shown below. <lb/>region R3 = [1..N/2, 1..M/2]; <lb/>--N and M can be set at runtime <lb/>R4 = [1..N, 1..M]; <lb/>type <lb/>part_list = record <lb/>count: integer; <lb/>list : array [1..Max] of particle; <lb/>end; <lb/>var <lb/>vortices : [R4] part_list; <lb/>ring4 <lb/>: [R4] ring; <lb/>ring3 <lb/>: [R3] ring; <lb/></body>

			<note place="headnote">A Portable Parallel N-body Solver <lb/></note>

			<page>5 <lb/></page>

			<body>direction sw2 = [2, -2]; <lb/>north = [-1, 0]; <lb/>The implementation of this solver is similar to that of one V-cycle of a multigrid method. <lb/>As shown below, a rst sweep starts from the nest level and builds outer approximations <lb/>of the velocity vector at all levels. A second sweep builds inner approximations from the <lb/>coarsest level down to the nest. These sweeps require inter-level communication. In <lb/>the descent phase, intra-level communication is needed to compute ring-ring interactions <lb/>between a cell and its well-separated neighbors whose parents are not well-separated from <lb/>the cell&apos;s parent. Such neighbors are at most three cells away. The eects of particles in <lb/>neighboring cells are computed directly at the nest level. <lb/>procedure fast(); <lb/>[R4] begin <lb/>initialize(); <lb/>ORAfinest(vortices, ring4); <lb/>go_up_43(ring3, ring4); <lb/>--inter-level communication <lb/>[R3] <lb/>go_up_32(ring2, ring3); <lb/>[Expand2(R2)] visit_2_neighbors(ring2); <lb/>--intra-level communication <lb/>[Expand3(R3)] visit_3_neighbors(ring2); <lb/>--intra-level communication <lb/>[R3] <lb/>go_down_23(ring2, ring3); <lb/>. . . <lb/>neighbor_contributions(vortices); --nearest neighbor communication <lb/>end; <lb/>Fig. 2. Intra-level Communication with 2-Neighbors. <lb/>Figure 2 shows the intra-level communication with 2-neighbors. The corresponding <lb/>ZPL code is shown below. First, for each cell, the southwest neighboring cell is copied to <lb/>the local cell. Then, a sweep is made around the box, accumulating the eects of ring-ring <lb/>interactions. To specify the data motion corresponding to the thick arrow in Figure 2, the <lb/>visit 2 neighbors() function is logically invoked in the shaded region shown in Figure 2 <lb/>(this is expressed above using a macro called Expand2 that expands to a list of regions). <lb/>Note that the same code applies to all cells, regardless of boundary conditions. The compiler <lb/>does not generate communication for \neighbors&quot; that lie outside the data space (i.e., when <lb/>the thick arrow enters the shaded region). The 3-neighbor communication is analogous, <lb/>although not all 3-neighbors are needed. <lb/>procedure visit_2_neighbors(var ring: [2] Box); --ring is a 2D ensemble array <lb/>var tmp : [R] Box; <lb/>i : integer; <lb/>begin <lb/></body>

			<page>6 <lb/></page>

			<note place="headnote">Lewis et al. <lb/></note>

			<body>tmp := ring; <lb/>--add the contributions of the 2-neighbors <lb/>tmp := tmp@sw2; <lb/>--translate to southwest <lb/>for i := 1 to 4 do <lb/>tmp := tmp@north; <lb/>add_contributions_OR_lm(ring, tmp); <lb/>end; <lb/>. . . <lb/>--repeat for north, east, south and west <lb/>end; <lb/>5 Conclusions <lb/>ZPL allows the elegant expression|even when compared against sequential programs|of <lb/>both the direct and fast N-body solvers. The direct solver achieved good performance on <lb/>two radically dierent architectures. We also expect good speedup for the fast solver. <lb/>In a complete wind simulation the fast solver must be invoked at each time step. <lb/>Each time step introduces new particles to the simulation to satisfy appropriate boundary <lb/>conditions. Furthermore, as particles already present in the simulation domain move, they <lb/>may cross cell boundaries of the nest grid level or may leave the domain altogether. <lb/>Particles must therefore be redistributed for the next time step of the N-body solver. Our <lb/>ZPL implementation species the movement of these particles across the simulation domain, <lb/>but leaves the details of inter-processor communication to the compiler. These will be shown <lb/>in an upcoming report. <lb/>A general issue with all multi-level computations is the mapping of the dierent levels <lb/>of the hierarchy to processors. For example, grids can be aligned eccentrically to minimize <lb/>communication (as shown above in our region denitions) or aligned concentrically to <lb/>maximize parallelism. The best choice will likely depend on the architecture. ZPL allows <lb/>these mappings to be specied at runtime, and we intend to explore these issues on various <lb/>machines. <lb/></body>

			<listBibl>References <lb/>[1] Gail Alverson, William Griswold, David Notkin, and Lawrence Snyder. A exible communica-<lb/>tion abstraction for nonshared memory parallel computing. In Proceedings of Supercomputing <lb/>&apos;90, November 1990. <lb/>[2] Christopher R. Anderson. An implementation of the fast multipole method without multipoles. <lb/>SIAM Journal of Sci. Stat. Computing, 13(4):923{947, July 1992. <lb/>[3] William Griswold, Gail Harrison, David Notkin, and Lawrence Snyder. Scalable abstractions <lb/>for parallel programming. In Proceedings of the Fifth Distributed Memory Computing <lb/>Conference, 1990. Charleston, South Carolina. <lb/>[4] Calvin Lin and Lawrence Snyder. A portable implementation of SIMPLE. International <lb/>Journal of Parallel Programming, 20(5):363{401, 1991. <lb/>[5] Calvin Lin and Lawrence Snyder. ZPL: An array sublanguage. In Uptal Banerjee, David <lb/>Gelernter, Alexandru Nicolau, and David Padua, editors, Languages and Compilers for Parallel <lb/>Computing, pages 96{114. Springer-Verlag, 1993. <lb/>[6] Lawrence Snyder. Foundations of practical parallel programming languages. In Proceedings <lb/>of the Second International Conference of the Austrian Center for Parallel Computation. <lb/>Springer-Verlag, 1993. <lb/>[7] George Turkiyyah, Dorothy Reed, and Jiyao Yang. Fast vortex methods for predicting wind-<lb/>induced pressures on building systems. submitted for publication, 1993. </listBibl>


	</text>
</tei>
