<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>1 Introduction<lb/></head>

			<p>The last year has brought enormous leaps in deep generative modeling across various data domains,<lb/> such as natural language <ref type="biblio">[50]</ref>, audio <ref type="biblio">[17]</ref>, and visual media <ref type="biblio">[38, 37, 40, 44, 15, 3, 7]</ref>. In this report,<lb/> we focus on the latter and unveil SDXL, a drastically improved version of Stable Diffusion. Stable<lb/> Diffusion is a latent text-to-image diffusion model (DM) which serves as the foundation for an<lb/> array of recent advancements in, e.g., 3D classification <ref type="biblio">[43]</ref>, controllable image editing <ref type="biblio">[54]</ref>, image<lb/> personalization <ref type="biblio">[10]</ref>, synthetic data augmentation <ref type="biblio">[48]</ref>, graphical user interface prototyping <ref type="biblio">[51]</ref>, etc.<lb/> Remarkably, the scope of applications has been extraordinarily extensive, encompassing fields as<lb/> diverse as music generation <ref type="biblio">[9]</ref> and reconstructing images from fMRI brain scans <ref type="biblio">[49]</ref>.<lb/></p>

			<p>User studies demonstrate that SDXL consistently surpasses all previous versions of Stable Diffusion<lb/> by a significant margin (see Fig. <ref type="figure">1</ref>). In this report, we present the design choices which lead to this<lb/> boost in performance encompassing i) a 3× larger UNet-backbone compared to previous Stable<lb/> Diffusion models (Sec. 2.1), ii) two simple yet effective additional conditioning techniques (Sec. 2.2)<lb/> which do not require any form of additional supervision, and iii) a separate diffusion-based refinement<lb/> model which applies a noising-denoising process <ref type="biblio">[28]</ref> to the latents produced by SDXL to improve<lb/> the visual quality of its samples (Sec. 2.5).<lb/></p>

			<p>A major concern in the field of visual media creation is that while black-box-models are often<lb/> recognized as state-of-the-art, the opacity of their architecture prevents faithfully assessing and<lb/> validating their performance. This lack of transparency hampers reproducibility, stifles innovation,<lb/> and prevents the community from building upon these models to further the progress of science and<lb/> art. Moreover, these closed-source strategies make it challenging to assess the biases and limitations<lb/> of these models in an impartial and objective way, which is crucial for their responsible and ethical<lb/> deployment. With SDXL we are releasing an open model that achieves competitive performance with<lb/> black-box image generation models (see Fig. <ref type="figure">10</ref> &amp; Fig. <ref type="figure">11</ref>).<lb/></p>

			<head>2 Improving Stable Diffusion<lb/></head>

			<p>In this section we present our improvements for the Stable Diffusion architecture. These are modular,<lb/> and can be used individually or together to extend any model. Although the following strategies are<lb/> implemented as extensions to latent diffusion models (LDMs) <ref type="biblio">[38]</ref>, most of them are also applicable<lb/> to their pixel-space counterparts.<lb/></p>

			<figure>Figure 1: Left: Comparing user preferences between SDXL and Stable Diffusion 1.5 &amp; 2.1. While SDXL already<lb/> clearly outperforms Stable Diffusion 1.5 &amp; 2.1, adding the additional refinement stage boosts performance. Right:<lb/>Visualization of the two-stage pipeline: We generate initial latents of size 128 × 128 using SDXL. Afterwards,<lb/> we utilize a specialized high-resolution refinement model and apply SDEdit <ref type="biblio">[28]</ref> on the latents generated in the<lb/> first step, using the same prompt. SDXL and the refinement model use the same autoencoder.<lb/></figure>

			<head>2.1 Architecture &amp; Scale<lb/></head>

			<p>Starting with the seminal works Ho et al. <ref type="biblio">[14]</ref> and Song et al. <ref type="biblio">[47]</ref>, which demonstrated that DMs<lb/> are powerful generative models for image synthesis, the convolutional UNet <ref type="biblio">[39]</ref> architecture has<lb/> been the dominant architecture for diffusion-based image synthesis. However, with the development<lb/></p>

			<figure type="table">Table 1: Comparison of SDXL and older Stable Diffusion models.<lb/> Model<lb/> SDXL<lb/> SD 1.4/1.5<lb/> SD 2.0/2.1<lb/> # of UNet params<lb/> 2.6B<lb/> 860M<lb/> 865M<lb/> Transformer blocks<lb/> [0, 2, 10]<lb/> [1, 1, 1, 1]<lb/> [1, 1, 1, 1]<lb/> Channel mult.<lb/> [1, 2, 4]<lb/> [1, 2, 4, 4]<lb/> [1, 2, 4, 4]<lb/> Text encoder<lb/> CLIP ViT-L &amp; OpenCLIP ViT-bigG CLIP ViT-L OpenCLIP ViT-H<lb/> Context dim.<lb/> 2048<lb/> 768<lb/> 1024<lb/> Pooled text emb.<lb/>OpenCLIP ViT-bigG<lb/> N/A<lb/> N/A<lb/></figure>

			<p>of foundational DMs <ref type="biblio">[40, 37, 38]</ref>, the underlying architecture has constantly evolved: from adding<lb/> self-attention and improved upscaling layers <ref type="biblio">[5]</ref>, over cross-attention for text-to-image synthesis <ref type="biblio">[38]</ref>,<lb/> to pure transformer-based architectures <ref type="biblio">[33]</ref>.<lb/></p>

			<p>We follow this trend and, following Hoogeboom et al. <ref type="biblio">[16]</ref>, shift the bulk of the transformer com-<lb/>putation to lower-level features in the UNet. In particular, and in contrast to the original Stable<lb/> Diffusion architecture, we use a heterogeneous distribution of transformer blocks within the UNet:<lb/> For efficiency reasons, we omit the transformer block at the highest feature level, use 2 and 10<lb/> blocks at the lower levels, and remove the lowest level (8× downsampling) in the UNet altogether<lb/> -see Tab. <ref type="table">1</ref> for a comparison between the architectures of Stable Diffusion 1.x &amp; 2.x and SDXL.<lb/> We opt for a more powerful pre-trained text encoder that we use for text conditioning. Specifically,<lb/> we use OpenCLIP ViT-bigG <ref type="biblio">[19]</ref> in combination with CLIP ViT-L <ref type="biblio">[34]</ref>, where we concatenate the<lb/> penultimate text encoder outputs along the channel-axis <ref type="biblio">[1]</ref>. Besides using cross-attention layers to<lb/> condition the model on the text-input, we follow <ref type="biblio">[30]</ref> and additionally condition the model on the<lb/> pooled text embedding from the OpenCLIP model. These changes result in a model size of 2.6B<lb/> parameters in the UNet, see Tab. <ref type="table">1</ref>. The text encoders have a total size of 817M parameters.<lb/></p>

			<head>2.2 Micro-Conditioning<lb/></head>

			<figure>Figure 2:<lb/> Height-vs-Width distribution of our<lb/> pre-training dataset. Without the proposed size-<lb/>conditioning, 39% of the data would be discarded due<lb/> to edge lengths smaller than 256 pixels as visualized<lb/> by the dashed black lines. Color intensity in each visu-<lb/>alized cell is proportional to the number of samples.<lb/></figure>

			<head>Conditioning the Model on Image Size</head>

			<p>A no-<lb/>torious shortcoming of the LDM paradigm <ref type="biblio">[38]</ref> is<lb/> the fact that training a model requires a minimal<lb/> image size, due to its two-stage architecture. The<lb/> two main approaches to tackle this problem are ei-<lb/>ther to discard all training images below a certain<lb/> minimal resolution (for example, Stable Diffu-<lb/>sion 1.4/1.5 discarded all images with any size<lb/> below 512 pixels), or, alternatively, upscale im-<lb/>ages that are too small. However, depending on<lb/> the desired image resolution, the former method<lb/> can lead to significant portions of the training<lb/> data being discarded, what will likely lead to a<lb/> loss in performance and hurt generalization. We<lb/> visualize such effects in Fig. <ref type="figure">2</ref> for the dataset on<lb/> which SDXL was pretrained. For this particular<lb/> choice of data, discarding all samples below our<lb/> pretraining resolution of 256 2 pixels would lead<lb/> to a significant 39% of discarded data. The second method, on the other hand, usually introduces<lb/> upscaling artifacts which may leak into the final model outputs, causing, for example, blurry samples.<lb/></p>

			<p>Instead, we propose to condition the UNet model on the original image resolution, which is trivially<lb/> available during training. In particular, we provide the original (i.e., before any rescaling) height<lb/> and width of the images as an additional conditioning to the model c size = (h original , w original ).<lb/> Each component is independently embedded using a Fourier feature encoding, and these encodings<lb/> are concatenated into a single vector that we feed into the model by adding it to the timestep<lb/> embedding <ref type="biblio">[5]</ref>.<lb/></p>

			<p>At inference time, a user can then set the desired apparent resolution of the image via this size-<lb/>conditioning. Evidently (see Fig. <ref type="figure">3</ref>), the model has learned to associate the conditioning c size with<lb/></p>

			<figure>c size = (64, 64)<lb/> c size = (128, 128),<lb/> c size = (256, 236),<lb/> c size = (512, 512),<lb/> &apos;A robot painted as graffiti on a brick wall. a sidewalk is in front of the wall, and grass is growing out of cracks in the concrete.&apos;<lb/> &apos;Panda mad scientist mixing sparkling chemicals, artstation.&apos;<lb/> Figure 3: The effects of varying the size-conditioning: We show draw 4 samples with the same random seed from<lb/> SDXL and vary the size-conditioning as depicted above each column. The image quality clearly increases when<lb/> conditioning on larger image sizes. Samples from the 512 2 model, see Sec. 2.5. Note: For this visualization, we<lb/> use the 512 × 512 pixel base model (see Sec. 2.5), since the effect of size conditioning is more clearly visible<lb/> before 1024 × 1024 finetuning. Best viewed zoomed in.<lb/></figure>

			<p>resolution-dependent image features, which can be leveraged to modify the appearance of an output<lb/> corresponding to a given prompt. Note that for the visualization shown in Fig. <ref type="figure">3</ref>, we visualize samples<lb/> generated by the 512 × 512 model (see Sec. 2.5 for details), since the effects of the size conditioning<lb/> are less clearly visible after the subsequent multi-aspect (ratio) finetuning which we use for our final<lb/> SDXL model.<lb/></p>

			<figure type="table">Table 2: Conditioning on the original spatial<lb/> size of the training examples improves per-<lb/>formance on class-conditional ImageNet [4]<lb/> on 512 2 resolution.<lb/> model<lb/> FID-5k ↓ IS-5k ↑<lb/> CIN-512-only<lb/> 43.84<lb/> 110.64<lb/> CIN-nocond<lb/> 39.76<lb/> 211.50<lb/> CIN-size-cond<lb/> 36.53<lb/> 215.34<lb/></figure>

			<p>We quantitatively assess the effects of this simple but ef-<lb/>fective conditioning technique by training and evaluating<lb/> three LDMs on class conditional ImageNet <ref type="biblio">[4]</ref> at spatial<lb/> size 512 2 : For the first model (CIN-512-only) we discard<lb/> all training examples with at least one edge smaller than<lb/> 512 pixels what results in a train dataset of only 70k im-<lb/>ages. For CIN-nocond we use all training examples but<lb/> without size conditioning. This additional conditioning is<lb/> only used for CIN-size-cond. After training we generate<lb/> 5k samples with 50 DDIM steps <ref type="biblio">[46]</ref> and (classifier-free)<lb/> guidance scale of 5 <ref type="biblio">[13]</ref> for every model and compute IS <ref type="biblio">[42]</ref> and FID <ref type="biblio">[12]</ref> (against the full val-<lb/>idation set). For CIN-size-cond we generate samples always conditioned on c size = (512, 512).<lb/> Tab. <ref type="table">2</ref> summarizes the results and verifies that CIN-size-cond improves upon the baseline models in<lb/> both metrics. We attribute the degraded performance of CIN-512-only to bad generalization due to<lb/> overfitting on the small training dataset while the effects of a mode of blurry samples in the sample<lb/> distribution of CIN-nocond result in a reduced FID score. Note that, although we find these classical<lb/> quantitative scores not to be suitable for evaluating the performance of foundational (text-to-image)<lb/> DMs <ref type="biblio">[40, 37, 38]</ref> (see App. F), they remain reasonable metrics on ImageNet as the neural backbones<lb/> of FID and IS have been trained on ImageNet itself.<lb/></p>

			<head>Conditioning the Model on Cropping Parameters</head>

			<p>The first two rows of Fig. <ref type="figure">4</ref> illustrate a typical<lb/> failure mode of previous SD models: Synthesized objects can be cropped, such as the cut-off head of<lb/> the cat in the left examples for SD 1-5 and SD 2-1. An intuitive explanation for this behavior is the<lb/> use of random cropping during training of the model: As collating a batch in DL frameworks such as<lb/></p>

			<figure>&apos;A propaganda poster depicting a cat dressed as french<lb/> emperor napoleon holding a piece of cheese.&apos;<lb/> &apos;a close-up of a fire spitting dragon,<lb/> cinematic shot.&apos;<lb/> SD 1-5<lb/> SD 2-1<lb/> SDXL<lb/> Figure 4: Comparison of the output of SDXL with previous versions of Stable Diffusion. For each prompt, we<lb/> show 3 random samples of the respective model for 50 steps of the DDIM sampler <ref type="biblio">[46]</ref> and cfg-scale 8.0 <ref type="biblio">[13]</ref>. Additional samples in Fig. <ref type="figure">14</ref>.<lb/></figure>

			<p>PyTorch <ref type="biblio">[32]</ref> requires tensors of the same size, a typical processing pipeline is to (i) resize an image<lb/> such that the shortest size matches the desired target size, followed by (ii) randomly cropping the<lb/> image along the longer axis. While random cropping is a natural form of data augmentation, it can<lb/> leak into the generated samples, causing the malicious effects shown above.<lb/></p>

			<p>To fix this problem, we propose another simple yet effective conditioning method: During dataloading,<lb/> we uniformly sample crop coordinates c top and c left (integers specifying the amount of pixels cropped<lb/> from the top-left corner along the height and width axes, respectively) and feed them into the model<lb/> as conditioning parameters via Fourier feature embeddings, similar to the size conditioning described<lb/> above. The concatenated embedding c crop is then used as an additional conditioning parameter.<lb/> We emphasize that this technique is not limited to LDMs and could be used for any DM. Note that<lb/> crop-and size-conditioning can be readily combined. In such a case, we concatenate the feature<lb/> embedding along the channel dimension, before adding it to the timestep embedding in the UNet.<lb/> Alg. 1 illustrates how we sample c crop and c size during training if such a combination is applied.<lb/></p>

			<p>Given that in our experience large scale datasets are, on average, object-centric, we set (c top , c left ) =<lb/> (0, 0) during inference and thereby obtain object-centered samples from the trained model.<lb/></p>

			<p>See Fig. <ref type="figure">5</ref> for an illustration: By tuning (c top , c left ), we can successfully simulate the amount of<lb/> cropping during inference. This is a form of conditioning-augmentation, and has been used in various<lb/> forms with autoregressive <ref type="biblio">[20]</ref> models, and more recently with diffusion models <ref type="biblio">[21]</ref>.<lb/></p>

			<p>While other methods like data bucketing <ref type="biblio">[31]</ref> successfully tackle the same task, we still benefit from<lb/> cropping-induced data augmentation, while making sure that it does not leak into the generation<lb/> process -we actually use it to our advantage to gain more control over the image synthesis process.<lb/> Furthermore, it is easy to implement and can be applied in an online fashion during training, without<lb/> additional data preprocessing.<lb/></p>

			<head>2.3 Multi-Aspect Training<lb/></head>

			<p>Real-world datasets include images of widely varying sizes and aspect-ratios (c.f. fig. <ref type="figure">2</ref>) While the<lb/> common output resolutions for text-to-image models are square images of 512 × 512 or 1024 × 1024<lb/> pixels, we argue that this is a rather unnatural choice, given the widespread distribution and use of<lb/> landscape (e.g., 16:9) or portrait format screens.<lb/></p>

			<p>Motivated by this, we finetune our model to handle multiple aspect-ratios simultaneously: We follow<lb/> common practice <ref type="biblio">[31]</ref> and partition the data into buckets of different aspect ratios, where we keep the<lb/> pixel count as close to 1024 2 pixels as possibly, varying height and width accordingly in multiples<lb/></p>

		<!-- not sure how to annotate this -->
			<figure>Algorithm 1 Conditioning pipeline for size-and crop-conditioning<lb/> Require: Training dataset of images D, target image size for training s = (h tgt , w tgt )<lb/> Require: Resizing function R, cropping function function C<lb/> Require: Model train step T<lb/> converged ← False<lb/> while not converged do<lb/> x ∼ D<lb/> w original ← width(x)<lb/> h original ← height(x)<lb/> c size ← (h original , w original )<lb/> x ← R(x, s)<lb/> ▷ resize smaller image size to target size s<lb/> if h original ≤ w original then<lb/> c left ∼ U (0, width(x) -s w )<lb/> ▷ sample c left from discrete uniform distribution<lb/> c top = 0<lb/> else if h original &gt; w original then<lb/> c top ∼ U (0, height(x) -s h )<lb/> ▷ sample c top from discrete uniform distribution<lb/> c left = 0<lb/> end if<lb/> c crop ← (c top , c left )<lb/> x ← C(x, s, c crop )<lb/> ▷ crop image to size s with top-left coordinate (c top , c left )<lb/> converged ← T (x, c size , c crop )<lb/> ▷ train model conditioned on c size and c crop<lb/> end while<lb/></figure>

		<figure>c crop = (0, 0)<lb/> c crop = (0, 256),<lb/> c crop = (256, 0),<lb/> c crop = (512, 512),<lb/> &apos;An astronaut riding a pig, highly realistic dslr photo, cinematic shot.&apos;<lb/> &apos;A capybara made of lego sitting in a realistic, natural field.&apos;<lb/> Figure 5: Varying the crop conditioning as discussed in Sec. 2.2. See Fig. 4 and Fig. 14 for samples from SD 1.5<lb/> and SD 2.1 which provide no explicit control of this parameter and thus introduce cropping artifacts. Samples<lb/> from the 512 2 model, see Sec. 2.5.<lb/></figure>

			<p>of 64. A full list of all aspect ratios used for training is provided in App. I. During optimization,<lb/> a training batch is composed of images from the same bucket, and we alternate between bucket<lb/> sizes for each training step. Additionally, the model receives the bucket size (or, target size) as a<lb/> conditioning, represented as a tuple of integers c ar = (h tgt , w tgt ) which are embedded into a Fourier<lb/> space in analogy to the size-and crop-conditionings described above.<lb/></p>

			<p>In practice, we apply multi-aspect training as a finetuning stage after pretraining the model at a<lb/> fixed aspect-ratio and resolution and combine it with the conditioning techniques introduced in<lb/> Sec. 2.2 via concatenation along the channel axis. Fig. <ref type="figure">16</ref> in App. J provides python-code for this<lb/> operation. Note that crop-conditioning and multi-aspect training are complementary operations, and<lb/> crop-conditioning then only works within the bucket boundaries (usually 64 pixels). For ease of<lb/> implementation, however, we opt to keep this control parameter for multi-aspect models.<lb/></p>

			<head>2.4 Improved Autoencoder<lb/></head>

			<figure type="table">Table 3: Autoencoder reconstruction performance on<lb/> the COCO2017 [26] validation split, images of size<lb/> 256 × 256 pixels. Note: Stable Diffusion 2.x uses an<lb/> improved version of Stable Diffusion 1.x&apos;s autoencoder,<lb/> where the decoder was finetuned with a reduced weight<lb/> on the perceptual loss [55], and used more compute.<lb/> Note that our new autoencoder is trained from scratch.<lb/> model<lb/> PNSR ↑ SSIM ↑ LPIPS ↓ rFID ↓<lb/> SDXL-VAE<lb/> 24.7<lb/> 0.73<lb/> 0.88<lb/> 4.4<lb/> SD-VAE 1.x<lb/> 23.4<lb/> 0.69<lb/> 0.96<lb/> 5.0<lb/> SD-VAE 2.x<lb/> 24.5<lb/> 0.71<lb/> 0.92<lb/> 4.7<lb/></figure>

			<p>Stable Diffusion is a LDM, operating in a pre-<lb/>trained, learned (and fixed) latent space of an<lb/> autoencoder. While the bulk of the semantic<lb/> composition is done by the LDM <ref type="biblio">[38]</ref>, we can<lb/> improve local, high-frequency details in gener-<lb/>ated images by improving the autoencoder. To<lb/> this end, we train the same autoencoder archi-<lb/>tecture used for the original Stable Diffusion at<lb/> a larger batch-size (256 vs 9) and additionally<lb/> track the weights with an exponential moving<lb/> average. The resulting autoencoder outperforms the original model in all evaluated reconstruction<lb/> metrics, see Tab. <ref type="table">3</ref>. We use this autoencoder for all of our experiments.<lb/></p>

			<head>2.5 Putting Everything Together<lb/></head>

			<p>We train the final model, SDXL, in a multi-stage procedure. SDXL uses the autoencoder from Sec. 2.4<lb/> and a discrete-time diffusion schedule <ref type="biblio">[14, 45]</ref> with 1000 steps. First, we pretrain a base model<lb/> (see Tab. <ref type="table">1</ref>) on an internal dataset whose height-and width-distribution is visualized in Fig. <ref type="figure">2</ref> for<lb/> 600 000 optimization steps at a resolution of 256 × 256 pixels and a batch-size of 2048, using size-<lb/>and crop-conditioning as described in Sec. 2.2. We continue training on 512 × 512 pixel images for<lb/> another 200 000 optimization steps, and finally utilize multi-aspect training (Sec. 2.3) in combination<lb/> with an offset-noise <ref type="biblio">[11, 25]</ref> level of 0.05 to train the model on different aspect ratios (Sec. 2.3,<lb/> App. I) of ∼ 1024 × 1024 pixel area.<lb/></p>

			<head>Refinement Stage </head>
			<p>Empirically, we find that the resulting model sometimes yields samples of low<lb/> local quality, see Fig. <ref type="figure">6</ref>. To improve sample quality, we train a separate LDM in the same latent space,<lb/> which is specialized on high-quality, high resolution data and employ a noising-denoising process as<lb/> introduced by SDEdit <ref type="biblio">[28]</ref> on the samples from the base model. We follow <ref type="biblio">[1]</ref> and specialize this<lb/> refinement model on the first 200 (discrete) noise scales. During inference, we render latents from<lb/> the base SDXL, and directly diffuse and denoise them in latent space with the refinement model (see<lb/> Fig. <ref type="figure">1</ref>), using the same text input. We note that this step is optional, but improves sample quality for<lb/> detailed backgrounds and human faces, as demonstrated in Fig. <ref type="figure">6</ref> and Fig. <ref type="figure">13</ref>.<lb/></p>

			<p>To assess the performance of our model (with and without refinement stage), we conduct a user<lb/> study, and let users pick their favorite generation from the following four models: SDXL, SDXL<lb/> (with refiner), Stable Diffusion 1.5 and Stable Diffusion 2.1. The results demonstrate the SDXL with<lb/> the refinement stage is the highest rated choice, and outperforms Stable Diffusion 1.5 &amp; 2.1 by a<lb/> significant margin (win rates: SDXL w/ refinement: 48.44%, SDXL base: 36.93%, Stable Diffusion<lb/> 1.5: 7.91%, Stable Diffusion 2.1: 6.71%). See Fig. <ref type="figure">1</ref>, which also provides an overview of the full<lb/> pipeline. However, when using classical performance metrics such as FID and CLIP scores the<lb/> improvements of SDXL over previous methods are not reflected as shown in Fig. <ref type="figure">12</ref> and discussed in<lb/> App. F. This aligns with and further backs the findings of Kirstain et al. <ref type="biblio">[23]</ref>.<lb/></p>

			<figure>Figure 6: 1024 2 samples (with zoom-ins) from SDXL without (left) and with (right) the refinement model<lb/> discussed. Prompt: &quot;Epic long distance cityscape photo of New York City flooded by the ocean and overgrown<lb/> buildings and jungle ruins in rainforest, at sunset, cinematic shot, highly detailed, 8k, golden light&quot;. See Fig. <ref type="figure">13</ref><lb/> for additional samples.<lb/></figure>

			<head>3 Future Work<lb/></head>

			<p>This report presents a preliminary analysis of improvements to the foundation model Stable Diffusion<lb/> for text-to-image synthesis. While we achieve significant improvements in synthesized image quality,<lb/> prompt adherence and composition, in the following, we discuss a few aspects for which we believe<lb/> the model may be improved further:<lb/></p>

			<list>
				<item>• Single stage: Currently, we generate the best samples from SDXL using a two-stage approach<lb/> with an additional refinement model. This results in having to load two large models into<lb/> memory, hampering accessibility and sampling speed. Future work should investigate ways<lb/> to provide a single stage of equal or better quality.<lb/></item>
				<item>• Text synthesis: While the scale and the larger text encoder (OpenCLIP ViT-bigG <ref type="biblio">[19]</ref>)<lb/> help to improve the text rendering capabilities over previous versions of Stable Diffusion,<lb/> incorporating byte-level tokenizers <ref type="biblio">[52, 27]</ref> or simply scaling the model to larger sizes <ref type="biblio">[53,<lb/> 40]</ref> may further improve text synthesis.<lb/></item>
				<item>• Architecture: During the exploration stage of this work, we briefly experimented with<lb/> transformer-based architectures such as UViT <ref type="biblio">[16]</ref> and DiT <ref type="biblio">[33]</ref>, but found no immediate<lb/> benefit. We remain, however, optimistic that a careful hyperparameter study will eventually<lb/> enable scaling to much larger transformer-dominated architectures.<lb/></item>
				<item>• Distillation: While our improvements over the original Stable Diffusion model are significant,<lb/> they come at the price of increased inference cost (both in VRAM and sampling speed).<lb/> Future work will thus focus on decreasing the compute needed for inference, and increased<lb/> sampling speed, for example through guidance-<ref type="biblio">[29]</ref>, knowledge-<ref type="biblio">[6, 22, 24]</ref> and progressive<lb/> distillation <ref type="biblio">[41, 2, 29</ref>].<lb/></item>
				<item>• Our model is trained in the discrete-time formulation of <ref type="biblio">[14]</ref>, and requires offset-noise <ref type="biblio">[11,<lb/> 25]</ref> for aesthetically pleasing results. The EDM-framework of Karras et al. <ref type="biblio">[21]</ref> is a<lb/> promising candidate for future model training, as its formulation in continuous time allows<lb/> for increased sampling flexibility and does not require noise-schedule corrections.</item>
			</list>


	</text>
</tei>
