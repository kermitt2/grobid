<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>I. INTRODUCTION<lb/></head>

			<p>Recent advancements in deep learning and GPU capabil-<lb/>ities <ref type="biblio">[12]</ref> have significantly improved computer vision&apos;s<lb/> detection and prediction. A major innovation is the use of<lb/> transformer models, first for Natural Language Processing<lb/> (NLP) in 2017 and later for visual tasks <ref type="biblio">[7]</ref>. Visual trans-<lb/>formers, especially those developed by Google Brain in 2020,<lb/> have outperformed traditional CNNs in accuracy, especially<lb/> with large datasets. However, their high computational<lb/> and memory requirements pose challenges for edge device<lb/> deployment <ref type="biblio">[32]</ref>, <ref type="biblio">[78]</ref>, primarily due to their reliance on<lb/> complex global attention mechanisms and MLPs. To mitigate<lb/> these demands, various strategies like multi-scale processing,<lb/> token dropping, early prediction, softmax elimination, and<lb/> efficient attention approaches have been researched. These<lb/> approaches are summarized in Section III. While these solu-<lb/>tions address certain aspects of the computational challenges,<lb/> they fall short of fully optimizing context-aware computation.<lb/> The main contributions of this paper are as follows:<lb/></p>
			<p>1) This paper introduces a novel context-aware<lb/> approximation technique for dynamic pruning of<lb/> computational trees in transformer models, diverging<lb/> significantly from existing methods. We identify an<lb/> underutilized potential in transformers for context-<lb/>based approximation, which we argue can greatly<lb/></p>

			<figure>FIGURE 1. (Left): Overall structure of original Visual Transformer (ViT) in [10]. (right): Encoder solution used in ViT, illustrating the implementation<lb/> details of Multi-Head Self Attention (MSA) from h scaled dot-product attention units.<lb/></figure>

			<p>enhance their efficiency with minimal accuracy impact,<lb/> broadening their application scope.<lb/> 2) We present the Incremental Resolution Enhancing<lb/> Transformer (IRET), a transformative model architec-<lb/>ture that employs attention-based input sampling.<lb/> 3) Utilizing learnable 2D lifting schemes, IRET processes<lb/> three input samples incrementally, thereby building<lb/> contextual awareness early. This architecture allows<lb/> IRET to use temporal attention scores for two key<lb/> functions: a) forget: discarding unattended tokens, and<lb/> b) focus: selectively enhancing the embedding size of<lb/> attended tokens by merging existing features with new<lb/> ones from a 2D lifting scheme output.<lb/></p>

			<p>This approach mirrors human visual perception, starting<lb/> with a broad context understanding and then focusing on<lb/> more pertinent image aspects. IRET thus uses minimal infor-<lb/>mation initially for context comprehension, subsequently<lb/> concentrating on key image tokens through incremental<lb/> sampling while ignoring less relevant ones. The remainder<lb/> of the paper is structured as follows: Section II covers<lb/> background information. Section III reviews related work.<lb/> Section IV details the IRET architecture. Section V presents<lb/> experimental evaluations. Finally, Section VI concludes the<lb/> paper.<lb/></p>

			<head>II. BACKGROUND<lb/></head>

			<p>Fig. <ref type="figure">1</ref>. (left) shows the Visual Transformer (ViT) <ref type="biblio">[10]<lb/></ref> architecture, and Fig. <ref type="figure">1</ref>. (right) captures the structure of its<lb/> encoder layer. In ViT the input image is split into fixed-size<lb/> patches by reshaping the image x ∈ R H ×W ×C into a sequence<lb/> of flattened 2D patches x p ∈ R N ×(P 2 .C) . The (H , W ) is the<lb/> image resolution, C is the number of channels, (P, P) is the<lb/> image patch resolution, and N = HW /P 2 is the number<lb/> of patches. The attention mechanism used in the encoder is<lb/> scaled dot-product attention suggested in <ref type="biblio">[56]</ref>. The inputs<lb/> are queries Q and keys K of dimension d k , and values V<lb/> of dimension d v . The encoder is designed to linearly project<lb/> the queries, keys, and values h times with different learned<lb/> linear projections to d k , d k , and d v dimensions, respectively.<lb/> As shown in Fig. <ref type="figure">1</ref>(right), each encoder layer uses h scaled<lb/> dot-product attention heads. Scaled dot-product attention<lb/> heads compute the matrix in Eq. 1 yielding d v -dimensional<lb/> output values that are later concatenated and projected. The<lb/> Multi-Head Self Attention (MSA), the function of which<lb/> is captured in Eq. 2, allows the model to jointly attend<lb/> to information from different representation subspaces at<lb/> different positions. Similar to BERT&apos;s class token <ref type="biblio">[9]</ref>, ViT<lb/> prepends a learnable embedding to embedded patches (z 0<lb/> 0 =<lb/> x class) , whose state at the output of the encoder (z 0<lb/> L ) serves as<lb/> the image representation y. Layernorm (LN ) is applied before<lb/> and residual connections after every block.<lb/></p>

			<formula>Attention(Q, K, V ) = Softmax(QK T / d k )V<lb/>
				(<label>1</label>)<lb/>
			</formula>

			<formula>MSA(Q, K , V ) = Concat(head i , . . . , head h )W O , (<label>2</label>)<lb/></formula>

			<formula>head i = Attention(QW<lb/> Q<lb/> i , KW K<lb/> i , VW V<lb/> i ) (<label>3</label>)<lb/></formula>

			<p>The Visual transformer function is captured using equa-<lb/>tions <ref type="formula">4 through 7</ref>:<lb/></p>

			<formula>z 0 = [x class ; x 1<lb/> p E; x 2<lb/> p E; . . . .; x N<lb/> p E] + E pos ,<lb/> E ∈ R (P 2 .C)<lb/> × D, E pos<lb/> (<label>4</label>)<lb/></formula> 
		
			<formula>z ′<lb/> l = MSA(LN (z l-1 )) + z l-1 , l = 1 . . . L<lb/> (<label>5</label>)<lb/></formula>
		
			<formula>z l = MLP(LN (z ′<lb/> l )) + z ′<lb/> l ,<lb/> l = 1 . . . L<lb/> (<label>6</label>)<lb/></formula>
		
			<formula>y = LN (z 0<lb/> l )<lb/> (<label>7</label>)<lb/></formula>

			<p>The classification head is attached to z 0<lb/> L and implemented<lb/> by an MLP with one hidden layer at pre-training and<lb/> one linear layer at fine-tuning. 1-Dimensional Position<lb/> embedding is added to the patch embeddings to retain<lb/> positional information. In a similar vein, DETR <ref type="biblio">[4]</ref> exploits<lb/> a pure transformer to create an end-to-end object detection<lb/> framework. Taking a different approach, DeiT <ref type="biblio">[55]</ref> enhances<lb/> ViT by introducing the distillation token, and leverages a<lb/> teacher model to decrease the necessary training data.<lb/></p>

			<figure>FIGURE 2. Timeline illustrating the proposed Multi-Scale Vision Transformer architectures [13], [17], [23], [34], [40], [60], [61], [75], [81], [82].<lb/></figure>

			<head>III. RELATED WORKS<lb/></head>

			<p>Several studies have focused on reducing the high computa-<lb/>tional complexity of vision transformers, resulting in models<lb/> with similar accuracy but lower complexity. This section<lb/> offers a brief overview of these approaches.<lb/></p>

			<head>A. MULTISCALE VIOSN TRANSFORMERS<lb/></head>

			<p>A widely adopted strategy for addressing the computational<lb/> complexity of vision transformers is pyramid-style process-<lb/>ing. This technique processes input images at multiple scales,<lb/> effectively capturing both coarse and fine contextual informa-<lb/>tion <ref type="biblio">[13]</ref>, <ref type="biblio">[42]</ref>, <ref type="biblio">[44]</ref>, <ref type="biblio">[68]</ref>. Fig. <ref type="figure">2</ref>. provides a summary of the<lb/> key approaches within this category. Numerous models have<lb/> successfully implemented this strategy, including: Pyramid<lb/> Vision Transformer (PVT) <ref type="biblio">[60]</ref>, Swin Transformer <ref type="biblio">[40]</ref>,<lb/> Multi-scale Vision Transformer (MViT) <ref type="biblio">[13]</ref>, PVT v2 <ref type="biblio">[61]</ref>,<lb/> and Wave-ViT <ref type="biblio">[75]</ref>.<lb/></p>

			<p>PVT takes in detailed image patches to effectively capture<lb/> fine-grained information for high-resolution representation.<lb/> PVT employs a pyramid structure that gradually reduces in<lb/> size, helping manage computational complexity in deeper<lb/> layers. The authors introduce a spatial-reduction attention<lb/> layer, which plays a role in conserving resources during<lb/> computation.<lb/></p>

			<p>The Swin Transformer introduces a hierarchical<lb/> transformer architecture that utilizes shifted windows for<lb/> representation computation. This approach ensures linear<lb/> computational complexity with respect to image size. The<lb/> model&apos;s early stages involve the processing of small patches,<lb/> with the gradual merging of neighboring patches in deeper<lb/> layers. The Swin Transformer adopts a shared key set among<lb/> patches within the same window, effectively mitigating<lb/> latency concerns associated with earlier sliding window-<lb/>based self-attention methods.<lb/></p>

			<p>MViT incorporates several channel-resolution stages, each<lb/> serving a distinct purpose. In the initial layers, the model<lb/> operates at an elevated spatial resolution coupled with a con-<lb/>strained channel dimension. As the network delves deeper,<lb/> spatial resolution diminishes while channel dimensions<lb/> expand significantly. This ingenious design culminates in the<lb/> formation of a feature pyramid, effectively encompassing a<lb/> comprehensive spectrum of features across different scales.<lb/></p>

			<p>PVT v2 offers reduced computational complexity while<lb/> preserving local image continuity. Additionally, the model<lb/> features a flexible position encoding scheme. In the Wave-<lb/>ViT, they utilize a down-sampling technique based on wavelet<lb/> transforms and integrate it with self-attention learning. In this<lb/> architecture, the wavelet block plays a central role. It employs<lb/> Discrete Wavelet Transform (DWT) to process the key and<lb/> value inputs separately, dividing them into four distinct<lb/> subbands. These subbands are then stacked together, followed<lb/> by a convolutional operation that maintains locality within<lb/> each subband. The output of this convolutional layer feeds<lb/> into both the multi-head attention and inverse DWT layers.<lb/></p>

			<p>While multi-scale designs effectively capture contextual<lb/> information at various resolutions, they typically rely on fixed<lb/> embedding dimensions and lack mechanisms for incremental<lb/> refinement or adaptive token management. Our proposed<lb/> solution addresses these gaps by employing smaller initial<lb/> embedding dimensions that incrementally increase based on<lb/> attention-driven sampling, enabling dynamic refinement of<lb/> resolution as the model processes the input. Additionally,<lb/> our approach incorporates token pruning guided by attention<lb/> scores, ensuring that computational resources are focused on<lb/> the most relevant features. This integration of context-aware<lb/> approximation with incremental resolution enhancement<lb/> complements existing multi-scale methods.<lb/></p>

			<head>B. PATCH AND TOKEN PRUNING<lb/></head>

			<p>Numerous studies have highlighted the sparse nature of<lb/> attention matrices within transformer models and identified<lb/> instances of token redundancy that don&apos;t significantly con-<lb/>tribute to final predictions. Building upon these observations,<lb/></p> 
		
			<figure>FIGURE 3. Timeline illustrating the proposed token and patch pruning approaches for transformer architectures [14], [19], [26], [31], [37], [38], [41], [48],<lb/> [63], [73], [74], [76].<lb/></figure>

			<p>various strategies have been introduced to trim these redun-<lb/>dant tokens and enhance efficiency <ref type="biblio">[5]</ref>, <ref type="biblio">[25]</ref>, <ref type="biblio">[28]</ref>, <ref type="biblio">[39]</ref>,<lb/> <ref type="biblio">[59]</ref>, <ref type="biblio">[67]</ref>. Unlike convolutional models, transformers can<lb/> leverage unstructured pruned inputs to their advantage. The<lb/> proposed techniques for token pruning can be classified<lb/> into two primary categories: static pruning and dynamic<lb/> pruning techniques. Certain methods apply a consistent<lb/> approach across different types of inputs. Other techniques<lb/> adapt their strategies based on the specific characteristics<lb/> of different inputs. Rao et al. <ref type="biblio">[48]</ref>, introduced DynamicViT.<lb/> This approach centers around enhancing the transformer<lb/> architecture by integrating a predictive module into specific<lb/> layers. The purpose of this module is to forecast the<lb/> importance score assigned to each token. Consequently,<lb/> tokens with lower scores undergo a hierarchical pruning<lb/> process. This strategy entails disconnecting tokens that have<lb/> been pruned from the remaining tokens within the attention<lb/> matrix. This task is achieved using the Gumbel-Softmax<lb/> masking strategy.<lb/></p>

			<p>Wang et al. <ref type="biblio">[63]</ref> demonstrated a correlation between the<lb/> complexity of input images and the required number of<lb/> tokens for accurate predictions. This suggests that simpler<lb/> images can be accurately predicted using fewer tokens. They<lb/> introduced DVT, a sequential transformer model designed to<lb/> process images with varying token counts. In their approach,<lb/> the concept of early exiting is utilized. This involves<lb/> halting computation if accurate predictions are achieved,<lb/> thereby bypassing the use of models with higher token<lb/> counts. To optimize the utilization of upper-level transformer<lb/> models and prevent computational inefficiencies, they reused<lb/> features and relationships.<lb/></p>

			<p>Kim et al. <ref type="biblio">[26]</ref>, introduced Learned Token Pruning (LTP).<lb/> This approach involves dynamically removing tokens that<lb/> are deemed less significant. This determination is made<lb/> by utilizing a threshold value, which the model learns<lb/> independently for each layer. Xu et al. <ref type="biblio">[74]</ref> proposed an<lb/> approach called Evo-ViT in which they introduced the<lb/> concept of slow-fast updating. This involves employing<lb/> separate computational pathways to update tokens based<lb/> on their informational relevance. This way, informative and<lb/> uninformative tokens are processed differently, helping to<lb/> maintain the spatial structure of the data during updates.<lb/> Liang et al. <ref type="biblio">[31]</ref> proposed EViT, which utilizes a token<lb/> reorganization strategy for detecting attentive tokens while<lb/> merging inattentive ones into a singular token. The unique<lb/> feature of this model is its independence from the necessity<lb/> of a fully trained ViT. Nevertheless, adjusting the target<lb/> ratio would still mandate retraining the model. Fayyaz et al.<lb/> <ref type="biblio">[14]</ref> presented ATS, a distinctive module referred to as a<lb/> differentiable and parameter-free Adaptive Token Sampler.<lb/> This module facilitates the dynamic selection of tokens<lb/> from input images based on attention scores, allowing for<lb/> variability in the number of chosen tokens for each image.<lb/> ATS can be seamlessly incorporated into a pre-trained model<lb/> to enhance its performance. Liu et al. <ref type="biblio">[37]</ref> established<lb/> that employing a technique known as PatchDropout, which<lb/> involves the random omission of input image patches, enables<lb/> efficient training of standard ViT models at high resolutions.<lb/> This method achieves a significant reduction of at least 50%<lb/> in both FLOPs and memory consumption on typical datasets<lb/> with natural images.<lb/></p>

			<p>Meng et al. <ref type="biblio">[41]</ref> introduced AdaViT, a framework that<lb/> autonomously determines the utilization of patches, self-<lb/>attention heads, and layers within ViT. The core innovation<lb/> involves integrating a lightweight multi-head subnetwork<lb/> (referred to as the decision network) into each transformer<lb/> block of the backbone network. This auxiliary network learns<lb/> to predict binary choices concerning patch embedding incor-<lb/>poration, self-attention head engagement, and block omission<lb/> across the network. Yin et al. <ref type="biblio">[76]</ref> introduced A-ViT, which<lb/></p>

			<figure>FIGURE 4. Timeline of proposed early exiting approaches for transformers <ref type="biblio">[3], [11], [15], [21], [33], [52], [53], [63], [70], [71], [72], [83], [84]</ref>.<lb/></figure>

			<p>involves an adaptive inference mechanism. This mechanism<lb/> intelligently stops the computation process for various tokens<lb/> at different depths, focusing computational resources only<lb/> on tokens that contribute significantly to discrimination. This<lb/> approach dynamically adjusts the allocation of computation<lb/> resources. A-ViT integrated with high-performance hard-<lb/>ware. This is facilitated by the removal of paused tokens from<lb/> ongoing computations, resulting in enhanced computational<lb/> efficiency. The entire halting process can be acquired through<lb/> the model&apos;s existing parameters.<lb/></p>

			<p>These methods effectively reduce computational com-<lb/>plexity but are not without challenges. Many approaches<lb/> rely heavily on retraining or manual parameter adjustments,<lb/> which limits their scalability and adaptability. Static methods<lb/> often lack contextual awareness, whereas dynamic methods<lb/> can introduce significant overhead or fail to maintain spatial<lb/> and semantic coherence. Our proposed architecture bridges<lb/> these gaps by incorporating a context-aware approxima-<lb/>tion mechanism with incremental resolution enhancement.<lb/> Unlike static approaches, it dynamically adjusts embedding<lb/> dimensions and utilizes token pruning based on attention<lb/> scores, ensuring computational resources are dedicated to<lb/> the most informative tokens. Additionally, the token pruning<lb/> mechanisms discussed in this subsection seamlessly integrate<lb/> with our model, boosting its efficiency and adaptability.<lb/></p>

			<head>C. EARLY TERMINATION<lb/></head>

			<p>Earlier, scholars introduced the concept of anytime prediction<lb/> in the realm of computer vision. Multiexit architectures<lb/> can be created from deep neural networks by introducing<lb/> branches that exit early after certain intermediate layers.<lb/> This transformation enables the inference process to adapt<lb/> dynamically, which proves beneficial for IoT applications<lb/> with strict latency demands. These applications often face<lb/> fluctuating communication and computation resources <ref type="biblio">[3]</ref>.<lb/> Building upon this idea, certain studies have extended this<lb/> approach to transformers, effectively striking a favorable<lb/> balance between prediction speed and accuracy. Elbayad et al.<lb/> <ref type="biblio">[11]</ref> introduced a depth-adaptive transformer in which<lb/> predictions occur at different network stages, with net-<lb/>work length and computation adjusting according to input<lb/> sequences. They trained the decoder using aligned and mixed<lb/> methods, examining sequence-specific versus token-specific<lb/> depth prediction approaches. Xin et al. <ref type="biblio">[70]</ref>, revealed that<lb/> different layers of BERT exhibit varying behaviors, with<lb/> some layers being redundant. As a solution, they introduced<lb/> DeeBERT, which enables samples to exit earlier through<lb/> off-ramps to improve efficiency. Zhou et al. <ref type="biblio">[83]</ref>, introduced<lb/> a solution known as Patience-based Early Exit (PABEE),<lb/> which involves incorporating an internal classifier within<lb/> each layer of a pretrained language model (PLM). This<lb/> approach is designed to halt the model&apos;s inference process<lb/> when the internal classifier&apos;s accuracy remains consistent for<lb/> a predetermined period, effectively mitigating unnecessary<lb/> processing. Sun et al. <ref type="biblio">[52]</ref>, constructed an ensemble model<lb/> that utilized internal classifiers. This approach capitalizes<lb/> on the internal classifiers being trained for predictions<lb/> on the same task. They introduced a voting strategy that<lb/> leverages predictions from all preceding internal classifiers.<lb/> This strategy aids in determining both the optimal timing for<lb/> exiting the process and the corresponding label assignment.<lb/> Li et al <ref type="biblio">[30]</ref> introduced an approach aimed at expediting<lb/> the inference process of a pre-trained sequence labeling<lb/> model. The proposed methodology includes two distinct<lb/> components: SENTEE, which operates at the sentence level<lb/> and enables early exiting in sequence labeling, and TOKEE,<lb/> an early-exit mechanism functioning at the token level.<lb/> LeeBERT <ref type="biblio">[84]</ref> introduces a training approach in which every<lb/> exit learns not only from the final layer but also from each<lb/> other. In the BERxiT paper <ref type="biblio">[71]</ref>, the focus was on rectifying<lb/> drawbacks in earlier early exit methods for BERT. These<lb/> methods were restricted to classification tasks and couldn&apos;t<lb/> fully leverage BERT&apos;s potential due to limited fine-tuning<lb/> strategies. The solution introduced was a &apos;&apos;learning-to-exit&apos;&apos;<lb/> module, extending early exits to diverse tasks and enhancing<lb/> BERT&apos;s utilization.<lb/></p>

			<figure>FIGURE 5. Softmax complexity reduction approaches in transformers <ref type="biblio">[6], [27], [35], [46], [51], [65], [80]</ref>.<lb/></figure>

			<p>He et al. <ref type="biblio">[21]</ref>, presented Magic Pyramid (MP) which<lb/> utilizes token pruning for width-wise computation reduction<lb/> and incorporates early exit strategies to address depth-wise<lb/> computation reduction, ultimately leading to improved<lb/> efficiency in model inference. Liao et al. <ref type="biblio">[33]</ref> presents a<lb/> solution for global early exits, utilizing information from<lb/> both preceding and subsequent layers to facilitate the exit<lb/> process. The Dynamic transformer <ref type="biblio">[63]</ref> was introduced<lb/> as an approach aimed at automatically determining the<lb/> optimal number of tokens necessary for processing each<lb/> input image. This was accomplished by utilizing a series of<lb/> interconnected transformers, with each one accommodating<lb/> an increasing number of tokens. Throughout the testing<lb/> phase, these transformers were sequentially activated in an<lb/> adaptable fashion. In essence, the inference process would<lb/> conclude once a prediction of sufficient confidence was<lb/> generated. In this study <ref type="biblio">[3]</ref>, seven distinct designs for early<lb/> exit branches are introduced, which can be integrated into<lb/> ViT backbones. By conducting comprehensive experiments<lb/> involving tasks such as image classification and crowd<lb/> counting -the latter involving regression, it is demonstrated<lb/> that these architectures offer valuable options for striking a<lb/> balance between classification accuracy and inference speed,<lb/> depending on the specific task.<lb/></p>

			<p>To conclude this subsection, we emphasize that the early<lb/> termination mechanisms discussed here offer a variety of<lb/> strategies for balancing computational efficiency and pre-<lb/>dictive accuracy. In our proposed architecture, classification<lb/> heads have been integrated into the IRET model to facilitate<lb/> real-time predictions, providing an inherent mechanism<lb/> for early exits. Furthermore, the diverse early prediction<lb/> techniques reviewed can be seamlessly incorporated into our<lb/> framework.<lb/></p>

			<head>D. SOFTMAX COMPLEXITY REDUCTION<lb/></head>

			<p>The softmax operation in transformers is a major computa-<lb/>tional bottleneck, especially with longer sequences. It relies<lb/> on costly exponential functions, and achieving numerical<lb/> stability often involves extra steps. Efforts to speed up,<lb/> approximate, or eliminate softmax have been made <ref type="biblio">[6]</ref>, <ref type="biblio">[27]</ref>,<lb/> <ref type="biblio">[51]</ref>, <ref type="biblio">[57]</ref>. Choromanski et al. <ref type="biblio">[6]</ref> proposed Performers,<lb/> for estimating regular full-rank-attention transformers with<lb/> linear space and time complexity. Unlike traditional methods<lb/> that rely on priors like sparsity or low-rankness, performers<lb/> employ fast attention to approximate softmax attention<lb/> kernels. This enables scalable and efficient modeling of atten-<lb/>tion mechanisms beyond softmax, addressing the quadratic<lb/> complexity challenge of conventional transformers.<lb/></p>

			<p>Koohpayegani et al. <ref type="biblio">[27]</ref> presentd SimA, an attention block<lb/> that replaces the softmax layer with L1-norm normalization,<lb/> simplifying attention to matrix multiplications. SimA main-<lb/>tains comparable accuracy to state-of-the-art transformer<lb/> variants like DeiT, XCiT, and CvT while removing the<lb/> computational overhead of Softmax.<lb/></p>

			<p>Stevens et al. <ref type="biblio">[51]</ref> introduced Softermax, an optimized<lb/> softmax algorithm designed for hardware efficiency. Soft-<lb/>ermax integrates base replacement, low-precision softmax<lb/> computations, and an online normalization calculation.<lb/> By leveraging the fine-tuning principles of transformer-<lb/>based networks, they apply Softermax-aware fine-tuning<lb/> to minimize accuracy loss without imposing additional<lb/> training burdens. Furthermore, they provided insights into<lb/> the microarchitecture required for implementing Softermax<lb/> in an inference accelerator. Qin et al. proposed cosFORMER,<lb/> which leverages non-negativity and a non-linear re-weighting<lb/> scheme in the softmax attention matrix to create a linear<lb/> transformer <ref type="biblio">[46]</ref>.<lb/></p>

			<p>The softmax overhead reduction methods discussed pro-<lb/>vide efficient alternatives to traditional softmax, improving<lb/> scalability and accuracy. By integrating these techniques into<lb/> our proposed model, we can further decrease computational<lb/> overhead and enhance performance.<lb/></p>

			<head>E. NOVEL ATTENTIONS<lb/></head>

			<p>The computational challenge of quadratic complexity in<lb/> self-attention has long been a prominent obstacle when<lb/> applying the models to tasks in computer vision. A pri-<lb/>mary research focus in enhancing the efficiency of vision<lb/></p> 
		
			<figure>FIGURE 6. Attention optimization approaches for transformer models <ref type="biblio">[16], [18], [22], [24], [36], [43], [45], [54], [58], [62], [66], [69], [77]</ref>.<lb/></figure>

			<p>transformers involves the reduction of computational costs<lb/> associated with self-attention modules <ref type="biblio">[2]</ref>, <ref type="biblio">[29]</ref>, <ref type="biblio">[47]</ref>,<lb/> <ref type="biblio">[50]</ref>, <ref type="biblio">[79]</ref>.<lb/></p>

			<p>In Flatten Transformer, the focused linear attention<lb/> module addresses the computational complexity issue of<lb/> self-attention by introducing a module that combines high<lb/> efficiency with expressiveness <ref type="biblio">[16]</ref>. CrossFormer and its<lb/> enhanced version, CrossFormer++, explicitly leverage fea-<lb/>tures of different scales, while addressing issues such as<lb/> self-attention map enlargement and amplitude explosion <ref type="biblio">[62]</ref>.<lb/> EfficientViT introduces a new building block and cascaded<lb/> group attention module to improve memory efficiency and<lb/> computational redundancy in transformer models, achieving<lb/> a commendable trade-off between speed and accuracy <ref type="biblio">[36]</ref>.<lb/> FasterViT <ref type="biblio">[18]</ref>, a hybrid CNN-ViT neural network amalga-<lb/>mates the swift local representation learning of CNNs with<lb/> ViT&apos;s global modeling capabilities. Within FasterViT, the<lb/> Hierarchical Attention (HAT) technique efficiently breaks<lb/> down the quadratic complexity of global self-attention<lb/> into multi-level attention mechanisms, effectively curtail-<lb/>ing computational costs. By harnessing efficient window-<lb/>based self-attention, individual windows are endowed with<lb/> dedicated carrier tokens, fostering both local and global<lb/> representation learning. HiLo <ref type="biblio">[45]</ref> encodes high frequen-<lb/>cies using local window self-attention and captures low<lb/> frequencies through global attention within the input feature<lb/> map.<lb/></p>

			<p>Castling-ViT <ref type="biblio">[77]</ref> tackles the challenge of enabling<lb/> ViTs to efficiently learn both global and local con-<lb/>text during inference. It uses linear-angular attention and<lb/> masked softmax-based quadratic attention during training<lb/> and switches to linear-angular attention alone during infer-<lb/>ence. The framework employs angular kernels for query-key<lb/> similarity, simplified by decomposing them into linear terms<lb/> and high-order residuals, and incorporates modules like<lb/> depthwise convolution and masked softmax attention to<lb/> efficiently learn global and local information.<lb/></p>

			<p>The attention computation reduction methods discussed<lb/> here provide innovative solutions to address the quadratic<lb/> complexity of self-attention while preserving accuracy.<lb/> Integrating these techniques into our proposed model can<lb/> further enhance its efficiency, enabling it to handle complex<lb/> tasks with reduced computational overhead and improved<lb/> scalability.<lb/></p>

			<head>IV. IRET: PROPOSED METHOD<lb/></head>

			<p>To adapt vision transformers for resource-constrained<lb/> devices, reduce their computational and memory require-<lb/>ments, and address the shortcomings of previous solutions<lb/> discussed in the previous section, we introduce IRET.<lb/></p>

			<head>A. ARCHITECTURE OF IRET<lb/></head>

			<p>The high-level architecture of IRET is shown in Fig. <ref type="figure">7</ref>. The<lb/> innovation in IRET is the ability to focus on attended tokens<lb/> in addition to forgetting unattended tokens.<lb/></p>

			<p>As illustrated in Fig. <ref type="figure">7</ref>, IRET replaces several transformer<lb/> encoder layers with IRET encoders. The architecture of an<lb/> IRET encoder is shown in Fig. <ref type="figure">8</ref>. IRET encoder pre-processes<lb/> the tokens for token dropping and token focusing before<lb/> performing the encoding. More specifically, similar to prior<lb/> work in <ref type="biblio">[14]</ref> and <ref type="biblio">[48]</ref>, IRET performs the token dropping<lb/> based on CLS token attention scores, dropping tokens with<lb/> low attention scores to prune the computational tree.<lb/></p>

			<p>However, as illustrated in Fig. <ref type="figure">8</ref> IRET also has an<lb/> attention-based mechanism for an incremental sampling of<lb/> the input image using an &apos;&apos;attention-based focusing&apos;&apos; module.<lb/> The focusing module received a new sample of the input<lb/> image using a learnable 2D-lifting scheme in <ref type="biblio">[49]</ref> that is<lb/> shared across IRET layers. Details of the 2D-lifting scheme<lb/> will be explained later. We refer to this input image sample<lb/> as a sub-band sample. Each generated sub-band is then<lb/> divided into patches with a 1-to-1 mapping relationship to<lb/> input image patches. Based on the attention-score of input<lb/> (existing) tokens, the token focusing module then decides for<lb/></p>

			<figure>FIGURE 7. The IRET architecture processes input through four sampling steps: initially with a scaled low-pass filter and then three times using<lb/> learnable 2-D lifting schemes. With each IRET layer, the embedding size of each token increases as it assimilates additional information.<lb/> Concurrently, before each IRET layer, less-attended tokens are dropped. Therefore, each IRET layer has dual roles: discarding unattended tokens and<lb/> focusing on attended ones through extra sampling. The transformer encoder&apos;s increasing size visualizes the growth in embedding size at each IRET<lb/> encoder.<lb/></figure>

			<figure>FIGURE 8. The IRET layer architecture utilizes the CLS token to identify unattended tokens, employing a token dropping method to remove them.<lb/> Additionally, it determines which tokens require more focus based on the CLS token. This process involves filtering patches from the input sample<lb/> created by the 2D lifting scheme, projecting these patches into new embeddings, and then concatenating new information to enhance the existing<lb/> token embeddings. By enlarging the embedding size, IRET increases focus on attended tokens.<lb/></figure>

			<p>each patch in the newly sampled sub-band to be ignored or<lb/> forwarded to the layer. If the corresponding token coming<lb/> from the previous encoder has an attention score above the<lb/> desired threshold, the token is deemed useful and is subjected<lb/> to embedding. The embedded information for each sub-band<lb/> that corresponds to an attended token is concatenated to the<lb/> embedding of that token, increasing the embedding size,<lb/> which is analogous to improving focus on that part. The<lb/> size of each encoder layer in Fig. <ref type="figure">7</ref> corresponds to the<lb/> embedding size of its token. Using this illustration, as shown<lb/> in Fig. <ref type="figure">7</ref>, each IRET encoder layer (shown in blue) increases<lb/> the embedding size (shown in dark blue), while each regular<lb/> transformer encoder layer maintains the embedding size.<lb/></p>

			<head>B. INPUT SAMPLING PROCEDURE<lb/></head>

			<p>To obtain input image samples for incorporation into IRET<lb/> layers, we investigated three methodologies: 1. Utilizing<lb/> the original input, 2. Employing DWT subbands, and 3.<lb/> Adopting a learnable sampling approach known as the<lb/> 2D-lifting Scheme. The outcomes obtained through these<lb/> various sampling approaches are thoroughly examined and<lb/> detailed in Section V, shedding light on the efficacy and<lb/> impact of the chosen sampling methods on the overall<lb/> performance of the model.<lb/></p>

			<head>Original Input-</head>
			<p>In this experiment which is the baseline,<lb/> we feed the downsampled original input three more times to<lb/> the model using the IRET layers. The goal of this experiment<lb/> is to check the ability of the model to learn new features from<lb/> the new embedded samples of the original input.<lb/></p>

			<head>DWT Subbands-</head>
			<p>An alternative sampling approach<lb/> involves the utilization of Discrete Wavelet Transform<lb/> (DWT). Numerous investigations have harnessed the capabil-<lb/>ities of DWT within the realm of computer vision to augment<lb/> diverse facets of image analysis and processing. DWT,<lb/> a mathematical technique adept at decomposing signals<lb/> or images into their fundamental frequency components,<lb/> provides a unique pathway for feature extraction, represen-<lb/>tation, and manipulation. We employ DWT to produce four<lb/></p>

			<figure>FIGURE 9. The Architecture of Learnable 2D Lifting Scheme. It receives the original image and learns four<lb/> output samples. S 1 , S 2 and, S 3 are used in the architecture of IRET.<lb/></figure>

			<p>subbands of images, each sized 112*112, denoted as LL,<lb/> LH, HL, and HH. Due to the LL subband containing a<lb/> greater entropy of information, it is assigned to the initial<lb/> layer. Simultaneously, the LH, HL, and HH subbands are<lb/> sequentially inserted into the subsequent IRET layers for<lb/> further processing. This distribution ensures an effective<lb/> utilization of the information content across the layers of the<lb/> model.<lb/></p>

			<head>2D-lifting Scheme-</head>

			<p>The architecture of the 2D-lifting<lb/> scheme <ref type="biblio">[49]</ref> used in the IRET layer is shown in Fig. <ref type="figure">9</ref>. The<lb/> lifting scheme is designed to take a signal, denoted as x, as its<lb/> input and produce two key outputs: the approximation sub-<lb/>band (c) and the details sub-band (d) of the wavelet transform.<lb/> The process of designing this lifting scheme involves three<lb/> distinct stages: Splitting the signal, Updater, and Predictor.<lb/> Eq. <ref type="formula">8 through 10</ref> describes the functionality of these stages.<lb/> The predictor and updater components are implemented<lb/> using CNN layers. Each CNN consists of two layers: the first<lb/> layer uses a kernel size of 1 × 3 (for horizontal processing) or<lb/> 3×1 (for vertical processing) and employs a ReLU activation<lb/> to enhance non-linear representations. The second layer uses<lb/> a 1×1 convolution with a tanh activation to stabilize the range<lb/> of outputs. The signal x is partitioned into two components in<lb/> splitting stage: an even component and an odd component.<lb/> The even component consists of all the values located at even<lb/> positions in the sequence and in the update operation is often<lb/> used as the basis for creating the approximation sub-band<lb/> c, which captures the low-frequency details of the signal.<lb/> The odd component consists of all the values located at odd<lb/> positions in the sequence and in the prediction operation<lb/> is used to derive the detail sub-band d, which captures the<lb/> high-frequency details of the signal.<lb/></p>

			<formula>x e [n] = x[2n], x o [n] = x[2n + 1], x : input signal<lb/> (<label>8</label>)<lb/></formula>



			<formula>c[n] = x e [n] + U (x o<lb/> L U [n]), U (.) = update operator (<label>9</label>)<lb/></formula>

			<formula>d[n] = x o [n] -P(c L P [n]), P(.) = prediction operator<lb/> (<label>10</label>)<lb/></formula>

			<p>The loss function of learnable updater and predictor is defined<lb/> as.<lb/></p>

			<formula>Loss(P) = n (P(c L P [n]) -x o [n]) 2<lb/> (<label>11</label>)<lb/></formula>

			<formula>Loss(U ) = n (U (x L U<lb/> o [n]) -(x o [n] -x e [n])) 2<lb/> (<label>12</label>)<lb/></formula>

			<p>For the predictor, the loss function minimizes the mean<lb/> squared error between the predicted and actual odd samples<lb/> and the updater&apos;s loss function minimizes the error in<lb/> approximating the difference between the odd and even<lb/> components.<lb/></p>

			<p>The initial convolutional layers extract discriminative<lb/> features from the data before downsampling. This is done<lb/> using two sequences of convolution, batch normalization, and<lb/> ReLU with a kernel size of 3 × 3.<lb/></p>

			<p>It&apos;s important to note that to minimize overhead, a portion<lb/> of the 2D-lifting scheme is shared across IRET encoder<lb/> layers. Nonetheless, each IRET encoder layer is fed by a<lb/> unique segment of the 2D-lifting scheme, ensuring it receives<lb/> a distinct sample. Additionally, this 2D-lifting scheme is<lb/> designed to be learnable, enabling its integration and training<lb/> alongside the rest of the model in an end-to-end manner.<lb/> During the training phase, the lifting scheme is trained as an<lb/> integral component of the IRET architecture. This approach<lb/> allows each IRET layer to adaptively incorporate new and<lb/> unique features, differentiating them from previous sampled<lb/> information for each token.<lb/></p>

			<p>To maintain the positional information of patches in<lb/> newly sampled images we employ a position embedding<lb/> layer to add this data to their embedding. Prior to adopting<lb/> learnable layers, we explored different sampling techniques<lb/> for the input image, like DWT, using each sub-band as a<lb/> separate input to the feature encoding layer. However, our<lb/></p>

			<figure>FIGURE 10. The IRET layer features two main functions: attention-based token dropping and focusing.<lb/> It eliminates unattended tokens using attention scores to simplify computation and enlarges the embedding<lb/> size for attended tokens with extra features from a 2-D lifting scheme. This process, akin to human brain<lb/> focusing, allows IRET to selectively prioritize certain tokens, thereby boosting accuracy and lowering<lb/> computational complexity.<lb/></figure>

			<p>findings indicated that a learnable lifting scheme, which<lb/> learns features based on model loss and trained alongside the<lb/> main model, yields the highest accuracy.<lb/></p>

			<p>Also, note that the input to IRET is a scaled version of<lb/> the input image. For example, input to the embedding layer<lb/> of DeiT is a 224 × 224 pixel image. For IRET, we take<lb/> a scaled 112 × 112 pixel image as input and also reduce<lb/> the embedding size of the first layer from 384 to 192.<lb/> subsequently in each IRET layer (that in the variant shown<lb/> in this proposal is positioned in layers 4, 5, and 6, the<lb/> embedding size of features is increased from 192 to 294,<lb/> 348, and 384 respectively bringing in additional 102, 54,<lb/> and 36 embedding dimensions with each added IRET layer.<lb/> Starting with a smaller embedding size and working with a<lb/> smaller embedding size in the first 6 layers of the IRET layers<lb/> allows a significant reduction of the computation. By working<lb/> with a smaller embedding size, IRET first decides where<lb/> to look for information in the input image. As the attention<lb/> scores highlight the importance of various input tokens, then<lb/> IRET layers stop processing unattended tokens, and more<lb/> importantly, bring in additional details for the features in<lb/> attended tokens.<lb/></p>

			<p>Fig. <ref type="figure">10</ref> visualizes the pre-processing function for token<lb/> dropping and token focusing in an IRET encoder layer. The<lb/> attention threshold, which is predefined, plays a crucial role<lb/> in determining the tokens to be dropped or focused. In the<lb/> left part of the figure, the attention matrix is depicted, with<lb/> the first row highlighted in red corresponding to the CLS<lb/> token. This token is essential for classification in the last<lb/> layer of transformers, as its attention to other tokens reflects<lb/> their importance. The attention scores in the CLS row are<lb/> what we use in IRET to decide if a token is to be forgotten<lb/> (drop) or focused by bringing additional information through<lb/> the use of a 2D-lifting scheme. In the right part of the<lb/> figure, the token dropping process is illustrated. At the first<lb/> layer, tokens with attention scores below the predefined<lb/> threshold are dropped. In the subsequent layer, the remaining<lb/> tokens are concatenated with their corresponding tokens<lb/> from the 2D-lifting scheme. This process increases the<lb/> embedding dimension, enabling the model to focus more on<lb/> these tokens, which is visualized by a clearer token at the<lb/> end.<lb/></p>

			<p>Fig <ref type="figure">11</ref> is another visualization of the token dropping<lb/> and focusing concept in an IRET encoder. As illustrated,<lb/> each IRET layer increases the details of each token with a<lb/> high attention score (this is visualized by increasing image<lb/> resolution, but in reality, this is achieved by increasing<lb/> embedding size), while dropping the unattended tokens.<lb/></p>

			<head>C. IMPROVED EARLY PREDICTION THROUGH MULTI-EXIT<lb/> ARCHITECTURE IN IRET<lb/></head>

			<p>As explained in section III, early termination is one of<lb/> the proposed methods for expediting model prediction.<lb/> We have further investigated and incorporated this feature<lb/> to enhance the speed of IRET. As depicted in Fig. <ref type="figure">12</ref>,<lb/> various input images present distinct content and pose<lb/> challenges, showcasing classification tasks with varying<lb/> levels of complexity. Therefore, our goal is to formulate<lb/> the IRET with multiple exit options, enabling it to perform<lb/> early classification after each encoder layer. The decision<lb/> to advance to the next encoder level is based on how sure<lb/> the model is about its prediction and our predetermined<lb/> confidence threshold. This formulation additionally enables<lb/> the application of IRET in low-power and real-time systems.<lb/></p>

			<p>Fig. <ref type="figure">13</ref> depicts the real-time behavior of the IRET system,<lb/> wherein the model&apos;s execution is halted based on a predefined<lb/> deadline. Consequently, the model adjusts its complexity<lb/> by reducing the number of layers when faced with tighter<lb/> deadlines, and conversely, it utilizes more layers when more<lb/> time is available for computation. In scenarios with low-<lb/>power models, computations can be halted to adhere to energy<lb/> constraints.<lb/></p>

			<p>It&apos;s crucial to consider that the energy constraints play a<lb/> pivotal role in determining at which layers IRET concludes its<lb/> operations. This variability stems from the diverse number of<lb/> skip computations influenced by context-aware computation<lb/></p>

			<figure>FIGURE 11. In IRET, the &apos;forget and focus&apos; concept hinges on CLS token attention values. Tokens with attention below a threshold are dropped<lb/> (&apos;forget&apos;), while those above the threshold see increased embedding size (&apos;focus&apos;) via a 2D-lifting scheme. The concept of focus is shown by<lb/> increased resolution.<lb/></figure>

			<figure>FIGURE 12. IRET exit behavior in low-power and energy aware system.<lb/></figure>

			<figure>FIGURE 13. IRET exit behavior in real-time system.<lb/></figure>

			<p>and patch elimination, especially when dealing with different<lb/> images. Figure <ref type="figure">14</ref> shows the overview of muti-exit IRET.<lb/> Extra heads are added to the model after each encoder layer<lb/> from layers 4 to 12. Classification before the first 3 layers is<lb/> not useful because the model has more understanding after<lb/> the 3 first layers.<lb/></p>

			<head>V. EXPERIMENTS<lb/></head>

			<p>Our model was developed based on the Facebook DeiT <ref type="biblio">[55]<lb/></ref> small model with hard distillation, utilizing the Timm<lb/> library <ref type="biblio">[64]</ref>. We conducted our experiments on the Ima-<lb/>geNet dataset <ref type="biblio">[8]</ref> using Nvidia A100 as the training<lb/> platform.<lb/></p>

			<figure>FIGURE 14. The IRET multi-exit architecture. Employing MLP heads at layers 4 to 12 enables the model to make early predictions, effectively bypassing<lb/> the need for computations in subsequent layers. This strategic approach enhances efficiency and accelerates decision-making in IRET.<lb/></figure>

			<p>To explore the most effective input sampling strategies,<lb/> three distinct approaches were evaluated. These experiments<lb/> aimed to address the challenge of balancing computational<lb/> efficiency with model performance by identifying a sub-<lb/>sampling mechanism that retains essential information while<lb/> reducing redundancy. The performance of each sampling<lb/> method was assessed using top-1 accuracy metric. Accuracy<lb/> was defined as the proportion of correctly classified images<lb/> over the total test set.<lb/></p>

			<p>The experimental results demonstrated that the<lb/> 2D-lifting scheme significantly outperformed the other meth-<lb/>ods, emerging as the most effective and impactful sampling<lb/> approach. This scheme was designed to dynamically adapt<lb/> the subsampling process to capture and preserve essential<lb/> image features, enabling improved learning by the IRET<lb/> layers.<lb/></p>

			<p>For the first sampling mechanism, we simply fed the<lb/> downsampled image to IRET layers as well Fig. <ref type="figure">15a</ref> shows<lb/> the results. In implementing the second sampling approach,<lb/> the Pytorch Wavelet library was utilized <ref type="biblio">[1]</ref>. However, this<lb/> method proved to be ineffective as the model struggled<lb/> to learn from subsequent samples introduced to the IRET<lb/> layers. Consequently, no notable improvement in accuracy<lb/> was observed following the insertion of LH, HL, and HH<lb/> subbands. The result of this method is illustrated in Fig. <ref type="figure">15b</ref>.<lb/></p>

			<p>These two sampling experiments served as motivation to<lb/> adopt a trainable subsampling approach. By doing so, the<lb/> model can gain valuable information from the inputs to the<lb/> IRET layers, potentially enhancing its learning capabilities.<lb/></p>

			<p>A learnable approach for subsampling images, such as<lb/> the 2D-lifting scheme discussed, introduces adaptability and<lb/> flexibility into the subsampling process. Unlike fixed or<lb/> predetermined subsampling methods, a learnable approach<lb/> allows the model to dynamically adjust and optimize the<lb/> subsampling strategy during training. Learnable subsampling<lb/> enables the model to adapt its sampling strategy based on<lb/> the specific characteristics and requirements of the given<lb/> task. Different tasks or datasets may benefit from different<lb/> subsampling patterns, and a learnable approach allows the<lb/> model to learn the most effective strategy for the task at hand.<lb/> Moreover, Images often contain complex relationships and<lb/> structures that may vary across different regions. A learnable<lb/> approach allows the model to capture and exploit these intri-<lb/>cate relationships during subsampling, potentially leading to<lb/> the extraction of more relevant and discriminative features.<lb/> Also, traditional subsampling methods may discard certain<lb/> information during the downsampling process. A learnable<lb/> approach has the potential to minimize information loss by<lb/> intelligently selecting which details to retain or discard based<lb/> on the model&apos;s learning experience. In essence, a learnable<lb/> subsampling approach empowers the model to actively par-<lb/>ticipate in the decision-making process, learning and refining<lb/> its subsampling strategy during training. This adaptability<lb/> can lead to more effective feature extraction, better task<lb/> performance, and improved generalization capabilities. Each<lb/> sample focuses on acquiring insights into a distinct aspect of<lb/> the image, thereby introducing new features to enhance the<lb/> model&apos;s understanding.<lb/></p>

			<p>The final model inputs are 112 × 112 pixels, with IRET<lb/> layers receiving 112 × 112 sub-bands generated by the 2D-<lb/>lifting scheme <ref type="biblio">[49]</ref>. To enhance trainability, we integrated<lb/> three additional classification heads, each corresponding to<lb/> a CLS token of an IRET layer. These heads contribute to the<lb/> total classification error during backpropagation, accelerating<lb/> the training of the IRET layer and 2D-lifting scheme. These<lb/> heads are removed post-training for inference. Training lasts<lb/> for 300 epochs or until accuracy plateaus. Data augmentation<lb/> included randomly omitting information from the 2D-lifting<lb/> scheme to assess IRET&apos;s incremental learning capability.<lb/> We evaluated IRET&apos;s performance in four scenarios: 1) Using<lb/> only the input image, 2) Adding the first sub-band sample to<lb/> the first IRET layer, 3) Incorporating two sub-band samples<lb/></p>

			<figure>FIGURE 15. Top-1 accuracy of the IRET for different input sampling mechanisms and different combinations of inputs. In each experiment,<lb/> we have tested four combinations: Only the downsampled input I, downsampled input plus one sample I + S1, downsampled input plus<lb/> two samples I + S1 + S2, and downsampled input plus three samples I + S1 + S2 + S3. (a) Original downsampled input is given to the IRET<lb/> layers. (b) DWT is used to generate the samples. (c,d) 2D Lifting mechanism is used for sample generation for DeiT small and tiny<lb/> respectively. (e) Comparing the top1 accuracy of all the experiments using all samples as input.<lb/></figure>

			<figure type="table">TABLE 1. IRET&apos;s accuracy, FLOP count, and parameter count based on various attention thresholds in the IRET layer, which affect token dropping and<lb/> focusing.<lb/></figure>

			<p>in the first and second IRET layers, and 4) Including all three<lb/> sub-band samples.<lb/></p>

			<p>Fig. <ref type="figure">15c</ref> presents the top-1 training accuracy of IRET<lb/> across these scenarios. The figure shows IRET&apos;s proficiency<lb/> in incremental learning, with diminishing accuracy gains<lb/> upon adding more sub-band samples. The first sub-band&apos;s<lb/> addition notably boosts accuracy, but subsequent samples<lb/> yield lesser improvements. This observation made us limit<lb/> the number of IRET layers. The embedding size distribution<lb/> across sub-bands also affects incremental learning rate<lb/></p>

			<figure>FIGURE 16. Change in accuracy and flop count as a function of attention threshold for token dropping and<lb/> token focusing.<lb/></figure>

			<figure>FIGURE 17. Token dropping of layers with pruning policy based on different threshold values. For smaller<lb/> threshold values, the model drops fewer tokens.<lb/></figure>

			<p>and final accuracy. As shown, the model&apos;s top-1 accuracy<lb/> improves from 67.5% to 75.93%, 77.52%, and 78.12% with<lb/> the addition of new information extracted from sampled sub-<lb/>bands. We have also developed the IRET-tiny using DeiT<lb/> tiny model. Fig. <ref type="figure">15d</ref> shows the IRET-tiny accuracy using<lb/> lifting-scheme with the top-1 accuracy close to 70%. Finally,<lb/> in Fig. <ref type="figure">15e</ref>, the top-1 accuracy of the experiments is compared<lb/> when they receive all samples. This serves as evidence of the<lb/> superiority of the DeiT small model and the utilization of the<lb/> 2D lifting scheme.<lb/></p>

			<p>As mentioned above, the IRET layer facilitates a dynamic<lb/> balance between computational complexity and model accu-<lb/>racy. In the realm of approximate computing, the ideal<lb/> scenario is achieving a substantial reduction in computational<lb/> complexity with only a minor impact on performance.<lb/></p>

			<p>IRET exemplifies this by enabling dynamic observation of<lb/> such trade-offs. The token dropping and focusing attention<lb/> threshold in each IRET layer is the control knob for this trade-<lb/>off. The threshold could be different for each IRET encoder.<lb/> However, for simplicity in this study, we apply a uniform<lb/> attention threshold across all IRET layers, leaving detailed<lb/> exploration of threshold variations for future research.<lb/></p>

			<p>Table <ref type="table">1</ref> presents the top-1 and top-5 accuracy, FLOP<lb/> count, and parameters of IRET under various attention<lb/> thresholds for token dropping and focusing. The IRET&apos;s<lb/> parameter count remains constant at 17.24M, but attention<lb/> thresholding reduces the number of parameters actively used<lb/> by discarding those related to dropped tokens. It&apos;s important<lb/> to differentiate between used parameters and those loaded<lb/> from memory, as data movement depends on the hardware<lb/></p>

			<figure>FIGURE 18. Visualizing forget and focus with attention threshold of 0.0004 for samples from the ImageNet dataset.<lb/></figure>

			<p>accelerator&apos;s architecture, including buffer sizes and mapping<lb/> solutions. Reduction in used parameters leads to decreased<lb/> data movement in the hardware accelerator, which we plan<lb/> to explore further in future work. Fig. <ref type="figure">16</ref> visualizes how<lb/> increasing the threshold size effectively prunes the model<lb/> with minimal impact on accuracy. This balance is achieved<lb/> by the token-dropping module reducing complexity and the<lb/> focusing module maintaining accuracy.<lb/> </p>
				
			<p>Fig. <ref type="figure">17</ref> presents the average pruning results across different<lb/> threshold values, illustrating how the number of dropped<lb/> tokens varies within each layer, as averaged over the<lb/> ImageNet test set. This figure demonstrates the sensitivity<lb/> of our pruning strategy to the attention threshold. In the<lb/> IRET model presented in this paper, there are 3 IRET encoder<lb/> layers. As illustrated, by increasing the pruning threshold, the<lb/> number of dropped tokens in each layer and total number<lb/> of dropped tokens increases. In the extreme case, with the<lb/> attention pruning threshold of 3E-3, as illustrated in this<lb/> figure, 109 tokens are dropped in layer 4 (IRET layer 1), 61 in<lb/> layer 5, and 15 in layer 6. In this case, from table 1, the top-1<lb/> accuracy of 71.11 and top-5 accuracy of 88.97 is achieved by<lb/> focusing on only 13 tokens.<lb/></p>

			<p>Fig. <ref type="figure">18</ref> showcases the practical impact of our pruning<lb/> strategy on individual samples. This figure visually compares<lb/> the original token distribution with the pruned results at<lb/> different layers, emphasizing how the model selectively<lb/> focuses on the most critical tokens. The visualizations<lb/> provide a clear depiction of how irrelevant or less important<lb/> tokens are gradually removed as the layers progress, allowing<lb/> the model to allocate more resources to the most informative<lb/> regions.<lb/></p>

			<p>Fig. <ref type="figure">19</ref> illustrates the trade-off between computational<lb/> complexity and accuracy for IRET, comparing it to prior<lb/> art solutions. Increasing the attention threshold in IRET<lb/> leads to a gradual decline in accuracy but with a significant<lb/> reduction in computational complexity. It&apos;s crucial to note<lb/> that the data points for ATS <ref type="biblio">[14]</ref>, DeiT <ref type="biblio">[55]</ref>, ResNet <ref type="biblio">[20]</ref>,<lb/> and AdaVIT <ref type="biblio">[41]</ref> represent different models. For ResNet,<lb/> the accuracies correspond to models with varying depths<lb/> from 18 to 152 layers. DeiT and ATS models differ in<lb/> embedding sizes (384, 318, 258, 192), meaning each point<lb/> reflects a distinct model architecture optimized for specific<lb/> accuracy. In contrast, all IRET data points are derived from<lb/> the same architecture, starting with an embedding size of<lb/> 192 and incrementally increasing it through the IRET encoder<lb/> layers to 294, 348 and 384 respectively. The variations<lb/> in IRET&apos;s FLOP count and accuracy are due to different<lb/> attention thresholds for token dropping, assumed uniform<lb/> across all layers in this study. Adjusting these thresholds<lb/> layer-wise in IRET, with incremental increases, could further<lb/> enhance accuracy.<lb/></p>

			<p>It is also worth noting that in IRET, token focusing<lb/> and dropping occur in layers 4, 5, and 6 (IRET layers),<lb/> whereas in ATS, token dropping is applied in all layers<lb/> past the third encoder. Combining IRET and ATS could<lb/> potentially yield higher accuracy. This approach, alongside<lb/> the exploration of various thresholds, learnable thresholds,<lb/> and the integration of ATS with other pruning techniques,<lb/> will be a focus of our future work. As shown, IRET<lb/> initially has slightly lower accuracy than ATS and DeiT<lb/> without token dropping. However, with the implementation<lb/> of the Focus concept and increased token dropping, IRET<lb/> achieves better accuracy than ATS and DeiT at similar FLOP<lb/> counts for higher attention thresholds. IRET&apos;s consistent<lb/> architecture and the FLOP reduction achieved solely through<lb/> threshold control, coupled with its superior accuracy in lower<lb/> FLOP count regions, positions it as an efficient solution<lb/> for edge applications balancing accuracy with computational<lb/> complexity, enabling its use in energy and latency-sensitive<lb/> applications.<lb/></p>

			<figure>FIGURE 19. Comparing the tradeoff between accuracy and flop count in IRET with that of prior art<lb/> solutions. Adopting the concept of Focus allows the IRET to enjoy a gentler drop in accuracy while<lb/> increasing the attention threshold used for token dropping and focusing.<lb/></figure>

			<figure type="table">TABLE 2. IRET with multi-head architecture Top-1 accuracy of heads.<lb/></figure>

			<figure type="table">TABLE 3. IRET with multi-head architecture GFLOPS count based on different confidence threshold.<lb/></figure>

			<head>A. MUTIHEADED IRET ENERGY-AWARE BEHAVIOR<lb/></head>

			<p>Table <ref type="table">2</ref> shows the Top-1 accuracy of MLP heads for different<lb/> variants of IRET. As you can see from head 7 we have an<lb/> acceptable prediction accuracy for all variants of IRET, so the<lb/> model gives us the possibility to do the prediction earlier,<lb/> especially for simple images, and avoid the computation of<lb/> final layers of the model and save power and energy. For the<lb/> real-time scenario, with a preset deadline, the model can do<lb/> the prediction with high confidence and meet the deadline.<lb/> The proposed multi-headed architecture provides an energy-<lb/>effective solution. This means that for some applications<lb/> using this architecture, we can do earlier predictions when<lb/> we reach the desired confidence level. We have tested the<lb/> effect of different confidence values on IRET. Fig. <ref type="figure">20</ref> shows<lb/> the behavior of the IRET variant with attention threshold =<lb/> 0.0004 based on different confidence values. As the figure<lb/> shows for smaller confidence values like 60 the model can<lb/> do the prediction for all the images using heads 4-6 so the<lb/></p>

			<figure>FIGURE 20. The effect of different confidence thresholds on the<lb/> multi-exit IRET. The attention threshold is 0.0004.<lb/> computation of the reset of layers is skipped. The dark orange<lb/> color shows the percentage of classified images using the<lb/> current head and the light orange shows the percentage of<lb/></figure>

			<figure>FIGURE 21. The effect of different attention thresholds on the multi-exit<lb/> IRET. The confidence threshold is 70.<lb/></figure>

			<p>classified images in the previous layers. But these predictions<lb/> are based on the confidence threshold so not all of the images<lb/> are classified correctly. The dark gray shows the percentage<lb/> of the correctly classified images in each head and the light<lb/> gray shows the percentage of the correctly classified images<lb/> in previous heads. By increasing the confidence threshold<lb/> the ability of the model to do the prediction in earlier layers<lb/> decreases and the models switch to later layers for prediction<lb/> so the computation requirement of the model and as a result<lb/> of that the energy requirement of it increases by increasing<lb/> the confidence. Moreover, the gap between the images<lb/> classified in each head and the number of correctly classified<lb/> images in each head decreases. Fig. <ref type="figure">21</ref> shows the effect of<lb/> different Attention threshold values for a constant confidence<lb/> threshold = 70. By increasing the Attention threshold more<lb/> tokens drop so the model needs more layers to extract the<lb/> features and do the classification. In both experiments, some<lb/> of the images remain unclassified. The gap between 100 and<lb/> the last bar shows the number of remaining images that are<lb/> not classified. Table <ref type="table">3</ref> shows the flop count of the different<lb/> variants of multi-head IRET based on different confidence<lb/> values. For smaller confidence values the GFLOPS of the<lb/> model reduces significantly. With a smaller confidence value,<lb/> the model can perform the prediction at an earlier head<lb/> and avoid the rest of the computation. However, increasing<lb/> confidence needs more understanding of the image features<lb/> by the model and more number layers and as a result more<lb/> computation.<lb/></p>

			<head>VI. CONCLUSION<lb/></head>

			<p>In this study, we introduced the IRET encoder, a novel<lb/> encoder layer that not only drops unattended tokens but<lb/> also enhances the model&apos;s focus on attended ones using<lb/> incremental input sampling and increased embedding size.<lb/> IRET transformer, constructed using a mix of IRET and<lb/> basic transformer encoders. Based on the choice of attention<lb/> threshold for token dropping and token focusing, IRET allows<lb/> us to trade accuracy for computational complexity. The<lb/> IRET&apos;s ability to focus attended tokens using incremental<lb/> input sampling allows a more graceful degradation in<lb/> accuracy in the result of dropping tokens compared to<lb/> prior art solutions. Notably, its computational complexity is<lb/> modulated through attention threshold adjustments, rather<lb/> than changes in embedding size or model architecture.<lb/> The combination of this unique feature alongside early<lb/> exiting renders IRET ideal for applications requiring a<lb/> balance between accuracy, energy efficiency, and latency<lb/> considerations.<lb/></p>

			<p>While IRET offers promising advancements, there are<lb/> areas for further improvement. Future work will explore<lb/> incorporating token pruning mechanisms, such as mixing<lb/> IRET with ATS, to enhance accuracy. Additionally, we aim<lb/> to investigate new attention computation methods to further<lb/> optimize performance. To address practical implementation<lb/> challenges, we plan to pursue a hardware-software co-design<lb/> approach, exploring both the hardware and algorithmic<lb/> aspects to make IRET more efficient and scalable across<lb/> diverse deployment scenarios. Moreover, we plan to leverage<lb/> the IRET architecture for tasks that demand high-resolution<lb/> inputs or involve datasets with limited structural regularities.<lb/> We will assess its performance and fine-tune it to ensure<lb/> optimal functioning for these specific applications.</p>


	</text>
</tei>
