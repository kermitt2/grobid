<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>1 INTRODUCTION<lb/></head>

			<p>Twitter is a micro-blogging platform that allows people to quickly<lb/> send short messages, called tweets, on the internet. This platform<lb/> became very popular, reaching 500 million users in February 2012<lb/> <ref type="biblio">[14]</ref>. On average, about 6000 tweets are published on Twitter per<lb/> second, which has triggered an increasing number of studies about<lb/> how to build a good simulation of twitter traffic <ref type="biblio">[5][7]</ref>. In particular,<lb/> Hawkes process, a self-activation process where previous events<lb/> can have impacts on later occurrences, gives a reasonable way to<lb/> conduct the simulation. However, previous research lacks visual-<lb/>ization of the detailed generating process which makes it hard for<lb/> practitioners to imagine what is achieved by the model <ref type="biblio">[6]</ref>. In this<lb/> research, we show a step-by-step approach of applying Hawkes<lb/> proess to the Twitter simulation. Simulating social media offers<lb/> important insights that can be leveraged to enhance marketing,<lb/> predicting web-crisis, analyzing information transmission, and ad-<lb/>justing trading strategies <ref type="biblio">[4][10]</ref>. In this paper, we emphasize an<lb/> important application-using importance sampling to simulate the<lb/> chance of &quot;Twitpocalypse&quot;, a bug that happened a few years ago due<lb/> to extremely high volume of tweets during a given period of time.<lb/> However, for other applications mentioned above, it&apos;s hard to ex-<lb/>tract relevant information from Hawkes process model because the<lb/> model doesn&apos;t reflect the specific activities on a micro-scale even<lb/> under our step-by-step implementation strategy. Some research<lb/> uses agent-based modeling that considers a specific set of users<lb/> in the model and makes them carry out certain activities follow-<lb/>ing some fixed rules <ref type="biblio">[4]</ref>. This approach provides valuable insights<lb/> but lacks mathematical rigorousness and evaluating numeric re-<lb/>sults becomes impossible. We realize that one important thing that<lb/> Hawkes process fails to incorporate is the network structure of<lb/> social media where the way users are connected can have a huge<lb/> impact on the performance of the simulation model. Therefore, we<lb/> design a modified version of Hawkes process which embeds this<lb/> traditional model within a graph structure that represents the user<lb/> relationship. This approach successfully combines the advantages<lb/> of both agent-based modeling which focuses on individual level<lb/> dynamics and Hawkes process which treats the entire social media<lb/> as a whole. This modification provides traditional Hawkes process<lb/> with a broader range of application while maintaining its original<lb/> self-activating property. The detailed implementation of this model<lb/> is also discussed in this article. We have not yet conducted a detailed<lb/> analysis of the statistical properties of the model, which we leave<lb/> to future work.<lb/></p>

			<head>2 HAWKES PROCESS<lb/></head>
			<head>2.1 Model description and implementation<lb/></head>

			<p>In previous studies, people have been relying on Hawkes process,<lb/> a self-exciting process brought up by Hawkes, A. G. in 1971, to<lb/> simulate activities that involve interactions between events <ref type="biblio">[11]</ref>.<lb/> For example, an interesting application is to use Hawkes process to<lb/> model the queues in front of nightclub <ref type="biblio">[3]</ref>. Usually, we use Poisson<lb/> process to model the arrivals of customers, or night-club visitors in<lb/> this case. However, the Poisson model ignores the fact that people<lb/> may have stronger interests in night-club that seem to be popular(i.e<lb/> a long queue outside the nightclub). Hawkes process successfully<lb/> remedied this issue by taking into account the mutual effects be-<lb/>tween different events. The original formulation of this process is<lb/> defined as following:<lb/> A self-exciting temporal point process N whose conditional intensity<lb/> function 𝜆 = 𝜆(𝑡), given a specific time t, is defined to be<lb/></p>

				<formula>𝜆(𝑡) = 𝜇 (𝑡) +<lb/> ∑︁<lb/> 𝑖:𝜏 𝑖 &lt;𝑡<lb/> 𝑣 (𝑡 -𝜏 𝑖 )<lb/></formula>

			<p>where the constant function 𝜇 (𝑡), also denoted as 𝜆 0 as 𝜆(𝑡) = 𝜇 (𝑡)<lb/> when 𝑡 = 0, is the initial rate of the process 𝑁 which is usually rep-<lb/>resented as a constant function, 𝜏 𝑖 are the points in time occurring<lb/> prior to time 𝑡, and 𝑣 is a monotone decreasing non-negative function<lb/> which governs the clustering density of 𝑁 .<lb/></p>

			<p>This model has also been implemented to generate simulations<lb/> of social media activities including tweets <ref type="biblio">[15]</ref>. Intuitively, Hawkes<lb/> process should be able to produce an accurate simulation of Twitter<lb/> activities due to its self-exciting property. However, in this article,<lb/> we will discuss some of the current limitations and bring up a po-<lb/>tential solution. We also found that most of the previous studies<lb/> were more focused on the theoretical part of this model and the<lb/> actual application was discussed but not explicitly carried out <ref type="biblio">[13]</ref>.<lb/> In this part, in order to present how this model actually works in<lb/> application, we write a simulation program using Python with the<lb/> help of Numpy. To show more details about how the model could<lb/> be generated step by step, we also visualized the entire process<lb/> using Matplotlib with relatively small parameters due to the lim-<lb/>ited computational resources. This example should provide those<lb/> who are not familiar with Hawkes process or are interested in how<lb/> the self-exciting property is displayed in practice a comprehensive<lb/> overview in a way that is easy to understand.<lb/></p>

			<head>Step 1: Generate the first generation. </head>
			<p>In order to make the cal-<lb/>culation more convenient, we implement the Hawkes process gen-<lb/>eration by generation. For the first generation 𝑘 = 0, we simulate<lb/> times using the homogeneous Poisson process with initial intensity<lb/> function 𝜇 : R → R defined as 𝜇 (𝑥) = 𝜆 0 if 𝑥 &gt; 0 and 0 otherwise.<lb/></p>

				<formula>𝑃 = {𝑡<lb/> (0)<lb/> 1 , ..., 𝑡<lb/> (0)<lb/> 𝑁<lb/> (0)<lb/> 𝑡<lb/> } ∼ 𝑃𝑃 (𝜆 0 ),<lb/> (Poisson process of intensity 𝜆 0 )<lb/></formula>

			<head>Step 2: Define the lambda function. </head>
			<p>The definition above has<lb/> already explained the most part of the model formulation except<lb/> the function 𝑣, which is supposed to be a monotone decreasing<lb/> non-negative function. This function reflects how the impact of<lb/> previous events diminishes over time but remains positive. In our<lb/> example, we just follow the convention of using exponential func-<lb/>tion to represent such trend. We will set parameters 𝑎, 𝑏 ∈ R + and<lb/> define the function 𝑣: R + → R + as 𝑣 (𝑥) = 𝑎 • 𝑒 -𝑏𝑥 .<lb/></p>

			<head>Step 3: Calculate the maximal intensity for each generation<lb/> and produce the next generation based on this maximal in-<lb/>tensity.</head>
			<p>To continue generating the other generations, we shall<lb/> find the greatest value that 𝜆(𝑡) can take given the current genera-<lb/>tion and use that intensity to generate the next generation. Then,<lb/> we shall select some events generated during this process to be<lb/> removed based on how likely these events would happen with the<lb/> real non-homogeneous intensity. Suppose we have the current gen-<lb/>eration of 𝑛 events {𝜏 𝑖 } 𝑛<lb/> 𝑖=1 . Since, the previous generation is a finite<lb/> set, the maximum exists for this function. Since the function 𝜆(𝑡)<lb/> is decreasing within each interval [𝜏 𝑖 , 𝜏 𝑖+1 ] with 𝑖 = 1, 2, . . . , 𝑛 -1,<lb/> 𝑚𝑎𝑥 {𝜆(𝑡) : 𝑡 ∈ R + } ∈ {𝜆(𝜏 𝑖 ) : 𝑖 ∈ 1, 2, . . . , 𝑛}. Thus, we just need<lb/> to compare a few values to get the maximum, which significantly<lb/> reduce the amount of computation.<lb/></p>

			<head>Step 4: Decide which points in the current generation should<lb/> be kept using random number generator. </head>
			<p>It&apos;s obvious that fol-<lb/>lowing Step 3, we would generate more events than we expected<lb/> to have because the intensity function doesn&apos;t always stay at its<lb/> maximum. Given a time 𝜏 generated based on the current genera-<lb/>tion, to decide if we should keep it, we calculate the chance of that<lb/> event by measuring the ratio of 𝜆(𝜏) to 𝑚𝑎𝑥 {𝜆(𝑡) : 𝑡 ∈ R + } which<lb/> has already been determined. When we finalize the 𝑘 𝑡ℎ generation,<lb/> we will denote the set of events as 𝑃 𝑘 and move on to generate the<lb/> 𝑘 + 1 𝑡ℎ generation using the same procedure.<lb/></p>

			<head>Step 5: Terminate the algorithm and build the superimposed<lb/> process </head>
			<p>For each generation, we will set a time threshold 𝑇 so that<lb/> events generated after 𝑇 will be discarded as time dimension can<lb/> go to infinity and we won&apos;t keep track of it. If there is no new event<lb/> generated based on the current generation, we will decide to termi-<lb/>nate the algorithm as the previous impacts have shrunk to minimal.<lb/> The randomness of this process makes the number of generations<lb/> hard to predict. However, some previous studies have shown that<lb/> we can compute the expected value and standard deviation with the<lb/> help of differential equations. We shall also experimentally validate<lb/> those results in our current study. Then, the entire Hawkes process<lb/> can be summarized by summing up all the previous generations<lb/></p>

				<formula>{𝑡 1 , .., 𝑡 𝑁 𝑇 } =<lb/> 𝐾-1<lb/> 𝑘=0<lb/> Π (𝑘)<lb/></formula>

			<p>where</p>

				<formula>𝑁 𝑡 = 𝐾-1<lb/> 𝑘=0 𝑁<lb/> (𝑘)<lb/> 𝑡 .<lb/></formula>

			<head>Step 6: Visualization </head>
			<p>The simulation result can be visualized using<lb/> Python. Here, as shown in Figure <ref type="figure">1</ref>, we pick two generations that<lb/> clearly demonstrate how the events are produced, generation by<lb/> generation.<lb/></p>

			<head>2.2 Complexity Analysis<lb/></head>

			<p>The efficiency of our implementation depends on the ratio between<lb/> the number of actual events that are accepted and the number<lb/> of total events generated. As shown in Figure <ref type="figure">2</ref>, this algorithm<lb/> works well with small sample size and small parameters but gets<lb/> less efficient with larger parameters and time span. Previously,<lb/> Ogata(<ref type="biblio">[11]</ref>) proposed a fast algorithm implementing Hawkes pro-<lb/>cess. Put briefly, their algorithm generates the entire Hawkes pro-<lb/>cess simultaneously instead of breaking it into several generations.<lb/> Here, we chose to use this step-by-step method for simplicity and<lb/> in order to clarify the mechanism and make better visualization.<lb/></p>

			<head>3 APPLICATION: RARE EVENT SIMULATION<lb/></head>

			<p>One important application of this model is that it can be utilized to<lb/> estimate the probability of an extremely rare event-&quot;twitpocalypse&quot;<lb/> <ref type="biblio">[2]</ref>. Twitter labeled each tweet with an unique ID and the largest<lb/> amount of IDs that can be maintained is 2, 147, 483, 647, the largest<lb/> number that can be stored as &quot;signed integer&quot;. A decade ago, more<lb/> than 20 billion tweets had been sent under 3 years, which went<lb/></p>

			<figure>Figure 1: Visualization of Hawkes Process<lb/>At the beginning, we set the threshold and parameters based on<lb/> which the first generation is produced. For each generation, we use<lb/> the maximum 𝜆 to generate a sample and keep those whose value, as<lb/> determined by the random number generator, fall under 𝜆(𝜏). We<lb/> can see that the number of points first goes up and then goes down as<lb/> we produce more generations. Eventually, the impact inserted by the<lb/> early events diminishes and the curve shifts to the right until no more<lb/> event is generated.<lb/></figure>

			<figure>Figure 2: Heatmap of Model&apos;s Complexity<lb/> We run experiments with different sets of parameters and output the<lb/> results in this heat map visualization. We can see that as 𝛼 gets<lb/> larger and 𝛽 gets smaller, the efficiency of our algorithm drops as less<lb/> points generated are used as the final output. Therefore, our method<lb/> might fit some specific scenarios but we should seek better<lb/> approaches when dealing with larger tasks.<lb/></figure>

			<p>far beyond the expectation of Twitter developers and caused a<lb/> crash. Moreover, a lot of third-party apps that were connected<lb/> with Twitter couldn&apos;t handle this large number and also crashed<lb/> during this incident. As Twitter gains increasing popularity, some<lb/> people have been suspicious about with what chance there will<lb/> another &quot;twitpocalypse&quot; and when it will happen. Due to Hawkes<lb/> Process&apos;s superior ability to simulate the nature of social media<lb/> streams, it can serve as a good tool to approximate the chance<lb/> of reaching an enormous traffic in social media with the aid of<lb/> importance sampling. In this section, we will introduce the most<lb/> common importance sampling method and discuss how this method<lb/> can be implemented under this Hawkes Process context.<lb/></p>

			<head>3.1 Importance Sampling<lb/></head>

			<p>In statistics, importance sampling is a popular variance reduction<lb/> strategy used to estimate a rare event that is hard to be obtained<lb/> using traditional ways such as direct calculation and Monte Carlo<lb/> estimation <ref type="biblio">[8]</ref>. Most of the distributions are too complicated to de-<lb/>rive a formula for computing probability so people usually just use<lb/> Monte Carlo estimation which runs a large number of experiments<lb/> and approximates the actual probability of a event with the em-<lb/>pirical probability. However, if the actual probability is extremely<lb/> low, as in the case of Twitpocalypse, we will need a huge amount<lb/> of experiments just to get one single occurrence of the event and<lb/> clearly this estimated probability is very unstable. Therefore, when<lb/> the chance of a certain event is really low, it is a common practice to<lb/> use importance sampling which reduced the variance and results in<lb/> a much higher robustness <ref type="biblio">[1]</ref>. The formal description of one way to<lb/> do importance sampling is following: Suppose we want to estimate<lb/> 𝜌 = E[𝜂 (𝑋 )] where 𝑋 is a random variable describing some obser-<lb/>vation, 𝜂 is an indicator function of some events, and 𝜌 is just the<lb/> probability of this set of events. Then, one way to estimate 𝜌 is to<lb/> generate a sequence of i.i.d. random numbers 𝑋 <ref type="biblio">(1)</ref> , 𝑋 <ref type="biblio">(2)</ref> , . . . , 𝑋 <ref type="biblio">(𝑛)<lb/></ref> and then compute the empirical probability<lb/></p>

				<formula>ρ =<lb/> 1<lb/> 𝑛<lb/> 𝑛<lb/> ∑︁<lb/> 𝑖=1<lb/> 𝜂 (𝑋 (𝑖) ).<lb/></formula>

			<p>However, as we just discussed, this could lead to an inaccurate and<lb/> unstable result if the probability we are estimating is too low. In<lb/> order to solve this, we can introduce a new variable variable 𝑌 and<lb/> generate a sequence of i.i.d. random numbers 𝑌 (1) , 𝑌 (2) , . . . , 𝑌 (𝑘) .<lb/> Suppose 𝑋 is equipped with a probability density function 𝑝 (•) and<lb/> 𝑌 is equipped with a probability density function 𝑞(•). Then, as<lb/> long as 𝑠𝑢𝑝𝑝𝑜𝑟𝑡 (𝑝 • 𝜂) ⊂ 𝑠𝑢𝑝𝑝𝑜𝑟𝑡 (𝑞 • 𝜂), we can show<lb/></p>

			<formula>ρ =<lb/> 1<lb/> 𝑛<lb/> 𝑛<lb/> ∑︁<lb/> 𝑖=1<lb/> 𝜂 (𝑋 (𝑖) )<lb/> =<lb/> 1<lb/> 𝑘<lb/> 𝑛<lb/> ∑︁<lb/> 𝑖=1<lb/> ∫<lb/> 𝜂 (𝑥 (𝑖) )𝑝 (𝑥 (𝑖) )𝑑𝑥<lb/> =<lb/> 1<lb/> 𝑘<lb/> 𝑛<lb/> ∑︁<lb/> 𝑖=1<lb/> ∫<lb/> 𝜂 (𝑥 (𝑖) )<lb/> 𝑝 (𝑥 (𝑖) )<lb/> 𝑞(𝑥 (𝑖) )<lb/> 𝑞(𝑥 (𝑖) )𝑑𝑥<lb/> = E 𝑞 [𝜂 (𝑌 )<lb/> 𝑝 (𝑌 )<lb/> 𝑞(𝑌 )<lb/> ]<lb/> =<lb/> 1<lb/> 𝑛<lb/> 𝑛<lb/> ∑︁<lb/> 𝑖=1<lb/> 𝜂 (𝑌 (𝑖) )<lb/> 𝑝 (𝑌 (𝑖) )<lb/> 𝑞(𝑌 (𝑖) )<lb/></formula>

			<p>In practice, we can define a random variable 𝑌 where the rare even<lb/> is much more likely to happen and run a Monte Carlo simulation<lb/> with this 𝑌 . This result has been proved to be unbiased. In the<lb/> next section, we will discuss the specific implementation of this<lb/> technique in Hawkes Process and how the probability of a rare<lb/> event, &quot;Twitpocalypse&quot;, can actually be estimated.<lb/></p>

			<head>3.2 How to implement on Hawkes Process<lb/></head>

			<p>For Hawkes Process, previous research has derived several theo-<lb/>rem regarding its limit behavior, expectation, and variance of the<lb/> intensity and number of events generated. In this paper, we will<lb/> demonstrate how we can put together the previous results and<lb/> apply them into an importance sampling implementation. Given a<lb/> certain set of parameters 𝛼, 𝛽, 𝜆 0 = 𝜇 (𝑡), the limit of intensity 𝜆 𝑡 as<lb/> 𝑡 → ∞ is formulated as<lb/></p>

				<formula>𝜆 ∞ =<lb/> 𝛽𝜆 0<lb/> 𝛽 -𝛼<lb/> .<lb/></formula>

			<p>Also, the formula for approximating the expected value has been<lb/> obtained by solving a differential equation system (<ref type="biblio">[3]</ref>),<lb/></p>

				<formula>E[𝑁 𝑡 ] =<lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/>  <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> <lb/> 𝜆 ∞ 𝑡 + 𝜆 0 -𝜆 ∞<lb/> 𝛽-𝛼 (1 -𝑒 -(𝛽-𝛼)𝑡 )<lb/> if 𝛼 &lt; 𝛽<lb/> 𝛽𝜆 0<lb/> (𝛼-𝛽) 2 + 𝜆 0<lb/> 𝛼-𝛽 (𝑒 (𝛼-𝛽)𝑡 -1) -<lb/>𝛽𝜆 0<lb/> 𝛼-𝛽 𝑡 if 𝛼 &gt; 𝛽<lb/> 𝛽𝜆 0<lb/> 2 𝑡 2 + 𝜆 0 𝑡<lb/> if 𝛼 = 𝛽<lb/> (<label>1</label>)<lb/></formula>

			<p>From these formula, it&apos;s clear that as we increase the initial intensity<lb/> 𝜆 0 , the expected number of events will increase. Therefore, our<lb/> strategy is to build another Hawkes Process with a larger initial<lb/> intensity λ0 such that the corresponding expected value is equal<lb/> to the extreme value we are trying to simulate. In this case, the<lb/> extreme value is just the maximum number of tweets that can be<lb/> stored as a &quot;signed integer&quot; and handled by the system, which is<lb/> approximately equal to 2 32 . Hence, the occurrence of this rare event<lb/> can be represented as 𝑁 𝑡 &gt; 2 32 with 𝑡 being the time period we are<lb/> interested in. Therefore, we can first compute E[𝑁 𝑡 |𝜆 0 ] and set<lb/></p>

				<formula>λ0 =<lb/> 2 32<lb/> E[𝑁 𝑡 |𝜆 0 ]<lb/> .<lb/></formula>

			<p>The next step is to compute the ratio between the likelihoods of<lb/> these two processes to fully construct the importance sampling. An-<lb/>other research has shown that the likelyhood function for Hawkes<lb/> Process takes the following form: Suppose our Hawkes Process takes<lb/> 𝜆 0 , 𝛼, 𝛽 as its parameters and 𝑡 as its time threshold. Then, by using<lb/> results on likelihood of point process, we can obtain the likelihood of<lb/> this Hawkes Process as<lb/></p>

				<formula>𝐿 𝜆 0 = 𝑒𝑥𝑝{<lb/> ∫ 𝑡<lb/> 0<lb/> (1 -𝜆(𝑠))𝑑𝑠 +<lb/> ∫ 𝑡<lb/> 0<lb/> 𝑙𝑛(𝜆(𝑠))𝑑𝑁 𝑠 }.<lb/></formula>

			<p>In particular, if we have the collection of tweets {𝑡 1 , 𝑡 2 , . . . , 𝑡 𝑛 }, the<lb/> above equation can be rewritten as<lb/></p>

				<formula>𝐿 𝜆 0 (𝑡 1 , 𝑡 2 , . . . , 𝑡 𝑛 ) = 𝑒𝑥𝑝{𝑡 -𝑡𝜆-<lb/>𝑛<lb/> ∑︁<lb/> 𝑖=1<lb/> 𝛼 (1-𝑒 -𝛽 (𝑡 -𝑡 𝑖 ) )+<lb/> 𝑛<lb/> ∑︁<lb/> 𝑖=1<lb/> 𝑙𝑛(𝜆(𝑡 𝑖 ))}.<lb/></formula>

			<p>Using the importance sampling technique mentioned in the last<lb/> section, if we take a very large 𝑚 and run the experiments with the<lb/> Hawkes Process with initial intensity λ0 as discussed above. We can<lb/> use the results, denoted as 𝑌 1 , 𝑌 2 , . . . , 𝑌 𝑚 where each 𝑌 𝑘 is a set of<lb/></p>

			<figure>Figure 3: Estimating the chance of Twitpocalypse with Im-<lb/>portance Sampling<lb/> We use Monte Carlo simulation technique to estimate the probability<lb/> of exceeding each of these 10 thresholds within 10 time units, each<lb/> with 100 experiments. Then, we calculate the estimated probability<lb/> and standard deviation for each threshold. We can see that as the<lb/> threshold gets larger, the chance of exceeding that threshold within a<lb/> given time period gets extremely low. Also, we can tell that the model<lb/> is very stable because the standard deviation is about 12 to 13 orders<lb/> of magnitude smaller than the expected value.<lb/></figure>

			<p>all the tweets 𝑡 𝑘,1 , 𝑡 𝑘,2 , . . . , 𝑡 𝑘,𝑛 generated in that experiment, to es-<lb/>timate the actual probability of original Hawkes Process generating<lb/> more than 2 31 tweets within 𝑡 time period<lb/></p>

			<formula>ρ =<lb/> 1<lb/> 𝑚<lb/> 𝑚<lb/> ∑︁<lb/> 𝑖=1<lb/> 𝜂 (𝑌 (𝑖) )<lb/> 𝐿 𝜆 0 (𝑌 (𝑖) )<lb/> 𝐿 λ0<lb/> (𝑌 (𝑖) )<lb/>.<lb/></formula>

			<p>Note: in this case, 𝜂 (𝑌 ( 𝑖)) is the indicator variable about if 𝑖 𝑡ℎ<lb/> experiment produces more than 2 32 tweets and this should appear<lb/> very frequent as we have carefully chosen a large λ0 . Estimating the<lb/> real probability of twitpocalypse would require significant amount<lb/> of computational resources due to the large sample size. In our<lb/> research, we evaluate the probability of generating more than 1000<lb/> tweets within 10 units of time which, according to the formula<lb/> presented above, is supposed to have an expected value of 34.548.<lb/> The numeric results are summarized in Table <ref type="table">3</ref>.<lb/></p>

			<head>4 LIMITATIONS OF THE TRADITIONAL<lb/> APPROACH<lb/></head>

			<p>However, the traditional approach of representing Twitter activities<lb/> using Hawkes process fails to consider the social network structure<lb/> and individual differences, which would limit the model&apos;s perfor-<lb/>mance in a more micro-scale simulation and prediction. The tradi-<lb/>tional model may have an important role in predicting extremely<lb/> rare events like &quot;Twitpocalypse&quot;, but the scope of applications is<lb/> restricted. For example, we may not find the traditional model very<lb/> helpful if we are interested in questions like &quot;under what kind of<lb/> social network structure will a given user&apos;s tweets be most likely<lb/> forwarded?&quot; In practice, a lot of tasks that require a simulation of<lb/> Twitter activities are focused on micro-scale Twitter activity <ref type="biblio">[9]<lb/></ref> and aiming to explore more detailed properties about Twitter usage<lb/> <ref type="biblio">[12]</ref>. By far, although modeling of social network has received an<lb/> increasing interest due to the data availability and growth in me-<lb/>dia usage, very few techniques have been developed for modeling<lb/> social media activities with a focus on the impact brought by a spe-<lb/>cific network structure. With the same set of users, if we represent<lb/> their relationships with a network structure, different networks<lb/> might generate totally different activity patterns. A good property<lb/> of Hawkes process is that it successfully captures how a series of<lb/> events interact and affect each other, which can&apos;t be represented<lb/> by the traditional Poisson process due to its memoryless property.<lb/> Therefore, we intend to incorporate this important property into<lb/> our model while embedding it with a graph structure so that more<lb/> detailed information is included. In the next section, we will give a<lb/> thorough description of the model design and a brief example of<lb/> its implementation.<lb/></p>

			<head>5 GRAPH MODEL<lb/></head>
			<head>5.1 Model Definition<lb/></head>

			<p>The traditional way of simulating Twitter activities using Hawkes<lb/> Process considers all the twitter activities as a whole but fails to cap-<lb/>ture individual difference. This method could be useful in measuring<lb/> and estimating the total Twitter volume but hard to be applied into<lb/> some more micro-level scenarios like predicting the number of<lb/> retweets that a message can generate. At this point, we also realize<lb/> that the structure of social network plays a significant role in users&apos;<lb/> twitter activity. For example, it might be easier for a user with more<lb/> followers to get more retweets. In this part, we will introduce a<lb/> more realistic simulation that embeds Hawkes Process into a graph<lb/> structure and simulate each user&apos;s twitter activity(represented by<lb/> node) based on the arrival of tweets sent by those the user is follow-<lb/>ing. In our model, each node will represent a twitter user and the<lb/> edges (directed) can represent if one user is following or followed<lb/> by another user.<lb/></p>

			<p>Definition 1. Given a group of users, if any of them is not fol-<lb/>lowed or following any user not from this group, this group is called a<lb/> closed Twitter network.<lb/></p>

			<p>Definition 2. If one user is followed by or following another user,<lb/> we say these two users are connected. If a is connected to b and b is<lb/> connected to c, then a is connected to c. In other words, transitivity<lb/> holds. Given a closed Twitter network, if every pair of users in this<lb/> network are connected, then this network is irreducible.<lb/></p>

			<p>Definition 3. Each closed and irreducible Twitter network with<lb/> 𝑛 users has a directed graph representation 𝐺 = (𝑉 , 𝐸) with 𝑉 =<lb/> {𝑉 1 , 𝑉 2 , . . . , 𝑉 𝑛 } (each vertex represents a user). For each pair of (𝑉 𝑖 , 𝑉 𝑗 )<lb/> such that 𝑉 𝑖 is following 𝑉 𝑗 , there is an edge (𝑉 𝑗 , 𝑉 𝑖 ) ∈ 𝐸.<lb/></p>

			<p>Definition 4. For each 𝑖 in {1, . . . , 𝑛}, there is an intensity 𝐼 𝑖 . If 𝑉 𝑖<lb/> doesn&apos;t follow anyone else on Twitter, then the number of tweets pub-<lb/>lished by 𝑉 𝑖 follows a Poisson process with intensity 𝐼 𝑖 . 𝑉 𝑖 &apos;s twitter ac-<lb/>tivity can be affected by the tweets published by all users that 𝑉 𝑖 is fol-<lb/>lowing. Let 𝑈 𝑖 = {𝑉 𝑗 : (𝑖, 𝑗) ∈ 𝐸}, namely the collection of users that<lb/> 𝑉 𝑖 is following. Let 𝑋 𝑖,𝜏 denote a tweet message published by the user<lb/> 𝑉 𝑖 at time 𝜏. Given a period of time 𝑇 , let 𝑀 𝑖 be the collection of tweet<lb/> messages such that 𝑀 𝑖 = {𝑋 𝑘,𝜏 : 𝑘 ∈ 𝑈 𝑖 , 𝜏 ∈ 𝑇 }. Then the retweet<lb/> activity of 𝑉 𝑖 during time 𝑇 can be modeled by a non-homogeneous<lb/> Poisson process with intensity 𝜆(𝑡) = {𝜏:𝜏 ≤𝑡,∃𝑘 ∈𝑈 𝑖 𝑋 𝑘,𝜏 ∈𝑀 𝑖 } 𝑔(𝑡 -𝜏).<lb/></p>

			<figure>Figure 4: Histogram of tweets generated over time<lb/></figure>

			<p>Instead of making the whole tweet generating process as a<lb/> Hawkes process, we assign a Hawkes function on each individ-<lb/>ual that also responds to others&apos; posts. This model inherits the<lb/> advantage of Hawkes model that takes the impacts of previous<lb/> events into account. However, the network structure of this model<lb/> makes it more suitable for social media simulation. Also, with this<lb/> graph structure, it becomes possible to do some in-depth analysis,<lb/> such as quantifying the impact generated by a KOL, with the help<lb/> of some graph theory approaches.<lb/></p>

			<head>5.2 Implementation Specifics<lb/></head>

			<p>The implementation of this model is similar to the original Hawkes<lb/> process. The major challenge in this model is that the function<lb/> of each vertex (user) changes according to other vertices. Our ap-<lb/>proach is that due to the memoryless property of exponential func-<lb/>tion, instead of generation by generation, we can run the simulation<lb/> step by step. The function for each vertex is fixed between two con-<lb/>secutive tweets in the whole network. Therefore, we can always<lb/> use the functions at current step to generate to generate the next<lb/> tweet for each individual and pick the earliest tweet to be the one<lb/> that actually takes place. Then, this tweet will help us update the<lb/> intensity function of all the users who are following this publisher<lb/> and run the next round of simulation. We experimented on a small<lb/> network with five vertices. With deliberate choice of parameters,<lb/> the tweets are generated at a very stable rate (Figure <ref type="figure">4</ref>), which is<lb/> similar to the real world situation. We can also see from Figure <ref type="figure">5<lb/></ref> that the user who follows more accounts is likely to produce more<lb/> tweets.<lb/></p>

			<p>The flexibility of this model opens up various options for appli-<lb/>cations. For example, we can assign customized parameters (𝛼, 𝛽)<lb/> to each vertex to represent different user segments. Also, we can<lb/> study the gain of adding a particular edge and the loss of deleting a<lb/> particular edge to measure how a minor change could insert impact<lb/> on the whole network.<lb/></p>

			<head>6 CHALLENGES AND FUTURE WORK<lb/></head>

			<p>The current implementation of the graph model is not as efficient as<lb/> the traditional Hawkes model. This is because we have to compare<lb/> the next tweet of each user in order to prepare and update the<lb/> model for the next step. The complexity of this implementation<lb/> algorithm could be very high as we have to do 𝑂 (|𝑉 (𝐺)|) times of<lb/></p>

			<figure>Figure 5: Visualization of network<lb/></figure>

			<p>computations and each computation goes through all the relevant<lb/> past tweets which stack up over time. A possible simplification<lb/> strategy is to clean up the previous tweets whose impacts become<lb/> minimal as they have been published for quite a while so that we<lb/> can avoid some unnecessary computations. Another challenge is<lb/> that it is really hard to employ analysis on the limit behavior and<lb/> other statistical properties such as expected value and variance<lb/> in this model due to the complicated and frequent interactions<lb/> between vertices. Our future work will be primarily focused on<lb/> developing these properties of the model to improve rigorousness<lb/> and robustness.<lb/></p>

			<head>7 CONCLUSION<lb/></head>

			<p>In this paper, we give an overview of the Hawkes process theory<lb/> and introduce a reproducible strategy to implement it using Python.<lb/> This step-by-step approach can help keep track of how the events<lb/> are generated and give researchers opportunities to observe the<lb/> trend and adjust the model in accordance with the real pattern<lb/> that is being studied. Also, we compare the complexity of our ap-<lb/>proach with an existing fast implementation of Hawkes process.<lb/> On the practical side, we demonstrate an interesting application<lb/> of Hawkes process that estimates the probability of an extremely<lb/> rare event known as &quot;Twitpocalypse&quot;. However, regarding social<lb/> media modeling, the scope provided by this traditional Hawkes<lb/> model is limited because the dynamics of social media activities<lb/> also depend on the network structure that is formed by the users. In<lb/> order to address this issue, we bring up a new model which embeds<lb/> the Hawkes process in a graph structure that incorporates both<lb/> the self-activating property and individual level features. Due to<lb/> the complexity of this new model, in this paper, we just briefly<lb/> illustrate the implementation method and run an experiment on a<lb/> small sample graph. Although there is still a lot of further work that<lb/> needs to be done to make this model more rigorous, the potential<lb/> of this model can be seen in its ability to consider very specific<lb/> situations and simulate the individual-level interactions.</p>


	</text>
</tei>
