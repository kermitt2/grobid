<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>1 INTRODUCTION<lb/></head>

			<p>Twitter is a micro-blogging platform that allows people to quickly<lb/> send short messages, called tweets, on the internet. This platform<lb/> became very popular, reaching 500 million users in February 2012<lb/> <ref type="biblio">[14]</ref>. On average, about 6000 tweets are published on Twitter per<lb/> second, which has triggered an increasing number of studies about<lb/> how to build a good simulation of twitter traffic <ref type="biblio">[5][7]</ref>. In particular,<lb/> Hawkes process, a self-activation process where previous events<lb/> can have impacts on later occurrences, gives a reasonable way to<lb/> conduct the simulation. However, previous research lacks visual-<lb/>ization of the detailed generating process which makes it hard for<lb/> practitioners to imagine what is achieved by the model <ref type="biblio">[6]</ref>. In this<lb/> research, we show a step-by-step approach of applying Hawkes<lb/> proess to the Twitter simulation. Simulating social media offers<lb/> important insights that can be leveraged to enhance marketing,<lb/> predicting web-crisis, analyzing information transmission, and ad-<lb/>justing trading strategies <ref type="biblio">[4][10]</ref>. In this paper, we emphasize an<lb/> important application-using importance sampling to simulate the<lb/> chance of &quot;Twitpocalypse&quot;, a bug that happened a few years ago due<lb/> to extremely high volume of tweets during a given period of time.<lb/> However, for other applications mentioned above, it&apos;s hard to ex-<lb/>tract relevant information from Hawkes process model because the<lb/> model doesn&apos;t reflect the specific activities on a micro-scale even<lb/> under our step-by-step implementation strategy. Some research<lb/> uses agent-based modeling that considers a specific set of users<lb/> in the model and makes them carry out certain activities follow-<lb/>ing some fixed rules <ref type="biblio">[4]</ref>. This approach provides valuable insights<lb/> but lacks mathematical rigorousness and evaluating numeric re-<lb/>sults becomes impossible. We realize that one important thing that<lb/> Hawkes process fails to incorporate is the network structure of<lb/> social media where the way users are connected can have a huge<lb/> impact on the performance of the simulation model. Therefore, we<lb/> design a modified version of Hawkes process which embeds this<lb/> traditional model within a graph structure that represents the user<lb/> relationship. This approach successfully combines the advantages<lb/> of both agent-based modeling which focuses on individual level<lb/> dynamics and Hawkes process which treats the entire social media<lb/> as a whole. This modification provides traditional Hawkes process<lb/> with a broader range of application while maintaining its original<lb/> self-activating property. The detailed implementation of this model<lb/> is also discussed in this article. We have not yet conducted a detailed<lb/> analysis of the statistical properties of the model, which we leave<lb/> to future work.<lb/></p>

			<head>2 HAWKES PROCESS<lb/></head>
			<head>2.1 Model description and implementation<lb/></head>

			<p>In previous studies, people have been relying on Hawkes process,<lb/> a self-exciting process brought up by Hawkes, A. G. in 1971, to<lb/> simulate activities that involve interactions between events <ref type="biblio">[11]</ref>.<lb/> For example, an interesting application is to use Hawkes process to<lb/> model the queues in front of nightclub <ref type="biblio">[3]</ref>. Usually, we use Poisson<lb/> process to model the arrivals of customers, or night-club visitors in<lb/> this case. However, the Poisson model ignores the fact that people<lb/> may have stronger interests in night-club that seem to be popular(i.e<lb/> a long queue outside the nightclub). Hawkes process successfully<lb/> remedied this issue by taking into account the mutual effects be-<lb/>tween different events. The original formulation of this process is<lb/> defined as following:<lb/> A self-exciting temporal point process N whose conditional intensity<lb/> function ğœ† = ğœ†(ğ‘¡), given a specific time t, is defined to be<lb/></p>

				<formula>ğœ†(ğ‘¡) = ğœ‡ (ğ‘¡) +<lb/> âˆ‘ï¸<lb/> ğ‘–:ğœ ğ‘– &lt;ğ‘¡<lb/> ğ‘£ (ğ‘¡ -ğœ ğ‘– )<lb/></formula>

			<p>where the constant function ğœ‡ (ğ‘¡), also denoted as ğœ† 0 as ğœ†(ğ‘¡) = ğœ‡ (ğ‘¡)<lb/> when ğ‘¡ = 0, is the initial rate of the process ğ‘ which is usually rep-<lb/>resented as a constant function, ğœ ğ‘– are the points in time occurring<lb/> prior to time ğ‘¡, and ğ‘£ is a monotone decreasing non-negative function<lb/> which governs the clustering density of ğ‘ .<lb/></p>

			<p>This model has also been implemented to generate simulations<lb/> of social media activities including tweets <ref type="biblio">[15]</ref>. Intuitively, Hawkes<lb/> process should be able to produce an accurate simulation of Twitter<lb/> activities due to its self-exciting property. However, in this article,<lb/> we will discuss some of the current limitations and bring up a po-<lb/>tential solution. We also found that most of the previous studies<lb/> were more focused on the theoretical part of this model and the<lb/> actual application was discussed but not explicitly carried out <ref type="biblio">[13]</ref>.<lb/> In this part, in order to present how this model actually works in<lb/> application, we write a simulation program using Python with the<lb/> help of Numpy. To show more details about how the model could<lb/> be generated step by step, we also visualized the entire process<lb/> using Matplotlib with relatively small parameters due to the lim-<lb/>ited computational resources. This example should provide those<lb/> who are not familiar with Hawkes process or are interested in how<lb/> the self-exciting property is displayed in practice a comprehensive<lb/> overview in a way that is easy to understand.<lb/></p>

			<head>Step 1: Generate the first generation. </head>
			<p>In order to make the cal-<lb/>culation more convenient, we implement the Hawkes process gen-<lb/>eration by generation. For the first generation ğ‘˜ = 0, we simulate<lb/> times using the homogeneous Poisson process with initial intensity<lb/> function ğœ‡ : R â†’ R defined as ğœ‡ (ğ‘¥) = ğœ† 0 if ğ‘¥ &gt; 0 and 0 otherwise.<lb/></p>

				<formula>ğ‘ƒ = {ğ‘¡<lb/> (0)<lb/> 1 , ..., ğ‘¡<lb/> (0)<lb/> ğ‘<lb/> (0)<lb/> ğ‘¡<lb/> } âˆ¼ ğ‘ƒğ‘ƒ (ğœ† 0 ),<lb/> (Poisson process of intensity ğœ† 0 )<lb/></formula>

			<head>Step 2: Define the lambda function. </head>
			<p>The definition above has<lb/> already explained the most part of the model formulation except<lb/> the function ğ‘£, which is supposed to be a monotone decreasing<lb/> non-negative function. This function reflects how the impact of<lb/> previous events diminishes over time but remains positive. In our<lb/> example, we just follow the convention of using exponential func-<lb/>tion to represent such trend. We will set parameters ğ‘, ğ‘ âˆˆ R + and<lb/> define the function ğ‘£: R + â†’ R + as ğ‘£ (ğ‘¥) = ğ‘ â€¢ ğ‘’ -ğ‘ğ‘¥ .<lb/></p>

			<head>Step 3: Calculate the maximal intensity for each generation<lb/> and produce the next generation based on this maximal in-<lb/>tensity.</head>
			<p>To continue generating the other generations, we shall<lb/> find the greatest value that ğœ†(ğ‘¡) can take given the current genera-<lb/>tion and use that intensity to generate the next generation. Then,<lb/> we shall select some events generated during this process to be<lb/> removed based on how likely these events would happen with the<lb/> real non-homogeneous intensity. Suppose we have the current gen-<lb/>eration of ğ‘› events {ğœ ğ‘– } ğ‘›<lb/> ğ‘–=1 . Since, the previous generation is a finite<lb/> set, the maximum exists for this function. Since the function ğœ†(ğ‘¡)<lb/> is decreasing within each interval [ğœ ğ‘– , ğœ ğ‘–+1 ] with ğ‘– = 1, 2, . . . , ğ‘› -1,<lb/> ğ‘šğ‘ğ‘¥ {ğœ†(ğ‘¡) : ğ‘¡ âˆˆ R + } âˆˆ {ğœ†(ğœ ğ‘– ) : ğ‘– âˆˆ 1, 2, . . . , ğ‘›}. Thus, we just need<lb/> to compare a few values to get the maximum, which significantly<lb/> reduce the amount of computation.<lb/></p>

			<head>Step 4: Decide which points in the current generation should<lb/> be kept using random number generator. </head>
			<p>It&apos;s obvious that fol-<lb/>lowing Step 3, we would generate more events than we expected<lb/> to have because the intensity function doesn&apos;t always stay at its<lb/> maximum. Given a time ğœ generated based on the current genera-<lb/>tion, to decide if we should keep it, we calculate the chance of that<lb/> event by measuring the ratio of ğœ†(ğœ) to ğ‘šğ‘ğ‘¥ {ğœ†(ğ‘¡) : ğ‘¡ âˆˆ R + } which<lb/> has already been determined. When we finalize the ğ‘˜ ğ‘¡â„ generation,<lb/> we will denote the set of events as ğ‘ƒ ğ‘˜ and move on to generate the<lb/> ğ‘˜ + 1 ğ‘¡â„ generation using the same procedure.<lb/></p>

			<head>Step 5: Terminate the algorithm and build the superimposed<lb/> process </head>
			<p>For each generation, we will set a time threshold ğ‘‡ so that<lb/> events generated after ğ‘‡ will be discarded as time dimension can<lb/> go to infinity and we won&apos;t keep track of it. If there is no new event<lb/> generated based on the current generation, we will decide to termi-<lb/>nate the algorithm as the previous impacts have shrunk to minimal.<lb/> The randomness of this process makes the number of generations<lb/> hard to predict. However, some previous studies have shown that<lb/> we can compute the expected value and standard deviation with the<lb/> help of differential equations. We shall also experimentally validate<lb/> those results in our current study. Then, the entire Hawkes process<lb/> can be summarized by summing up all the previous generations<lb/></p>

				<formula>{ğ‘¡ 1 , .., ğ‘¡ ğ‘ ğ‘‡ } =<lb/> ğ¾-1<lb/> ğ‘˜=0<lb/> Î  (ğ‘˜)<lb/></formula>

			<p>where</p>

				<formula>ğ‘ ğ‘¡ = ğ¾-1<lb/> ğ‘˜=0 ğ‘<lb/> (ğ‘˜)<lb/> ğ‘¡ .<lb/></formula>

			<head>Step 6: Visualization </head>
			<p>The simulation result can be visualized using<lb/> Python. Here, as shown in Figure <ref type="figure">1</ref>, we pick two generations that<lb/> clearly demonstrate how the events are produced, generation by<lb/> generation.<lb/></p>

			<head>2.2 Complexity Analysis<lb/></head>

			<p>The efficiency of our implementation depends on the ratio between<lb/> the number of actual events that are accepted and the number<lb/> of total events generated. As shown in Figure <ref type="figure">2</ref>, this algorithm<lb/> works well with small sample size and small parameters but gets<lb/> less efficient with larger parameters and time span. Previously,<lb/> Ogata(<ref type="biblio">[11]</ref>) proposed a fast algorithm implementing Hawkes pro-<lb/>cess. Put briefly, their algorithm generates the entire Hawkes pro-<lb/>cess simultaneously instead of breaking it into several generations.<lb/> Here, we chose to use this step-by-step method for simplicity and<lb/> in order to clarify the mechanism and make better visualization.<lb/></p>

			<head>3 APPLICATION: RARE EVENT SIMULATION<lb/></head>

			<p>One important application of this model is that it can be utilized to<lb/> estimate the probability of an extremely rare event-&quot;twitpocalypse&quot;<lb/> <ref type="biblio">[2]</ref>. Twitter labeled each tweet with an unique ID and the largest<lb/> amount of IDs that can be maintained is 2, 147, 483, 647, the largest<lb/> number that can be stored as &quot;signed integer&quot;. A decade ago, more<lb/> than 20 billion tweets had been sent under 3 years, which went<lb/></p>

			<figure>Figure 1: Visualization of Hawkes Process<lb/>At the beginning, we set the threshold and parameters based on<lb/> which the first generation is produced. For each generation, we use<lb/> the maximum ğœ† to generate a sample and keep those whose value, as<lb/> determined by the random number generator, fall under ğœ†(ğœ). We<lb/> can see that the number of points first goes up and then goes down as<lb/> we produce more generations. Eventually, the impact inserted by the<lb/> early events diminishes and the curve shifts to the right until no more<lb/> event is generated.<lb/></figure>

			<head>Figure 2: Heatmap of Model&apos;s Complexity<lb/> We run experiments with different sets of parameters and output the<lb/> results in this heat map visualization. We can see that as ğ›¼ gets<lb/> larger and ğ›½ gets smaller, the efficiency of our algorithm drops as less<lb/> points generated are used as the final output. Therefore, our method<lb/> might fit some specific scenarios but we should seek better<lb/> approaches when dealing with larger tasks.<lb/></figure>

			<p>far beyond the expectation of Twitter developers and caused a<lb/> crash. Moreover, a lot of third-party apps that were connected<lb/> with Twitter couldn&apos;t handle this large number and also crashed<lb/> during this incident. As Twitter gains increasing popularity, some<lb/> people have been suspicious about with what chance there will<lb/> another &quot;twitpocalypse&quot; and when it will happen. Due to Hawkes<lb/> Process&apos;s superior ability to simulate the nature of social media<lb/> streams, it can serve as a good tool to approximate the chance<lb/> of reaching an enormous traffic in social media with the aid of<lb/> importance sampling. In this section, we will introduce the most<lb/> common importance sampling method and discuss how this method<lb/> can be implemented under this Hawkes Process context.<lb/></p>

			<head>3.1 Importance Sampling<lb/></head>

			<p>In statistics, importance sampling is a popular variance reduction<lb/> strategy used to estimate a rare event that is hard to be obtained<lb/> using traditional ways such as direct calculation and Monte Carlo<lb/> estimation <ref type="biblio">[8]</ref>. Most of the distributions are too complicated to de-<lb/>rive a formula for computing probability so people usually just use<lb/> Monte Carlo estimation which runs a large number of experiments<lb/> and approximates the actual probability of a event with the em-<lb/>pirical probability. However, if the actual probability is extremely<lb/> low, as in the case of Twitpocalypse, we will need a huge amount<lb/> of experiments just to get one single occurrence of the event and<lb/> clearly this estimated probability is very unstable. Therefore, when<lb/> the chance of a certain event is really low, it is a common practice to<lb/> use importance sampling which reduced the variance and results in<lb/> a much higher robustness <ref type="biblio">[1]</ref>. The formal description of one way to<lb/> do importance sampling is following: Suppose we want to estimate<lb/> ğœŒ = E[ğœ‚ (ğ‘‹ )] where ğ‘‹ is a random variable describing some obser-<lb/>vation, ğœ‚ is an indicator function of some events, and ğœŒ is just the<lb/> probability of this set of events. Then, one way to estimate ğœŒ is to<lb/> generate a sequence of i.i.d. random numbers ğ‘‹ <ref type="biblio">(1)</ref> , ğ‘‹ <ref type="biblio">(2)</ref> , . . . , ğ‘‹ <ref type="biblio">(ğ‘›)<lb/></ref> and then compute the empirical probability<lb/></p>

				<formula>Ï =<lb/> 1<lb/> ğ‘›<lb/> ğ‘›<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> ğœ‚ (ğ‘‹ (ğ‘–) ).<lb/></formula>

			<p>However, as we just discussed, this could lead to an inaccurate and<lb/> unstable result if the probability we are estimating is too low. In<lb/> order to solve this, we can introduce a new variable variable ğ‘Œ and<lb/> generate a sequence of i.i.d. random numbers ğ‘Œ (1) , ğ‘Œ (2) , . . . , ğ‘Œ (ğ‘˜) .<lb/> Suppose ğ‘‹ is equipped with a probability density function ğ‘ (â€¢) and<lb/> ğ‘Œ is equipped with a probability density function ğ‘(â€¢). Then, as<lb/> long as ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ (ğ‘ â€¢ ğœ‚) âŠ‚ ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ (ğ‘ â€¢ ğœ‚), we can show<lb/></p>

			<formula>Ï =<lb/> 1<lb/> ğ‘›<lb/> ğ‘›<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> ğœ‚ (ğ‘‹ (ğ‘–) )<lb/> =<lb/> 1<lb/> ğ‘˜<lb/> ğ‘›<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> âˆ«<lb/> ğœ‚ (ğ‘¥ (ğ‘–) )ğ‘ (ğ‘¥ (ğ‘–) )ğ‘‘ğ‘¥<lb/> =<lb/> 1<lb/> ğ‘˜<lb/> ğ‘›<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> âˆ«<lb/> ğœ‚ (ğ‘¥ (ğ‘–) )<lb/> ğ‘ (ğ‘¥ (ğ‘–) )<lb/> ğ‘(ğ‘¥ (ğ‘–) )<lb/> ğ‘(ğ‘¥ (ğ‘–) )ğ‘‘ğ‘¥<lb/> = E ğ‘ [ğœ‚ (ğ‘Œ )<lb/> ğ‘ (ğ‘Œ )<lb/> ğ‘(ğ‘Œ )<lb/> ]<lb/> =<lb/> 1<lb/> ğ‘›<lb/> ğ‘›<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> ğœ‚ (ğ‘Œ (ğ‘–) )<lb/> ğ‘ (ğ‘Œ (ğ‘–) )<lb/> ğ‘(ğ‘Œ (ğ‘–) )<lb/></formula>

			<p>In practice, we can define a random variable ğ‘Œ where the rare even<lb/> is much more likely to happen and run a Monte Carlo simulation<lb/> with this ğ‘Œ . This result has been proved to be unbiased. In the<lb/> next section, we will discuss the specific implementation of this<lb/> technique in Hawkes Process and how the probability of a rare<lb/> event, &quot;Twitpocalypse&quot;, can actually be estimated.<lb/></p>

			<head>3.2 How to implement on Hawkes Process<lb/></head>

			<p>For Hawkes Process, previous research has derived several theo-<lb/>rem regarding its limit behavior, expectation, and variance of the<lb/> intensity and number of events generated. In this paper, we will<lb/> demonstrate how we can put together the previous results and<lb/> apply them into an importance sampling implementation. Given a<lb/> certain set of parameters ğ›¼, ğ›½, ğœ† 0 = ğœ‡ (ğ‘¡), the limit of intensity ğœ† ğ‘¡ as<lb/> ğ‘¡ â†’ âˆ is formulated as<lb/></p>

				<formula>ğœ† âˆ =<lb/> ğ›½ğœ† 0<lb/> ğ›½ -ğ›¼<lb/> .<lb/></formula>

			<p>Also, the formula for approximating the expected value has been<lb/> obtained by solving a differential equation system (<ref type="biblio">[3]</ref>),<lb/></p>

				<formula>E[ğ‘ ğ‘¡ ] =<lb/> ï£±<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´ ï£²<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£´<lb/> ï£³<lb/> ğœ† âˆ ğ‘¡ + ğœ† 0 -ğœ† âˆ<lb/> ğ›½-ğ›¼ (1 -ğ‘’ -(ğ›½-ğ›¼)ğ‘¡ )<lb/> if ğ›¼ &lt; ğ›½<lb/> ğ›½ğœ† 0<lb/> (ğ›¼-ğ›½) 2 + ğœ† 0<lb/> ğ›¼-ğ›½ (ğ‘’ (ğ›¼-ğ›½)ğ‘¡ -1) -<lb/>ğ›½ğœ† 0<lb/> ğ›¼-ğ›½ ğ‘¡ if ğ›¼ &gt; ğ›½<lb/> ğ›½ğœ† 0<lb/> 2 ğ‘¡ 2 + ğœ† 0 ğ‘¡<lb/> if ğ›¼ = ğ›½<lb/> (<label>1</label>)<lb/></formula>

			<p>From these formula, it&apos;s clear that as we increase the initial intensity<lb/> ğœ† 0 , the expected number of events will increase. Therefore, our<lb/> strategy is to build another Hawkes Process with a larger initial<lb/> intensity Î»0 such that the corresponding expected value is equal<lb/> to the extreme value we are trying to simulate. In this case, the<lb/> extreme value is just the maximum number of tweets that can be<lb/> stored as a &quot;signed integer&quot; and handled by the system, which is<lb/> approximately equal to 2 32 . Hence, the occurrence of this rare event<lb/> can be represented as ğ‘ ğ‘¡ &gt; 2 32 with ğ‘¡ being the time period we are<lb/> interested in. Therefore, we can first compute E[ğ‘ ğ‘¡ |ğœ† 0 ] and set<lb/></p>

				<formula>Î»0 =<lb/> 2 32<lb/> E[ğ‘ ğ‘¡ |ğœ† 0 ]<lb/> .<lb/></formula>

			<p>The next step is to compute the ratio between the likelihoods of<lb/> these two processes to fully construct the importance sampling. An-<lb/>other research has shown that the likelyhood function for Hawkes<lb/> Process takes the following form: Suppose our Hawkes Process takes<lb/> ğœ† 0 , ğ›¼, ğ›½ as its parameters and ğ‘¡ as its time threshold. Then, by using<lb/> results on likelihood of point process, we can obtain the likelihood of<lb/> this Hawkes Process as<lb/></p>

				<formula>ğ¿ ğœ† 0 = ğ‘’ğ‘¥ğ‘{<lb/> âˆ« ğ‘¡<lb/> 0<lb/> (1 -ğœ†(ğ‘ ))ğ‘‘ğ‘  +<lb/> âˆ« ğ‘¡<lb/> 0<lb/> ğ‘™ğ‘›(ğœ†(ğ‘ ))ğ‘‘ğ‘ ğ‘  }.<lb/></formula>

			<p>In particular, if we have the collection of tweets {ğ‘¡ 1 , ğ‘¡ 2 , . . . , ğ‘¡ ğ‘› }, the<lb/> above equation can be rewritten as<lb/></p>

				<formula>ğ¿ ğœ† 0 (ğ‘¡ 1 , ğ‘¡ 2 , . . . , ğ‘¡ ğ‘› ) = ğ‘’ğ‘¥ğ‘{ğ‘¡ -ğ‘¡ğœ†-<lb/>ğ‘›<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> ğ›¼ (1-ğ‘’ -ğ›½ (ğ‘¡ -ğ‘¡ ğ‘– ) )+<lb/> ğ‘›<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> ğ‘™ğ‘›(ğœ†(ğ‘¡ ğ‘– ))}.<lb/></formula>

			<p>Using the importance sampling technique mentioned in the last<lb/> section, if we take a very large ğ‘š and run the experiments with the<lb/> Hawkes Process with initial intensity Î»0 as discussed above. We can<lb/> use the results, denoted as ğ‘Œ 1 , ğ‘Œ 2 , . . . , ğ‘Œ ğ‘š where each ğ‘Œ ğ‘˜ is a set of<lb/></p>

			<figure>Figure 3: Estimating the chance of Twitpocalypse with Im-<lb/>portance Sampling<lb/> We use Monte Carlo simulation technique to estimate the probability<lb/> of exceeding each of these 10 thresholds within 10 time units, each<lb/> with 100 experiments. Then, we calculate the estimated probability<lb/> and standard deviation for each threshold. We can see that as the<lb/> threshold gets larger, the chance of exceeding that threshold within a<lb/> given time period gets extremely low. Also, we can tell that the model<lb/> is very stable because the standard deviation is about 12 to 13 orders<lb/> of magnitude smaller than the expected value.<lb/></figure>

			<p>all the tweets ğ‘¡ ğ‘˜,1 , ğ‘¡ ğ‘˜,2 , . . . , ğ‘¡ ğ‘˜,ğ‘› generated in that experiment, to es-<lb/>timate the actual probability of original Hawkes Process generating<lb/> more than 2 31 tweets within ğ‘¡ time period<lb/></p>

			<formula>Ï =<lb/> 1<lb/> ğ‘š<lb/> ğ‘š<lb/> âˆ‘ï¸<lb/> ğ‘–=1<lb/> ğœ‚ (ğ‘Œ (ğ‘–) )<lb/> ğ¿ ğœ† 0 (ğ‘Œ (ğ‘–) )<lb/> ğ¿ Î»0<lb/> (ğ‘Œ (ğ‘–) )<lb/>.<lb/></formula>

			<p>Note: in this case, ğœ‚ (ğ‘Œ ( ğ‘–)) is the indicator variable about if ğ‘– ğ‘¡â„<lb/> experiment produces more than 2 32 tweets and this should appear<lb/> very frequent as we have carefully chosen a large Î»0 . Estimating the<lb/> real probability of twitpocalypse would require significant amount<lb/> of computational resources due to the large sample size. In our<lb/> research, we evaluate the probability of generating more than 1000<lb/> tweets within 10 units of time which, according to the formula<lb/> presented above, is supposed to have an expected value of 34.548.<lb/> The numeric results are summarized in Table <ref type="table">3</ref>.<lb/></p>

			<head>4 LIMITATIONS OF THE TRADITIONAL<lb/> APPROACH<lb/></head>

			<p>However, the traditional approach of representing Twitter activities<lb/> using Hawkes process fails to consider the social network structure<lb/> and individual differences, which would limit the model&apos;s perfor-<lb/>mance in a more micro-scale simulation and prediction. The tradi-<lb/>tional model may have an important role in predicting extremely<lb/> rare events like &quot;Twitpocalypse&quot;, but the scope of applications is<lb/> restricted. For example, we may not find the traditional model very<lb/> helpful if we are interested in questions like &quot;under what kind of<lb/> social network structure will a given user&apos;s tweets be most likely<lb/> forwarded?&quot; In practice, a lot of tasks that require a simulation of<lb/> Twitter activities are focused on micro-scale Twitter activity <ref type="biblio">[9]<lb/></ref> and aiming to explore more detailed properties about Twitter usage<lb/> <ref type="biblio">[12]</ref>. By far, although modeling of social network has received an<lb/> increasing interest due to the data availability and growth in me-<lb/>dia usage, very few techniques have been developed for modeling<lb/> social media activities with a focus on the impact brought by a spe-<lb/>cific network structure. With the same set of users, if we represent<lb/> their relationships with a network structure, different networks<lb/> might generate totally different activity patterns. A good property<lb/> of Hawkes process is that it successfully captures how a series of<lb/> events interact and affect each other, which can&apos;t be represented<lb/> by the traditional Poisson process due to its memoryless property.<lb/> Therefore, we intend to incorporate this important property into<lb/> our model while embedding it with a graph structure so that more<lb/> detailed information is included. In the next section, we will give a<lb/> thorough description of the model design and a brief example of<lb/> its implementation.<lb/></p>

			<head>5 GRAPH MODEL<lb/></head>
			<head>5.1 Model Definition<lb/></head>

			<p>The traditional way of simulating Twitter activities using Hawkes<lb/> Process considers all the twitter activities as a whole but fails to cap-<lb/>ture individual difference. This method could be useful in measuring<lb/> and estimating the total Twitter volume but hard to be applied into<lb/> some more micro-level scenarios like predicting the number of<lb/> retweets that a message can generate. At this point, we also realize<lb/> that the structure of social network plays a significant role in users&apos;<lb/> twitter activity. For example, it might be easier for a user with more<lb/> followers to get more retweets. In this part, we will introduce a<lb/> more realistic simulation that embeds Hawkes Process into a graph<lb/> structure and simulate each user&apos;s twitter activity(represented by<lb/> node) based on the arrival of tweets sent by those the user is follow-<lb/>ing. In our model, each node will represent a twitter user and the<lb/> edges (directed) can represent if one user is following or followed<lb/> by another user.<lb/></p>

			<p>Definition 1. Given a group of users, if any of them is not fol-<lb/>lowed or following any user not from this group, this group is called a<lb/> closed Twitter network.<lb/></p>

			<p>Definition 2. If one user is followed by or following another user,<lb/> we say these two users are connected. If a is connected to b and b is<lb/> connected to c, then a is connected to c. In other words, transitivity<lb/> holds. Given a closed Twitter network, if every pair of users in this<lb/> network are connected, then this network is irreducible.<lb/></p>

			<p>Definition 3. Each closed and irreducible Twitter network with<lb/> ğ‘› users has a directed graph representation ğº = (ğ‘‰ , ğ¸) with ğ‘‰ =<lb/> {ğ‘‰ 1 , ğ‘‰ 2 , . . . , ğ‘‰ ğ‘› } (each vertex represents a user). For each pair of (ğ‘‰ ğ‘– , ğ‘‰ ğ‘— )<lb/> such that ğ‘‰ ğ‘– is following ğ‘‰ ğ‘— , there is an edge (ğ‘‰ ğ‘— , ğ‘‰ ğ‘– ) âˆˆ ğ¸.<lb/></p>

			<p>Definition 4. For each ğ‘– in {1, . . . , ğ‘›}, there is an intensity ğ¼ ğ‘– . If ğ‘‰ ğ‘–<lb/> doesn&apos;t follow anyone else on Twitter, then the number of tweets pub-<lb/>lished by ğ‘‰ ğ‘– follows a Poisson process with intensity ğ¼ ğ‘– . ğ‘‰ ğ‘– &apos;s twitter ac-<lb/>tivity can be affected by the tweets published by all users that ğ‘‰ ğ‘– is fol-<lb/>lowing. Let ğ‘ˆ ğ‘– = {ğ‘‰ ğ‘— : (ğ‘–, ğ‘—) âˆˆ ğ¸}, namely the collection of users that<lb/> ğ‘‰ ğ‘– is following. Let ğ‘‹ ğ‘–,ğœ denote a tweet message published by the user<lb/> ğ‘‰ ğ‘– at time ğœ. Given a period of time ğ‘‡ , let ğ‘€ ğ‘– be the collection of tweet<lb/> messages such that ğ‘€ ğ‘– = {ğ‘‹ ğ‘˜,ğœ : ğ‘˜ âˆˆ ğ‘ˆ ğ‘– , ğœ âˆˆ ğ‘‡ }. Then the retweet<lb/> activity of ğ‘‰ ğ‘– during time ğ‘‡ can be modeled by a non-homogeneous<lb/> Poisson process with intensity ğœ†(ğ‘¡) = {ğœ:ğœ â‰¤ğ‘¡,âˆƒğ‘˜ âˆˆğ‘ˆ ğ‘– ğ‘‹ ğ‘˜,ğœ âˆˆğ‘€ ğ‘– } ğ‘”(ğ‘¡ -ğœ).<lb/></p>

			<figure>Figure 4: Histogram of tweets generated over time<lb/></figure>

			<p>Instead of making the whole tweet generating process as a<lb/> Hawkes process, we assign a Hawkes function on each individ-<lb/>ual that also responds to others&apos; posts. This model inherits the<lb/> advantage of Hawkes model that takes the impacts of previous<lb/> events into account. However, the network structure of this model<lb/> makes it more suitable for social media simulation. Also, with this<lb/> graph structure, it becomes possible to do some in-depth analysis,<lb/> such as quantifying the impact generated by a KOL, with the help<lb/> of some graph theory approaches.<lb/></p>

			<head>5.2 Implementation Specifics<lb/></head>

			<p>The implementation of this model is similar to the original Hawkes<lb/> process. The major challenge in this model is that the function<lb/> of each vertex (user) changes according to other vertices. Our ap-<lb/>proach is that due to the memoryless property of exponential func-<lb/>tion, instead of generation by generation, we can run the simulation<lb/> step by step. The function for each vertex is fixed between two con-<lb/>secutive tweets in the whole network. Therefore, we can always<lb/> use the functions at current step to generate to generate the next<lb/> tweet for each individual and pick the earliest tweet to be the one<lb/> that actually takes place. Then, this tweet will help us update the<lb/> intensity function of all the users who are following this publisher<lb/> and run the next round of simulation. We experimented on a small<lb/> network with five vertices. With deliberate choice of parameters,<lb/> the tweets are generated at a very stable rate (Figure <ref type="figure">4</ref>), which is<lb/> similar to the real world situation. We can also see from Figure <ref type="figure">5<lb/></ref> that the user who follows more accounts is likely to produce more<lb/> tweets.<lb/></p>

			<p>The flexibility of this model opens up various options for appli-<lb/>cations. For example, we can assign customized parameters (ğ›¼, ğ›½)<lb/> to each vertex to represent different user segments. Also, we can<lb/> study the gain of adding a particular edge and the loss of deleting a<lb/> particular edge to measure how a minor change could insert impact<lb/> on the whole network.<lb/></p>

			<head>6 CHALLENGES AND FUTURE WORK<lb/></head>

			<p>The current implementation of the graph model is not as efficient as<lb/> the traditional Hawkes model. This is because we have to compare<lb/> the next tweet of each user in order to prepare and update the<lb/> model for the next step. The complexity of this implementation<lb/> algorithm could be very high as we have to do ğ‘‚ (|ğ‘‰ (ğº)|) times of<lb/></p>

			<figure>Figure 5: Visualization of network<lb/></figure>

			<p>computations and each computation goes through all the relevant<lb/> past tweets which stack up over time. A possible simplification<lb/> strategy is to clean up the previous tweets whose impacts become<lb/> minimal as they have been published for quite a while so that we<lb/> can avoid some unnecessary computations. Another challenge is<lb/> that it is really hard to employ analysis on the limit behavior and<lb/> other statistical properties such as expected value and variance<lb/> in this model due to the complicated and frequent interactions<lb/> between vertices. Our future work will be primarily focused on<lb/> developing these properties of the model to improve rigorousness<lb/> and robustness.<lb/></p>

			<head>7 CONCLUSION<lb/></head>

			<p>In this paper, we give an overview of the Hawkes process theory<lb/> and introduce a reproducible strategy to implement it using Python.<lb/> This step-by-step approach can help keep track of how the events<lb/> are generated and give researchers opportunities to observe the<lb/> trend and adjust the model in accordance with the real pattern<lb/> that is being studied. Also, we compare the complexity of our ap-<lb/>proach with an existing fast implementation of Hawkes process.<lb/> On the practical side, we demonstrate an interesting application<lb/> of Hawkes process that estimates the probability of an extremely<lb/> rare event known as &quot;Twitpocalypse&quot;. However, regarding social<lb/> media modeling, the scope provided by this traditional Hawkes<lb/> model is limited because the dynamics of social media activities<lb/> also depend on the network structure that is formed by the users. In<lb/> order to address this issue, we bring up a new model which embeds<lb/> the Hawkes process in a graph structure that incorporates both<lb/> the self-activating property and individual level features. Due to<lb/> the complexity of this new model, in this paper, we just briefly<lb/> illustrate the implementation method and run an experiment on a<lb/> small sample graph. Although there is still a lot of further work that<lb/> needs to be done to make this model more rigorous, the potential<lb/> of this model can be seen in its ability to consider very specific<lb/> situations and simulate the individual-level interactions.</p>


	</text>
</tei>
