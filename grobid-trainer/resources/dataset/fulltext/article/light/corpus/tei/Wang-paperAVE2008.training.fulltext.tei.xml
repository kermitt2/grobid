<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_Wang-paperAVE2008"/>
	</teiHeader>
	<text xml:lang="en">

			<head level="1">1 Introduction and Related Work<lb/></head>

			<p>Answer Validation is an important step for Question Answering (QA) systems, which aims to validate the<lb/> answers extracted from natural language texts, and select the most proper answers for the final output.<lb/></p>

			<p>Using Recognizing Textual Entailment (RTE-1 – <ref type="biblio">Dagan et al., 2006</ref>; RTE-2 – <ref type="biblio">Bar-Haim et al., 2006</ref>) to do<lb/> answer validation has shown a great success <ref type="biblio">(Peñas et al., 2007)</ref>. We also developed our own RTE system and<lb/> participated in AVE2007. The RTE system proposed a new sentence representation extracted from the<lb/> dependency structure, and utilized the Subsequence Kernel method <ref type="biblio">(Bunescu and Mooney, 2006)</ref> to perform<lb/> machine learning. We have achieved fairly high results on both the RTE-2 data set <ref type="biblio">(Wang and Neumann, 2007a)</ref><lb/> and the RTE-3 data set <ref type="biblio">(Wang and Neumann, 2007b)</ref>, especially on Information Extraction (IE) and QA pairs.<lb/> However, on the AVE data sets, we still found much space for the improvement. Therefore, based on the<lb/> system we developed last year, our motivation this year is to see whether using extra information, e.g. named-<lb/>entity (NE) recognition, question analysis, etc., can make further improvement on the final results.<lb/> This report will start with a brief introduction of our RTE system and then followed by the whole AVE<lb/> system. The results of our two submission runs will be shown in section 4, and in the end, we will summarize<lb/> our work.<lb/></p>

			<head level="1">2 The RTE System<lb/></head>

			<p>The RTE system <ref type="biblio">(Wang and Neumann, 2007a</ref>; <ref type="biblio">Wang and Neumann, 2007b)</ref> is developed for RTE-3 Challenge<lb/> <ref type="biblio">(Giampiccolo et al., 2007)</ref>. The system contains a main approach with two backup strategies. The main approach<lb/> extracts parts of the dependency structures to form a new representation, named Tree Skeleton, as the feature<lb/> space and then applies Subsequence Kernels to represent TSs and perform Machine Learning. The backup<lb/> strategies will deal with the T-H pairs which cannot be solved by the main approach. One backup strategy is<lb/> called Triple Matcher, as it calculates the overlapping ratio on top of the dependency structures in a triple<lb/> representation; the other is simply a Bag-of-Words (BoW) method, which calculates the overlapping ratio of<lb/> words in T and H.<lb/></p>

			<p>The main approach starts with processing H, since it is usually textually shorter than T, and the dependency<lb/> structure also simpler. Tree skeletons are extracted based on the dependency structures derived by Minipar <ref type="biblio">(Lin,<lb/> 1998)</ref> for English and SMES <ref type="biblio">(Neumann and Piskorski, 2002)</ref> for German. There are nouns in the lower part of<lb/> the parse tree, and they share a common parent node, which is (usually) a verb in the upper part. Since content<lb/> words usually convey most of the meaning of the sentence, we will mark the nouns as Topic Words and the verb<lb/> as the Root Node. Together with the dependency paths in between, they form a subtree of the original<lb/> dependency structure, which can be viewed as an extended version of Predicate-Argument Structure <ref type="biblio">(Gildea and<lb/> Palmer, 2002)</ref>. We call the subtree Tree Skeleton, the topic words Foot Nodes, and the dependency path from the<lb/> noun to the root node Spine. If there are two foot nodes, the corresponding spines will be the Left Spine and the<lb/> Right Spine.<lb/></p>

			<p>On top of the tree skeleton of H, the tree skeleton of T can also be extracted. We assume that if the entailment<lb/> holds from T to H, at least, they will share the same topics. Since in practice, there are different expressions for<lb/> the same entity, we have applied some fuzzy matching techniques to correspond the topic words in T and H, like<lb/> initialism, partial matching, etc. Once we successfully identify the topic words in T, we trace up along the<lb/> dependency parse tree to find the lowest common parent node, which will be marked as the root node of the tree<lb/> skeleton of T 1 .<lb/></p>

			<p>After some generalizations, we merge the two tree skeletons by 1) excluding the longest common prefixes for<lb/> left spines and 2) excluding the longest common suffixes for right spines. Finally, we will get the dissimilarity of<lb/> the two tree skeletons and we call it Spine Differences, i.e. Left Spine Difference (LSD) and Right Spine<lb/> Difference (RSD). Then, since all the remaining symbols are POS tags and (generalized) dependency relation<lb/> tags, they altogether form a Closed-Class Symbol (CCS) set. The spine difference is thus a sequence of CCSs. To<lb/> represent it, we have utilized a Subsequence Kernel and a Collocation Kernel <ref type="biblio">(Wang and Neumann, 2007a)</ref>.<lb/></p>

			<p>We have also considered the comparison between root nodes and their adjacent dependency relations. We<lb/> have observed that some adjacent dependency relations of the root node (e.g. &lt;SUBJ&gt;or &lt;OBJ&gt;) can play<lb/> important roles in predicting the entailment relationship. For instance, the verb &quot;sell&quot; has a direction of the action<lb/> from the subject to the object. In addition, the verb &quot;sell&quot; and &quot;buy&quot; convey totally different semantics.<lb/> Therefore, we assign them two extra simple kernels named Verb Consistence (VC) and Verb Relation<lb/> Consistence (VRC). The former indicates whether two root nodes have a similar meaning, and the latter<lb/> indicates whether the relations are contradictive (e.g. &lt;SUBJ&gt; and &lt;OBJ&gt; are contradictive).<lb/></p>

			<p>Finally, the main approach is assisted by two backup strategies: one is called the Triple Similarity and the<lb/> other is called the BoW Similarity. Chief requirements for the backup strategy are robustness and simplicity.<lb/> Accordingly, we construct a similarity function, which operates on two triple (dependency structure represented<lb/> in the form of &lt;head, relation, modifier&gt;) sets and determines how many triples of H are contained in T. The<lb/> core assumption here is that the higher the number of matching triple elements, the more similar both sets are,<lb/> and the more likely it is that T entails H. The function uses an approximate matching function. Different cases<lb/> (i.e. ignoring either the parent node or the child node, or the relation between nodes) might provide different<lb/> indications for the similarity of T and H. We then sum them up using different weights and divide the result by<lb/> the cardinality of H for normalization. The BoW similarity score is calculated by dividing the number of<lb/> overlapping words between T and H by the total number of words in H after a simple tokenization according to<lb/> the space between words.<lb/></p>

			<!--note place="footnote">1 The Root Node of T is not necessary to be a verb, instead, it could be a noun, a preposition, or even a dependency relation.<lb/></note-->

			<head level="1">3<lb/> The AVE System<lb/></head>

			<figure>
				Fig. 1.
				Our AVE system uses the RTE system (Tera – Textual Entailment Recognition for Application) as a core component.<lb/> The preprocessing module mainly adapts questions, their corresponding answers, and supporting documents into Text (T)-<lb/>Hypothesis (H) pairs, assisted by some manually designed patterns. The post-processing module (i.e. the Answer Validation<lb/> in the picture) will validate each answer and select a most proper one based on the output of the RTE system. The new<lb/> modules added are the NE Recognition and Question Analysis. Thus, we will have extra information like NEs in the answers,<lb/> Expected Answer Types (EATs), etc.<lb/>
			</figure>

			<head level="2">3.1 Preprocessing and Post-processing<lb/></head>

			<p>Since the input of the AVE task is a list of questions, their corresponding answers and the documents containing<lb/> these answers, we need to adapt them into T-H pairs for the RTE system. For instance, the question is,<lb/> How many &quot;Superside&quot; world championships did Steve Webster win between 1987 and 2004?<lb/> (id=87) 2<lb/></p>

			<p>The QA system gives out several candidate answers to this question, as follows,<lb/> ten (id=87_1)<lb/> 24 (id=87_2)<lb/> …</p>

			<p>Each answer will have one supporting document where the answer comes from, like this,<lb/> The most successful sidecar racer in Superside has been Steve Webster MBE, who has won ten<lb/> world championships between 1987 and 2004. (id=87_1)<lb/></p>

			<p>The assumption here is that if the answer is relevant to the question, the document which contains the answer<lb/> should entail the statement derived by combining the question and the answer. This section will mainly focus on<lb/> the combination of the question and the answer and in the next sections the RTE system and how to deal with the<lb/> output of the system will be described.<lb/></p>

			<p>In order to combine the question and the answer into a statement, we need some language patterns. Normally,<lb/> we have different types of questions, such as Who-questions asking about persons, What-questions asking about<lb/> definitions, etc. Therefore, we manually construct some language patterns for the input questions. For the<lb/> example given above (id=87), we will apply the following pattern,<lb/> Steve Webster won &lt;Answer&gt; &quot;Superside&quot; world championships between 1987 and 2004.<lb/> (id=87)<lb/></p>

			<p>Consequently, we substitute the &lt;Answer&gt; by each candidate answer to form Hs – hypotheses. Since the<lb/> supporting documents are naturally the Ts – texts, the T-H pairs are built up accordingly,<lb/> Id: 87_1<lb/> Entailment: Unknown<lb/> Text: The most successful sidecar racer in Superside has been Steve Webster MBE, who has<lb/> won ten world championships between 1987 and 2004.<lb/> Hypothesis: Steve Webster won ten &quot;Superside&quot; world championships between 1987 and 2004.<lb/></p>

			<p>These T-H pairs can be the input for any generic RTE systems. In practice, after applying our RTE system, if<lb/> the T-H pairs are covered by our main approach, we will directly use the answers; if not, we will use a threshold<lb/></p>

			<!--note place="footnote">2 The &quot;id&quot; comes from AVE 2008 test data, i.e. &quot;AVE2008-annotated-test-EN.xml&quot;.<lb/></note-->

			<p>to decide the answer based on the two similarity scores. Therefore, every T-H pair has a triple similarity score<lb/> and a BoW similarity score, and for some of the T-H pairs, we directly know whether the entailment holds. The<lb/> post-processing is straightforward, the &quot;YES&quot; entailment cases will be validated answers and the &quot;NO&quot;<lb/> entailment cases will be rejected answers. In addition, the selected answers (i.e. the best answers) will naturally<lb/> be the pairs covered by our main approach or (if not,) with the highest similarity scores.<lb/></p>

			<head level="2">3.2 Additional Components<lb/></head>

			<p>The RTE system is used as a core component of the AVE system. Based on the error analysis of last year&apos;s<lb/> results, this year we use additional components to filter out noisy candidates. Therefore, two extra components<lb/> are added to the architecture, the NE recognizer and the question analyzer. For NE recognition, we use<lb/> StanfordNER <ref type="biblio">(Finkel et al., 2005)</ref> for English and SPPC <ref type="biblio">(Neumann and Piskorski, 2002)</ref> for German; and for<lb/> question analysis, we use the SMES system <ref type="biblio">(Neumann and Piskorski, 2002)</ref>. The detailed workflow is as follows,<lb/></p>

			<list>
				<item>1. Annotate NEs in H, store them in an NE list; if the answer is an NE, store the NE type as A&apos;_Type;<lb/></item>

				<item>2. Analyze the question and obtain expected answer type, store it as A_Type;<lb/></item>

				<item>3. Synthesize all the information, i.e. NE list, A_Type, A&apos;_Type, BoW similarity, Triple similarity, etc.<lb/> As for the example mentioned above (id=87), the additional information will be,<lb/> NE list: Steve Webster (person), 1987 (date), 2004 (date);<lb/> A_Type: Number<lb/> A&apos;_Type: Number<lb/></item>
			</list>	

			<p>Then, heuristic rules are straightforward to be applied, e.g. checking the consistence between<lb/> A_Type and A&apos;_Type, checking whether all (or how many of) the NEs also appear in the documents,<lb/> etc. All these results together with the outputs of the RTE system will be synthesized to make the final<lb/> decision.<lb/></p>

			<head level="1">4 Results<lb/></head>

			<p>We have submitted two runs for this year&apos;s AVE tasks, one for English and one for German. In the following, we<lb/> will first show the table of the results and then present an error analysis.<lb/></p>

			<figure type="table">
				Table 1. Results of our submissions compared with last year&apos;s<lb/>

				Submission Runs<lb/> Recall Precision<lb/> F-measure<lb/> Estimated QA<lb/> Performance<lb/> QA Accuracy<lb/> 100% VALIDATED (EN)<lb/> 1 0.08<lb/> 0.14<lb/> N/A<lb/> N/A<lb/> 50%VALIDATED (EN)<lb/> 0.5<lb/> 0.08<lb/> 0.13<lb/> N/A<lb/> N/A<lb/> Perfect Selection (EN)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.56<lb/> 0.34<lb/> Best QA System (EN)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.21<lb/> 0.21<lb/> dfki07-run1 (EN)<lb/> 0.62<lb/> 0.37<lb/> 0.46<lb/> N/A<lb/> 0.16<lb/> dfki07-run2 (EN)<lb/> 0.71<lb/> 0.44<lb/> 0.55<lb/> N/A<lb/> 0.21<lb/> dfki08run1 (EN)<lb/> 0.78<lb/> 0.54<lb/> 0.64<lb/> 0.34<lb/> 0.24<lb/> 100% VALIDATED(DE)<lb/> 1 0.12<lb/> 0.21<lb/> N/A<lb/> N/A<lb/> 50% VALIDATED (DE)<lb/> 0.5<lb/> 0.12<lb/> 0.19<lb/> N/A<lb/> N/A<lb/> Perfect Selection (DE)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.77<lb/> 0.52<lb/> Best QA System (DE)<lb/> N/A<lb/> N/A<lb/> N/A<lb/> 0.38<lb/> 0.38<lb/> dfki08run1 (DE)<lb/> 0.71<lb/> 0.54<lb/> 0.61<lb/> 0.52<lb/> 0.43<lb/>
			</figure>

			<p>In the table, we notice that both for English and German, our validation system outperforms the best QA<lb/> systems, which suggests the necessity of the validation step. Although there is a gap between the system<lb/> performance and the perfect selection, the results are quite satisfactory. If we compare this year&apos;s results with<lb/> last year&apos;s, the additional information does improve the results significantly.<lb/></p>

			<p>Comparing the recall and precision, for both languages, the latter is worse. Therefore, we did some error<lb/> analysis to see whether there is still some space for improvements. An interesting example in the English data is<lb/> as follows,<lb/> Question: What is the name of the best known piece by Jeremiah Clarke? (id=0011)<lb/> Answer: a rondo (id=0011_7)<lb/> Document: The most famous piece known by that name, however, is a composition by<lb/> Jeremiah Clarke, properly a rondo for keyboard named Prince of Denmark&apos;s March.<lb/></p>

			<p>Our system wrongly validated this answer, because &quot;a rondo&quot; is not the name of that music work. In fact,<lb/> what we need here is a special proper name recognizer which can differentiate whether the noun is a name for a<lb/> music work.<lb/></p>

			<p>In the German data, other kinds of errors occur. For instance,<lb/> Question: Wer war Russlands Verteidigungsminister 1994? (id=0020 3 )<lb/> Answer: Pawel Gratschow (id=0020_6)<lb/> Document: Wie der russische Verteidigungsminister Pawel Gratschow am Mittwoch in Tiflis<lb/> weiter bekanntgab, will Rußland insgesamt fünf Militärstützpunkte in den Kaukasus-<lb/>Republiken Georgien, Armenien und Aserbaidschan einrichten. 1994-02-02<lb/></p>

			<p>The key problem here is that the year &quot;1994&quot; in the document might not be the year when the event happened,<lb/> but the year of the report. This asks us to further synthesize the information we have, i.e. NE annotation and<lb/> dependency parsing, to make better use of them.<lb/></p>

			<head level="1">5 Conclusion and Future Work<lb/></head>

			<p>To sum up, in this paper, we described our participation of AVE 2008. Based on the experience of last year&apos;s<lb/> participation, apart from the RTE core system, we add two extra components, NE recognizer and question<lb/> analyzer, to further improve the results. The strategy is quite successful according to the comparison of system<lb/> performances.<lb/></p>

			<p>However, the problem has not been fully solved. Due to the noisy web data, filtering some documents in the<lb/> preprocessing step could be even more effective than working on the post-processing phase. Another direction<lb/> considered by us is to take a closer look at the different performances between different languages.<lb/></p>

			<!--note place="footnote">3 This &quot;id&quot; comes from &quot;AVE2008-annotated-test-DE.xml&quot;.</note-->

	</text>
</tei>
