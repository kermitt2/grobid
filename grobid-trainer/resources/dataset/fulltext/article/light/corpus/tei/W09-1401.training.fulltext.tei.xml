<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>1 Introduction<lb/></head>

			<p>The history of text mining (TM) shows that shared<lb/> tasks based on carefully curated resources, such<lb/> as those organized in the MUC <ref type="biblio">(Chinchor, 1998)</ref>,<lb/> TREC <ref type="biblio">(Voorhees, 2007)</ref> and ACE <ref type="biblio">(Strassel et al.,<lb/> 2008)</ref> events, have significantly contributed to the<lb/> progress of their respective fields. This has also been<lb/> the case in bio-TM. Examples include the TREC Ge-<lb/>nomics track <ref type="biblio">(Hersh et al., 2007)</ref>, JNLPBA <ref type="biblio">(Kim et<lb/> al., 2004)</ref>, LLL <ref type="biblio">(NÃ©dellec, 2005)</ref>, and BioCreative<lb/> <ref type="biblio">(Hirschman et al., 2007)</ref>. While the first two ad-<lb/>dressed bio-IR (information retrieval) and bio-NER<lb/> (named entity recognition), respectively, the last two<lb/> focused on bio-IE (information extraction), seeking<lb/> relations between bio-molecules. With the emer-<lb/>gence of NER systems with performance capable of<lb/> supporting practical applications, the recent interest<lb/> of the bio-TM community is shifting toward IE.<lb/></p>

			<p>Similarly to LLL and BioCreative, the<lb/> BioNLP&apos;09 Shared Task (the BioNLP task, here-<lb/>after) also addresses bio-IE, but takes a definitive<lb/> step further toward finer-grained IE. While LLL and<lb/> BioCreative focus on a rather simple representation<lb/> of relations of bio-molecules, i.e. protein-protein<lb/> interactions (PPI), the BioNLP task concerns the<lb/> detailed behavior of bio-molecules, characterized as<lb/> bio-molecular events (bio-events). The difference in<lb/> focus is motivated in part by different applications<lb/> envisioned as being supported by the IE methods.<lb/> For example, BioCreative aims to support curation<lb/> of PPI databases such as MINT <ref type="biblio">(Chatr-aryamontri<lb/> et al., 2007)</ref>, for a long time one of the primary tasks<lb/> of bioinformatics. The BioNLP task aims to support<lb/> the development of more detailed and structured<lb/> databases, e.g. pathway <ref type="biblio">(Bader et al., 2006)</ref> or Gene<lb/> Ontology Annotation (GOA) <ref type="biblio">(Camon et al., 2004)<lb/></ref> databases, which are gaining increasing interest<lb/> in bioinformatics research in response to recent<lb/> advances in molecular biology.<lb/></p>

			<p>As the first shared task of its type, the BioNLP<lb/> task aimed to define a bounded, well-defined bio-<lb/>event extraction task, considering both the actual<lb/> needs and the state of the art in bio-TM technology<lb/> and to pursue it as a community-wide effort. The<lb/> key challenge was in finding a good balance between<lb/> the utility and the feasibility of the task, which was<lb/> also limited by the resources available. Special con-<lb/>sideration was given to providing evaluation at di-<lb/>verse levels and aspects, so that the results can drive<lb/> continuous efforts in relevant directions. The pa-<lb/>per discusses the design and implementation of the<lb/> BioNLP task, and reports the results with analysis.<lb/></p>

			<figure type="table">Type<lb/> Primary Args.<lb/> Second. Args.<lb/> Gene expression<lb/> T(P)<lb/> Transcription<lb/> T(P)<lb/> Protein catabolism<lb/> T(P)<lb/> Phosphorylation<lb/> T(P)<lb/> Site<lb/> Localization<lb/> T(P)<lb/> AtLoc, ToLoc<lb/> Binding<lb/> T(P)+<lb/> Site+<lb/> Regulation<lb/> T(P/Ev), C(P/Ev) Site, CSite<lb/> Positive regulation<lb/> T(P/Ev), C(P/Ev) Site, CSite<lb/> Negative regulation T(P/Ev), C(P/Ev) Site, CSite<lb/> Table <ref type="table">1</ref>: Event types and their arguments. The type of the<lb/> filler entity is specified in parenthesis. The filler entity<lb/> of the secondary arguments are all of Entity type which<lb/> represents any entity but proteins: T=Theme, C=Cause,<lb/> P=Protein, Ev=Event.<lb/></figure>

			<head>2 Task setting<lb/></head>

			<p>To focus efforts on the novel aspects of the event<lb/> extraction task, is was assumed that named entity<lb/> recognition has already been performed and the task<lb/> was begun with a given set of gold protein anno-<lb/>tation. This is the only feature of the task setting<lb/> that notably detracts from its realism. However,<lb/> given that state-of-the-art protein annotation meth-<lb/>ods show a practically applicable level of perfor-<lb/>mance, i.e. 88% F-score <ref type="biblio">(Wilbur et al., 2007)</ref>, we<lb/> believe the choice is reasonable and has several ad-<lb/>vantages, including focus on event extraction and ef-<lb/>fective evaluation and analysis.<lb/></p>

			<head>2.1 Target event types<lb/></head>

			<p>Table <ref type="table">1</ref> shows the event types addressed in the<lb/> BioNLP task. The event types were selected from<lb/> the GENIA ontology, with consideration given to<lb/> their importance and the number of annotated in-<lb/>stances in the GENIA corpus. The selected event<lb/> types all concern protein biology, implying that they<lb/> take proteins as their theme. The first three types<lb/> concern protein metabolism, i.e. protein production<lb/> and breakdown. Phosphorylation is a representa-<lb/>tive protein modification event, and Localization and<lb/> Binding are representative fundamental molecular<lb/> events. Regulation (including its sub-types, Posi-<lb/>tive and Negative regulation) represents regulatory<lb/> events and causal relations. The last five are uni-<lb/>versal but frequently occur on proteins. For the bio-<lb/>logical interpretation of the event types, readers are<lb/> referred to Gene Ontology (GO) and the GENIA on-<lb/>tology.<lb/></p>

			<figure>The failure of p65 translocation to the nucleus . . .<lb/> T3<lb/> (Protein, 40-46)<lb/> T2<lb/> (Localization, 19-32)<lb/> E1<lb/> (Type:T2, Theme:T3, ToLoc:T1)<lb/> T1<lb/> (Entity, 15-18)<lb/> M1 (Negation E1)<lb/> Figure 1: Example event annotation. The protein an-<lb/>notation T3 is given as a starting point. The extraction<lb/> of annotation in bold is required for Task 1, T1 and the<lb/> ToLoc:T1 argument for Task 2, and M1 for Task 3.<lb/></figure>

			<p>As shown in Table <ref type="table">1</ref>, the theme or themes of all<lb/> events are considered primary arguments, that is, ar-<lb/>guments that are critical to identifying the event. For<lb/> regulation events, the entity or event stated as the<lb/> cause of the regulation is also regarded as a primary<lb/> argument. For some event types, other arguments<lb/> detailing of the events are also defined (Secondary<lb/> Args. in Table <ref type="table">1</ref>).<lb/></p>

			<p>From a computational point of view, the event<lb/> types represent different levels of complexity. When<lb/> only primary arguments are considered, the first five<lb/> event types require only unary arguments, and the<lb/> task can be cast as relation extraction between a<lb/> predicate (event trigger) and an argument (Protein).<lb/> The Binding type is more complex in requiring the<lb/> detection of an arbitrary number of arguments. Reg-<lb/>ulation events always take a Theme argument and,<lb/> when expressed, also a Cause argument. Note that a<lb/> Regulation event may take another event as its theme<lb/> or cause, a unique feature of the BioNLP task com-<lb/>pared to other event extraction tasks, e.g. ACE.<lb/></p>

			<head>2.2 Representation<lb/></head>

			<p>In the BioNLP task, events are expressed using three<lb/> different types of entities. Text-bound entities (t-<lb/>entities hereafter) are represented as text spans with<lb/> associated class information. The t-entities include<lb/> event triggers (Localization, Binding, etc), protein<lb/> references (Protein) and references to other entities<lb/> (Entity). A t-entity is represented by a pair, (entity-<lb/>type, text-span), and assigned an id with the pre-<lb/>fix &quot;T&quot;, e.g. T1-T3 in Figure <ref type="figure">1</ref>. An event is ex-<lb/>pressed as an n-tuple of typed t-entities, and has<lb/> a id with prefix &quot;E&quot;, e.g. E1. An event modifi-<lb/>cation is expressed by a pair, (predicate-negation-<lb/>or-speculation, event-id), and has an id with prefix<lb/> &quot;M&quot;, e.g. M1.<lb/></p>

			<figure type="table">Item<lb/> Training<lb/> Devel.<lb/> Test<lb/> Abstract<lb/> 800<lb/> 150<lb/> 260<lb/> Sentence<lb/> 7,449<lb/> 1,450<lb/> 2,447<lb/> Word<lb/> 176,146<lb/> 33,937<lb/> 57,367<lb/> Event<lb/> 8,597 / 8,615 1,809 / 1,815 3,182 / 3,193<lb/> Table 2: Statistics of the data sets.<lb/></figure>

			<p>For events,<lb/> Task1/Task2 shown separately as secondary arguments<lb/> may introduce additional differentiation of events.<lb/></p>

			<head>2.3 Subtasks<lb/></head>

			<p>The BioNLP task targets semantically rich event ex-<lb/>traction, involving the extraction of several different<lb/> classes of information. To facilitate evaluation on<lb/> different aspects of the overall task, the task is di-<lb/>vided to three sub-tasks addressing event extraction<lb/> at different levels of specificity.<lb/></p> 

			<p>Task 1. Core event detection detection of typed,<lb/> text-bound events and assignment of given pro-<lb/>teins as their primary arguments.<lb/></p>

			<p>Task 2. Event enrichment recognition of sec-<lb/>ondary arguments that further specify the<lb/> events extracted in Task 1.<lb/></p>

			<p>Task 3. Negation/Speculation detection detection<lb/> of negations and speculation statements<lb/> concerning extracted events.<lb/></p>

			<p>Task 1 serves as the backbone of the shared task and<lb/> is mandatory for all participants. Task 2 involves the<lb/> recognition of Entity type t-entities and assignment<lb/> of those as secondary event arguments. Task 3 ad-<lb/>dresses the recognition of negated or speculatively<lb/> expressed events without specific binding to text. An<lb/> example is given in Fig. <ref type="figure">1</ref>.<lb/></p>

			<head>3 Data preparation<lb/></head>

			<p>The BioNLP task data were prepared based on the<lb/> GENIA event corpus. The data for the training and<lb/> development sets were derived from the publicly<lb/> available event corpus <ref type="biblio">(Kim et al., 2008)</ref>, and the<lb/> data for the test set from an unpublished portion of<lb/> the corpus. Table <ref type="table">2</ref> shows statistics of the data sets.<lb/></p>

			<p>For data preparation, in addition to filtering out<lb/> irrelevant annotations from the original GENIA cor-<lb/>pus, some new types of annotation were added to<lb/> make the event annotation more appropriate for the<lb/> purposes of the shared task. The following sections<lb/> describe the key changes to the corpus.<lb/></p>

			<head>3.1 Gene-or-gene-product annotation<lb/></head>

			<p>The named entity (NE) annotation of the GENIA<lb/> corpus has been somewhat controversial due to dif-<lb/>ferences in annotation principles compared to other<lb/> biomedical NE corpora. For instance, the NE an-<lb/>notation in the widely applied GENETAG corpus<lb/> <ref type="biblio">(Tanabe et al., 2005)</ref> does not differentiate proteins<lb/> from genes, while GENIA annotation does. Such<lb/> differences have caused significant inconsistency in<lb/> methods and resources following different annota-<lb/>tion schemes. To remove or reduce the inconsis-<lb/>tency, GENETAG-style NE annotation, which we<lb/> term gene-or-gene-product (GGP) annotation, has<lb/> been added to the GENIA corpus, with appropriate<lb/> revision of the original annotation. For details, we<lb/> refer to <ref type="biblio">(Ohta et al., 2009)</ref>. The NE annotation used<lb/> in the BioNLP task data is based on this annotation.<lb/></p>

			<head>3.2 Argument revision<lb/></head>

			<p>The GENIA event annotation was made based on<lb/> the GENIA event ontology, which uses a loose typ-<lb/>ing system for the arguments of each event class.<lb/> For example, in Figure <ref type="figure">2</ref>(a), it is expressed that<lb/> the binding event involves two proteins, TRAF2<lb/> and CD40, and that, in the case of CD40, its cy-<lb/>toplasmic domain takes part in the binding. With-<lb/>out constraints on the type of theme arguments,<lb/> the following two annotations are both legitimate:<lb/></p>

			<p>(Type:Binding, Theme:TRAF2, Theme:CD40)<lb/> (Type:Binding, Theme:TRAF2,<lb/> Theme:CD40 cytoplasmic domain)<lb/></p>

			<p>The two can be seen as specifying the same event<lb/> at different levels of specificity 1 . Although both al-<lb/>ternatives are reasonable, the need to have consis-<lb/>tent training and evaluation data requires a consis-<lb/>tent choice to be made for the shared task.<lb/> Thus, we fix the types of all non-event<lb/> primary arguments to be proteins (specifically<lb/> GGPs). For GENIA event annotations involving<lb/> themes other than proteins, additional argument<lb/> types were introduced, for example, as follows:<lb/></p>

			<figure>(a)<lb/> TRAF2 is a â¦ which binds to the CD40 cytoplasmic domain<lb/> GGP<lb/> GGP<lb/> PDR<lb/> (b)<lb/> HMG-I binds to GATA motifs<lb/> GGP<lb/> DDR<lb/> (c)<lb/> alpha B2 bound the PEBP2 site within the GM-CSF promoter<lb/> GGP<lb/> GGP<lb/> DDR<lb/> DDR<lb/> Figure 2: Entity annotation to example sentences<lb/> from (a) PMID10080948, (b) PMID7575565, and (c)<lb/> PMID7605990 (simplified).<lb/></figure> 

			<figure>(a)<lb/> Ah receptor recognizes the B cell transcription factor, BSAP<lb/> (b)<lb/> Grf40 binds to linker for activation of T cells (LAT)<lb/> (c)<lb/> expression of p21(WAF1/CIP1) and p27(KIP1)<lb/> (d)<lb/> included both p50/p50 and p50/p65 dimers<lb/> (e)<lb/> IL-4 Stat, also known as Stat6<lb/> Figure 3: Equivalent entities in example sentences from<lb/> (a) PMID7541987 (simplified), (b) PMID10224278, (c)<lb/> PMID10090931, (d) PMID9243743, (e) PMID7635985.<lb/></figure> 

			<p>(Type:Binding, Theme1:TRAF2, Theme2:CD40,<lb/> Site2:cytoplasmic domain)<lb/></p>

			<p>Note that the protein, CD40, and its domain, cyto-<lb/>plasmic domain, are associated by argument num-<lb/>bering. To resolve issues related to the mapping<lb/> between proteins and related entities systematically,<lb/> we introduced partial static relation annotation for<lb/> relations such as Part-Whole, drawing in part on<lb/> similar annotation of the BioInfer corpus <ref type="biblio">(Pyysalo<lb/> et al., 2007)</ref>. For details of this part of the revision<lb/> process, we refer to <ref type="biblio">(Pyysalo et al., 2009)</ref>.<lb/></p>

			<p>Figure <ref type="figure">2</ref> shows some challenging cases. In (b),<lb/> the site GATA motifs is not identified as an argument<lb/> of the binding event, because the protein containing<lb/> it is not stated. In (c), among the two sites (PEBP2<lb/> site and promoter) of the gene GM-CSF, only the<lb/> more specific one, PEBP2, is annotated.<lb/></p>

			<head>3.3 Equivalent entity references<lb/></head>

			<p>Alternative names for the same object are fre-<lb/>quently introduced in biomedical texts, typically<lb/> through apposition. This is illustrated in Figure <ref type="figure">3(a)</ref>,<lb/> where the two expressions B cell transcription fac-<lb/>tor and BSAP are in apposition and refer to the<lb/> same protein. Consequently, in this case the fol-<lb/>lowing two annotations represent the same event:<lb/></p>

			<p>(Type:Binding, Theme:Ah receptor,<lb/> Theme:B cell transcription factor)<lb/> (Type:Binding, Theme:Ah receptor, Theme:BSAP)<lb/></p>

			<p>In the GENIA event corpus only one of these is an-<lb/>notated, with preference given to shorter names over<lb/> longer descriptive ones. Thus of the above exam-<lb/>ple events, the latter would be annotated. How-<lb/>ever, as both express the same event, in the shared<lb/> task evaluation either alternative was accepted as<lb/> correct extraction of the event. In order to im-<lb/>plement this aspect of the evaluation, expressions<lb/> of equivalent entities were annotated as follows:<lb/></p>

			<p>Eq (B cell transcription factor, BSAP)<lb/></p>

			<p>The equivalent entity annotation in the revised GE-<lb/>NIA corpus covers also cases other than simple ap-<lb/>position, illustrated in Figure <ref type="figure">3</ref>. A frequent case in<lb/> biomedical literature involves use of the slash sym-<lb/>bol (&quot;/&quot;) to state synonyms. The slash symbol is<lb/> ambiguous as it is used also to indicate dimerized<lb/> proteins. In the case of p50/p50, the two p50 are<lb/> annotated as equivalent because they represent the<lb/> same proteins at the same state. Note that although<lb/> rare, also explicitly introduced aliases are annotated,<lb/> as in Figure <ref type="figure">3(e)</ref>.<lb/></p>

			<head>4 Evaluation<lb/></head>

			<p>For the evaluation, the participants were given the<lb/> test data with gold annotation only for proteins. The<lb/> evaluation was then carried out by comparing the<lb/> annotation predicted by each participant to the gold<lb/> annotation. For the comparison, equality of anno-<lb/>tations is defined as described in Section 4.1. The<lb/> evaluation results are reported using the standard<lb/> recall/precision/f-score metrics, under different cri-<lb/>teria defined through the equalities.<lb/></p>

			<head>4.1 Equalities and Strict matching<lb/></head>

			<p>Equality of events is defined as follows:<lb/></p>

			<p>Event Equality equality holds between any two<lb/> events when (1) the event types are the same,<lb/> (2) the event triggers are the same, and (3) the<lb/> arguments are fully matched.<lb/></p>

			<p>A full matching of arguments between two events<lb/> means there is a perfect 1-to-1 mapping between the<lb/> two sets of arguments. Equality of individual argu-<lb/>ments is defined as follows:<lb/></p>

			<p>Argument Equality equality holds between any<lb/> two arguments when (1) the role types are the<lb/> same, and (2-1) both are t-entities and equality<lb/> holds between them, or (2-2) both are events<lb/> and equality holds between them.<lb/></p>

			<p>Due to the condition (2-2), event equality is defined<lb/> recursively for events referring to events. Equality<lb/> of t-entities is defined as follows:<lb/></p>

			<p>T-entity Equality equality holds between any two<lb/> t-entities when (1) the entity types are the same,<lb/> and (2) the spans are the same.<lb/></p>

			<p>Any two text spans (beg1, end1) and (beg2, end2),<lb/> are the same iff beg1 = beg2 and end1 = end2.<lb/> Note that the event triggers are also t-entities thus<lb/> their equality is defined by the t-entity equality.<lb/></p>

			<head>4.2 Evaluation modes<lb/></head>

			<p>Various evaluation modes can be defined by varying<lb/> equivalence criteria. In the following, we describe<lb/> three fundamental variants applied in the evaluation.<lb/></p>

			<head>Strict matching</head>

			<p>The strict matching mode requires<lb/> exact equality, as defined in section 4.1. As some<lb/> of its requirements may be viewed as unnecessarily<lb/> precise, practically motivated relaxed variants, de-<lb/>scribed in the following, are also applied.<lb/></p>

			<head>Approximate span matching</head> 

			<p>The approximate<lb/> span matching mode is defined by relaxing the<lb/> requirement for text span matching for t-entities.<lb/> Specifically, a given span is equivalent to a gold<lb/> span if it is entirely contained within an extension<lb/> of the gold span by one word both to the left and<lb/> to the right, that is, beg1 â¥ ebeg2 and end1 â¤<lb/> eend2, where (beg1, end1) is the given span and<lb/> (ebeg2, eend2) is the extended gold span.<lb/></p>

			<head>Approximate recursive matching</head>

			<p>In strict match-<lb/>ing, for a regulation event to be correct, the events it<lb/> refers to as theme or cause must also be be strictly<lb/> correct. The approximate recursive matching mode<lb/> is defined by relaxing the requirement for recursive<lb/> event matching, so that an event can match even<lb/> if the events it refers to are only partially correct.<lb/></p>

			<figure type="table">Event<lb/> Release date<lb/> Announcement Dec 8<lb/> Sample data<lb/> Dec 15<lb/> Training data<lb/> Jan 19 â 21, Feb 2 (rev1), Feb 10 (rev2)<lb/> Devel. data<lb/> Feb 7<lb/> Test data<lb/> Feb 22 â Mar 2<lb/> Submission<lb/> Mar 2 â Mar 9<lb/> Table 3: Shared task schedule. The arrows indicate a<lb/> change of schedule.<lb/></figure>

			<p>Specifically, for partial matching, only Theme argu-<lb/>ments are considered: events can match even if re-<lb/>ferred events differ in non-Theme arguments.<lb/></p>

			<head>5 Schedule<lb/></head>

			<p>The BioNLP task was held for 12 weeks, from the<lb/> sample data release to the final submission. It in-<lb/>cluded 5 weeks of system design period with sam-<lb/>ple data, 6 weeks of system development period with<lb/> training and development data, and a 1 week test pe-<lb/>riod. The system development period was originally<lb/> planned for 5 weeks but extended by 1 week due to<lb/> the delay of the training data release and the revi-<lb/>sion. Table <ref type="table">3</ref> shows key dates of the schedule.<lb/></p>

			<head>6 Supporting Resources<lb/></head>

			<p>To allow participants to focus development efforts<lb/> on novel aspects of event extraction, we prepared<lb/> publicly available BioNLP resources readily avail-<lb/>able for the shared task. Several fundamental<lb/> BioNLP tools were provided through U-Compare<lb/> <ref type="biblio">(Kano et al., 2009)</ref> 2 , which included tools for to-<lb/>kenization, sentence segmentation, part-of-speech<lb/> tagging, chunking and syntactic parsing.<lb/></p>

			<p>Participants were also provided with the syntactic<lb/> analyses created by a selection of parsers. We ap-<lb/>plied two mainstream Penn Treebank (PTB) phrase<lb/> structure parsers: the Bikel parser 3 , implementing<lb/> Collins&apos; parsing model <ref type="biblio">(Bikel, 2004</ref>) and trained<lb/> on PTB, and the reranking parser of <ref type="biblio">(Charniak<lb/> and Johnson, 2005)</ref> with the self-trained biomed-<lb/>ical parsing model of <ref type="biblio">(McClosky and Charniak,<lb/> 2008)</ref> 4 . We also applied the GDep 5 , native de-<lb/>pendency parser trained on the GENIA Treebank<lb/></p> 

			<figure type="table">NLP<lb/> Task<lb/> Team<lb/> Task<lb/> Org<lb/> Word<lb/> Chunking<lb/> Parsing<lb/> Trigger<lb/> Argument<lb/> Ext. Resources<lb/> UTurku<lb/> 1--<lb/>3C+2BI<lb/> Porter<lb/> MC<lb/> SVM<lb/> SVM (SVMlight)<lb/> JULIELab<lb/> 1--1C+2L+2B<lb/> OpenNLP<lb/> OpenNLP<lb/> GDep<lb/> Dict+Stat<lb/> SVM(libSVM)<lb/> UniProt, Mesh,<lb/> Porter<lb/> ME(Mallet)<lb/> GOA, UMLS<lb/> ConcordU<lb/> 1-3<lb/> 3C<lb/> Stanford<lb/> Stanford<lb/> Dict+Stat<lb/> Rules<lb/> WordNet, VerbNet,<lb/> UMLS<lb/> UT+DBCLS 12-<lb/>2C<lb/> Porter<lb/> MC<lb/> Dict<lb/> MLN(thebeast)<lb/> CCG<lb/> VIBGhent<lb/> 1-3<lb/> 2C+1B<lb/> Porter,<lb/> Stanford<lb/> Dict<lb/> SVM(libSVM)<lb/> UTokyo<lb/> 1--<lb/>3C<lb/> GTag<lb/> Dict<lb/> ME(liblinear)<lb/> UIMA<lb/> Enju<lb/> UNSW<lb/> 1--<lb/>1C+1B<lb/> GDep<lb/> CRF<lb/> Rules<lb/> WordNet, MetaMap<lb/> UZurich<lb/> 1--<lb/>3C<lb/> LingPipe,<lb/> LTChunk<lb/> Pro3Gres<lb/> Dict<lb/> Rules<lb/> Morpha<lb/> ASU+HU+BU 123<lb/> 6C+2BI<lb/> Porter<lb/> BioLG,<lb/> Dict<lb/> Rules<lb/> Lucene<lb/> Charniak<lb/> Rules<lb/> Cam<lb/> 1--<lb/>3C<lb/> Porter<lb/> RASP<lb/> Dict<lb/> Rules<lb/> UAntwerp<lb/> 12-<lb/>3C<lb/> GTag<lb/> GDep<lb/> MBL<lb/> MBL(TiMBL)<lb/> Rules<lb/> UNIMAN<lb/> 1--<lb/>4C+2BI<lb/> Porter<lb/> GDep<lb/> Dict, CRF<lb/> SVM<lb/> MeSH, GO<lb/> GTag<lb/> Rules<lb/> SCAI<lb/> 1--<lb/>1C<lb/> Rules<lb/> UAveiro<lb/> 1--<lb/>1C+1L<lb/> NooJ<lb/> NooJ<lb/> Rules<lb/> BioLexicon<lb/> USzeged<lb/> 1-3<lb/> 3C+1B<lb/> GTag<lb/> Dict, VSM<lb/> C4.5(WEKA)<lb/> BioScope<lb/> Rules<lb/> NICTA<lb/> 1-3<lb/> 4C<lb/> GTag<lb/> ERG<lb/> CRF(CRF++)<lb/> Rules<lb/> JULIE<lb/> CNBMadrid<lb/> 12-<lb/>2C+1B<lb/> Porter,<lb/> GTag<lb/> CBR<lb/> GTag<lb/> Rules<lb/> CCP-BTMG 123<lb/> 7C<lb/> LingPipe<lb/> LingPipe OpenDMAP LingPipe, CM<lb/> Rules<lb/> GO, SO, MIO,<lb/> UIMA<lb/> CIPS-ASU<lb/> 1--<lb/>3C<lb/> MontyTagger Custom<lb/> Stanford<lb/> CRF(ABNER)<lb/> Rules,<lb/> NB(WEKA)<lb/> UMich<lb/> 1--<lb/>2C<lb/> Stanford<lb/> MC<lb/> Dict<lb/> SVM(SVMlight)<lb/> PIKB<lb/> 1--<lb/>5C+2B<lb/> MIRA<lb/> MIRA<lb/> KoreaU<lb/> 1--<lb/>5C<lb/> GTag<lb/> GDep<lb/> Rules, ME<lb/> ME<lb/> WSJ<lb/> Table 4: Profiles of the participants: GTag=GENIAtagger, MLN=Markov Logic Network, UMLS=UMLS SPE-<lb/>CIALIST Lexicon/tools, MC=McClosky-Charniak, GDep=Genia Dependency Parser, Stanford=Stanford Parser,<lb/> CBR=Case-Based Reasoning, CM=ConceptMapper.<lb/></figure>

			<p><ref type="biblio">(Tateisi et al., 2005)</ref>, and a version of the C&amp;C CCG<lb/> deep parser 6 adapted to biomedical text <ref type="biblio">(Rimell and<lb/> Clark, 2008)</ref>.<lb/></p> 

			<p>The text of all documents was segmented and to-<lb/>kenized using the GENIA Sentence Splitter and the<lb/> GENIA Tagger, provided by U-Compare. The same<lb/> segmentation was enforced for all parsers, which<lb/> were run using default settings. Both the native out-<lb/>put of each parser and a representation in the popular<lb/> Stanford Dependency (SD) format (de <ref type="biblio">Marneffe et<lb/> al., 2006)</ref> were provided. The SD representation was<lb/> created using the Stanford tools 7 to convert from the<lb/> PTB scheme, the custom conversion introduced by<lb/> <ref type="biblio">(Rimell and Clark, 2008)</ref> for the C&amp;C CCG parser,<lb/> and a simple format-only conversion for GDep.<lb/></p>

			<head>7 Results and Discussion<lb/></head> 

			<head>7.1 Participation<lb/></head>

			<p>In total, 42 teams showed interest in the shared task<lb/> and registered for participation, and 24 teams sub-<lb/>mitted final results. All 24 teams participated in the<lb/> obligatory Task 1, six in each of Tasks 2 and 3, and<lb/> two teams completed all the three tasks.<lb/></p>

			<p>Table <ref type="table">4</ref> shows a profile of the 22 final teams,<lb/> excepting two who wished to remain anonymous.<lb/> A brief examination on the team organization (the<lb/> Org column) shows a computer science background<lb/> (C) to be most frequent among participants, with<lb/> less frequent participation from bioinformaticians<lb/> (BI), biologists (B) and liguists (L). This may be<lb/> attributed in part to the fact that the event extrac-<lb/>tion task required complex computational modeling.<lb/> The role of computer scientists may be emphasized<lb/> in part due to the fact that the task was novel to most<lb/> participants, requiring particular efforts in frame-<lb/>work design and implementation and computational<lb/> resources. This also suggests there is room for im-<lb/>provement from more input from biologists.<lb/></p>

			<head>7.2 Evaluation results<lb/></head>

			<p>The final evaluation results of Task 1 are shown in<lb/> Table <ref type="table">5</ref>. The results on the five event types involv-<lb/></p>

			<figure type="table">Team<lb/> Simple Event<lb/> Binding<lb/> Regulation<lb/> All<lb/> UTurku<lb/> 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95<lb/> JULIELab<lb/> 59.81 / 79.80 / 68.38 49.57 / 35.25 / 41.20 35.03 / 34.18 / 34.60 45.82 / 47.52 / 46.66<lb/> ConcordU<lb/> 49.75 / 81.44 / 61.76 20.46 / 40.57 / 27.20 27.47 / 49.89 / 35.43 34.98 / 61.59 / 44.62<lb/> UT+DBCLS<lb/> 55.75 / 72.74 / 63.12 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35<lb/> VIBGhent<lb/> 54.48 / 79.31 / 64.59 38.04 / 38.60 / 38.32 17.36 / 31.61 / 22.41 33.41 / 51.55 / 40.54<lb/> UTokyo<lb/> 45.69 / 72.19 / 55.96 34.58 / 50.63 / 41.10 14.22 / 34.26 / 20.09 28.13 / 53.56 / 36.88<lb/> UNSW<lb/> 45.85 / 69.94 / 55.39 23.63 / 37.27 / 28.92 16.58 / 28.27 / 20.90 28.22 / 45.78 / 34.92<lb/> UZurich<lb/> 44.92 / 66.62 / 53.66 30.84 / 37.28 / 33.75 14.82 / 30.21 / 19.89 27.75 / 46.60 / 34.78<lb/> ASU+HU+BU 45.09 / 76.80 / 56.82 19.88 / 44.52 / 27.49 05.20 / 33.46 / 09.01 21.62 / 62.21 / 32.09<lb/> Cam<lb/> 39.17 / 76.40 / 51.79 12.68 / 31.88 / 18.14 09.98 / 37.76 / 15.79 21.12 / 56.90 / 30.80<lb/> UAntwerp<lb/> 41.29 / 65.68 / 50.70 12.97 / 31.03 / 18.29 11.07 / 29.85 / 16.15 22.50 / 47.70 / 30.58<lb/> UNIMAN<lb/> 50.00 / 63.21 / 55.83 12.68 / 40.37 / 19.30 04.05 / 16.75 / 06.53 22.06 / 48.61 / 30.35<lb/> SCAI<lb/> 43.74 / 70.73 / 54.05 28.82 / 35.21 / 31.70 12.64 / 16.55 / 14.33 25.96 / 36.26 / 30.26<lb/> UAveiro<lb/> 43.57 / 71.63 / 54.18 13.54 / 34.06 / 19.38 06.29 / 21.05 / 09.69 20.93 / 49.30 / 29.38<lb/> Team 24<lb/> 41.29 / 64.72 / 50.41 22.77 / 35.43 / 27.72 09.38 / 19.23 / 12.61 22.69 / 40.55 / 29.10<lb/> USzeged<lb/> 47.63 / 44.44 / 45.98 15.27 / 25.73 / 19.17 04.17 / 18.21 / 06.79 21.53 / 36.99 / 27.21<lb/> NICTA<lb/> 31.13 / 77.31 / 44.39 16.71 / 29.00 / 21.21 07.80 / 18.12 / 10.91 17.44 / 39.99 / 24.29<lb/> CNBMadrid<lb/> 50.25 / 46.59 / 48.35 33.14 / 20.54 / 25.36 12.22 / 07.99 / 09.67 28.63 / 20.88 / 24.15<lb/> CCP-BTMG<lb/> 28.17 / 87.63 / 42.64 12.68 / 40.00 / 19.26 03.09 / 48.11 / 05.80 13.45 / 71.81 / 22.66<lb/> CIPS-ASU<lb/> 39.68 / 38.60 / 39.13 17.29 / 31.58 / 22.35 11.86 / 08.15 / 09.66 22.78 / 19.03 / 20.74<lb/> UMich<lb/> 52.71 / 25.89 / 34.73 31.70 / 12.61 / 18.05 14.22 / 06.56 / 08.98 30.42 / 14.11 / 19.28<lb/> PIKB<lb/> 26.65 / 75.72 / 39.42 07.20 / 39.68 / 12.20 01.09 / 30.51 / 02.10 11.25 / 66.54 / 19.25<lb/> Team 09<lb/> 27.16 / 43.61 / 33.47 03.17 / 09.82 / 04.79 02.42 / 11.90 / 04.02 11.69 / 31.42 / 17.04<lb/> KoreaU<lb/> 20.56 / 66.39 / 31.40 12.97 / 50.00 / 20.59 00.67 / 37.93 / 01.31 09.40 / 61.65 / 16.31<lb/> Table 5: Evaluation results of Task 1 (recall / precision / f-score).<lb/></figure> 

			<figure type="table">Team<lb/> All<lb/> Site for Phospho.(56) AtLoc &amp; ToLoc (65)<lb/> All Second Args.<lb/> UT+DBCLS<lb/> 35.86 / 54.08 / 43.12 71.43 / 71.43 / 71.43<lb/> 23.08 / 88.24 / 36.59<lb/> 32.14 / 72.41 / 44.52<lb/> UAntwerp<lb/> 21.52 / 45.77 / 29.27 00.00 / 00.00 / 00.00 01.54 /100.00 / 03.03 06.63 / 52.00 / 11.76<lb/> ASU+HU+BU 19.70 / 56.87 / 29.26 00.00 / 00.00 / 00.00<lb/> 00.00 / 00.00 / 00.00<lb/> 00.00 / 00.00 / 00.00<lb/> Team 24<lb/> 22.08 / 38.28 / 28.01 55.36 / 93.94 / 69.66<lb/> 21.54 / 66.67 / 32.56<lb/> 30.10 / 76.62 / 43.22<lb/> CCP-BTMG<lb/> 13.25 / 70.97 / 22.33 30.36 /100.00 / 46.58 00.00 / 00.00 / 00.00 08.67 /100.00 / 15.96<lb/> CNBMadrid<lb/> 25.02 / 18.32 / 21.15 85.71 / 57.14 / 68.57<lb/> 32.31 / 47.73 / 38.53<lb/> 50.00 / 09.71 / 16.27<lb/> Table 6: Evaluation results for Task 2.<lb/></figure>

			<p>ing only a single primary theme argument are shown<lb/> in one merged class, &quot;Simple Event&quot;. The broad per-<lb/>formance range (31% -70%) indicates even the ex-<lb/>traction of simple events is not a trivial task. How-<lb/>ever, the top-ranked systems show encouraging per-<lb/>formance, achieving or approaching 70% f-score.<lb/></p>

			<p>The performance ranges for Binding (5% -44%)<lb/> and Regulation (1% -40%) events show their ex-<lb/>traction to be clearly more challenging. It is in-<lb/>teresting that while most systems show better per-<lb/>formance for binding over regulation events, the<lb/> systems [ConcordU] and [UT+DBCLS] are better<lb/> for regulation, showing somewhat reduced perfor-<lb/>mance for Binding events. This is in particular con-<lb/>trast to the following two systems, [ViBGhent] and<lb/> [UTokyo], which show far better performance for<lb/> Binding than Regulation events. As one possible<lb/> explanation, we find that the latter two differentiate<lb/> binding events by their number of themes, while the<lb/> former two give no specific treatment to multi-theme<lb/> binding events. Such observations and comparisons<lb/> are a clear benefit of a community-wide shared task.<lb/></p>

			<p>Table <ref type="table">6</ref> shows the evaluation results for the teams<lb/> who participated in Task 2. The &quot;All&quot; column shows<lb/> the overall performance of the systems for Task 2,<lb/> while the &quot;All Second Args.&quot; column shows the<lb/> performance of finding only the secondary argu-<lb/>ments. The evaluation results show considerable<lb/> differences between the criteria. For example, the<lb/> system [Team 24] shows performance comparable<lb/> to the top ranked system in finding secondary argu-<lb/>ments, although its overall performance for Task 2<lb/> is more limited. Table 6 also shows the three sys-<lb/>tems, [UT+DBCLS], [Team 24] and [CNBMadrid],<lb/> </p>

			<figure type="table">Team<lb/> Negation<lb/> Speculation<lb/> ConcordU<lb/> 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27<lb/> VIBGhent<lb/> 10.57 / 45.10 / 17.13 08.65 / 15.79 / 11.18<lb/> ASU+HU+BU 03.96 / 27.27 / 06.92 06.25 / 28.26 / 10.24<lb/> NICTA<lb/> 05.29 / 34.48 / 09.17 04.81 / 30.30 / 08.30<lb/> USzeged<lb/> 05.29 / 01.94 / 02.84 12.02 / 03.88 / 05.87<lb/> CCP-BTMG 01.76 / 05.26 / 02.64 06.73 / 13.33 / 08.95<lb/> Table 7: Evaluation results for Task 3.<lb/></figure>

			<figure>0<lb/> 10<lb/> 20<lb/> 30<lb/> 40<lb/> 50<lb/> 60<lb/> 02/18 02/21 02/24 02/27 03/02 03/05 03/08<lb/> daily average<lb/> Figure 4: Scatterplot of the evaluation results on the de-<lb/>velopment data during the system development period.<lb/></figure>

			<p>show performance at a practical level in particular in<lb/> finding specific sites of phosphorylation.<lb/></p>

			<p>As shown in Table <ref type="table">7</ref>, the performance range for<lb/> Task 3 is very low although the representation of the<lb/> task is as simple as the simple events. We attribute<lb/> the reason to the fact that Task 3 is the only task of<lb/> which the annotation is not bound to textual clue,<lb/> thus no text-bound annotation was provided.<lb/></p>

			<p>Figure <ref type="figure">4</ref> shows a scatter plot of the performance<lb/> of the participating systems during the system devel-<lb/>opment period. The performance evaluation comes<lb/> from the log of the online evaluation system on the<lb/> development data. It shows the best performance<lb/> and the average performance of the participating<lb/> systems were trending upwards up until the dead-<lb/>line of final submission, which indicates there is still<lb/> much potential for improvement.<lb/></p>

			<head>7.3 Ensemble<lb/></head>

			<p>Table <ref type="table">8</ref> shows experimental results of a system en-<lb/>semble using the final submissions. For the ex-<lb/>periments, the top 3-10 systems were chosen, and<lb/> the output of each system treated as a weighted<lb/> vote 8 . Three weighting schemes were used; &quot;Equal&quot;<lb/> weights each vote equally; &quot;Averaged&quot; weights each<lb/> </p>

			<figure type="table">Ensemble Equal Averaged Event Type<lb/> Top 3<lb/> 53.19<lb/> 53.19<lb/> 54.08<lb/> Top 4<lb/> 54.34<lb/> 54.34<lb/> 55.21<lb/> Top 5<lb/> 54.77<lb/> 55.03<lb/> 55.10<lb/> Top 6<lb/> 55.13<lb/> 55.77<lb/> 55.96<lb/> Top 7<lb/> 54.33<lb/> 55.45<lb/> 55.73<lb/> Top 10<lb/> 52.79<lb/> 54.63<lb/> 55.18<lb/> Table 8: Experimental results of system ensemble.<lb/></figure>

			<p>vote by the overall f-score of the system; &quot;Event<lb/> Type&quot; weights each vote by the f-score of the sys-<lb/>tem for the specific event type. The best score,<lb/> 55.96%, was obtained by the &quot;Event Type&quot; weight-<lb/>ing scheme, showing a 4% unit improvement over<lb/> the best individual system. While using the final<lb/> scores for weighting uses data that would not be<lb/> available in practice, similar weighting could likely<lb/> be obtained e.g. using performance on the devel-<lb/>opment data. The experiment demonstrates that an<lb/> f-score better than 55% can be achieved simply by<lb/> combining the strengths of the systems.<lb/></p>

			<head>8 Conclusion<lb/></head>

			<p>Meeting with the community-wide participation, the<lb/> BioNLP Shared Task was successful in introducing<lb/> fine-grained event extraction to the domain. The<lb/> evaluation results of the final submissions from the<lb/> participants are both promising and encouraging for<lb/> the future of this approach to IE. It has been revealed<lb/> that state-of-the-art performance in event extraction<lb/> is approaching a practically applicable level for sim-<lb/>ple events, and also that there are many remain-<lb/>ing challenges in the extraction of complex events.<lb/> A brief analysis suggests that the submitted data<lb/> together with the system descriptions are rich re-<lb/>sources for finding directions for improvements. Fi-<lb/>nally, the experience of the shared task participants<lb/> provides an invaluable basis for cooperation in fac-<lb/>ing further challenges.</p>

	</text>
</tei>
