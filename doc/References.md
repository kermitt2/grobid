## Project reference

To cite this work, you can simply refer to the github project:

```
GROBID (2008-2019) <https://github.com/kermitt2/grobid>
```

(please do not include a particular person name to emphasize the project and tool!)

Here's a BibTeX entry:

```
@misc{GROBID, 
    title = {GROBID}, 
    howpublished = {\url{https://github.com/kermitt2/grobid}}, 
    publisher = {GitHub},
    year = {2008 --- 2019},
    archivePrefix = {swh},
    eprint = {1:dir:6a298c1b2008913d62e01e5bc967510500f80710}
}
```

## Presentations on Grobid

[GROBID in 30 slides](grobid-04-2015.pdf) (2015).

[GROBID in 20 slides](GROBID.pdf) (2012).

## Papers around Grobid

Note: if you want to cite this work, please cite the software project as mentioned above and not a particular paper. 

P. Lopez. [GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications](https://lekythos.library.ucy.ac.cy/bitstream/handle/10797/14013/ECDL069.pdf?sequence=1). Proceedings of the 13th European Conference on Digital Library (ECDL), Corfu, Greece, 2009.

P. Lopez. Automatic Extraction and Resolution of Bibliographical References in Patent Documents. First Information Retrieval Facility Conference (IRFC), Vienna, May 2010. LNCS 6107, pp. 120-135. Springer, Heidelberg, 2010.

Joseph Boyd. [Automatic Metadata Extraction The High Energy Physics Use Case](https://preprints.cern.ch/record/2039361/files/CERN-THESIS-2015-105.pdf). Master Thesis, EPFL, Switzerland, 2015. 

## Evaluation and usages

Phil Gooch and Kris Jack, [How well does Mendeleyâ€™s Metadata Extraction Work?](https://krisjack.wordpress.com/2015/03/12/how-well-does-mendeleys-metadata-extraction-work/), 2015

M. Lipinski, K. Yao, C. Breitinger, J. Beel, and B. Gipp. [Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents](http://docear.org/papers/Evaluation_of_Header_Metadata_Extraction_Approaches_and_Tools_for_Scientific_PDF_Documents.pdf), in Proceedings of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL), Indianapolis, IN, USA, 2013. 

Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney and Dan S. Weld. [GORC: A large contextual citation graph of academic papers](https://arxiv.org/pdf/1911.02782.pdf), [arXiv:1911.02782](https://arxiv.org/abs/1911.02782), [github](https://github.com/allenai/s2-gorc), 2019

[Meta-eval](https://github.com/allenai/meta-eval), 2015

D. Tkaczyk, A. Collins, P. Sheridan, & J. Beel. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case. [arXiv:1802.01168](https://arxiv.org/pdf/1802.01168), 2018.

## Articles on CRF for bibliographical extraction

Fuchun Peng and Andrew McCallum. Accurate Information Extraction from Research Papers using Conditional Random Fields. Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL), 2004.

Isaac G. Councill, C. Lee Giles, Min-Yen Kan. ParsCit: An open-source CRF reference string parsing package. In Proceedings of the Language Resources and Evaluation Conference (LREC), Marrakesh, Morrocco, 2008.

## Other similar Open Source tools

[parsCit](https://github.com/knmnyn/ParsCit)

[CERMINE](https://github.com/CeON/CERMINE)

[Science Parse](https://github.com/allenai/science-parse) 

[science Parse v2](https://github.com/allenai/spv2) 

[Metatagger](https://github.com/iesl/rexa1-metatagger)

[BILBO](https://github.com/OpenEdition/bilbo)

CiteSeerX page on [Scholarly Information Extraction](http://csxstatic.ist.psu.edu/downloads/software#Services) which lists tools and related information (now a bit outdated). 
